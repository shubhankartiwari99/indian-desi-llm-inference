{
  "best_metric": 17.081350326538086,
  "best_model_checkpoint": "/Users/shubhankartiwari/indian-desi-llm-inference/artifacts/alignment_lora/runs/run_01/checkpoint-449",
  "epoch": 1.9977753058954395,
  "eval_steps": 500,
  "global_step": 449,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.11,
      "grad_norm": 322.4986572265625,
      "learning_rate": 1.1111111111111113e-05,
      "loss": 25.7789,
      "step": 25
    },
    {
      "epoch": 0.22,
      "grad_norm": 554.7561645507812,
      "learning_rate": 1.9882491186839016e-05,
      "loss": 25.9763,
      "step": 50
    },
    {
      "epoch": 0.33,
      "grad_norm": 775.477294921875,
      "learning_rate": 1.929494712103408e-05,
      "loss": 25.9193,
      "step": 75
    },
    {
      "epoch": 0.44,
      "grad_norm": 2440.143310546875,
      "learning_rate": 1.870740305522914e-05,
      "loss": 24.5676,
      "step": 100
    },
    {
      "epoch": 0.56,
      "grad_norm": 479.1619567871094,
      "learning_rate": 1.8119858989424208e-05,
      "loss": 24.26,
      "step": 125
    },
    {
      "epoch": 0.67,
      "grad_norm": 1100.8450927734375,
      "learning_rate": 1.7532314923619274e-05,
      "loss": 25.5683,
      "step": 150
    },
    {
      "epoch": 0.78,
      "grad_norm": 670.5160522460938,
      "learning_rate": 1.6944770857814337e-05,
      "loss": 25.4756,
      "step": 175
    },
    {
      "epoch": 0.89,
      "grad_norm": 1110.58251953125,
      "learning_rate": 1.6357226792009403e-05,
      "loss": 24.0502,
      "step": 200
    },
    {
      "epoch": 1.0,
      "eval_loss": 19.822389602661133,
      "eval_runtime": 12.2988,
      "eval_samples_per_second": 16.262,
      "eval_steps_per_second": 4.065,
      "step": 224
    },
    {
      "epoch": 1.0,
      "grad_norm": 12191.451171875,
      "learning_rate": 1.5769682726204466e-05,
      "loss": 24.9174,
      "step": 225
    },
    {
      "epoch": 1.11,
      "grad_norm": 309.3616027832031,
      "learning_rate": 1.5182138660399531e-05,
      "loss": 24.6033,
      "step": 250
    },
    {
      "epoch": 1.22,
      "grad_norm": 380.8612060546875,
      "learning_rate": 1.4594594594594596e-05,
      "loss": 22.472,
      "step": 275
    },
    {
      "epoch": 1.33,
      "grad_norm": 332.8778076171875,
      "learning_rate": 1.400705052878966e-05,
      "loss": 24.3577,
      "step": 300
    },
    {
      "epoch": 1.45,
      "grad_norm": 590.0335693359375,
      "learning_rate": 1.3419506462984727e-05,
      "loss": 23.6863,
      "step": 325
    },
    {
      "epoch": 1.56,
      "grad_norm": 665.9624633789062,
      "learning_rate": 1.283196239717979e-05,
      "loss": 24.1604,
      "step": 350
    },
    {
      "epoch": 1.67,
      "grad_norm": 1915.9942626953125,
      "learning_rate": 1.2244418331374854e-05,
      "loss": 24.5685,
      "step": 375
    },
    {
      "epoch": 1.78,
      "grad_norm": 391.995361328125,
      "learning_rate": 1.165687426556992e-05,
      "loss": 23.7795,
      "step": 400
    },
    {
      "epoch": 1.89,
      "grad_norm": 588.6574096679688,
      "learning_rate": 1.1069330199764983e-05,
      "loss": 22.6208,
      "step": 425
    },
    {
      "epoch": 2.0,
      "eval_loss": 17.081350326538086,
      "eval_runtime": 14.2003,
      "eval_samples_per_second": 14.084,
      "eval_steps_per_second": 3.521,
      "step": 449
    }
  ],
  "logging_steps": 25,
  "max_steps": 896,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "total_flos": 476031279955968.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
