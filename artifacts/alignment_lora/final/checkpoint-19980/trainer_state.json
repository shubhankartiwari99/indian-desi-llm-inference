{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 19980,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 19426.6171875,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 23.9683,
      "step": 1
    },
    {
      "epoch": 0.0,
      "grad_norm": 2712.36279296875,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 18.1095,
      "step": 2
    },
    {
      "epoch": 0.0,
      "grad_norm": 6541.81640625,
      "learning_rate": 5.000000000000001e-07,
      "loss": 31.7174,
      "step": 3
    },
    {
      "epoch": 0.0,
      "grad_norm": 4642.04931640625,
      "learning_rate": 6.666666666666667e-07,
      "loss": 21.015,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 348756.5625,
      "learning_rate": 8.333333333333333e-07,
      "loss": 19.0694,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 6293.4580078125,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 24.7658,
      "step": 6
    },
    {
      "epoch": 0.01,
      "grad_norm": 27946.95703125,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 16.0835,
      "step": 7
    },
    {
      "epoch": 0.01,
      "grad_norm": 12839.095703125,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 18.5419,
      "step": 8
    },
    {
      "epoch": 0.01,
      "grad_norm": 8342.5234375,
      "learning_rate": 1.5e-06,
      "loss": 22.3654,
      "step": 9
    },
    {
      "epoch": 0.01,
      "grad_norm": 204295.75,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 28.4639,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 95740.8046875,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 30.6698,
      "step": 11
    },
    {
      "epoch": 0.01,
      "grad_norm": 3480.7021484375,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 16.5736,
      "step": 12
    },
    {
      "epoch": 0.01,
      "grad_norm": 10814.189453125,
      "learning_rate": 2.166666666666667e-06,
      "loss": 15.3993,
      "step": 13
    },
    {
      "epoch": 0.01,
      "grad_norm": 14224.33984375,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 37.6125,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 2547.51611328125,
      "learning_rate": 2.5e-06,
      "loss": 17.7142,
      "step": 15
    },
    {
      "epoch": 0.02,
      "grad_norm": 13396.3623046875,
      "learning_rate": 2.666666666666667e-06,
      "loss": 25.6733,
      "step": 16
    },
    {
      "epoch": 0.02,
      "grad_norm": 8084.17919921875,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 32.5975,
      "step": 17
    },
    {
      "epoch": 0.02,
      "grad_norm": 6555.4365234375,
      "learning_rate": 3e-06,
      "loss": 17.0737,
      "step": 18
    },
    {
      "epoch": 0.02,
      "grad_norm": 9448.9619140625,
      "learning_rate": 3.166666666666667e-06,
      "loss": 30.0001,
      "step": 19
    },
    {
      "epoch": 0.02,
      "grad_norm": 31357.375,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 28.1346,
      "step": 20
    },
    {
      "epoch": 0.02,
      "grad_norm": 7923.6416015625,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 31.5666,
      "step": 21
    },
    {
      "epoch": 0.02,
      "grad_norm": 9472.2412109375,
      "learning_rate": 3.666666666666667e-06,
      "loss": 30.9423,
      "step": 22
    },
    {
      "epoch": 0.02,
      "grad_norm": 6404.15185546875,
      "learning_rate": 3.833333333333334e-06,
      "loss": 27.9806,
      "step": 23
    },
    {
      "epoch": 0.02,
      "grad_norm": 2412.272216796875,
      "learning_rate": 4.000000000000001e-06,
      "loss": 29.419,
      "step": 24
    },
    {
      "epoch": 0.03,
      "grad_norm": 19928.865234375,
      "learning_rate": 4.166666666666667e-06,
      "loss": 22.7418,
      "step": 25
    },
    {
      "epoch": 0.03,
      "grad_norm": 5459.26904296875,
      "learning_rate": 4.333333333333334e-06,
      "loss": 29.0482,
      "step": 26
    },
    {
      "epoch": 0.03,
      "grad_norm": 24947.0078125,
      "learning_rate": 4.5e-06,
      "loss": 34.1953,
      "step": 27
    },
    {
      "epoch": 0.03,
      "grad_norm": 4263.39306640625,
      "learning_rate": 4.666666666666667e-06,
      "loss": 36.6887,
      "step": 28
    },
    {
      "epoch": 0.03,
      "grad_norm": 14417.4052734375,
      "learning_rate": 4.833333333333333e-06,
      "loss": 27.361,
      "step": 29
    },
    {
      "epoch": 0.03,
      "grad_norm": 4921.15673828125,
      "learning_rate": 5e-06,
      "loss": 13.1438,
      "step": 30
    },
    {
      "epoch": 0.03,
      "grad_norm": 6019.08935546875,
      "learning_rate": 5.166666666666667e-06,
      "loss": 20.8912,
      "step": 31
    },
    {
      "epoch": 0.03,
      "grad_norm": 5161.97705078125,
      "learning_rate": 5.333333333333334e-06,
      "loss": 19.3286,
      "step": 32
    },
    {
      "epoch": 0.03,
      "grad_norm": 10570.173828125,
      "learning_rate": 5.500000000000001e-06,
      "loss": 18.6533,
      "step": 33
    },
    {
      "epoch": 0.03,
      "grad_norm": 5661.56005859375,
      "learning_rate": 5.666666666666667e-06,
      "loss": 27.6278,
      "step": 34
    },
    {
      "epoch": 0.04,
      "grad_norm": 9068.9287109375,
      "learning_rate": 5.833333333333334e-06,
      "loss": 19.4946,
      "step": 35
    },
    {
      "epoch": 0.04,
      "grad_norm": 6551.5517578125,
      "learning_rate": 6e-06,
      "loss": 34.0688,
      "step": 36
    },
    {
      "epoch": 0.04,
      "grad_norm": 8402.5380859375,
      "learning_rate": 6.166666666666667e-06,
      "loss": 26.3404,
      "step": 37
    },
    {
      "epoch": 0.04,
      "grad_norm": 3169.636474609375,
      "learning_rate": 6.333333333333334e-06,
      "loss": 24.6237,
      "step": 38
    },
    {
      "epoch": 0.04,
      "grad_norm": 11943.3642578125,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 21.0539,
      "step": 39
    },
    {
      "epoch": 0.04,
      "grad_norm": 3705.986328125,
      "learning_rate": 6.666666666666667e-06,
      "loss": 27.9686,
      "step": 40
    },
    {
      "epoch": 0.04,
      "grad_norm": 3769.466064453125,
      "learning_rate": 6.833333333333333e-06,
      "loss": 24.0913,
      "step": 41
    },
    {
      "epoch": 0.04,
      "grad_norm": 16617.9921875,
      "learning_rate": 7.000000000000001e-06,
      "loss": 28.1527,
      "step": 42
    },
    {
      "epoch": 0.04,
      "grad_norm": 40124.51171875,
      "learning_rate": 7.166666666666667e-06,
      "loss": 30.8527,
      "step": 43
    },
    {
      "epoch": 0.04,
      "grad_norm": 7684.35400390625,
      "learning_rate": 7.333333333333334e-06,
      "loss": 24.7386,
      "step": 44
    },
    {
      "epoch": 0.05,
      "grad_norm": 13184.126953125,
      "learning_rate": 7.5e-06,
      "loss": 18.5226,
      "step": 45
    },
    {
      "epoch": 0.05,
      "grad_norm": 31556.388671875,
      "learning_rate": 7.666666666666667e-06,
      "loss": 32.4754,
      "step": 46
    },
    {
      "epoch": 0.05,
      "grad_norm": 11985.7314453125,
      "learning_rate": 7.833333333333333e-06,
      "loss": 16.0325,
      "step": 47
    },
    {
      "epoch": 0.05,
      "grad_norm": 6076.97021484375,
      "learning_rate": 8.000000000000001e-06,
      "loss": 24.5192,
      "step": 48
    },
    {
      "epoch": 0.05,
      "grad_norm": 5328.6953125,
      "learning_rate": 8.166666666666668e-06,
      "loss": 34.2428,
      "step": 49
    },
    {
      "epoch": 0.05,
      "grad_norm": 6626.0693359375,
      "learning_rate": 8.333333333333334e-06,
      "loss": 23.9109,
      "step": 50
    },
    {
      "epoch": 0.05,
      "grad_norm": 11652.2763671875,
      "learning_rate": 8.500000000000002e-06,
      "loss": 34.6427,
      "step": 51
    },
    {
      "epoch": 0.05,
      "grad_norm": 2128.8486328125,
      "learning_rate": 8.666666666666668e-06,
      "loss": 40.654,
      "step": 52
    },
    {
      "epoch": 0.05,
      "grad_norm": 13948.369140625,
      "learning_rate": 8.833333333333334e-06,
      "loss": 19.5804,
      "step": 53
    },
    {
      "epoch": 0.05,
      "grad_norm": 5313.767578125,
      "learning_rate": 9e-06,
      "loss": 32.1603,
      "step": 54
    },
    {
      "epoch": 0.06,
      "grad_norm": 3134.908447265625,
      "learning_rate": 9.166666666666666e-06,
      "loss": 31.2902,
      "step": 55
    },
    {
      "epoch": 0.06,
      "grad_norm": 6596.79296875,
      "learning_rate": 9.333333333333334e-06,
      "loss": 38.5693,
      "step": 56
    },
    {
      "epoch": 0.06,
      "grad_norm": 45133.1015625,
      "learning_rate": 9.5e-06,
      "loss": 27.8014,
      "step": 57
    },
    {
      "epoch": 0.06,
      "grad_norm": 5267.53173828125,
      "learning_rate": 9.666666666666667e-06,
      "loss": 15.0727,
      "step": 58
    },
    {
      "epoch": 0.06,
      "grad_norm": 10860.8056640625,
      "learning_rate": 9.833333333333333e-06,
      "loss": 37.5134,
      "step": 59
    },
    {
      "epoch": 0.06,
      "grad_norm": 6549.3466796875,
      "learning_rate": 1e-05,
      "loss": 38.2987,
      "step": 60
    },
    {
      "epoch": 0.06,
      "grad_norm": 10834.298828125,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 29.6409,
      "step": 61
    },
    {
      "epoch": 0.06,
      "grad_norm": 5499.0087890625,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 21.2077,
      "step": 62
    },
    {
      "epoch": 0.06,
      "grad_norm": 4177.91845703125,
      "learning_rate": 1.05e-05,
      "loss": 35.8046,
      "step": 63
    },
    {
      "epoch": 0.06,
      "grad_norm": 84666.0,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 31.9912,
      "step": 64
    },
    {
      "epoch": 0.07,
      "grad_norm": 7415.12841796875,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 18.1279,
      "step": 65
    },
    {
      "epoch": 0.07,
      "grad_norm": 5868.3828125,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 17.7629,
      "step": 66
    },
    {
      "epoch": 0.07,
      "grad_norm": 4436.99658203125,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 21.6399,
      "step": 67
    },
    {
      "epoch": 0.07,
      "grad_norm": 27621.6015625,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 31.4325,
      "step": 68
    },
    {
      "epoch": 0.07,
      "grad_norm": 44119.55078125,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 39.7303,
      "step": 69
    },
    {
      "epoch": 0.07,
      "grad_norm": 15596.53515625,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 36.0269,
      "step": 70
    },
    {
      "epoch": 0.07,
      "grad_norm": 77744.6640625,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 22.7075,
      "step": 71
    },
    {
      "epoch": 0.07,
      "grad_norm": 28549.39453125,
      "learning_rate": 1.2e-05,
      "loss": 23.9909,
      "step": 72
    },
    {
      "epoch": 0.07,
      "grad_norm": 1326.114990234375,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 22.8262,
      "step": 73
    },
    {
      "epoch": 0.07,
      "grad_norm": 33945.3125,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 20.4152,
      "step": 74
    },
    {
      "epoch": 0.08,
      "grad_norm": 3468.857666015625,
      "learning_rate": 1.25e-05,
      "loss": 14.7391,
      "step": 75
    },
    {
      "epoch": 0.08,
      "grad_norm": 5523.60693359375,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 39.8298,
      "step": 76
    },
    {
      "epoch": 0.08,
      "grad_norm": 30893.369140625,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 32.9351,
      "step": 77
    },
    {
      "epoch": 0.08,
      "grad_norm": 23155.2265625,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 20.2554,
      "step": 78
    },
    {
      "epoch": 0.08,
      "grad_norm": 10230.625,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 37.4164,
      "step": 79
    },
    {
      "epoch": 0.08,
      "grad_norm": 11297.4677734375,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 38.9745,
      "step": 80
    },
    {
      "epoch": 0.08,
      "grad_norm": 23815.33203125,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 21.0184,
      "step": 81
    },
    {
      "epoch": 0.08,
      "grad_norm": 26590.158203125,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 22.1453,
      "step": 82
    },
    {
      "epoch": 0.08,
      "grad_norm": 25841.37109375,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 30.1794,
      "step": 83
    },
    {
      "epoch": 0.08,
      "grad_norm": 10515.6435546875,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 19.5161,
      "step": 84
    },
    {
      "epoch": 0.09,
      "grad_norm": 4369.0634765625,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 25.2746,
      "step": 85
    },
    {
      "epoch": 0.09,
      "grad_norm": 21044.140625,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 22.6468,
      "step": 86
    },
    {
      "epoch": 0.09,
      "grad_norm": 16490.580078125,
      "learning_rate": 1.45e-05,
      "loss": 29.6144,
      "step": 87
    },
    {
      "epoch": 0.09,
      "grad_norm": 22677.92578125,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 21.0265,
      "step": 88
    },
    {
      "epoch": 0.09,
      "grad_norm": 8711.3203125,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 15.3589,
      "step": 89
    },
    {
      "epoch": 0.09,
      "grad_norm": 7286.23388671875,
      "learning_rate": 1.5e-05,
      "loss": 22.4558,
      "step": 90
    },
    {
      "epoch": 0.09,
      "grad_norm": 13029.373046875,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 19.0455,
      "step": 91
    },
    {
      "epoch": 0.09,
      "grad_norm": 2001.707763671875,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 21.795,
      "step": 92
    },
    {
      "epoch": 0.09,
      "grad_norm": 8908.1416015625,
      "learning_rate": 1.55e-05,
      "loss": 14.5071,
      "step": 93
    },
    {
      "epoch": 0.09,
      "grad_norm": 16806.57421875,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 43.2108,
      "step": 94
    },
    {
      "epoch": 0.1,
      "grad_norm": 7179.541015625,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 14.8925,
      "step": 95
    },
    {
      "epoch": 0.1,
      "grad_norm": 20760.740234375,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 24.5903,
      "step": 96
    },
    {
      "epoch": 0.1,
      "grad_norm": 27919.017578125,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 24.6239,
      "step": 97
    },
    {
      "epoch": 0.1,
      "grad_norm": 10210.0009765625,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 17.4535,
      "step": 98
    },
    {
      "epoch": 0.1,
      "grad_norm": 15286.8076171875,
      "learning_rate": 1.65e-05,
      "loss": 15.0445,
      "step": 99
    },
    {
      "epoch": 0.1,
      "grad_norm": 20980.14453125,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 23.7012,
      "step": 100
    },
    {
      "epoch": 0.1,
      "grad_norm": 10622.8134765625,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 34.5291,
      "step": 101
    },
    {
      "epoch": 0.1,
      "grad_norm": 10333.8505859375,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 27.5652,
      "step": 102
    },
    {
      "epoch": 0.1,
      "grad_norm": 11251.65234375,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 20.3363,
      "step": 103
    },
    {
      "epoch": 0.1,
      "grad_norm": 2092.83642578125,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 13.0447,
      "step": 104
    },
    {
      "epoch": 0.11,
      "grad_norm": 3918.583984375,
      "learning_rate": 1.75e-05,
      "loss": 34.9495,
      "step": 105
    },
    {
      "epoch": 0.11,
      "grad_norm": 9804.708984375,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 22.6172,
      "step": 106
    },
    {
      "epoch": 0.11,
      "grad_norm": 10946.9228515625,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 33.0009,
      "step": 107
    },
    {
      "epoch": 0.11,
      "grad_norm": 231349.890625,
      "learning_rate": 1.8e-05,
      "loss": 24.4758,
      "step": 108
    },
    {
      "epoch": 0.11,
      "grad_norm": 1875.1134033203125,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 32.6441,
      "step": 109
    },
    {
      "epoch": 0.11,
      "grad_norm": 5099.67236328125,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 21.5389,
      "step": 110
    },
    {
      "epoch": 0.11,
      "grad_norm": 6915.49951171875,
      "learning_rate": 1.85e-05,
      "loss": 36.9482,
      "step": 111
    },
    {
      "epoch": 0.11,
      "grad_norm": 9540.9287109375,
      "learning_rate": 1.866666666666667e-05,
      "loss": 15.6458,
      "step": 112
    },
    {
      "epoch": 0.11,
      "grad_norm": 15243.0380859375,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 25.4789,
      "step": 113
    },
    {
      "epoch": 0.11,
      "grad_norm": 2931.868408203125,
      "learning_rate": 1.9e-05,
      "loss": 28.3439,
      "step": 114
    },
    {
      "epoch": 0.12,
      "grad_norm": 14142.4482421875,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 22.564,
      "step": 115
    },
    {
      "epoch": 0.12,
      "grad_norm": 2435.78369140625,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 17.6539,
      "step": 116
    },
    {
      "epoch": 0.12,
      "grad_norm": 29427.13671875,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 35.6292,
      "step": 117
    },
    {
      "epoch": 0.12,
      "grad_norm": 13644.7666015625,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 43.7505,
      "step": 118
    },
    {
      "epoch": 0.12,
      "grad_norm": 6313.96826171875,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 15.3689,
      "step": 119
    },
    {
      "epoch": 0.12,
      "grad_norm": 11503.7255859375,
      "learning_rate": 2e-05,
      "loss": 34.4523,
      "step": 120
    },
    {
      "epoch": 0.12,
      "grad_norm": 3707.015380859375,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 17.6003,
      "step": 121
    },
    {
      "epoch": 0.12,
      "grad_norm": 4571.49267578125,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 18.6372,
      "step": 122
    },
    {
      "epoch": 0.12,
      "grad_norm": 10859.537109375,
      "learning_rate": 2.05e-05,
      "loss": 48.9797,
      "step": 123
    },
    {
      "epoch": 0.12,
      "grad_norm": 7467.58935546875,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 37.7188,
      "step": 124
    },
    {
      "epoch": 0.13,
      "grad_norm": 5731.36767578125,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 25.6115,
      "step": 125
    },
    {
      "epoch": 0.13,
      "grad_norm": 10359.228515625,
      "learning_rate": 2.1e-05,
      "loss": 24.4065,
      "step": 126
    },
    {
      "epoch": 0.13,
      "grad_norm": 8725.8994140625,
      "learning_rate": 2.116666666666667e-05,
      "loss": 36.8322,
      "step": 127
    },
    {
      "epoch": 0.13,
      "grad_norm": 12424.4560546875,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 21.8621,
      "step": 128
    },
    {
      "epoch": 0.13,
      "grad_norm": 2635.866455078125,
      "learning_rate": 2.15e-05,
      "loss": 30.2376,
      "step": 129
    },
    {
      "epoch": 0.13,
      "grad_norm": 14739.4677734375,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 20.0094,
      "step": 130
    },
    {
      "epoch": 0.13,
      "grad_norm": 3818.294189453125,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 17.5221,
      "step": 131
    },
    {
      "epoch": 0.13,
      "grad_norm": 5054.60107421875,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 36.8672,
      "step": 132
    },
    {
      "epoch": 0.13,
      "grad_norm": 37820.07421875,
      "learning_rate": 2.216666666666667e-05,
      "loss": 37.7192,
      "step": 133
    },
    {
      "epoch": 0.13,
      "grad_norm": 8295.322265625,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 27.072,
      "step": 134
    },
    {
      "epoch": 0.14,
      "grad_norm": 10738.2490234375,
      "learning_rate": 2.25e-05,
      "loss": 37.941,
      "step": 135
    },
    {
      "epoch": 0.14,
      "grad_norm": 9490.1318359375,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 37.7804,
      "step": 136
    },
    {
      "epoch": 0.14,
      "grad_norm": 3481.20068359375,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 17.4586,
      "step": 137
    },
    {
      "epoch": 0.14,
      "grad_norm": 3436.892578125,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 23.8828,
      "step": 138
    },
    {
      "epoch": 0.14,
      "grad_norm": 11117.4443359375,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 18.2675,
      "step": 139
    },
    {
      "epoch": 0.14,
      "grad_norm": 3881.9619140625,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 17.3514,
      "step": 140
    },
    {
      "epoch": 0.14,
      "grad_norm": 2229.3896484375,
      "learning_rate": 2.35e-05,
      "loss": 15.0569,
      "step": 141
    },
    {
      "epoch": 0.14,
      "grad_norm": 8774.7119140625,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 26.1996,
      "step": 142
    },
    {
      "epoch": 0.14,
      "grad_norm": 4765.3349609375,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 23.3441,
      "step": 143
    },
    {
      "epoch": 0.14,
      "grad_norm": 6671.443359375,
      "learning_rate": 2.4e-05,
      "loss": 20.5203,
      "step": 144
    },
    {
      "epoch": 0.15,
      "grad_norm": 20951.767578125,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 30.4916,
      "step": 145
    },
    {
      "epoch": 0.15,
      "grad_norm": 7591.1796875,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 23.6689,
      "step": 146
    },
    {
      "epoch": 0.15,
      "grad_norm": 8432.984375,
      "learning_rate": 2.45e-05,
      "loss": 27.7546,
      "step": 147
    },
    {
      "epoch": 0.15,
      "grad_norm": 1440.3370361328125,
      "learning_rate": 2.466666666666667e-05,
      "loss": 26.5664,
      "step": 148
    },
    {
      "epoch": 0.15,
      "grad_norm": 23986.591796875,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 28.0412,
      "step": 149
    },
    {
      "epoch": 0.15,
      "grad_norm": 6081.8505859375,
      "learning_rate": 2.5e-05,
      "loss": 26.9247,
      "step": 150
    },
    {
      "epoch": 0.15,
      "grad_norm": 4436.7109375,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 27.9745,
      "step": 151
    },
    {
      "epoch": 0.15,
      "grad_norm": 40790.89453125,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 28.9974,
      "step": 152
    },
    {
      "epoch": 0.15,
      "grad_norm": 35810.375,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 41.1232,
      "step": 153
    },
    {
      "epoch": 0.15,
      "grad_norm": 9110.9248046875,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 26.7704,
      "step": 154
    },
    {
      "epoch": 0.16,
      "grad_norm": 7617.3740234375,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 30.7463,
      "step": 155
    },
    {
      "epoch": 0.16,
      "grad_norm": 2864.674072265625,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 29.7975,
      "step": 156
    },
    {
      "epoch": 0.16,
      "grad_norm": 3961.849609375,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 15.3685,
      "step": 157
    },
    {
      "epoch": 0.16,
      "grad_norm": 8108.6875,
      "learning_rate": 2.633333333333333e-05,
      "loss": 39.3449,
      "step": 158
    },
    {
      "epoch": 0.16,
      "grad_norm": 17944.703125,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 17.4062,
      "step": 159
    },
    {
      "epoch": 0.16,
      "grad_norm": 10463.5283203125,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 41.5949,
      "step": 160
    },
    {
      "epoch": 0.16,
      "grad_norm": 5416.3427734375,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 17.4177,
      "step": 161
    },
    {
      "epoch": 0.16,
      "grad_norm": 7910.23388671875,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 18.6143,
      "step": 162
    },
    {
      "epoch": 0.16,
      "grad_norm": 3299.92578125,
      "learning_rate": 2.716666666666667e-05,
      "loss": 19.4044,
      "step": 163
    },
    {
      "epoch": 0.16,
      "grad_norm": 9246.9609375,
      "learning_rate": 2.733333333333333e-05,
      "loss": 25.9544,
      "step": 164
    },
    {
      "epoch": 0.17,
      "grad_norm": 12498.0302734375,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 22.5543,
      "step": 165
    },
    {
      "epoch": 0.17,
      "grad_norm": 19271.54296875,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 26.214,
      "step": 166
    },
    {
      "epoch": 0.17,
      "grad_norm": 4294.42724609375,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 11.716,
      "step": 167
    },
    {
      "epoch": 0.17,
      "grad_norm": 10706.6181640625,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 31.0214,
      "step": 168
    },
    {
      "epoch": 0.17,
      "grad_norm": 15087.1474609375,
      "learning_rate": 2.816666666666667e-05,
      "loss": 24.1333,
      "step": 169
    },
    {
      "epoch": 0.17,
      "grad_norm": 3687.19140625,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 31.0187,
      "step": 170
    },
    {
      "epoch": 0.17,
      "grad_norm": 2222.2060546875,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 32.0467,
      "step": 171
    },
    {
      "epoch": 0.17,
      "grad_norm": 7063.54248046875,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 22.5083,
      "step": 172
    },
    {
      "epoch": 0.17,
      "grad_norm": 8819.55078125,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 18.8996,
      "step": 173
    },
    {
      "epoch": 0.17,
      "grad_norm": 7469.72119140625,
      "learning_rate": 2.9e-05,
      "loss": 22.605,
      "step": 174
    },
    {
      "epoch": 0.18,
      "grad_norm": 10452.1357421875,
      "learning_rate": 2.916666666666667e-05,
      "loss": 52.3018,
      "step": 175
    },
    {
      "epoch": 0.18,
      "grad_norm": 10822.6513671875,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 23.6367,
      "step": 176
    },
    {
      "epoch": 0.18,
      "grad_norm": 6678.66357421875,
      "learning_rate": 2.95e-05,
      "loss": 24.4936,
      "step": 177
    },
    {
      "epoch": 0.18,
      "grad_norm": 2731.048583984375,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 22.934,
      "step": 178
    },
    {
      "epoch": 0.18,
      "grad_norm": 7749.1689453125,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 16.3014,
      "step": 179
    },
    {
      "epoch": 0.18,
      "grad_norm": 3828.69580078125,
      "learning_rate": 3e-05,
      "loss": 20.7825,
      "step": 180
    },
    {
      "epoch": 0.18,
      "grad_norm": 13749.736328125,
      "learning_rate": 3.016666666666667e-05,
      "loss": 47.2974,
      "step": 181
    },
    {
      "epoch": 0.18,
      "grad_norm": 10525.8037109375,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 32.1284,
      "step": 182
    },
    {
      "epoch": 0.18,
      "grad_norm": 2486.653076171875,
      "learning_rate": 3.05e-05,
      "loss": 32.2968,
      "step": 183
    },
    {
      "epoch": 0.18,
      "grad_norm": 7254.89208984375,
      "learning_rate": 3.066666666666667e-05,
      "loss": 21.0235,
      "step": 184
    },
    {
      "epoch": 0.19,
      "grad_norm": 2096.44580078125,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 17.3471,
      "step": 185
    },
    {
      "epoch": 0.19,
      "grad_norm": 14532.265625,
      "learning_rate": 3.1e-05,
      "loss": 24.3364,
      "step": 186
    },
    {
      "epoch": 0.19,
      "grad_norm": 8895.5390625,
      "learning_rate": 3.116666666666667e-05,
      "loss": 20.1615,
      "step": 187
    },
    {
      "epoch": 0.19,
      "grad_norm": 14529.078125,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 23.6102,
      "step": 188
    },
    {
      "epoch": 0.19,
      "grad_norm": 26931.31640625,
      "learning_rate": 3.15e-05,
      "loss": 24.9033,
      "step": 189
    },
    {
      "epoch": 0.19,
      "grad_norm": 50235.796875,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 43.586,
      "step": 190
    },
    {
      "epoch": 0.19,
      "grad_norm": 5760.0576171875,
      "learning_rate": 3.183333333333334e-05,
      "loss": 17.2392,
      "step": 191
    },
    {
      "epoch": 0.19,
      "grad_norm": 6799.216796875,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 24.3063,
      "step": 192
    },
    {
      "epoch": 0.19,
      "grad_norm": 7178.119140625,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 19.0843,
      "step": 193
    },
    {
      "epoch": 0.19,
      "grad_norm": 29251.373046875,
      "learning_rate": 3.233333333333333e-05,
      "loss": 18.0391,
      "step": 194
    },
    {
      "epoch": 0.2,
      "grad_norm": 5771.33544921875,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 23.2377,
      "step": 195
    },
    {
      "epoch": 0.2,
      "grad_norm": 59044.6015625,
      "learning_rate": 3.266666666666667e-05,
      "loss": 19.4095,
      "step": 196
    },
    {
      "epoch": 0.2,
      "grad_norm": 2591.782958984375,
      "learning_rate": 3.283333333333333e-05,
      "loss": 15.7825,
      "step": 197
    },
    {
      "epoch": 0.2,
      "grad_norm": 14101.71484375,
      "learning_rate": 3.3e-05,
      "loss": 25.4712,
      "step": 198
    },
    {
      "epoch": 0.2,
      "grad_norm": 80529.5625,
      "learning_rate": 3.316666666666667e-05,
      "loss": 16.2775,
      "step": 199
    },
    {
      "epoch": 0.2,
      "grad_norm": 9530.6708984375,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 24.6213,
      "step": 200
    },
    {
      "epoch": 0.2,
      "grad_norm": 6969.2138671875,
      "learning_rate": 3.35e-05,
      "loss": 19.0759,
      "step": 201
    },
    {
      "epoch": 0.2,
      "grad_norm": 18261.166015625,
      "learning_rate": 3.366666666666667e-05,
      "loss": 20.464,
      "step": 202
    },
    {
      "epoch": 0.2,
      "grad_norm": 1497.0,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 14.4991,
      "step": 203
    },
    {
      "epoch": 0.2,
      "grad_norm": 14745.591796875,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 35.8197,
      "step": 204
    },
    {
      "epoch": 0.21,
      "grad_norm": 23732.89453125,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 16.1742,
      "step": 205
    },
    {
      "epoch": 0.21,
      "grad_norm": 1625.832763671875,
      "learning_rate": 3.433333333333333e-05,
      "loss": 19.3367,
      "step": 206
    },
    {
      "epoch": 0.21,
      "grad_norm": 6273.763671875,
      "learning_rate": 3.45e-05,
      "loss": 28.6472,
      "step": 207
    },
    {
      "epoch": 0.21,
      "grad_norm": 10135.4599609375,
      "learning_rate": 3.466666666666667e-05,
      "loss": 29.7263,
      "step": 208
    },
    {
      "epoch": 0.21,
      "grad_norm": 9893.5244140625,
      "learning_rate": 3.483333333333334e-05,
      "loss": 22.7973,
      "step": 209
    },
    {
      "epoch": 0.21,
      "grad_norm": 18803.205078125,
      "learning_rate": 3.5e-05,
      "loss": 36.2913,
      "step": 210
    },
    {
      "epoch": 0.21,
      "grad_norm": 14399.06640625,
      "learning_rate": 3.516666666666667e-05,
      "loss": 26.0483,
      "step": 211
    },
    {
      "epoch": 0.21,
      "grad_norm": 3698.886962890625,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 13.6519,
      "step": 212
    },
    {
      "epoch": 0.21,
      "grad_norm": 1723.1956787109375,
      "learning_rate": 3.55e-05,
      "loss": 43.3431,
      "step": 213
    },
    {
      "epoch": 0.21,
      "grad_norm": 10352.986328125,
      "learning_rate": 3.566666666666667e-05,
      "loss": 23.0206,
      "step": 214
    },
    {
      "epoch": 0.22,
      "grad_norm": 2734.44580078125,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 17.1165,
      "step": 215
    },
    {
      "epoch": 0.22,
      "grad_norm": 11336.7646484375,
      "learning_rate": 3.6e-05,
      "loss": 31.424,
      "step": 216
    },
    {
      "epoch": 0.22,
      "grad_norm": 10542.154296875,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 40.4458,
      "step": 217
    },
    {
      "epoch": 0.22,
      "grad_norm": 2077.479248046875,
      "learning_rate": 3.633333333333333e-05,
      "loss": 33.7804,
      "step": 218
    },
    {
      "epoch": 0.22,
      "grad_norm": 4581.67041015625,
      "learning_rate": 3.65e-05,
      "loss": 15.9899,
      "step": 219
    },
    {
      "epoch": 0.22,
      "grad_norm": 5193.146484375,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 24.5206,
      "step": 220
    },
    {
      "epoch": 0.22,
      "grad_norm": 19435.689453125,
      "learning_rate": 3.683333333333334e-05,
      "loss": 17.5565,
      "step": 221
    },
    {
      "epoch": 0.22,
      "grad_norm": 28088.3203125,
      "learning_rate": 3.7e-05,
      "loss": 27.3704,
      "step": 222
    },
    {
      "epoch": 0.22,
      "grad_norm": 4158.08984375,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 41.1338,
      "step": 223
    },
    {
      "epoch": 0.22,
      "grad_norm": 119648.8203125,
      "learning_rate": 3.733333333333334e-05,
      "loss": 28.0615,
      "step": 224
    },
    {
      "epoch": 0.23,
      "grad_norm": 13913.1123046875,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 21.4616,
      "step": 225
    },
    {
      "epoch": 0.23,
      "grad_norm": 768.767822265625,
      "learning_rate": 3.766666666666667e-05,
      "loss": 39.6444,
      "step": 226
    },
    {
      "epoch": 0.23,
      "grad_norm": 11958.2314453125,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 20.4996,
      "step": 227
    },
    {
      "epoch": 0.23,
      "grad_norm": 1406.879638671875,
      "learning_rate": 3.8e-05,
      "loss": 29.8867,
      "step": 228
    },
    {
      "epoch": 0.23,
      "grad_norm": 10679.71875,
      "learning_rate": 3.816666666666667e-05,
      "loss": 22.891,
      "step": 229
    },
    {
      "epoch": 0.23,
      "grad_norm": 24322.93359375,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 27.4289,
      "step": 230
    },
    {
      "epoch": 0.23,
      "grad_norm": 8918.0517578125,
      "learning_rate": 3.85e-05,
      "loss": 31.0209,
      "step": 231
    },
    {
      "epoch": 0.23,
      "grad_norm": 24647.541015625,
      "learning_rate": 3.866666666666667e-05,
      "loss": 16.2118,
      "step": 232
    },
    {
      "epoch": 0.23,
      "grad_norm": 16779.548828125,
      "learning_rate": 3.883333333333333e-05,
      "loss": 23.475,
      "step": 233
    },
    {
      "epoch": 0.23,
      "grad_norm": 4117.11083984375,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 19.1886,
      "step": 234
    },
    {
      "epoch": 0.24,
      "grad_norm": 11908.13671875,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 26.0437,
      "step": 235
    },
    {
      "epoch": 0.24,
      "grad_norm": 19710.48046875,
      "learning_rate": 3.933333333333333e-05,
      "loss": 27.8236,
      "step": 236
    },
    {
      "epoch": 0.24,
      "grad_norm": 87926.71875,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 29.5952,
      "step": 237
    },
    {
      "epoch": 0.24,
      "grad_norm": 25860.603515625,
      "learning_rate": 3.966666666666667e-05,
      "loss": 33.369,
      "step": 238
    },
    {
      "epoch": 0.24,
      "grad_norm": 7753.81787109375,
      "learning_rate": 3.983333333333333e-05,
      "loss": 27.1706,
      "step": 239
    },
    {
      "epoch": 0.24,
      "grad_norm": 2934.855712890625,
      "learning_rate": 4e-05,
      "loss": 17.0428,
      "step": 240
    },
    {
      "epoch": 0.24,
      "grad_norm": 6503.65576171875,
      "learning_rate": 4.016666666666667e-05,
      "loss": 22.2887,
      "step": 241
    },
    {
      "epoch": 0.24,
      "grad_norm": 1044.3497314453125,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 39.0713,
      "step": 242
    },
    {
      "epoch": 0.24,
      "grad_norm": 5783.29248046875,
      "learning_rate": 4.05e-05,
      "loss": 30.0699,
      "step": 243
    },
    {
      "epoch": 0.24,
      "grad_norm": 9894.9375,
      "learning_rate": 4.066666666666667e-05,
      "loss": 16.2848,
      "step": 244
    },
    {
      "epoch": 0.25,
      "grad_norm": 14542.33203125,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 25.9018,
      "step": 245
    },
    {
      "epoch": 0.25,
      "grad_norm": 4753.52880859375,
      "learning_rate": 4.1e-05,
      "loss": 19.2612,
      "step": 246
    },
    {
      "epoch": 0.25,
      "grad_norm": 13132.4345703125,
      "learning_rate": 4.116666666666667e-05,
      "loss": 56.3287,
      "step": 247
    },
    {
      "epoch": 0.25,
      "grad_norm": 15767.09375,
      "learning_rate": 4.133333333333333e-05,
      "loss": 19.9376,
      "step": 248
    },
    {
      "epoch": 0.25,
      "grad_norm": 7465.978515625,
      "learning_rate": 4.15e-05,
      "loss": 19.1521,
      "step": 249
    },
    {
      "epoch": 0.25,
      "grad_norm": 49489.10546875,
      "learning_rate": 4.166666666666667e-05,
      "loss": 33.6566,
      "step": 250
    },
    {
      "epoch": 0.25,
      "grad_norm": 25768.021484375,
      "learning_rate": 4.183333333333334e-05,
      "loss": 32.2055,
      "step": 251
    },
    {
      "epoch": 0.25,
      "grad_norm": 21972.591796875,
      "learning_rate": 4.2e-05,
      "loss": 36.0672,
      "step": 252
    },
    {
      "epoch": 0.25,
      "grad_norm": 9420.572265625,
      "learning_rate": 4.216666666666667e-05,
      "loss": 45.9238,
      "step": 253
    },
    {
      "epoch": 0.25,
      "grad_norm": 6333.912109375,
      "learning_rate": 4.233333333333334e-05,
      "loss": 22.7291,
      "step": 254
    },
    {
      "epoch": 0.26,
      "grad_norm": 4184.09423828125,
      "learning_rate": 4.25e-05,
      "loss": 15.4761,
      "step": 255
    },
    {
      "epoch": 0.26,
      "grad_norm": 8697.869140625,
      "learning_rate": 4.266666666666667e-05,
      "loss": 25.4396,
      "step": 256
    },
    {
      "epoch": 0.26,
      "grad_norm": 10183.0,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 22.9955,
      "step": 257
    },
    {
      "epoch": 0.26,
      "grad_norm": 6770.681640625,
      "learning_rate": 4.3e-05,
      "loss": 39.9505,
      "step": 258
    },
    {
      "epoch": 0.26,
      "grad_norm": 5272.6416015625,
      "learning_rate": 4.316666666666667e-05,
      "loss": 21.4382,
      "step": 259
    },
    {
      "epoch": 0.26,
      "grad_norm": 5545.953125,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 35.1112,
      "step": 260
    },
    {
      "epoch": 0.26,
      "grad_norm": 29670.658203125,
      "learning_rate": 4.35e-05,
      "loss": 34.1049,
      "step": 261
    },
    {
      "epoch": 0.26,
      "grad_norm": 26051.71875,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 32.9017,
      "step": 262
    },
    {
      "epoch": 0.26,
      "grad_norm": 2958.191162109375,
      "learning_rate": 4.383333333333334e-05,
      "loss": 24.5684,
      "step": 263
    },
    {
      "epoch": 0.26,
      "grad_norm": 7260.77490234375,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 37.4163,
      "step": 264
    },
    {
      "epoch": 0.27,
      "grad_norm": 4917.50537109375,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 14.6485,
      "step": 265
    },
    {
      "epoch": 0.27,
      "grad_norm": 14357.236328125,
      "learning_rate": 4.433333333333334e-05,
      "loss": 17.0494,
      "step": 266
    },
    {
      "epoch": 0.27,
      "grad_norm": 51370.296875,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 32.2367,
      "step": 267
    },
    {
      "epoch": 0.27,
      "grad_norm": 20579.537109375,
      "learning_rate": 4.466666666666667e-05,
      "loss": 29.2164,
      "step": 268
    },
    {
      "epoch": 0.27,
      "grad_norm": 7061.384765625,
      "learning_rate": 4.483333333333333e-05,
      "loss": 19.5609,
      "step": 269
    },
    {
      "epoch": 0.27,
      "grad_norm": 10563.0234375,
      "learning_rate": 4.5e-05,
      "loss": 20.2432,
      "step": 270
    },
    {
      "epoch": 0.27,
      "grad_norm": 5079.16650390625,
      "learning_rate": 4.516666666666667e-05,
      "loss": 16.6571,
      "step": 271
    },
    {
      "epoch": 0.27,
      "grad_norm": 10258.4404296875,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 19.4993,
      "step": 272
    },
    {
      "epoch": 0.27,
      "grad_norm": 134881.765625,
      "learning_rate": 4.55e-05,
      "loss": 14.9834,
      "step": 273
    },
    {
      "epoch": 0.27,
      "grad_norm": 14149.0849609375,
      "learning_rate": 4.566666666666667e-05,
      "loss": 28.6019,
      "step": 274
    },
    {
      "epoch": 0.28,
      "grad_norm": 25753.091796875,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 28.5915,
      "step": 275
    },
    {
      "epoch": 0.28,
      "grad_norm": 2152.2822265625,
      "learning_rate": 4.600000000000001e-05,
      "loss": 35.03,
      "step": 276
    },
    {
      "epoch": 0.28,
      "grad_norm": 2155.417236328125,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 12.8574,
      "step": 277
    },
    {
      "epoch": 0.28,
      "grad_norm": 2495.80517578125,
      "learning_rate": 4.633333333333333e-05,
      "loss": 24.2357,
      "step": 278
    },
    {
      "epoch": 0.28,
      "grad_norm": 2750.478759765625,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 47.6506,
      "step": 279
    },
    {
      "epoch": 0.28,
      "grad_norm": 6062.81005859375,
      "learning_rate": 4.666666666666667e-05,
      "loss": 38.4716,
      "step": 280
    },
    {
      "epoch": 0.28,
      "grad_norm": 32669.330078125,
      "learning_rate": 4.683333333333334e-05,
      "loss": 33.9416,
      "step": 281
    },
    {
      "epoch": 0.28,
      "grad_norm": 9417.169921875,
      "learning_rate": 4.7e-05,
      "loss": 15.9333,
      "step": 282
    },
    {
      "epoch": 0.28,
      "grad_norm": 8659.400390625,
      "learning_rate": 4.716666666666667e-05,
      "loss": 15.5609,
      "step": 283
    },
    {
      "epoch": 0.28,
      "grad_norm": 32889.27734375,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 20.0305,
      "step": 284
    },
    {
      "epoch": 0.29,
      "grad_norm": 6141.609375,
      "learning_rate": 4.75e-05,
      "loss": 34.9508,
      "step": 285
    },
    {
      "epoch": 0.29,
      "grad_norm": 20563.0078125,
      "learning_rate": 4.766666666666667e-05,
      "loss": 25.2313,
      "step": 286
    },
    {
      "epoch": 0.29,
      "grad_norm": 2438.39013671875,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 28.3801,
      "step": 287
    },
    {
      "epoch": 0.29,
      "grad_norm": 20808.79296875,
      "learning_rate": 4.8e-05,
      "loss": 19.8754,
      "step": 288
    },
    {
      "epoch": 0.29,
      "grad_norm": 8805.2373046875,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 41.7177,
      "step": 289
    },
    {
      "epoch": 0.29,
      "grad_norm": 25644.93359375,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 38.5869,
      "step": 290
    },
    {
      "epoch": 0.29,
      "grad_norm": 29385.103515625,
      "learning_rate": 4.85e-05,
      "loss": 27.7109,
      "step": 291
    },
    {
      "epoch": 0.29,
      "grad_norm": 4927.73583984375,
      "learning_rate": 4.866666666666667e-05,
      "loss": 25.9968,
      "step": 292
    },
    {
      "epoch": 0.29,
      "grad_norm": 22757.4609375,
      "learning_rate": 4.883333333333334e-05,
      "loss": 44.8249,
      "step": 293
    },
    {
      "epoch": 0.29,
      "grad_norm": 2186.56982421875,
      "learning_rate": 4.9e-05,
      "loss": 15.1143,
      "step": 294
    },
    {
      "epoch": 0.3,
      "grad_norm": 5914.80615234375,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 16.7857,
      "step": 295
    },
    {
      "epoch": 0.3,
      "grad_norm": 4089.6396484375,
      "learning_rate": 4.933333333333334e-05,
      "loss": 23.3706,
      "step": 296
    },
    {
      "epoch": 0.3,
      "grad_norm": 3855.492431640625,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 15.4817,
      "step": 297
    },
    {
      "epoch": 0.3,
      "grad_norm": 65150.3515625,
      "learning_rate": 4.966666666666667e-05,
      "loss": 24.6474,
      "step": 298
    },
    {
      "epoch": 0.3,
      "grad_norm": 9408.822265625,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 24.5847,
      "step": 299
    },
    {
      "epoch": 0.3,
      "grad_norm": 9331.609375,
      "learning_rate": 5e-05,
      "loss": 17.7913,
      "step": 300
    },
    {
      "epoch": 0.3,
      "grad_norm": 5026.5361328125,
      "learning_rate": 5.0166666666666675e-05,
      "loss": 18.9018,
      "step": 301
    },
    {
      "epoch": 0.3,
      "grad_norm": 5299.9287109375,
      "learning_rate": 5.0333333333333335e-05,
      "loss": 19.1435,
      "step": 302
    },
    {
      "epoch": 0.3,
      "grad_norm": 12570.2158203125,
      "learning_rate": 5.05e-05,
      "loss": 21.7965,
      "step": 303
    },
    {
      "epoch": 0.3,
      "grad_norm": 22721.291015625,
      "learning_rate": 5.0666666666666674e-05,
      "loss": 26.9298,
      "step": 304
    },
    {
      "epoch": 0.31,
      "grad_norm": 2461.341064453125,
      "learning_rate": 5.0833333333333333e-05,
      "loss": 14.2507,
      "step": 305
    },
    {
      "epoch": 0.31,
      "grad_norm": 4659.09375,
      "learning_rate": 5.1000000000000006e-05,
      "loss": 18.2031,
      "step": 306
    },
    {
      "epoch": 0.31,
      "grad_norm": 5060.76123046875,
      "learning_rate": 5.116666666666667e-05,
      "loss": 17.5642,
      "step": 307
    },
    {
      "epoch": 0.31,
      "grad_norm": 24364.876953125,
      "learning_rate": 5.133333333333333e-05,
      "loss": 47.9452,
      "step": 308
    },
    {
      "epoch": 0.31,
      "grad_norm": 3880.1806640625,
      "learning_rate": 5.1500000000000005e-05,
      "loss": 16.7956,
      "step": 309
    },
    {
      "epoch": 0.31,
      "grad_norm": 13290.458984375,
      "learning_rate": 5.166666666666667e-05,
      "loss": 20.1749,
      "step": 310
    },
    {
      "epoch": 0.31,
      "grad_norm": 3931.83984375,
      "learning_rate": 5.183333333333333e-05,
      "loss": 33.8325,
      "step": 311
    },
    {
      "epoch": 0.31,
      "grad_norm": 32137.283203125,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 34.3024,
      "step": 312
    },
    {
      "epoch": 0.31,
      "grad_norm": 3819.5087890625,
      "learning_rate": 5.216666666666666e-05,
      "loss": 13.8259,
      "step": 313
    },
    {
      "epoch": 0.31,
      "grad_norm": 18127.333984375,
      "learning_rate": 5.2333333333333336e-05,
      "loss": 20.6802,
      "step": 314
    },
    {
      "epoch": 0.32,
      "grad_norm": 11218.7861328125,
      "learning_rate": 5.25e-05,
      "loss": 19.4204,
      "step": 315
    },
    {
      "epoch": 0.32,
      "grad_norm": 8995.9921875,
      "learning_rate": 5.266666666666666e-05,
      "loss": 41.5223,
      "step": 316
    },
    {
      "epoch": 0.32,
      "grad_norm": 9155.759765625,
      "learning_rate": 5.2833333333333335e-05,
      "loss": 35.8847,
      "step": 317
    },
    {
      "epoch": 0.32,
      "grad_norm": 24384.5546875,
      "learning_rate": 5.300000000000001e-05,
      "loss": 32.3867,
      "step": 318
    },
    {
      "epoch": 0.32,
      "grad_norm": 14353.4208984375,
      "learning_rate": 5.316666666666667e-05,
      "loss": 46.4036,
      "step": 319
    },
    {
      "epoch": 0.32,
      "grad_norm": 20891.046875,
      "learning_rate": 5.333333333333333e-05,
      "loss": 19.1027,
      "step": 320
    },
    {
      "epoch": 0.32,
      "grad_norm": 13165.56640625,
      "learning_rate": 5.3500000000000006e-05,
      "loss": 28.6503,
      "step": 321
    },
    {
      "epoch": 0.32,
      "grad_norm": 9437.7705078125,
      "learning_rate": 5.3666666666666666e-05,
      "loss": 25.8198,
      "step": 322
    },
    {
      "epoch": 0.32,
      "grad_norm": 1721.8511962890625,
      "learning_rate": 5.383333333333334e-05,
      "loss": 21.6127,
      "step": 323
    },
    {
      "epoch": 0.32,
      "grad_norm": 13707.0830078125,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 28.3079,
      "step": 324
    },
    {
      "epoch": 0.33,
      "grad_norm": 7211.537109375,
      "learning_rate": 5.4166666666666664e-05,
      "loss": 14.084,
      "step": 325
    },
    {
      "epoch": 0.33,
      "grad_norm": 7848.02734375,
      "learning_rate": 5.433333333333334e-05,
      "loss": 20.1677,
      "step": 326
    },
    {
      "epoch": 0.33,
      "grad_norm": 8945.8310546875,
      "learning_rate": 5.45e-05,
      "loss": 27.4908,
      "step": 327
    },
    {
      "epoch": 0.33,
      "grad_norm": 56943.6875,
      "learning_rate": 5.466666666666666e-05,
      "loss": 16.1365,
      "step": 328
    },
    {
      "epoch": 0.33,
      "grad_norm": 36445.640625,
      "learning_rate": 5.4833333333333336e-05,
      "loss": 29.1485,
      "step": 329
    },
    {
      "epoch": 0.33,
      "grad_norm": 6011.5615234375,
      "learning_rate": 5.500000000000001e-05,
      "loss": 19.6655,
      "step": 330
    },
    {
      "epoch": 0.33,
      "grad_norm": 4127.67431640625,
      "learning_rate": 5.516666666666667e-05,
      "loss": 37.5954,
      "step": 331
    },
    {
      "epoch": 0.33,
      "grad_norm": 7871.544921875,
      "learning_rate": 5.5333333333333334e-05,
      "loss": 16.7777,
      "step": 332
    },
    {
      "epoch": 0.33,
      "grad_norm": 1530.329345703125,
      "learning_rate": 5.550000000000001e-05,
      "loss": 47.3147,
      "step": 333
    },
    {
      "epoch": 0.33,
      "grad_norm": 6867.19921875,
      "learning_rate": 5.566666666666667e-05,
      "loss": 15.7229,
      "step": 334
    },
    {
      "epoch": 0.34,
      "grad_norm": 6822.08837890625,
      "learning_rate": 5.583333333333334e-05,
      "loss": 15.4994,
      "step": 335
    },
    {
      "epoch": 0.34,
      "grad_norm": 7197.1787109375,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 21.2982,
      "step": 336
    },
    {
      "epoch": 0.34,
      "grad_norm": 6173.04052734375,
      "learning_rate": 5.6166666666666665e-05,
      "loss": 38.5845,
      "step": 337
    },
    {
      "epoch": 0.34,
      "grad_norm": 25231.220703125,
      "learning_rate": 5.633333333333334e-05,
      "loss": 42.1343,
      "step": 338
    },
    {
      "epoch": 0.34,
      "grad_norm": 3134.335693359375,
      "learning_rate": 5.65e-05,
      "loss": 20.779,
      "step": 339
    },
    {
      "epoch": 0.34,
      "grad_norm": 5242.01611328125,
      "learning_rate": 5.666666666666667e-05,
      "loss": 21.8963,
      "step": 340
    },
    {
      "epoch": 0.34,
      "grad_norm": 8771.4599609375,
      "learning_rate": 5.683333333333334e-05,
      "loss": 18.9965,
      "step": 341
    },
    {
      "epoch": 0.34,
      "grad_norm": 10904.791015625,
      "learning_rate": 5.6999999999999996e-05,
      "loss": 28.9243,
      "step": 342
    },
    {
      "epoch": 0.34,
      "grad_norm": 575.2186889648438,
      "learning_rate": 5.716666666666667e-05,
      "loss": 21.9566,
      "step": 343
    },
    {
      "epoch": 0.34,
      "grad_norm": 9396.1279296875,
      "learning_rate": 5.7333333333333336e-05,
      "loss": 15.0129,
      "step": 344
    },
    {
      "epoch": 0.35,
      "grad_norm": 69784.21875,
      "learning_rate": 5.7499999999999995e-05,
      "loss": 16.7538,
      "step": 345
    },
    {
      "epoch": 0.35,
      "grad_norm": 50964.1328125,
      "learning_rate": 5.766666666666667e-05,
      "loss": 25.2813,
      "step": 346
    },
    {
      "epoch": 0.35,
      "grad_norm": 5502.70849609375,
      "learning_rate": 5.783333333333334e-05,
      "loss": 24.6185,
      "step": 347
    },
    {
      "epoch": 0.35,
      "grad_norm": 19133.263671875,
      "learning_rate": 5.8e-05,
      "loss": 23.8476,
      "step": 348
    },
    {
      "epoch": 0.35,
      "grad_norm": 5496.8515625,
      "learning_rate": 5.8166666666666667e-05,
      "loss": 30.1991,
      "step": 349
    },
    {
      "epoch": 0.35,
      "grad_norm": 123914.328125,
      "learning_rate": 5.833333333333334e-05,
      "loss": 30.1911,
      "step": 350
    },
    {
      "epoch": 0.35,
      "grad_norm": 8478.9296875,
      "learning_rate": 5.85e-05,
      "loss": 14.7205,
      "step": 351
    },
    {
      "epoch": 0.35,
      "grad_norm": 28482.578125,
      "learning_rate": 5.866666666666667e-05,
      "loss": 23.4459,
      "step": 352
    },
    {
      "epoch": 0.35,
      "grad_norm": 5698.8642578125,
      "learning_rate": 5.883333333333334e-05,
      "loss": 18.6203,
      "step": 353
    },
    {
      "epoch": 0.35,
      "grad_norm": 5729.447265625,
      "learning_rate": 5.9e-05,
      "loss": 23.9948,
      "step": 354
    },
    {
      "epoch": 0.36,
      "grad_norm": 26858.951171875,
      "learning_rate": 5.916666666666667e-05,
      "loss": 34.4041,
      "step": 355
    },
    {
      "epoch": 0.36,
      "grad_norm": 8201.40234375,
      "learning_rate": 5.9333333333333343e-05,
      "loss": 20.507,
      "step": 356
    },
    {
      "epoch": 0.36,
      "grad_norm": 6519.2255859375,
      "learning_rate": 5.95e-05,
      "loss": 25.7919,
      "step": 357
    },
    {
      "epoch": 0.36,
      "grad_norm": 10039.73828125,
      "learning_rate": 5.966666666666667e-05,
      "loss": 30.8222,
      "step": 358
    },
    {
      "epoch": 0.36,
      "grad_norm": 7474.302734375,
      "learning_rate": 5.983333333333334e-05,
      "loss": 29.8942,
      "step": 359
    },
    {
      "epoch": 0.36,
      "grad_norm": 3415.429931640625,
      "learning_rate": 6e-05,
      "loss": 14.1358,
      "step": 360
    },
    {
      "epoch": 0.36,
      "grad_norm": 19706.71484375,
      "learning_rate": 6.0166666666666674e-05,
      "loss": 37.972,
      "step": 361
    },
    {
      "epoch": 0.36,
      "grad_norm": 20187.935546875,
      "learning_rate": 6.033333333333334e-05,
      "loss": 28.3771,
      "step": 362
    },
    {
      "epoch": 0.36,
      "grad_norm": 2219.44873046875,
      "learning_rate": 6.05e-05,
      "loss": 24.5954,
      "step": 363
    },
    {
      "epoch": 0.36,
      "grad_norm": 9021.3271484375,
      "learning_rate": 6.066666666666667e-05,
      "loss": 19.1304,
      "step": 364
    },
    {
      "epoch": 0.37,
      "grad_norm": 3313.689453125,
      "learning_rate": 6.083333333333333e-05,
      "loss": 15.8108,
      "step": 365
    },
    {
      "epoch": 0.37,
      "grad_norm": 85849.5703125,
      "learning_rate": 6.1e-05,
      "loss": 27.9896,
      "step": 366
    },
    {
      "epoch": 0.37,
      "grad_norm": 8184.720703125,
      "learning_rate": 6.116666666666667e-05,
      "loss": 24.3768,
      "step": 367
    },
    {
      "epoch": 0.37,
      "grad_norm": 4826.404296875,
      "learning_rate": 6.133333333333334e-05,
      "loss": 23.1752,
      "step": 368
    },
    {
      "epoch": 0.37,
      "grad_norm": 7891.998046875,
      "learning_rate": 6.15e-05,
      "loss": 22.4063,
      "step": 369
    },
    {
      "epoch": 0.37,
      "grad_norm": 14198.875,
      "learning_rate": 6.166666666666667e-05,
      "loss": 16.5251,
      "step": 370
    },
    {
      "epoch": 0.37,
      "grad_norm": 3171.879638671875,
      "learning_rate": 6.183333333333334e-05,
      "loss": 34.6284,
      "step": 371
    },
    {
      "epoch": 0.37,
      "grad_norm": 866.8056640625,
      "learning_rate": 6.2e-05,
      "loss": 17.036,
      "step": 372
    },
    {
      "epoch": 0.37,
      "grad_norm": 12232.49609375,
      "learning_rate": 6.216666666666667e-05,
      "loss": 29.686,
      "step": 373
    },
    {
      "epoch": 0.37,
      "grad_norm": 4512.5302734375,
      "learning_rate": 6.233333333333334e-05,
      "loss": 16.778,
      "step": 374
    },
    {
      "epoch": 0.38,
      "grad_norm": 3200.88232421875,
      "learning_rate": 6.25e-05,
      "loss": 24.9569,
      "step": 375
    },
    {
      "epoch": 0.38,
      "grad_norm": 18712.0546875,
      "learning_rate": 6.266666666666667e-05,
      "loss": 21.2155,
      "step": 376
    },
    {
      "epoch": 0.38,
      "grad_norm": 4190.91357421875,
      "learning_rate": 6.283333333333333e-05,
      "loss": 26.3596,
      "step": 377
    },
    {
      "epoch": 0.38,
      "grad_norm": 23704.953125,
      "learning_rate": 6.3e-05,
      "loss": 19.7529,
      "step": 378
    },
    {
      "epoch": 0.38,
      "grad_norm": 2244.299560546875,
      "learning_rate": 6.316666666666668e-05,
      "loss": 14.7293,
      "step": 379
    },
    {
      "epoch": 0.38,
      "grad_norm": 8563.189453125,
      "learning_rate": 6.333333333333333e-05,
      "loss": 36.04,
      "step": 380
    },
    {
      "epoch": 0.38,
      "grad_norm": 2934.77392578125,
      "learning_rate": 6.35e-05,
      "loss": 22.7532,
      "step": 381
    },
    {
      "epoch": 0.38,
      "grad_norm": 7568.59423828125,
      "learning_rate": 6.366666666666668e-05,
      "loss": 32.8996,
      "step": 382
    },
    {
      "epoch": 0.38,
      "grad_norm": 45291.4375,
      "learning_rate": 6.383333333333333e-05,
      "loss": 22.6959,
      "step": 383
    },
    {
      "epoch": 0.38,
      "grad_norm": 6600.33837890625,
      "learning_rate": 6.400000000000001e-05,
      "loss": 22.5736,
      "step": 384
    },
    {
      "epoch": 0.39,
      "grad_norm": 28305.5625,
      "learning_rate": 6.416666666666668e-05,
      "loss": 30.621,
      "step": 385
    },
    {
      "epoch": 0.39,
      "grad_norm": 9936.9990234375,
      "learning_rate": 6.433333333333333e-05,
      "loss": 44.1005,
      "step": 386
    },
    {
      "epoch": 0.39,
      "grad_norm": 6638.8232421875,
      "learning_rate": 6.450000000000001e-05,
      "loss": 18.5205,
      "step": 387
    },
    {
      "epoch": 0.39,
      "grad_norm": 83431.515625,
      "learning_rate": 6.466666666666666e-05,
      "loss": 23.5203,
      "step": 388
    },
    {
      "epoch": 0.39,
      "grad_norm": 18369.46875,
      "learning_rate": 6.483333333333333e-05,
      "loss": 19.3339,
      "step": 389
    },
    {
      "epoch": 0.39,
      "grad_norm": 4306.56103515625,
      "learning_rate": 6.500000000000001e-05,
      "loss": 18.6214,
      "step": 390
    },
    {
      "epoch": 0.39,
      "grad_norm": 4048.126708984375,
      "learning_rate": 6.516666666666666e-05,
      "loss": 16.8218,
      "step": 391
    },
    {
      "epoch": 0.39,
      "grad_norm": 8280.8505859375,
      "learning_rate": 6.533333333333334e-05,
      "loss": 21.9585,
      "step": 392
    },
    {
      "epoch": 0.39,
      "grad_norm": 16263.9462890625,
      "learning_rate": 6.55e-05,
      "loss": 13.8195,
      "step": 393
    },
    {
      "epoch": 0.39,
      "grad_norm": 5033.82763671875,
      "learning_rate": 6.566666666666666e-05,
      "loss": 18.1056,
      "step": 394
    },
    {
      "epoch": 0.4,
      "grad_norm": 3887.593017578125,
      "learning_rate": 6.583333333333334e-05,
      "loss": 14.0595,
      "step": 395
    },
    {
      "epoch": 0.4,
      "grad_norm": 5153.298828125,
      "learning_rate": 6.6e-05,
      "loss": 22.6597,
      "step": 396
    },
    {
      "epoch": 0.4,
      "grad_norm": 11339.7666015625,
      "learning_rate": 6.616666666666667e-05,
      "loss": 40.2502,
      "step": 397
    },
    {
      "epoch": 0.4,
      "grad_norm": 3329.0361328125,
      "learning_rate": 6.633333333333334e-05,
      "loss": 23.3167,
      "step": 398
    },
    {
      "epoch": 0.4,
      "grad_norm": 102161.46875,
      "learning_rate": 6.65e-05,
      "loss": 18.9601,
      "step": 399
    },
    {
      "epoch": 0.4,
      "grad_norm": 186503.40625,
      "learning_rate": 6.666666666666667e-05,
      "loss": 24.0648,
      "step": 400
    },
    {
      "epoch": 0.4,
      "grad_norm": 8326.0185546875,
      "learning_rate": 6.683333333333334e-05,
      "loss": 22.6457,
      "step": 401
    },
    {
      "epoch": 0.4,
      "grad_norm": 29934.306640625,
      "learning_rate": 6.7e-05,
      "loss": 19.9089,
      "step": 402
    },
    {
      "epoch": 0.4,
      "grad_norm": 266581.09375,
      "learning_rate": 6.716666666666667e-05,
      "loss": 31.3355,
      "step": 403
    },
    {
      "epoch": 0.4,
      "grad_norm": 12102.671875,
      "learning_rate": 6.733333333333333e-05,
      "loss": 20.289,
      "step": 404
    },
    {
      "epoch": 0.41,
      "grad_norm": 17918.28125,
      "learning_rate": 6.750000000000001e-05,
      "loss": 23.5251,
      "step": 405
    },
    {
      "epoch": 0.41,
      "grad_norm": 16568.662109375,
      "learning_rate": 6.766666666666667e-05,
      "loss": 25.0528,
      "step": 406
    },
    {
      "epoch": 0.41,
      "grad_norm": 6861.89990234375,
      "learning_rate": 6.783333333333333e-05,
      "loss": 38.7422,
      "step": 407
    },
    {
      "epoch": 0.41,
      "grad_norm": 15767.884765625,
      "learning_rate": 6.800000000000001e-05,
      "loss": 32.0113,
      "step": 408
    },
    {
      "epoch": 0.41,
      "grad_norm": 8605.8515625,
      "learning_rate": 6.816666666666667e-05,
      "loss": 19.788,
      "step": 409
    },
    {
      "epoch": 0.41,
      "grad_norm": 3044.3525390625,
      "learning_rate": 6.833333333333333e-05,
      "loss": 19.3278,
      "step": 410
    },
    {
      "epoch": 0.41,
      "grad_norm": 1539.2591552734375,
      "learning_rate": 6.850000000000001e-05,
      "loss": 38.2834,
      "step": 411
    },
    {
      "epoch": 0.41,
      "grad_norm": 3148.458984375,
      "learning_rate": 6.866666666666666e-05,
      "loss": 27.8424,
      "step": 412
    },
    {
      "epoch": 0.41,
      "grad_norm": 4217.21923828125,
      "learning_rate": 6.883333333333334e-05,
      "loss": 27.2995,
      "step": 413
    },
    {
      "epoch": 0.41,
      "grad_norm": 50622.1328125,
      "learning_rate": 6.9e-05,
      "loss": 33.0626,
      "step": 414
    },
    {
      "epoch": 0.42,
      "grad_norm": 8938.84765625,
      "learning_rate": 6.916666666666666e-05,
      "loss": 25.2149,
      "step": 415
    },
    {
      "epoch": 0.42,
      "grad_norm": 11168.68359375,
      "learning_rate": 6.933333333333334e-05,
      "loss": 16.8118,
      "step": 416
    },
    {
      "epoch": 0.42,
      "grad_norm": 1849.118408203125,
      "learning_rate": 6.95e-05,
      "loss": 19.9453,
      "step": 417
    },
    {
      "epoch": 0.42,
      "grad_norm": 11184.1650390625,
      "learning_rate": 6.966666666666668e-05,
      "loss": 25.9331,
      "step": 418
    },
    {
      "epoch": 0.42,
      "grad_norm": 15017.5859375,
      "learning_rate": 6.983333333333334e-05,
      "loss": 15.401,
      "step": 419
    },
    {
      "epoch": 0.42,
      "grad_norm": 6646.64892578125,
      "learning_rate": 7e-05,
      "loss": 22.9256,
      "step": 420
    },
    {
      "epoch": 0.42,
      "grad_norm": 7084.775390625,
      "learning_rate": 7.016666666666667e-05,
      "loss": 20.0822,
      "step": 421
    },
    {
      "epoch": 0.42,
      "grad_norm": 22740.99609375,
      "learning_rate": 7.033333333333334e-05,
      "loss": 33.0227,
      "step": 422
    },
    {
      "epoch": 0.42,
      "grad_norm": 889.5618896484375,
      "learning_rate": 7.05e-05,
      "loss": 22.0063,
      "step": 423
    },
    {
      "epoch": 0.42,
      "grad_norm": 9473.1083984375,
      "learning_rate": 7.066666666666667e-05,
      "loss": 28.7598,
      "step": 424
    },
    {
      "epoch": 0.43,
      "grad_norm": 13069.9921875,
      "learning_rate": 7.083333333333334e-05,
      "loss": 40.9119,
      "step": 425
    },
    {
      "epoch": 0.43,
      "grad_norm": 12091.0654296875,
      "learning_rate": 7.1e-05,
      "loss": 20.9743,
      "step": 426
    },
    {
      "epoch": 0.43,
      "grad_norm": 10420.947265625,
      "learning_rate": 7.116666666666667e-05,
      "loss": 19.0347,
      "step": 427
    },
    {
      "epoch": 0.43,
      "grad_norm": 1687.6837158203125,
      "learning_rate": 7.133333333333334e-05,
      "loss": 25.4189,
      "step": 428
    },
    {
      "epoch": 0.43,
      "grad_norm": 5977.40869140625,
      "learning_rate": 7.15e-05,
      "loss": 19.0546,
      "step": 429
    },
    {
      "epoch": 0.43,
      "grad_norm": 4697.7001953125,
      "learning_rate": 7.166666666666667e-05,
      "loss": 24.1908,
      "step": 430
    },
    {
      "epoch": 0.43,
      "grad_norm": 6669.7255859375,
      "learning_rate": 7.183333333333334e-05,
      "loss": 42.016,
      "step": 431
    },
    {
      "epoch": 0.43,
      "grad_norm": 8364.43359375,
      "learning_rate": 7.2e-05,
      "loss": 20.4308,
      "step": 432
    },
    {
      "epoch": 0.43,
      "grad_norm": 4365.63916015625,
      "learning_rate": 7.216666666666667e-05,
      "loss": 33.348,
      "step": 433
    },
    {
      "epoch": 0.43,
      "grad_norm": 9513.353515625,
      "learning_rate": 7.233333333333335e-05,
      "loss": 23.3631,
      "step": 434
    },
    {
      "epoch": 0.44,
      "grad_norm": 7548.19140625,
      "learning_rate": 7.25e-05,
      "loss": 23.6687,
      "step": 435
    },
    {
      "epoch": 0.44,
      "grad_norm": 4514.16455078125,
      "learning_rate": 7.266666666666667e-05,
      "loss": 19.559,
      "step": 436
    },
    {
      "epoch": 0.44,
      "grad_norm": 110143.7421875,
      "learning_rate": 7.283333333333335e-05,
      "loss": 18.8997,
      "step": 437
    },
    {
      "epoch": 0.44,
      "grad_norm": 24178.068359375,
      "learning_rate": 7.3e-05,
      "loss": 41.6002,
      "step": 438
    },
    {
      "epoch": 0.44,
      "grad_norm": 12967.0673828125,
      "learning_rate": 7.316666666666668e-05,
      "loss": 20.781,
      "step": 439
    },
    {
      "epoch": 0.44,
      "grad_norm": 4375.498046875,
      "learning_rate": 7.333333333333333e-05,
      "loss": 35.2414,
      "step": 440
    },
    {
      "epoch": 0.44,
      "grad_norm": 17372.46875,
      "learning_rate": 7.35e-05,
      "loss": 32.2287,
      "step": 441
    },
    {
      "epoch": 0.44,
      "grad_norm": 15479.626953125,
      "learning_rate": 7.366666666666668e-05,
      "loss": 38.7121,
      "step": 442
    },
    {
      "epoch": 0.44,
      "grad_norm": 10943.544921875,
      "learning_rate": 7.383333333333333e-05,
      "loss": 26.0698,
      "step": 443
    },
    {
      "epoch": 0.44,
      "grad_norm": 5533.2265625,
      "learning_rate": 7.4e-05,
      "loss": 16.8263,
      "step": 444
    },
    {
      "epoch": 0.45,
      "grad_norm": 14870.529296875,
      "learning_rate": 7.416666666666668e-05,
      "loss": 26.6075,
      "step": 445
    },
    {
      "epoch": 0.45,
      "grad_norm": 5919.845703125,
      "learning_rate": 7.433333333333333e-05,
      "loss": 14.2319,
      "step": 446
    },
    {
      "epoch": 0.45,
      "grad_norm": 2064.550537109375,
      "learning_rate": 7.450000000000001e-05,
      "loss": 33.2139,
      "step": 447
    },
    {
      "epoch": 0.45,
      "grad_norm": 11520.0009765625,
      "learning_rate": 7.466666666666667e-05,
      "loss": 16.4821,
      "step": 448
    },
    {
      "epoch": 0.45,
      "grad_norm": 2625.761962890625,
      "learning_rate": 7.483333333333333e-05,
      "loss": 28.9171,
      "step": 449
    },
    {
      "epoch": 0.45,
      "grad_norm": 5920.23046875,
      "learning_rate": 7.500000000000001e-05,
      "loss": 20.8221,
      "step": 450
    },
    {
      "epoch": 0.45,
      "grad_norm": 4558.720703125,
      "learning_rate": 7.516666666666667e-05,
      "loss": 12.7493,
      "step": 451
    },
    {
      "epoch": 0.45,
      "grad_norm": 12240.8505859375,
      "learning_rate": 7.533333333333334e-05,
      "loss": 26.0678,
      "step": 452
    },
    {
      "epoch": 0.45,
      "grad_norm": 13981.7470703125,
      "learning_rate": 7.55e-05,
      "loss": 26.6258,
      "step": 453
    },
    {
      "epoch": 0.45,
      "grad_norm": 5625.34423828125,
      "learning_rate": 7.566666666666667e-05,
      "loss": 17.4435,
      "step": 454
    },
    {
      "epoch": 0.46,
      "grad_norm": 40843.77734375,
      "learning_rate": 7.583333333333334e-05,
      "loss": 23.9551,
      "step": 455
    },
    {
      "epoch": 0.46,
      "grad_norm": 14622.671875,
      "learning_rate": 7.6e-05,
      "loss": 21.395,
      "step": 456
    },
    {
      "epoch": 0.46,
      "grad_norm": 7005.05859375,
      "learning_rate": 7.616666666666667e-05,
      "loss": 19.8134,
      "step": 457
    },
    {
      "epoch": 0.46,
      "grad_norm": 3766.375244140625,
      "learning_rate": 7.633333333333334e-05,
      "loss": 23.9878,
      "step": 458
    },
    {
      "epoch": 0.46,
      "grad_norm": 14925.212890625,
      "learning_rate": 7.65e-05,
      "loss": 27.3377,
      "step": 459
    },
    {
      "epoch": 0.46,
      "grad_norm": 5906.64306640625,
      "learning_rate": 7.666666666666667e-05,
      "loss": 54.0922,
      "step": 460
    },
    {
      "epoch": 0.46,
      "grad_norm": 19392.19140625,
      "learning_rate": 7.683333333333334e-05,
      "loss": 17.655,
      "step": 461
    },
    {
      "epoch": 0.46,
      "grad_norm": 12744.44140625,
      "learning_rate": 7.7e-05,
      "loss": 18.5741,
      "step": 462
    },
    {
      "epoch": 0.46,
      "grad_norm": 4875.94970703125,
      "learning_rate": 7.716666666666667e-05,
      "loss": 17.5209,
      "step": 463
    },
    {
      "epoch": 0.46,
      "grad_norm": 6817.84228515625,
      "learning_rate": 7.733333333333333e-05,
      "loss": 15.8312,
      "step": 464
    },
    {
      "epoch": 0.47,
      "grad_norm": 14358.8125,
      "learning_rate": 7.75e-05,
      "loss": 22.193,
      "step": 465
    },
    {
      "epoch": 0.47,
      "grad_norm": 2715.6005859375,
      "learning_rate": 7.766666666666667e-05,
      "loss": 15.1129,
      "step": 466
    },
    {
      "epoch": 0.47,
      "grad_norm": 35456.21875,
      "learning_rate": 7.783333333333333e-05,
      "loss": 19.8533,
      "step": 467
    },
    {
      "epoch": 0.47,
      "grad_norm": 7533.859375,
      "learning_rate": 7.800000000000001e-05,
      "loss": 22.5711,
      "step": 468
    },
    {
      "epoch": 0.47,
      "grad_norm": 4500.04443359375,
      "learning_rate": 7.816666666666666e-05,
      "loss": 20.7289,
      "step": 469
    },
    {
      "epoch": 0.47,
      "grad_norm": 34607.43359375,
      "learning_rate": 7.833333333333333e-05,
      "loss": 19.3394,
      "step": 470
    },
    {
      "epoch": 0.47,
      "grad_norm": 2288.94970703125,
      "learning_rate": 7.850000000000001e-05,
      "loss": 21.9403,
      "step": 471
    },
    {
      "epoch": 0.47,
      "grad_norm": 13817.966796875,
      "learning_rate": 7.866666666666666e-05,
      "loss": 35.3462,
      "step": 472
    },
    {
      "epoch": 0.47,
      "grad_norm": 3622.77685546875,
      "learning_rate": 7.883333333333334e-05,
      "loss": 14.2097,
      "step": 473
    },
    {
      "epoch": 0.47,
      "grad_norm": 6325.78466796875,
      "learning_rate": 7.900000000000001e-05,
      "loss": 26.4332,
      "step": 474
    },
    {
      "epoch": 0.48,
      "grad_norm": 4757.56494140625,
      "learning_rate": 7.916666666666666e-05,
      "loss": 16.2406,
      "step": 475
    },
    {
      "epoch": 0.48,
      "grad_norm": 9756.259765625,
      "learning_rate": 7.933333333333334e-05,
      "loss": 26.8308,
      "step": 476
    },
    {
      "epoch": 0.48,
      "grad_norm": 2155.6171875,
      "learning_rate": 7.950000000000001e-05,
      "loss": 14.0258,
      "step": 477
    },
    {
      "epoch": 0.48,
      "grad_norm": 1920.709228515625,
      "learning_rate": 7.966666666666666e-05,
      "loss": 16.925,
      "step": 478
    },
    {
      "epoch": 0.48,
      "grad_norm": 2648.64404296875,
      "learning_rate": 7.983333333333334e-05,
      "loss": 31.9745,
      "step": 479
    },
    {
      "epoch": 0.48,
      "grad_norm": 81692.9140625,
      "learning_rate": 8e-05,
      "loss": 38.3541,
      "step": 480
    },
    {
      "epoch": 0.48,
      "grad_norm": 27646.8828125,
      "learning_rate": 8.016666666666667e-05,
      "loss": 20.5584,
      "step": 481
    },
    {
      "epoch": 0.48,
      "grad_norm": 105115.296875,
      "learning_rate": 8.033333333333334e-05,
      "loss": 21.9338,
      "step": 482
    },
    {
      "epoch": 0.48,
      "grad_norm": 35767.85546875,
      "learning_rate": 8.05e-05,
      "loss": 37.8384,
      "step": 483
    },
    {
      "epoch": 0.48,
      "grad_norm": 9536.3603515625,
      "learning_rate": 8.066666666666667e-05,
      "loss": 17.9809,
      "step": 484
    },
    {
      "epoch": 0.49,
      "grad_norm": 1820.9815673828125,
      "learning_rate": 8.083333333333334e-05,
      "loss": 11.7728,
      "step": 485
    },
    {
      "epoch": 0.49,
      "grad_norm": 12527.7275390625,
      "learning_rate": 8.1e-05,
      "loss": 23.0127,
      "step": 486
    },
    {
      "epoch": 0.49,
      "grad_norm": 10804.2255859375,
      "learning_rate": 8.116666666666667e-05,
      "loss": 17.4095,
      "step": 487
    },
    {
      "epoch": 0.49,
      "grad_norm": 11045.9140625,
      "learning_rate": 8.133333333333334e-05,
      "loss": 20.5856,
      "step": 488
    },
    {
      "epoch": 0.49,
      "grad_norm": 41932.0546875,
      "learning_rate": 8.15e-05,
      "loss": 21.8718,
      "step": 489
    },
    {
      "epoch": 0.49,
      "grad_norm": 7737.57177734375,
      "learning_rate": 8.166666666666667e-05,
      "loss": 17.6454,
      "step": 490
    },
    {
      "epoch": 0.49,
      "grad_norm": 16521.87109375,
      "learning_rate": 8.183333333333333e-05,
      "loss": 15.0587,
      "step": 491
    },
    {
      "epoch": 0.49,
      "grad_norm": 5245.4462890625,
      "learning_rate": 8.2e-05,
      "loss": 19.0908,
      "step": 492
    },
    {
      "epoch": 0.49,
      "grad_norm": 19119.8046875,
      "learning_rate": 8.216666666666667e-05,
      "loss": 24.7247,
      "step": 493
    },
    {
      "epoch": 0.49,
      "grad_norm": 13555.5634765625,
      "learning_rate": 8.233333333333333e-05,
      "loss": 23.172,
      "step": 494
    },
    {
      "epoch": 0.5,
      "grad_norm": 8402.427734375,
      "learning_rate": 8.25e-05,
      "loss": 20.9195,
      "step": 495
    },
    {
      "epoch": 0.5,
      "grad_norm": 5223.08056640625,
      "learning_rate": 8.266666666666667e-05,
      "loss": 17.7405,
      "step": 496
    },
    {
      "epoch": 0.5,
      "grad_norm": 11704.5732421875,
      "learning_rate": 8.283333333333335e-05,
      "loss": 29.3574,
      "step": 497
    },
    {
      "epoch": 0.5,
      "grad_norm": 8386.982421875,
      "learning_rate": 8.3e-05,
      "loss": 32.6034,
      "step": 498
    },
    {
      "epoch": 0.5,
      "grad_norm": 10057.740234375,
      "learning_rate": 8.316666666666666e-05,
      "loss": 37.8597,
      "step": 499
    },
    {
      "epoch": 0.5,
      "grad_norm": 7998.1904296875,
      "learning_rate": 8.333333333333334e-05,
      "loss": 27.4275,
      "step": 500
    },
    {
      "epoch": 0.5,
      "grad_norm": 20484.669921875,
      "learning_rate": 8.35e-05,
      "loss": 19.109,
      "step": 501
    },
    {
      "epoch": 0.5,
      "grad_norm": 11851.5302734375,
      "learning_rate": 8.366666666666668e-05,
      "loss": 20.7464,
      "step": 502
    },
    {
      "epoch": 0.5,
      "grad_norm": 11556.0703125,
      "learning_rate": 8.383333333333334e-05,
      "loss": 31.2429,
      "step": 503
    },
    {
      "epoch": 0.5,
      "grad_norm": 32791.1328125,
      "learning_rate": 8.4e-05,
      "loss": 28.7661,
      "step": 504
    },
    {
      "epoch": 0.51,
      "grad_norm": 7219.001953125,
      "learning_rate": 8.416666666666668e-05,
      "loss": 24.3153,
      "step": 505
    },
    {
      "epoch": 0.51,
      "grad_norm": 3173.60107421875,
      "learning_rate": 8.433333333333334e-05,
      "loss": 36.3131,
      "step": 506
    },
    {
      "epoch": 0.51,
      "grad_norm": 1627.9169921875,
      "learning_rate": 8.450000000000001e-05,
      "loss": 48.1017,
      "step": 507
    },
    {
      "epoch": 0.51,
      "grad_norm": 12136.8994140625,
      "learning_rate": 8.466666666666667e-05,
      "loss": 16.7059,
      "step": 508
    },
    {
      "epoch": 0.51,
      "grad_norm": 5956.73486328125,
      "learning_rate": 8.483333333333334e-05,
      "loss": 18.5152,
      "step": 509
    },
    {
      "epoch": 0.51,
      "grad_norm": 7560.46240234375,
      "learning_rate": 8.5e-05,
      "loss": 23.3961,
      "step": 510
    },
    {
      "epoch": 0.51,
      "grad_norm": 50642.0625,
      "learning_rate": 8.516666666666667e-05,
      "loss": 19.0131,
      "step": 511
    },
    {
      "epoch": 0.51,
      "grad_norm": 48677.41796875,
      "learning_rate": 8.533333333333334e-05,
      "loss": 33.7204,
      "step": 512
    },
    {
      "epoch": 0.51,
      "grad_norm": 2938.78662109375,
      "learning_rate": 8.55e-05,
      "loss": 21.0031,
      "step": 513
    },
    {
      "epoch": 0.51,
      "grad_norm": 6082.140625,
      "learning_rate": 8.566666666666667e-05,
      "loss": 27.6879,
      "step": 514
    },
    {
      "epoch": 0.52,
      "grad_norm": 32795.6328125,
      "learning_rate": 8.583333333333334e-05,
      "loss": 18.3064,
      "step": 515
    },
    {
      "epoch": 0.52,
      "grad_norm": 7019.83251953125,
      "learning_rate": 8.6e-05,
      "loss": 18.0724,
      "step": 516
    },
    {
      "epoch": 0.52,
      "grad_norm": 13533.021484375,
      "learning_rate": 8.616666666666667e-05,
      "loss": 18.556,
      "step": 517
    },
    {
      "epoch": 0.52,
      "grad_norm": 28761.037109375,
      "learning_rate": 8.633333333333334e-05,
      "loss": 17.1519,
      "step": 518
    },
    {
      "epoch": 0.52,
      "grad_norm": 4737.94091796875,
      "learning_rate": 8.65e-05,
      "loss": 13.5988,
      "step": 519
    },
    {
      "epoch": 0.52,
      "grad_norm": 11713.39453125,
      "learning_rate": 8.666666666666667e-05,
      "loss": 32.5886,
      "step": 520
    },
    {
      "epoch": 0.52,
      "grad_norm": 17313.59375,
      "learning_rate": 8.683333333333333e-05,
      "loss": 32.701,
      "step": 521
    },
    {
      "epoch": 0.52,
      "grad_norm": 13955.6640625,
      "learning_rate": 8.7e-05,
      "loss": 24.7873,
      "step": 522
    },
    {
      "epoch": 0.52,
      "grad_norm": 23776.8515625,
      "learning_rate": 8.716666666666668e-05,
      "loss": 27.2172,
      "step": 523
    },
    {
      "epoch": 0.52,
      "grad_norm": 17764.1015625,
      "learning_rate": 8.733333333333333e-05,
      "loss": 17.5239,
      "step": 524
    },
    {
      "epoch": 0.53,
      "grad_norm": 6997.125,
      "learning_rate": 8.75e-05,
      "loss": 28.7387,
      "step": 525
    },
    {
      "epoch": 0.53,
      "grad_norm": 7076.27734375,
      "learning_rate": 8.766666666666668e-05,
      "loss": 30.962,
      "step": 526
    },
    {
      "epoch": 0.53,
      "grad_norm": 3092.74365234375,
      "learning_rate": 8.783333333333333e-05,
      "loss": 13.7161,
      "step": 527
    },
    {
      "epoch": 0.53,
      "grad_norm": 4859.73193359375,
      "learning_rate": 8.800000000000001e-05,
      "loss": 43.4001,
      "step": 528
    },
    {
      "epoch": 0.53,
      "grad_norm": 3271.125244140625,
      "learning_rate": 8.816666666666668e-05,
      "loss": 18.8928,
      "step": 529
    },
    {
      "epoch": 0.53,
      "grad_norm": 56151.66015625,
      "learning_rate": 8.833333333333333e-05,
      "loss": 25.7137,
      "step": 530
    },
    {
      "epoch": 0.53,
      "grad_norm": 2293.80517578125,
      "learning_rate": 8.850000000000001e-05,
      "loss": 13.3104,
      "step": 531
    },
    {
      "epoch": 0.53,
      "grad_norm": 2861.732421875,
      "learning_rate": 8.866666666666668e-05,
      "loss": 20.7424,
      "step": 532
    },
    {
      "epoch": 0.53,
      "grad_norm": 17223.19921875,
      "learning_rate": 8.883333333333333e-05,
      "loss": 22.8237,
      "step": 533
    },
    {
      "epoch": 0.53,
      "grad_norm": 1347.3372802734375,
      "learning_rate": 8.900000000000001e-05,
      "loss": 15.2692,
      "step": 534
    },
    {
      "epoch": 0.54,
      "grad_norm": 6719.3388671875,
      "learning_rate": 8.916666666666667e-05,
      "loss": 31.7134,
      "step": 535
    },
    {
      "epoch": 0.54,
      "grad_norm": 15137.5361328125,
      "learning_rate": 8.933333333333334e-05,
      "loss": 38.6457,
      "step": 536
    },
    {
      "epoch": 0.54,
      "grad_norm": 2330.755615234375,
      "learning_rate": 8.950000000000001e-05,
      "loss": 33.1782,
      "step": 537
    },
    {
      "epoch": 0.54,
      "grad_norm": 12273.9033203125,
      "learning_rate": 8.966666666666666e-05,
      "loss": 23.0512,
      "step": 538
    },
    {
      "epoch": 0.54,
      "grad_norm": 13681.7841796875,
      "learning_rate": 8.983333333333334e-05,
      "loss": 28.1175,
      "step": 539
    },
    {
      "epoch": 0.54,
      "grad_norm": 24160.083984375,
      "learning_rate": 9e-05,
      "loss": 18.7318,
      "step": 540
    },
    {
      "epoch": 0.54,
      "grad_norm": 8581.3857421875,
      "learning_rate": 9.016666666666667e-05,
      "loss": 33.3651,
      "step": 541
    },
    {
      "epoch": 0.54,
      "grad_norm": 6370.900390625,
      "learning_rate": 9.033333333333334e-05,
      "loss": 16.8568,
      "step": 542
    },
    {
      "epoch": 0.54,
      "grad_norm": 6744.4677734375,
      "learning_rate": 9.05e-05,
      "loss": 20.1226,
      "step": 543
    },
    {
      "epoch": 0.54,
      "grad_norm": 4770.23486328125,
      "learning_rate": 9.066666666666667e-05,
      "loss": 13.0483,
      "step": 544
    },
    {
      "epoch": 0.55,
      "grad_norm": 6151.78466796875,
      "learning_rate": 9.083333333333334e-05,
      "loss": 27.908,
      "step": 545
    },
    {
      "epoch": 0.55,
      "grad_norm": 8314.439453125,
      "learning_rate": 9.1e-05,
      "loss": 15.3047,
      "step": 546
    },
    {
      "epoch": 0.55,
      "grad_norm": 3095.983154296875,
      "learning_rate": 9.116666666666667e-05,
      "loss": 16.7281,
      "step": 547
    },
    {
      "epoch": 0.55,
      "grad_norm": 10939.845703125,
      "learning_rate": 9.133333333333334e-05,
      "loss": 19.605,
      "step": 548
    },
    {
      "epoch": 0.55,
      "grad_norm": 5163.39892578125,
      "learning_rate": 9.15e-05,
      "loss": 21.8613,
      "step": 549
    },
    {
      "epoch": 0.55,
      "grad_norm": 11379.1376953125,
      "learning_rate": 9.166666666666667e-05,
      "loss": 21.7049,
      "step": 550
    },
    {
      "epoch": 0.55,
      "grad_norm": 6912.11865234375,
      "learning_rate": 9.183333333333333e-05,
      "loss": 15.7256,
      "step": 551
    },
    {
      "epoch": 0.55,
      "grad_norm": 27147.857421875,
      "learning_rate": 9.200000000000001e-05,
      "loss": 26.8559,
      "step": 552
    },
    {
      "epoch": 0.55,
      "grad_norm": 13924.2822265625,
      "learning_rate": 9.216666666666667e-05,
      "loss": 24.776,
      "step": 553
    },
    {
      "epoch": 0.55,
      "grad_norm": 10253.451171875,
      "learning_rate": 9.233333333333333e-05,
      "loss": 13.654,
      "step": 554
    },
    {
      "epoch": 0.56,
      "grad_norm": 2549.06005859375,
      "learning_rate": 9.250000000000001e-05,
      "loss": 15.6539,
      "step": 555
    },
    {
      "epoch": 0.56,
      "grad_norm": 30324.59375,
      "learning_rate": 9.266666666666666e-05,
      "loss": 31.055,
      "step": 556
    },
    {
      "epoch": 0.56,
      "grad_norm": 11963.7744140625,
      "learning_rate": 9.283333333333334e-05,
      "loss": 14.296,
      "step": 557
    },
    {
      "epoch": 0.56,
      "grad_norm": 1694.303466796875,
      "learning_rate": 9.300000000000001e-05,
      "loss": 18.312,
      "step": 558
    },
    {
      "epoch": 0.56,
      "grad_norm": 21979.994140625,
      "learning_rate": 9.316666666666666e-05,
      "loss": 39.0338,
      "step": 559
    },
    {
      "epoch": 0.56,
      "grad_norm": 9920.962890625,
      "learning_rate": 9.333333333333334e-05,
      "loss": 23.5772,
      "step": 560
    },
    {
      "epoch": 0.56,
      "grad_norm": 31826.833984375,
      "learning_rate": 9.350000000000001e-05,
      "loss": 18.0631,
      "step": 561
    },
    {
      "epoch": 0.56,
      "grad_norm": 3772.789306640625,
      "learning_rate": 9.366666666666668e-05,
      "loss": 25.7211,
      "step": 562
    },
    {
      "epoch": 0.56,
      "grad_norm": 21734.224609375,
      "learning_rate": 9.383333333333334e-05,
      "loss": 19.9896,
      "step": 563
    },
    {
      "epoch": 0.56,
      "grad_norm": 4736.00146484375,
      "learning_rate": 9.4e-05,
      "loss": 39.0147,
      "step": 564
    },
    {
      "epoch": 0.57,
      "grad_norm": 19782.818359375,
      "learning_rate": 9.416666666666667e-05,
      "loss": 17.9292,
      "step": 565
    },
    {
      "epoch": 0.57,
      "grad_norm": 2960.087158203125,
      "learning_rate": 9.433333333333334e-05,
      "loss": 18.5308,
      "step": 566
    },
    {
      "epoch": 0.57,
      "grad_norm": 54449.88671875,
      "learning_rate": 9.449999999999999e-05,
      "loss": 16.6904,
      "step": 567
    },
    {
      "epoch": 0.57,
      "grad_norm": 9331.7734375,
      "learning_rate": 9.466666666666667e-05,
      "loss": 18.9681,
      "step": 568
    },
    {
      "epoch": 0.57,
      "grad_norm": 15292.4169921875,
      "learning_rate": 9.483333333333334e-05,
      "loss": 19.6281,
      "step": 569
    },
    {
      "epoch": 0.57,
      "grad_norm": 10229.005859375,
      "learning_rate": 9.5e-05,
      "loss": 35.4184,
      "step": 570
    },
    {
      "epoch": 0.57,
      "grad_norm": 8029.40625,
      "learning_rate": 9.516666666666667e-05,
      "loss": 25.3306,
      "step": 571
    },
    {
      "epoch": 0.57,
      "grad_norm": 2218.109130859375,
      "learning_rate": 9.533333333333334e-05,
      "loss": 22.9304,
      "step": 572
    },
    {
      "epoch": 0.57,
      "grad_norm": 17987.349609375,
      "learning_rate": 9.55e-05,
      "loss": 22.0291,
      "step": 573
    },
    {
      "epoch": 0.57,
      "grad_norm": 9745.1201171875,
      "learning_rate": 9.566666666666667e-05,
      "loss": 34.0092,
      "step": 574
    },
    {
      "epoch": 0.58,
      "grad_norm": 27449.912109375,
      "learning_rate": 9.583333333333334e-05,
      "loss": 32.4707,
      "step": 575
    },
    {
      "epoch": 0.58,
      "grad_norm": 12072.802734375,
      "learning_rate": 9.6e-05,
      "loss": 27.9623,
      "step": 576
    },
    {
      "epoch": 0.58,
      "grad_norm": 28745.8203125,
      "learning_rate": 9.616666666666667e-05,
      "loss": 34.1577,
      "step": 577
    },
    {
      "epoch": 0.58,
      "grad_norm": 17934.447265625,
      "learning_rate": 9.633333333333335e-05,
      "loss": 19.9995,
      "step": 578
    },
    {
      "epoch": 0.58,
      "grad_norm": 8866.9326171875,
      "learning_rate": 9.65e-05,
      "loss": 18.2061,
      "step": 579
    },
    {
      "epoch": 0.58,
      "grad_norm": 22706.75,
      "learning_rate": 9.666666666666667e-05,
      "loss": 32.8279,
      "step": 580
    },
    {
      "epoch": 0.58,
      "grad_norm": 4568.974609375,
      "learning_rate": 9.683333333333335e-05,
      "loss": 14.5181,
      "step": 581
    },
    {
      "epoch": 0.58,
      "grad_norm": 2600.528564453125,
      "learning_rate": 9.7e-05,
      "loss": 17.6983,
      "step": 582
    },
    {
      "epoch": 0.58,
      "grad_norm": 7024.80224609375,
      "learning_rate": 9.716666666666667e-05,
      "loss": 21.3356,
      "step": 583
    },
    {
      "epoch": 0.58,
      "grad_norm": 6422.40380859375,
      "learning_rate": 9.733333333333335e-05,
      "loss": 30.8962,
      "step": 584
    },
    {
      "epoch": 0.59,
      "grad_norm": 27232.267578125,
      "learning_rate": 9.75e-05,
      "loss": 21.983,
      "step": 585
    },
    {
      "epoch": 0.59,
      "grad_norm": 41027.83984375,
      "learning_rate": 9.766666666666668e-05,
      "loss": 26.2146,
      "step": 586
    },
    {
      "epoch": 0.59,
      "grad_norm": 9129.6689453125,
      "learning_rate": 9.783333333333334e-05,
      "loss": 11.5973,
      "step": 587
    },
    {
      "epoch": 0.59,
      "grad_norm": 5015.3310546875,
      "learning_rate": 9.8e-05,
      "loss": 27.7977,
      "step": 588
    },
    {
      "epoch": 0.59,
      "grad_norm": 10325.2177734375,
      "learning_rate": 9.816666666666668e-05,
      "loss": 32.2814,
      "step": 589
    },
    {
      "epoch": 0.59,
      "grad_norm": 12662.0537109375,
      "learning_rate": 9.833333333333333e-05,
      "loss": 19.2869,
      "step": 590
    },
    {
      "epoch": 0.59,
      "grad_norm": 4447.44970703125,
      "learning_rate": 9.850000000000001e-05,
      "loss": 15.4551,
      "step": 591
    },
    {
      "epoch": 0.59,
      "grad_norm": 7665.435546875,
      "learning_rate": 9.866666666666668e-05,
      "loss": 18.7547,
      "step": 592
    },
    {
      "epoch": 0.59,
      "grad_norm": 39059.1015625,
      "learning_rate": 9.883333333333333e-05,
      "loss": 20.28,
      "step": 593
    },
    {
      "epoch": 0.59,
      "grad_norm": 6411.63671875,
      "learning_rate": 9.900000000000001e-05,
      "loss": 23.5091,
      "step": 594
    },
    {
      "epoch": 0.6,
      "grad_norm": 6290.44873046875,
      "learning_rate": 9.916666666666667e-05,
      "loss": 21.0298,
      "step": 595
    },
    {
      "epoch": 0.6,
      "grad_norm": 1678.0843505859375,
      "learning_rate": 9.933333333333334e-05,
      "loss": 13.3339,
      "step": 596
    },
    {
      "epoch": 0.6,
      "grad_norm": 15618.947265625,
      "learning_rate": 9.95e-05,
      "loss": 18.3282,
      "step": 597
    },
    {
      "epoch": 0.6,
      "grad_norm": 5785.8212890625,
      "learning_rate": 9.966666666666667e-05,
      "loss": 22.0407,
      "step": 598
    },
    {
      "epoch": 0.6,
      "grad_norm": 6946.8603515625,
      "learning_rate": 9.983333333333334e-05,
      "loss": 21.0888,
      "step": 599
    },
    {
      "epoch": 0.6,
      "grad_norm": 2144.928466796875,
      "learning_rate": 0.0001,
      "loss": 13.067,
      "step": 600
    },
    {
      "epoch": 0.6,
      "grad_norm": 8456.99609375,
      "learning_rate": 9.999484004127967e-05,
      "loss": 20.1486,
      "step": 601
    },
    {
      "epoch": 0.6,
      "grad_norm": 10073.892578125,
      "learning_rate": 9.998968008255934e-05,
      "loss": 19.6491,
      "step": 602
    },
    {
      "epoch": 0.6,
      "grad_norm": 11667.3056640625,
      "learning_rate": 9.998452012383901e-05,
      "loss": 19.3334,
      "step": 603
    },
    {
      "epoch": 0.6,
      "grad_norm": 6664.2373046875,
      "learning_rate": 9.997936016511869e-05,
      "loss": 25.5048,
      "step": 604
    },
    {
      "epoch": 0.61,
      "grad_norm": 7449.20849609375,
      "learning_rate": 9.997420020639835e-05,
      "loss": 17.456,
      "step": 605
    },
    {
      "epoch": 0.61,
      "grad_norm": 7053.8720703125,
      "learning_rate": 9.996904024767802e-05,
      "loss": 32.7637,
      "step": 606
    },
    {
      "epoch": 0.61,
      "grad_norm": 11373.40625,
      "learning_rate": 9.996388028895769e-05,
      "loss": 19.8811,
      "step": 607
    },
    {
      "epoch": 0.61,
      "grad_norm": 49525.76171875,
      "learning_rate": 9.995872033023736e-05,
      "loss": 21.5478,
      "step": 608
    },
    {
      "epoch": 0.61,
      "grad_norm": 6930.43408203125,
      "learning_rate": 9.995356037151703e-05,
      "loss": 17.889,
      "step": 609
    },
    {
      "epoch": 0.61,
      "grad_norm": 30022.521484375,
      "learning_rate": 9.994840041279671e-05,
      "loss": 28.8515,
      "step": 610
    },
    {
      "epoch": 0.61,
      "grad_norm": 9934.2236328125,
      "learning_rate": 9.994324045407637e-05,
      "loss": 25.6596,
      "step": 611
    },
    {
      "epoch": 0.61,
      "grad_norm": 6861.455078125,
      "learning_rate": 9.993808049535604e-05,
      "loss": 35.2295,
      "step": 612
    },
    {
      "epoch": 0.61,
      "grad_norm": 33495.515625,
      "learning_rate": 9.99329205366357e-05,
      "loss": 32.0689,
      "step": 613
    },
    {
      "epoch": 0.61,
      "grad_norm": 13814.095703125,
      "learning_rate": 9.992776057791538e-05,
      "loss": 31.0501,
      "step": 614
    },
    {
      "epoch": 0.62,
      "grad_norm": 2124.5966796875,
      "learning_rate": 9.992260061919505e-05,
      "loss": 14.014,
      "step": 615
    },
    {
      "epoch": 0.62,
      "grad_norm": 3137.922119140625,
      "learning_rate": 9.991744066047473e-05,
      "loss": 20.7575,
      "step": 616
    },
    {
      "epoch": 0.62,
      "grad_norm": 9812.4072265625,
      "learning_rate": 9.991228070175439e-05,
      "loss": 27.6463,
      "step": 617
    },
    {
      "epoch": 0.62,
      "grad_norm": 19270.158203125,
      "learning_rate": 9.990712074303405e-05,
      "loss": 16.2565,
      "step": 618
    },
    {
      "epoch": 0.62,
      "grad_norm": 14795.37109375,
      "learning_rate": 9.990196078431373e-05,
      "loss": 26.4094,
      "step": 619
    },
    {
      "epoch": 0.62,
      "grad_norm": 13383.052734375,
      "learning_rate": 9.98968008255934e-05,
      "loss": 17.7239,
      "step": 620
    },
    {
      "epoch": 0.62,
      "grad_norm": 7323.69091796875,
      "learning_rate": 9.989164086687307e-05,
      "loss": 19.296,
      "step": 621
    },
    {
      "epoch": 0.62,
      "grad_norm": 52365.57421875,
      "learning_rate": 9.988648090815274e-05,
      "loss": 29.034,
      "step": 622
    },
    {
      "epoch": 0.62,
      "grad_norm": 9954.6923828125,
      "learning_rate": 9.988132094943241e-05,
      "loss": 33.2936,
      "step": 623
    },
    {
      "epoch": 0.62,
      "grad_norm": 11972.59375,
      "learning_rate": 9.987616099071207e-05,
      "loss": 28.3872,
      "step": 624
    },
    {
      "epoch": 0.63,
      "grad_norm": 20972.74609375,
      "learning_rate": 9.987100103199175e-05,
      "loss": 29.0683,
      "step": 625
    },
    {
      "epoch": 0.63,
      "grad_norm": 10501.5234375,
      "learning_rate": 9.986584107327142e-05,
      "loss": 20.0598,
      "step": 626
    },
    {
      "epoch": 0.63,
      "grad_norm": 7618.70703125,
      "learning_rate": 9.98606811145511e-05,
      "loss": 42.2724,
      "step": 627
    },
    {
      "epoch": 0.63,
      "grad_norm": 3610.59228515625,
      "learning_rate": 9.985552115583076e-05,
      "loss": 29.7715,
      "step": 628
    },
    {
      "epoch": 0.63,
      "grad_norm": 8015.88232421875,
      "learning_rate": 9.985036119711043e-05,
      "loss": 32.5876,
      "step": 629
    },
    {
      "epoch": 0.63,
      "grad_norm": 37453.09375,
      "learning_rate": 9.984520123839009e-05,
      "loss": 19.2749,
      "step": 630
    },
    {
      "epoch": 0.63,
      "grad_norm": 33416.1484375,
      "learning_rate": 9.984004127966977e-05,
      "loss": 18.4064,
      "step": 631
    },
    {
      "epoch": 0.63,
      "grad_norm": 5745.177734375,
      "learning_rate": 9.983488132094944e-05,
      "loss": 18.8664,
      "step": 632
    },
    {
      "epoch": 0.63,
      "grad_norm": 21569.568359375,
      "learning_rate": 9.982972136222911e-05,
      "loss": 26.4128,
      "step": 633
    },
    {
      "epoch": 0.63,
      "grad_norm": 14597.34375,
      "learning_rate": 9.982456140350878e-05,
      "loss": 16.7139,
      "step": 634
    },
    {
      "epoch": 0.64,
      "grad_norm": 1749.19287109375,
      "learning_rate": 9.981940144478845e-05,
      "loss": 21.9957,
      "step": 635
    },
    {
      "epoch": 0.64,
      "grad_norm": 4459.52587890625,
      "learning_rate": 9.981424148606811e-05,
      "loss": 22.183,
      "step": 636
    },
    {
      "epoch": 0.64,
      "grad_norm": 13393.2216796875,
      "learning_rate": 9.980908152734779e-05,
      "loss": 26.5642,
      "step": 637
    },
    {
      "epoch": 0.64,
      "grad_norm": 4305.84521484375,
      "learning_rate": 9.980392156862746e-05,
      "loss": 14.8656,
      "step": 638
    },
    {
      "epoch": 0.64,
      "grad_norm": 81539.03125,
      "learning_rate": 9.979876160990712e-05,
      "loss": 34.4005,
      "step": 639
    },
    {
      "epoch": 0.64,
      "grad_norm": 24874.9140625,
      "learning_rate": 9.97936016511868e-05,
      "loss": 17.988,
      "step": 640
    },
    {
      "epoch": 0.64,
      "grad_norm": 3762.713623046875,
      "learning_rate": 9.978844169246646e-05,
      "loss": 25.6812,
      "step": 641
    },
    {
      "epoch": 0.64,
      "grad_norm": 25758.09375,
      "learning_rate": 9.978328173374613e-05,
      "loss": 35.1604,
      "step": 642
    },
    {
      "epoch": 0.64,
      "grad_norm": 50603.609375,
      "learning_rate": 9.97781217750258e-05,
      "loss": 36.2128,
      "step": 643
    },
    {
      "epoch": 0.64,
      "grad_norm": 45020.6484375,
      "learning_rate": 9.977296181630548e-05,
      "loss": 23.5197,
      "step": 644
    },
    {
      "epoch": 0.65,
      "grad_norm": 9734.7822265625,
      "learning_rate": 9.976780185758514e-05,
      "loss": 15.555,
      "step": 645
    },
    {
      "epoch": 0.65,
      "grad_norm": 2163.5810546875,
      "learning_rate": 9.976264189886482e-05,
      "loss": 23.8509,
      "step": 646
    },
    {
      "epoch": 0.65,
      "grad_norm": 6786.1064453125,
      "learning_rate": 9.975748194014448e-05,
      "loss": 18.5597,
      "step": 647
    },
    {
      "epoch": 0.65,
      "grad_norm": 9592.291015625,
      "learning_rate": 9.975232198142415e-05,
      "loss": 20.3931,
      "step": 648
    },
    {
      "epoch": 0.65,
      "grad_norm": 2647.746826171875,
      "learning_rate": 9.974716202270383e-05,
      "loss": 45.5013,
      "step": 649
    },
    {
      "epoch": 0.65,
      "grad_norm": 56135.98046875,
      "learning_rate": 9.97420020639835e-05,
      "loss": 19.33,
      "step": 650
    },
    {
      "epoch": 0.65,
      "grad_norm": 120777.3515625,
      "learning_rate": 9.973684210526316e-05,
      "loss": 20.9873,
      "step": 651
    },
    {
      "epoch": 0.65,
      "grad_norm": 21209.3984375,
      "learning_rate": 9.973168214654284e-05,
      "loss": 24.0065,
      "step": 652
    },
    {
      "epoch": 0.65,
      "grad_norm": 7268.55126953125,
      "learning_rate": 9.97265221878225e-05,
      "loss": 26.3654,
      "step": 653
    },
    {
      "epoch": 0.65,
      "grad_norm": 5950.3271484375,
      "learning_rate": 9.972136222910217e-05,
      "loss": 21.4811,
      "step": 654
    },
    {
      "epoch": 0.66,
      "grad_norm": 26526.99609375,
      "learning_rate": 9.971620227038185e-05,
      "loss": 29.8094,
      "step": 655
    },
    {
      "epoch": 0.66,
      "grad_norm": 16508.328125,
      "learning_rate": 9.97110423116615e-05,
      "loss": 30.1334,
      "step": 656
    },
    {
      "epoch": 0.66,
      "grad_norm": 9058.0322265625,
      "learning_rate": 9.970588235294118e-05,
      "loss": 19.5711,
      "step": 657
    },
    {
      "epoch": 0.66,
      "grad_norm": 25053.8125,
      "learning_rate": 9.970072239422084e-05,
      "loss": 42.186,
      "step": 658
    },
    {
      "epoch": 0.66,
      "grad_norm": 15270.9404296875,
      "learning_rate": 9.969556243550052e-05,
      "loss": 20.7898,
      "step": 659
    },
    {
      "epoch": 0.66,
      "grad_norm": 41769.0546875,
      "learning_rate": 9.969040247678019e-05,
      "loss": 25.6768,
      "step": 660
    },
    {
      "epoch": 0.66,
      "grad_norm": 7539.2802734375,
      "learning_rate": 9.968524251805987e-05,
      "loss": 23.4036,
      "step": 661
    },
    {
      "epoch": 0.66,
      "grad_norm": 4058.095458984375,
      "learning_rate": 9.968008255933953e-05,
      "loss": 24.9842,
      "step": 662
    },
    {
      "epoch": 0.66,
      "grad_norm": 5074.98095703125,
      "learning_rate": 9.96749226006192e-05,
      "loss": 22.9887,
      "step": 663
    },
    {
      "epoch": 0.66,
      "grad_norm": 13134.9375,
      "learning_rate": 9.966976264189886e-05,
      "loss": 20.2768,
      "step": 664
    },
    {
      "epoch": 0.67,
      "grad_norm": 34022.72265625,
      "learning_rate": 9.966460268317854e-05,
      "loss": 23.4488,
      "step": 665
    },
    {
      "epoch": 0.67,
      "grad_norm": 31545.283203125,
      "learning_rate": 9.965944272445821e-05,
      "loss": 28.9817,
      "step": 666
    },
    {
      "epoch": 0.67,
      "grad_norm": 5447.609375,
      "learning_rate": 9.965428276573789e-05,
      "loss": 26.8495,
      "step": 667
    },
    {
      "epoch": 0.67,
      "grad_norm": 10845.72265625,
      "learning_rate": 9.964912280701755e-05,
      "loss": 24.0681,
      "step": 668
    },
    {
      "epoch": 0.67,
      "grad_norm": 13667.958984375,
      "learning_rate": 9.964396284829722e-05,
      "loss": 18.3999,
      "step": 669
    },
    {
      "epoch": 0.67,
      "grad_norm": 6356.55810546875,
      "learning_rate": 9.963880288957688e-05,
      "loss": 40.8419,
      "step": 670
    },
    {
      "epoch": 0.67,
      "grad_norm": 7318.13818359375,
      "learning_rate": 9.963364293085656e-05,
      "loss": 21.9644,
      "step": 671
    },
    {
      "epoch": 0.67,
      "grad_norm": 1833.4725341796875,
      "learning_rate": 9.962848297213623e-05,
      "loss": 15.4409,
      "step": 672
    },
    {
      "epoch": 0.67,
      "grad_norm": 12386.7763671875,
      "learning_rate": 9.962332301341589e-05,
      "loss": 19.0196,
      "step": 673
    },
    {
      "epoch": 0.67,
      "grad_norm": 56827.17578125,
      "learning_rate": 9.961816305469557e-05,
      "loss": 19.6856,
      "step": 674
    },
    {
      "epoch": 0.68,
      "grad_norm": 4730.83740234375,
      "learning_rate": 9.961300309597523e-05,
      "loss": 19.2352,
      "step": 675
    },
    {
      "epoch": 0.68,
      "grad_norm": 21378.783203125,
      "learning_rate": 9.96078431372549e-05,
      "loss": 25.7968,
      "step": 676
    },
    {
      "epoch": 0.68,
      "grad_norm": 21005.54296875,
      "learning_rate": 9.960268317853458e-05,
      "loss": 25.3915,
      "step": 677
    },
    {
      "epoch": 0.68,
      "grad_norm": 33999.22265625,
      "learning_rate": 9.959752321981425e-05,
      "loss": 25.5706,
      "step": 678
    },
    {
      "epoch": 0.68,
      "grad_norm": 7453.72412109375,
      "learning_rate": 9.959236326109391e-05,
      "loss": 15.9364,
      "step": 679
    },
    {
      "epoch": 0.68,
      "grad_norm": 47555.38671875,
      "learning_rate": 9.958720330237359e-05,
      "loss": 21.0056,
      "step": 680
    },
    {
      "epoch": 0.68,
      "grad_norm": 9168.1064453125,
      "learning_rate": 9.958204334365325e-05,
      "loss": 20.0083,
      "step": 681
    },
    {
      "epoch": 0.68,
      "grad_norm": 3000.615966796875,
      "learning_rate": 9.957688338493292e-05,
      "loss": 14.5823,
      "step": 682
    },
    {
      "epoch": 0.68,
      "grad_norm": 7834.86376953125,
      "learning_rate": 9.95717234262126e-05,
      "loss": 31.6801,
      "step": 683
    },
    {
      "epoch": 0.68,
      "grad_norm": 4507.26708984375,
      "learning_rate": 9.956656346749227e-05,
      "loss": 21.8273,
      "step": 684
    },
    {
      "epoch": 0.69,
      "grad_norm": 12155.5908203125,
      "learning_rate": 9.956140350877193e-05,
      "loss": 18.1194,
      "step": 685
    },
    {
      "epoch": 0.69,
      "grad_norm": 2645.363037109375,
      "learning_rate": 9.955624355005161e-05,
      "loss": 45.7971,
      "step": 686
    },
    {
      "epoch": 0.69,
      "grad_norm": 15706.9912109375,
      "learning_rate": 9.955108359133127e-05,
      "loss": 19.6706,
      "step": 687
    },
    {
      "epoch": 0.69,
      "grad_norm": 12221.9150390625,
      "learning_rate": 9.954592363261094e-05,
      "loss": 22.1659,
      "step": 688
    },
    {
      "epoch": 0.69,
      "grad_norm": 2910.715576171875,
      "learning_rate": 9.954076367389062e-05,
      "loss": 31.1095,
      "step": 689
    },
    {
      "epoch": 0.69,
      "grad_norm": 10990.041015625,
      "learning_rate": 9.953560371517028e-05,
      "loss": 25.7698,
      "step": 690
    },
    {
      "epoch": 0.69,
      "grad_norm": 15911.6171875,
      "learning_rate": 9.953044375644995e-05,
      "loss": 27.6463,
      "step": 691
    },
    {
      "epoch": 0.69,
      "grad_norm": 12854.384765625,
      "learning_rate": 9.952528379772961e-05,
      "loss": 28.5801,
      "step": 692
    },
    {
      "epoch": 0.69,
      "grad_norm": 11509.04296875,
      "learning_rate": 9.952012383900929e-05,
      "loss": 20.218,
      "step": 693
    },
    {
      "epoch": 0.69,
      "grad_norm": 7483.509765625,
      "learning_rate": 9.951496388028896e-05,
      "loss": 17.1043,
      "step": 694
    },
    {
      "epoch": 0.7,
      "grad_norm": 24982.7890625,
      "learning_rate": 9.950980392156864e-05,
      "loss": 43.421,
      "step": 695
    },
    {
      "epoch": 0.7,
      "grad_norm": 14273.0517578125,
      "learning_rate": 9.95046439628483e-05,
      "loss": 21.0251,
      "step": 696
    },
    {
      "epoch": 0.7,
      "grad_norm": 8052.41357421875,
      "learning_rate": 9.949948400412797e-05,
      "loss": 46.8274,
      "step": 697
    },
    {
      "epoch": 0.7,
      "grad_norm": 3425.82763671875,
      "learning_rate": 9.949432404540763e-05,
      "loss": 13.1918,
      "step": 698
    },
    {
      "epoch": 0.7,
      "grad_norm": 8645.0126953125,
      "learning_rate": 9.948916408668731e-05,
      "loss": 18.8068,
      "step": 699
    },
    {
      "epoch": 0.7,
      "grad_norm": 16055.6142578125,
      "learning_rate": 9.948400412796698e-05,
      "loss": 30.2808,
      "step": 700
    },
    {
      "epoch": 0.7,
      "grad_norm": 31611.904296875,
      "learning_rate": 9.947884416924666e-05,
      "loss": 34.5709,
      "step": 701
    },
    {
      "epoch": 0.7,
      "grad_norm": 7883.83740234375,
      "learning_rate": 9.947368421052632e-05,
      "loss": 14.552,
      "step": 702
    },
    {
      "epoch": 0.7,
      "grad_norm": 28704.326171875,
      "learning_rate": 9.946852425180599e-05,
      "loss": 28.3391,
      "step": 703
    },
    {
      "epoch": 0.7,
      "grad_norm": 11329.9970703125,
      "learning_rate": 9.946336429308565e-05,
      "loss": 23.5452,
      "step": 704
    },
    {
      "epoch": 0.71,
      "grad_norm": 18496.33984375,
      "learning_rate": 9.945820433436534e-05,
      "loss": 24.082,
      "step": 705
    },
    {
      "epoch": 0.71,
      "grad_norm": 30634.091796875,
      "learning_rate": 9.9453044375645e-05,
      "loss": 20.3418,
      "step": 706
    },
    {
      "epoch": 0.71,
      "grad_norm": 3207.048095703125,
      "learning_rate": 9.944788441692468e-05,
      "loss": 19.7367,
      "step": 707
    },
    {
      "epoch": 0.71,
      "grad_norm": 4738.90625,
      "learning_rate": 9.944272445820434e-05,
      "loss": 20.5511,
      "step": 708
    },
    {
      "epoch": 0.71,
      "grad_norm": 9550.0732421875,
      "learning_rate": 9.9437564499484e-05,
      "loss": 14.0888,
      "step": 709
    },
    {
      "epoch": 0.71,
      "grad_norm": 4907.669921875,
      "learning_rate": 9.943240454076367e-05,
      "loss": 17.6845,
      "step": 710
    },
    {
      "epoch": 0.71,
      "grad_norm": 4021.82568359375,
      "learning_rate": 9.942724458204335e-05,
      "loss": 25.7331,
      "step": 711
    },
    {
      "epoch": 0.71,
      "grad_norm": 13110.34375,
      "learning_rate": 9.942208462332302e-05,
      "loss": 26.8957,
      "step": 712
    },
    {
      "epoch": 0.71,
      "grad_norm": 2626.927978515625,
      "learning_rate": 9.941692466460268e-05,
      "loss": 14.9746,
      "step": 713
    },
    {
      "epoch": 0.71,
      "grad_norm": 4741.70849609375,
      "learning_rate": 9.941176470588236e-05,
      "loss": 22.7192,
      "step": 714
    },
    {
      "epoch": 0.72,
      "grad_norm": 4489.28564453125,
      "learning_rate": 9.940660474716202e-05,
      "loss": 14.9021,
      "step": 715
    },
    {
      "epoch": 0.72,
      "grad_norm": 16250.12109375,
      "learning_rate": 9.94014447884417e-05,
      "loss": 25.3934,
      "step": 716
    },
    {
      "epoch": 0.72,
      "grad_norm": 15738.20703125,
      "learning_rate": 9.939628482972137e-05,
      "loss": 23.9666,
      "step": 717
    },
    {
      "epoch": 0.72,
      "grad_norm": 25524.76953125,
      "learning_rate": 9.939112487100104e-05,
      "loss": 23.2266,
      "step": 718
    },
    {
      "epoch": 0.72,
      "grad_norm": 12525.7861328125,
      "learning_rate": 9.93859649122807e-05,
      "loss": 20.9566,
      "step": 719
    },
    {
      "epoch": 0.72,
      "grad_norm": 14313.2353515625,
      "learning_rate": 9.938080495356038e-05,
      "loss": 18.7603,
      "step": 720
    },
    {
      "epoch": 0.72,
      "grad_norm": 140141.84375,
      "learning_rate": 9.937564499484004e-05,
      "loss": 21.9157,
      "step": 721
    },
    {
      "epoch": 0.72,
      "grad_norm": 29924.91796875,
      "learning_rate": 9.937048503611973e-05,
      "loss": 17.9588,
      "step": 722
    },
    {
      "epoch": 0.72,
      "grad_norm": 15498.103515625,
      "learning_rate": 9.936532507739939e-05,
      "loss": 23.3018,
      "step": 723
    },
    {
      "epoch": 0.72,
      "grad_norm": 7300.55224609375,
      "learning_rate": 9.936016511867906e-05,
      "loss": 27.6863,
      "step": 724
    },
    {
      "epoch": 0.73,
      "grad_norm": 8455.2265625,
      "learning_rate": 9.935500515995872e-05,
      "loss": 30.0666,
      "step": 725
    },
    {
      "epoch": 0.73,
      "grad_norm": 2071.38232421875,
      "learning_rate": 9.934984520123838e-05,
      "loss": 13.1246,
      "step": 726
    },
    {
      "epoch": 0.73,
      "grad_norm": 3307.19091796875,
      "learning_rate": 9.934468524251806e-05,
      "loss": 13.3857,
      "step": 727
    },
    {
      "epoch": 0.73,
      "grad_norm": 5649.08642578125,
      "learning_rate": 9.933952528379773e-05,
      "loss": 34.6807,
      "step": 728
    },
    {
      "epoch": 0.73,
      "grad_norm": 5436.3662109375,
      "learning_rate": 9.933436532507741e-05,
      "loss": 23.9219,
      "step": 729
    },
    {
      "epoch": 0.73,
      "grad_norm": 15436.7919921875,
      "learning_rate": 9.932920536635707e-05,
      "loss": 16.2667,
      "step": 730
    },
    {
      "epoch": 0.73,
      "grad_norm": 6986.64892578125,
      "learning_rate": 9.932404540763674e-05,
      "loss": 17.6667,
      "step": 731
    },
    {
      "epoch": 0.73,
      "grad_norm": 10328.701171875,
      "learning_rate": 9.93188854489164e-05,
      "loss": 18.0344,
      "step": 732
    },
    {
      "epoch": 0.73,
      "grad_norm": 5051.48291015625,
      "learning_rate": 9.931372549019609e-05,
      "loss": 27.4489,
      "step": 733
    },
    {
      "epoch": 0.73,
      "grad_norm": 14416.9072265625,
      "learning_rate": 9.930856553147575e-05,
      "loss": 17.5218,
      "step": 734
    },
    {
      "epoch": 0.74,
      "grad_norm": 19420.92578125,
      "learning_rate": 9.930340557275543e-05,
      "loss": 14.5293,
      "step": 735
    },
    {
      "epoch": 0.74,
      "grad_norm": 41615.7109375,
      "learning_rate": 9.929824561403509e-05,
      "loss": 26.308,
      "step": 736
    },
    {
      "epoch": 0.74,
      "grad_norm": 91262.9375,
      "learning_rate": 9.929308565531476e-05,
      "loss": 23.2414,
      "step": 737
    },
    {
      "epoch": 0.74,
      "grad_norm": 3290.092041015625,
      "learning_rate": 9.928792569659442e-05,
      "loss": 22.1925,
      "step": 738
    },
    {
      "epoch": 0.74,
      "grad_norm": 7043.8212890625,
      "learning_rate": 9.928276573787411e-05,
      "loss": 27.283,
      "step": 739
    },
    {
      "epoch": 0.74,
      "grad_norm": 45090.99609375,
      "learning_rate": 9.927760577915377e-05,
      "loss": 20.2695,
      "step": 740
    },
    {
      "epoch": 0.74,
      "grad_norm": 2818.166748046875,
      "learning_rate": 9.927244582043345e-05,
      "loss": 17.3497,
      "step": 741
    },
    {
      "epoch": 0.74,
      "grad_norm": 8227.802734375,
      "learning_rate": 9.926728586171311e-05,
      "loss": 26.5538,
      "step": 742
    },
    {
      "epoch": 0.74,
      "grad_norm": 72809.546875,
      "learning_rate": 9.926212590299278e-05,
      "loss": 19.816,
      "step": 743
    },
    {
      "epoch": 0.74,
      "grad_norm": 15125.5576171875,
      "learning_rate": 9.925696594427244e-05,
      "loss": 28.3021,
      "step": 744
    },
    {
      "epoch": 0.75,
      "grad_norm": 9270.8720703125,
      "learning_rate": 9.925180598555212e-05,
      "loss": 19.3565,
      "step": 745
    },
    {
      "epoch": 0.75,
      "grad_norm": 31758.84765625,
      "learning_rate": 9.92466460268318e-05,
      "loss": 17.3617,
      "step": 746
    },
    {
      "epoch": 0.75,
      "grad_norm": 957.0426025390625,
      "learning_rate": 9.924148606811145e-05,
      "loss": 21.4812,
      "step": 747
    },
    {
      "epoch": 0.75,
      "grad_norm": 4637.35546875,
      "learning_rate": 9.923632610939113e-05,
      "loss": 26.4988,
      "step": 748
    },
    {
      "epoch": 0.75,
      "grad_norm": 13096.3251953125,
      "learning_rate": 9.923116615067079e-05,
      "loss": 30.8758,
      "step": 749
    },
    {
      "epoch": 0.75,
      "grad_norm": 18608.6875,
      "learning_rate": 9.922600619195048e-05,
      "loss": 27.562,
      "step": 750
    },
    {
      "epoch": 0.75,
      "grad_norm": 18367.810546875,
      "learning_rate": 9.922084623323014e-05,
      "loss": 32.3515,
      "step": 751
    },
    {
      "epoch": 0.75,
      "grad_norm": 8351.0703125,
      "learning_rate": 9.921568627450981e-05,
      "loss": 16.0118,
      "step": 752
    },
    {
      "epoch": 0.75,
      "grad_norm": 12669.87109375,
      "learning_rate": 9.921052631578947e-05,
      "loss": 18.8992,
      "step": 753
    },
    {
      "epoch": 0.75,
      "grad_norm": 2607.89111328125,
      "learning_rate": 9.920536635706915e-05,
      "loss": 28.0914,
      "step": 754
    },
    {
      "epoch": 0.76,
      "grad_norm": 11344.931640625,
      "learning_rate": 9.920020639834881e-05,
      "loss": 15.95,
      "step": 755
    },
    {
      "epoch": 0.76,
      "grad_norm": 9957.4443359375,
      "learning_rate": 9.91950464396285e-05,
      "loss": 17.0936,
      "step": 756
    },
    {
      "epoch": 0.76,
      "grad_norm": 6983.294921875,
      "learning_rate": 9.918988648090816e-05,
      "loss": 17.8801,
      "step": 757
    },
    {
      "epoch": 0.76,
      "grad_norm": 13905.25,
      "learning_rate": 9.918472652218783e-05,
      "loss": 27.1733,
      "step": 758
    },
    {
      "epoch": 0.76,
      "grad_norm": 10416.1884765625,
      "learning_rate": 9.91795665634675e-05,
      "loss": 30.9591,
      "step": 759
    },
    {
      "epoch": 0.76,
      "grad_norm": 8486.1171875,
      "learning_rate": 9.917440660474717e-05,
      "loss": 27.8801,
      "step": 760
    },
    {
      "epoch": 0.76,
      "grad_norm": 4990.7333984375,
      "learning_rate": 9.916924664602684e-05,
      "loss": 15.8573,
      "step": 761
    },
    {
      "epoch": 0.76,
      "grad_norm": 98074.03125,
      "learning_rate": 9.91640866873065e-05,
      "loss": 25.7769,
      "step": 762
    },
    {
      "epoch": 0.76,
      "grad_norm": 11460.20703125,
      "learning_rate": 9.915892672858618e-05,
      "loss": 16.6815,
      "step": 763
    },
    {
      "epoch": 0.76,
      "grad_norm": 9169.068359375,
      "learning_rate": 9.915376676986584e-05,
      "loss": 33.5316,
      "step": 764
    },
    {
      "epoch": 0.77,
      "grad_norm": 24253.98046875,
      "learning_rate": 9.914860681114551e-05,
      "loss": 21.4574,
      "step": 765
    },
    {
      "epoch": 0.77,
      "grad_norm": 14860.962890625,
      "learning_rate": 9.914344685242518e-05,
      "loss": 18.5159,
      "step": 766
    },
    {
      "epoch": 0.77,
      "grad_norm": 33621.234375,
      "learning_rate": 9.913828689370486e-05,
      "loss": 38.0704,
      "step": 767
    },
    {
      "epoch": 0.77,
      "grad_norm": 15520.109375,
      "learning_rate": 9.913312693498452e-05,
      "loss": 29.3981,
      "step": 768
    },
    {
      "epoch": 0.77,
      "grad_norm": 25125.224609375,
      "learning_rate": 9.91279669762642e-05,
      "loss": 24.1249,
      "step": 769
    },
    {
      "epoch": 0.77,
      "grad_norm": 165241.265625,
      "learning_rate": 9.912280701754386e-05,
      "loss": 23.6926,
      "step": 770
    },
    {
      "epoch": 0.77,
      "grad_norm": 31574.9609375,
      "learning_rate": 9.911764705882353e-05,
      "loss": 25.8035,
      "step": 771
    },
    {
      "epoch": 0.77,
      "grad_norm": 12263.3857421875,
      "learning_rate": 9.91124871001032e-05,
      "loss": 21.2303,
      "step": 772
    },
    {
      "epoch": 0.77,
      "grad_norm": 9084.45703125,
      "learning_rate": 9.910732714138288e-05,
      "loss": 19.1368,
      "step": 773
    },
    {
      "epoch": 0.77,
      "grad_norm": 31885.125,
      "learning_rate": 9.910216718266254e-05,
      "loss": 28.6691,
      "step": 774
    },
    {
      "epoch": 0.78,
      "grad_norm": 7303.67578125,
      "learning_rate": 9.909700722394222e-05,
      "loss": 16.961,
      "step": 775
    },
    {
      "epoch": 0.78,
      "grad_norm": 4650.54541015625,
      "learning_rate": 9.909184726522188e-05,
      "loss": 18.2634,
      "step": 776
    },
    {
      "epoch": 0.78,
      "grad_norm": 6208.12109375,
      "learning_rate": 9.908668730650155e-05,
      "loss": 15.5836,
      "step": 777
    },
    {
      "epoch": 0.78,
      "grad_norm": 7262.78955078125,
      "learning_rate": 9.908152734778123e-05,
      "loss": 17.8835,
      "step": 778
    },
    {
      "epoch": 0.78,
      "grad_norm": 8022.08154296875,
      "learning_rate": 9.907636738906089e-05,
      "loss": 16.4555,
      "step": 779
    },
    {
      "epoch": 0.78,
      "grad_norm": 4410.10595703125,
      "learning_rate": 9.907120743034056e-05,
      "loss": 15.4596,
      "step": 780
    },
    {
      "epoch": 0.78,
      "grad_norm": 53859.6640625,
      "learning_rate": 9.906604747162023e-05,
      "loss": 28.6708,
      "step": 781
    },
    {
      "epoch": 0.78,
      "grad_norm": 13486.7724609375,
      "learning_rate": 9.90608875128999e-05,
      "loss": 19.02,
      "step": 782
    },
    {
      "epoch": 0.78,
      "grad_norm": 3367.590087890625,
      "learning_rate": 9.905572755417956e-05,
      "loss": 15.6195,
      "step": 783
    },
    {
      "epoch": 0.78,
      "grad_norm": 40162.3125,
      "learning_rate": 9.905056759545925e-05,
      "loss": 17.1567,
      "step": 784
    },
    {
      "epoch": 0.79,
      "grad_norm": 9891.173828125,
      "learning_rate": 9.904540763673891e-05,
      "loss": 16.2604,
      "step": 785
    },
    {
      "epoch": 0.79,
      "grad_norm": 11227.666015625,
      "learning_rate": 9.904024767801858e-05,
      "loss": 18.3344,
      "step": 786
    },
    {
      "epoch": 0.79,
      "grad_norm": 11265.603515625,
      "learning_rate": 9.903508771929825e-05,
      "loss": 17.7607,
      "step": 787
    },
    {
      "epoch": 0.79,
      "grad_norm": 31737.8359375,
      "learning_rate": 9.902992776057792e-05,
      "loss": 23.563,
      "step": 788
    },
    {
      "epoch": 0.79,
      "grad_norm": 5992.595703125,
      "learning_rate": 9.902476780185758e-05,
      "loss": 25.1581,
      "step": 789
    },
    {
      "epoch": 0.79,
      "grad_norm": 11803.423828125,
      "learning_rate": 9.901960784313727e-05,
      "loss": 24.7004,
      "step": 790
    },
    {
      "epoch": 0.79,
      "grad_norm": 18604.033203125,
      "learning_rate": 9.901444788441693e-05,
      "loss": 23.806,
      "step": 791
    },
    {
      "epoch": 0.79,
      "grad_norm": 6566.2529296875,
      "learning_rate": 9.90092879256966e-05,
      "loss": 22.2058,
      "step": 792
    },
    {
      "epoch": 0.79,
      "grad_norm": 4199.90283203125,
      "learning_rate": 9.900412796697627e-05,
      "loss": 17.1932,
      "step": 793
    },
    {
      "epoch": 0.79,
      "grad_norm": 5115.90869140625,
      "learning_rate": 9.899896800825594e-05,
      "loss": 18.5392,
      "step": 794
    },
    {
      "epoch": 0.8,
      "grad_norm": 10816.4248046875,
      "learning_rate": 9.899380804953561e-05,
      "loss": 26.795,
      "step": 795
    },
    {
      "epoch": 0.8,
      "grad_norm": 39511.67578125,
      "learning_rate": 9.898864809081529e-05,
      "loss": 30.0862,
      "step": 796
    },
    {
      "epoch": 0.8,
      "grad_norm": 5841.95458984375,
      "learning_rate": 9.898348813209495e-05,
      "loss": 22.5317,
      "step": 797
    },
    {
      "epoch": 0.8,
      "grad_norm": 1976.4259033203125,
      "learning_rate": 9.897832817337461e-05,
      "loss": 12.6807,
      "step": 798
    },
    {
      "epoch": 0.8,
      "grad_norm": 16071.8291015625,
      "learning_rate": 9.897316821465429e-05,
      "loss": 21.847,
      "step": 799
    },
    {
      "epoch": 0.8,
      "grad_norm": 16219.67578125,
      "learning_rate": 9.896800825593395e-05,
      "loss": 20.1407,
      "step": 800
    },
    {
      "epoch": 0.8,
      "grad_norm": 5203.35546875,
      "learning_rate": 9.896284829721363e-05,
      "loss": 13.7873,
      "step": 801
    },
    {
      "epoch": 0.8,
      "grad_norm": 47456.13671875,
      "learning_rate": 9.89576883384933e-05,
      "loss": 25.2231,
      "step": 802
    },
    {
      "epoch": 0.8,
      "grad_norm": 27556.244140625,
      "learning_rate": 9.895252837977297e-05,
      "loss": 28.5556,
      "step": 803
    },
    {
      "epoch": 0.8,
      "grad_norm": 11698.6005859375,
      "learning_rate": 9.894736842105263e-05,
      "loss": 22.2,
      "step": 804
    },
    {
      "epoch": 0.81,
      "grad_norm": 4374.48974609375,
      "learning_rate": 9.89422084623323e-05,
      "loss": 16.2528,
      "step": 805
    },
    {
      "epoch": 0.81,
      "grad_norm": 58736.50390625,
      "learning_rate": 9.893704850361198e-05,
      "loss": 31.8583,
      "step": 806
    },
    {
      "epoch": 0.81,
      "grad_norm": 29749.001953125,
      "learning_rate": 9.893188854489165e-05,
      "loss": 20.4279,
      "step": 807
    },
    {
      "epoch": 0.81,
      "grad_norm": 6783.369140625,
      "learning_rate": 9.892672858617132e-05,
      "loss": 19.6368,
      "step": 808
    },
    {
      "epoch": 0.81,
      "grad_norm": 2745.81689453125,
      "learning_rate": 9.892156862745099e-05,
      "loss": 15.0129,
      "step": 809
    },
    {
      "epoch": 0.81,
      "grad_norm": 14002.453125,
      "learning_rate": 9.891640866873065e-05,
      "loss": 29.1481,
      "step": 810
    },
    {
      "epoch": 0.81,
      "grad_norm": 5809.7099609375,
      "learning_rate": 9.891124871001033e-05,
      "loss": 19.1207,
      "step": 811
    },
    {
      "epoch": 0.81,
      "grad_norm": 15129.1181640625,
      "learning_rate": 9.890608875129e-05,
      "loss": 23.1693,
      "step": 812
    },
    {
      "epoch": 0.81,
      "grad_norm": 25965.583984375,
      "learning_rate": 9.890092879256967e-05,
      "loss": 22.8345,
      "step": 813
    },
    {
      "epoch": 0.81,
      "grad_norm": 17725.36328125,
      "learning_rate": 9.889576883384934e-05,
      "loss": 26.4679,
      "step": 814
    },
    {
      "epoch": 0.82,
      "grad_norm": 5800.06103515625,
      "learning_rate": 9.8890608875129e-05,
      "loss": 19.3479,
      "step": 815
    },
    {
      "epoch": 0.82,
      "grad_norm": 9612.7138671875,
      "learning_rate": 9.888544891640867e-05,
      "loss": 15.5412,
      "step": 816
    },
    {
      "epoch": 0.82,
      "grad_norm": 16142.2353515625,
      "learning_rate": 9.888028895768833e-05,
      "loss": 23.1551,
      "step": 817
    },
    {
      "epoch": 0.82,
      "grad_norm": 16142.6083984375,
      "learning_rate": 9.887512899896802e-05,
      "loss": 47.8914,
      "step": 818
    },
    {
      "epoch": 0.82,
      "grad_norm": 19711.876953125,
      "learning_rate": 9.886996904024768e-05,
      "loss": 40.4719,
      "step": 819
    },
    {
      "epoch": 0.82,
      "grad_norm": 5365.70556640625,
      "learning_rate": 9.886480908152736e-05,
      "loss": 17.2016,
      "step": 820
    },
    {
      "epoch": 0.82,
      "grad_norm": 13710.1943359375,
      "learning_rate": 9.885964912280702e-05,
      "loss": 22.4417,
      "step": 821
    },
    {
      "epoch": 0.82,
      "grad_norm": 5380.3818359375,
      "learning_rate": 9.885448916408669e-05,
      "loss": 26.1556,
      "step": 822
    },
    {
      "epoch": 0.82,
      "grad_norm": 12545.1904296875,
      "learning_rate": 9.884932920536637e-05,
      "loss": 23.6967,
      "step": 823
    },
    {
      "epoch": 0.82,
      "grad_norm": 5715.16943359375,
      "learning_rate": 9.884416924664604e-05,
      "loss": 15.6874,
      "step": 824
    },
    {
      "epoch": 0.83,
      "grad_norm": 5166.28759765625,
      "learning_rate": 9.88390092879257e-05,
      "loss": 17.3116,
      "step": 825
    },
    {
      "epoch": 0.83,
      "grad_norm": 2599.188720703125,
      "learning_rate": 9.883384932920538e-05,
      "loss": 16.8906,
      "step": 826
    },
    {
      "epoch": 0.83,
      "grad_norm": 190365.3125,
      "learning_rate": 9.882868937048504e-05,
      "loss": 32.2557,
      "step": 827
    },
    {
      "epoch": 0.83,
      "grad_norm": 6408.76416015625,
      "learning_rate": 9.882352941176471e-05,
      "loss": 23.0898,
      "step": 828
    },
    {
      "epoch": 0.83,
      "grad_norm": 2576.32275390625,
      "learning_rate": 9.881836945304439e-05,
      "loss": 15.7899,
      "step": 829
    },
    {
      "epoch": 0.83,
      "grad_norm": 7840.44287109375,
      "learning_rate": 9.881320949432406e-05,
      "loss": 22.169,
      "step": 830
    },
    {
      "epoch": 0.83,
      "grad_norm": 5864.83984375,
      "learning_rate": 9.880804953560372e-05,
      "loss": 24.8549,
      "step": 831
    },
    {
      "epoch": 0.83,
      "grad_norm": 32876.04296875,
      "learning_rate": 9.88028895768834e-05,
      "loss": 30.8633,
      "step": 832
    },
    {
      "epoch": 0.83,
      "grad_norm": 100053.734375,
      "learning_rate": 9.879772961816306e-05,
      "loss": 34.988,
      "step": 833
    },
    {
      "epoch": 0.83,
      "grad_norm": 107456.21875,
      "learning_rate": 9.879256965944273e-05,
      "loss": 18.8224,
      "step": 834
    },
    {
      "epoch": 0.84,
      "grad_norm": 111127.234375,
      "learning_rate": 9.87874097007224e-05,
      "loss": 24.6729,
      "step": 835
    },
    {
      "epoch": 0.84,
      "grad_norm": 9831.8544921875,
      "learning_rate": 9.878224974200207e-05,
      "loss": 27.4978,
      "step": 836
    },
    {
      "epoch": 0.84,
      "grad_norm": 4008.512451171875,
      "learning_rate": 9.877708978328174e-05,
      "loss": 16.3425,
      "step": 837
    },
    {
      "epoch": 0.84,
      "grad_norm": 4959.2705078125,
      "learning_rate": 9.87719298245614e-05,
      "loss": 19.4625,
      "step": 838
    },
    {
      "epoch": 0.84,
      "grad_norm": 18730.685546875,
      "learning_rate": 9.876676986584108e-05,
      "loss": 19.7033,
      "step": 839
    },
    {
      "epoch": 0.84,
      "grad_norm": 9288.9267578125,
      "learning_rate": 9.876160990712075e-05,
      "loss": 21.4703,
      "step": 840
    },
    {
      "epoch": 0.84,
      "grad_norm": 10183.876953125,
      "learning_rate": 9.875644994840043e-05,
      "loss": 17.1472,
      "step": 841
    },
    {
      "epoch": 0.84,
      "grad_norm": 39883.13671875,
      "learning_rate": 9.875128998968009e-05,
      "loss": 27.8971,
      "step": 842
    },
    {
      "epoch": 0.84,
      "grad_norm": 5672.8330078125,
      "learning_rate": 9.874613003095976e-05,
      "loss": 13.9697,
      "step": 843
    },
    {
      "epoch": 0.84,
      "grad_norm": 17058.423828125,
      "learning_rate": 9.874097007223942e-05,
      "loss": 28.3168,
      "step": 844
    },
    {
      "epoch": 0.85,
      "grad_norm": 13602.5908203125,
      "learning_rate": 9.87358101135191e-05,
      "loss": 14.0532,
      "step": 845
    },
    {
      "epoch": 0.85,
      "grad_norm": 18191.22265625,
      "learning_rate": 9.873065015479877e-05,
      "loss": 24.6468,
      "step": 846
    },
    {
      "epoch": 0.85,
      "grad_norm": 4678.4794921875,
      "learning_rate": 9.872549019607845e-05,
      "loss": 21.5084,
      "step": 847
    },
    {
      "epoch": 0.85,
      "grad_norm": 5019.47705078125,
      "learning_rate": 9.87203302373581e-05,
      "loss": 18.7157,
      "step": 848
    },
    {
      "epoch": 0.85,
      "grad_norm": 8067.5361328125,
      "learning_rate": 9.871517027863778e-05,
      "loss": 29.8396,
      "step": 849
    },
    {
      "epoch": 0.85,
      "grad_norm": 57384.296875,
      "learning_rate": 9.871001031991744e-05,
      "loss": 21.98,
      "step": 850
    },
    {
      "epoch": 0.85,
      "grad_norm": 2293.677490234375,
      "learning_rate": 9.870485036119712e-05,
      "loss": 12.208,
      "step": 851
    },
    {
      "epoch": 0.85,
      "grad_norm": 28661.8984375,
      "learning_rate": 9.869969040247679e-05,
      "loss": 42.7943,
      "step": 852
    },
    {
      "epoch": 0.85,
      "grad_norm": 3800.031494140625,
      "learning_rate": 9.869453044375645e-05,
      "loss": 24.4141,
      "step": 853
    },
    {
      "epoch": 0.85,
      "grad_norm": 7604.99658203125,
      "learning_rate": 9.868937048503613e-05,
      "loss": 23.5284,
      "step": 854
    },
    {
      "epoch": 0.86,
      "grad_norm": 36019.01171875,
      "learning_rate": 9.868421052631579e-05,
      "loss": 24.1778,
      "step": 855
    },
    {
      "epoch": 0.86,
      "grad_norm": 7810.11962890625,
      "learning_rate": 9.867905056759546e-05,
      "loss": 17.3412,
      "step": 856
    },
    {
      "epoch": 0.86,
      "grad_norm": 27275.8515625,
      "learning_rate": 9.867389060887514e-05,
      "loss": 22.865,
      "step": 857
    },
    {
      "epoch": 0.86,
      "grad_norm": 22067.0390625,
      "learning_rate": 9.866873065015481e-05,
      "loss": 32.0892,
      "step": 858
    },
    {
      "epoch": 0.86,
      "grad_norm": 15193.2275390625,
      "learning_rate": 9.866357069143447e-05,
      "loss": 23.5827,
      "step": 859
    },
    {
      "epoch": 0.86,
      "grad_norm": 15744.072265625,
      "learning_rate": 9.865841073271415e-05,
      "loss": 19.1888,
      "step": 860
    },
    {
      "epoch": 0.86,
      "grad_norm": 6551.935546875,
      "learning_rate": 9.865325077399381e-05,
      "loss": 24.1325,
      "step": 861
    },
    {
      "epoch": 0.86,
      "grad_norm": 4117.44091796875,
      "learning_rate": 9.864809081527348e-05,
      "loss": 18.1508,
      "step": 862
    },
    {
      "epoch": 0.86,
      "grad_norm": 28154.0703125,
      "learning_rate": 9.864293085655316e-05,
      "loss": 21.739,
      "step": 863
    },
    {
      "epoch": 0.86,
      "grad_norm": 13785.90234375,
      "learning_rate": 9.863777089783283e-05,
      "loss": 15.7706,
      "step": 864
    },
    {
      "epoch": 0.87,
      "grad_norm": 22398.34375,
      "learning_rate": 9.863261093911249e-05,
      "loss": 29.6416,
      "step": 865
    },
    {
      "epoch": 0.87,
      "grad_norm": 19873.87890625,
      "learning_rate": 9.862745098039217e-05,
      "loss": 22.5782,
      "step": 866
    },
    {
      "epoch": 0.87,
      "grad_norm": 5595.986328125,
      "learning_rate": 9.862229102167183e-05,
      "loss": 16.85,
      "step": 867
    },
    {
      "epoch": 0.87,
      "grad_norm": 6892.77490234375,
      "learning_rate": 9.86171310629515e-05,
      "loss": 14.7015,
      "step": 868
    },
    {
      "epoch": 0.87,
      "grad_norm": 67375.2578125,
      "learning_rate": 9.861197110423118e-05,
      "loss": 22.5146,
      "step": 869
    },
    {
      "epoch": 0.87,
      "grad_norm": 28205.228515625,
      "learning_rate": 9.860681114551084e-05,
      "loss": 18.7218,
      "step": 870
    },
    {
      "epoch": 0.87,
      "grad_norm": 7521.87841796875,
      "learning_rate": 9.860165118679051e-05,
      "loss": 14.4575,
      "step": 871
    },
    {
      "epoch": 0.87,
      "grad_norm": 21602.72265625,
      "learning_rate": 9.859649122807017e-05,
      "loss": 19.2898,
      "step": 872
    },
    {
      "epoch": 0.87,
      "grad_norm": 7059.96875,
      "learning_rate": 9.859133126934985e-05,
      "loss": 19.8029,
      "step": 873
    },
    {
      "epoch": 0.87,
      "grad_norm": 2629.210693359375,
      "learning_rate": 9.858617131062952e-05,
      "loss": 16.2455,
      "step": 874
    },
    {
      "epoch": 0.88,
      "grad_norm": 838.436767578125,
      "learning_rate": 9.85810113519092e-05,
      "loss": 22.8088,
      "step": 875
    },
    {
      "epoch": 0.88,
      "grad_norm": 41696.17578125,
      "learning_rate": 9.857585139318886e-05,
      "loss": 19.4402,
      "step": 876
    },
    {
      "epoch": 0.88,
      "grad_norm": 2990.40966796875,
      "learning_rate": 9.857069143446853e-05,
      "loss": 17.0725,
      "step": 877
    },
    {
      "epoch": 0.88,
      "grad_norm": 32336.29296875,
      "learning_rate": 9.856553147574819e-05,
      "loss": 14.5716,
      "step": 878
    },
    {
      "epoch": 0.88,
      "grad_norm": 6431.3828125,
      "learning_rate": 9.856037151702787e-05,
      "loss": 23.2132,
      "step": 879
    },
    {
      "epoch": 0.88,
      "grad_norm": 4678.02978515625,
      "learning_rate": 9.855521155830754e-05,
      "loss": 15.1025,
      "step": 880
    },
    {
      "epoch": 0.88,
      "grad_norm": 186236.875,
      "learning_rate": 9.855005159958722e-05,
      "loss": 44.8007,
      "step": 881
    },
    {
      "epoch": 0.88,
      "grad_norm": 6827.486328125,
      "learning_rate": 9.854489164086688e-05,
      "loss": 33.2704,
      "step": 882
    },
    {
      "epoch": 0.88,
      "grad_norm": 14969.265625,
      "learning_rate": 9.853973168214655e-05,
      "loss": 20.6599,
      "step": 883
    },
    {
      "epoch": 0.88,
      "grad_norm": 4433.53125,
      "learning_rate": 9.853457172342621e-05,
      "loss": 16.6329,
      "step": 884
    },
    {
      "epoch": 0.89,
      "grad_norm": 20614.73828125,
      "learning_rate": 9.852941176470589e-05,
      "loss": 18.0184,
      "step": 885
    },
    {
      "epoch": 0.89,
      "grad_norm": 9462.072265625,
      "learning_rate": 9.852425180598556e-05,
      "loss": 12.1658,
      "step": 886
    },
    {
      "epoch": 0.89,
      "grad_norm": 1408.4208984375,
      "learning_rate": 9.851909184726522e-05,
      "loss": 15.2421,
      "step": 887
    },
    {
      "epoch": 0.89,
      "grad_norm": 47302.8203125,
      "learning_rate": 9.85139318885449e-05,
      "loss": 33.4233,
      "step": 888
    },
    {
      "epoch": 0.89,
      "grad_norm": 2893.6611328125,
      "learning_rate": 9.850877192982456e-05,
      "loss": 14.8602,
      "step": 889
    },
    {
      "epoch": 0.89,
      "grad_norm": 2182.6064453125,
      "learning_rate": 9.850361197110423e-05,
      "loss": 15.9521,
      "step": 890
    },
    {
      "epoch": 0.89,
      "grad_norm": 7308.58935546875,
      "learning_rate": 9.849845201238391e-05,
      "loss": 18.6724,
      "step": 891
    },
    {
      "epoch": 0.89,
      "grad_norm": 3282.540283203125,
      "learning_rate": 9.849329205366358e-05,
      "loss": 16.8159,
      "step": 892
    },
    {
      "epoch": 0.89,
      "grad_norm": 2224.379638671875,
      "learning_rate": 9.848813209494324e-05,
      "loss": 14.8069,
      "step": 893
    },
    {
      "epoch": 0.89,
      "grad_norm": 5461.40625,
      "learning_rate": 9.848297213622292e-05,
      "loss": 18.934,
      "step": 894
    },
    {
      "epoch": 0.9,
      "grad_norm": 17947.166015625,
      "learning_rate": 9.847781217750258e-05,
      "loss": 26.9064,
      "step": 895
    },
    {
      "epoch": 0.9,
      "grad_norm": 51670.8125,
      "learning_rate": 9.847265221878225e-05,
      "loss": 28.7578,
      "step": 896
    },
    {
      "epoch": 0.9,
      "grad_norm": 7124.90234375,
      "learning_rate": 9.846749226006193e-05,
      "loss": 19.6815,
      "step": 897
    },
    {
      "epoch": 0.9,
      "grad_norm": 3836.673095703125,
      "learning_rate": 9.84623323013416e-05,
      "loss": 24.7194,
      "step": 898
    },
    {
      "epoch": 0.9,
      "grad_norm": 19612.119140625,
      "learning_rate": 9.845717234262126e-05,
      "loss": 16.0764,
      "step": 899
    },
    {
      "epoch": 0.9,
      "grad_norm": 5015.46826171875,
      "learning_rate": 9.845201238390094e-05,
      "loss": 23.4503,
      "step": 900
    },
    {
      "epoch": 0.9,
      "grad_norm": 5542.76416015625,
      "learning_rate": 9.84468524251806e-05,
      "loss": 15.08,
      "step": 901
    },
    {
      "epoch": 0.9,
      "grad_norm": 5572.2021484375,
      "learning_rate": 9.844169246646027e-05,
      "loss": 15.6022,
      "step": 902
    },
    {
      "epoch": 0.9,
      "grad_norm": 14973.845703125,
      "learning_rate": 9.843653250773995e-05,
      "loss": 16.4087,
      "step": 903
    },
    {
      "epoch": 0.9,
      "grad_norm": 5143.42822265625,
      "learning_rate": 9.843137254901961e-05,
      "loss": 15.3341,
      "step": 904
    },
    {
      "epoch": 0.91,
      "grad_norm": 9280.5888671875,
      "learning_rate": 9.842621259029928e-05,
      "loss": 17.2465,
      "step": 905
    },
    {
      "epoch": 0.91,
      "grad_norm": 4294.392578125,
      "learning_rate": 9.842105263157894e-05,
      "loss": 31.915,
      "step": 906
    },
    {
      "epoch": 0.91,
      "grad_norm": 9951.0869140625,
      "learning_rate": 9.841589267285862e-05,
      "loss": 34.3524,
      "step": 907
    },
    {
      "epoch": 0.91,
      "grad_norm": 22095.126953125,
      "learning_rate": 9.841073271413829e-05,
      "loss": 27.5412,
      "step": 908
    },
    {
      "epoch": 0.91,
      "grad_norm": 17502.931640625,
      "learning_rate": 9.840557275541797e-05,
      "loss": 16.6679,
      "step": 909
    },
    {
      "epoch": 0.91,
      "grad_norm": 2821.904052734375,
      "learning_rate": 9.840041279669763e-05,
      "loss": 22.0943,
      "step": 910
    },
    {
      "epoch": 0.91,
      "grad_norm": 9054.97265625,
      "learning_rate": 9.83952528379773e-05,
      "loss": 19.2171,
      "step": 911
    },
    {
      "epoch": 0.91,
      "grad_norm": 4106.19580078125,
      "learning_rate": 9.839009287925696e-05,
      "loss": 19.524,
      "step": 912
    },
    {
      "epoch": 0.91,
      "grad_norm": 7245.04345703125,
      "learning_rate": 9.838493292053664e-05,
      "loss": 22.052,
      "step": 913
    },
    {
      "epoch": 0.91,
      "grad_norm": 5703.81298828125,
      "learning_rate": 9.837977296181631e-05,
      "loss": 27.6868,
      "step": 914
    },
    {
      "epoch": 0.92,
      "grad_norm": 24504.13671875,
      "learning_rate": 9.837461300309599e-05,
      "loss": 21.0627,
      "step": 915
    },
    {
      "epoch": 0.92,
      "grad_norm": 2860.588623046875,
      "learning_rate": 9.836945304437565e-05,
      "loss": 14.3439,
      "step": 916
    },
    {
      "epoch": 0.92,
      "grad_norm": 6383.470703125,
      "learning_rate": 9.836429308565532e-05,
      "loss": 16.1604,
      "step": 917
    },
    {
      "epoch": 0.92,
      "grad_norm": 28183.32421875,
      "learning_rate": 9.835913312693498e-05,
      "loss": 25.6283,
      "step": 918
    },
    {
      "epoch": 0.92,
      "grad_norm": 11970.49609375,
      "learning_rate": 9.835397316821466e-05,
      "loss": 29.4677,
      "step": 919
    },
    {
      "epoch": 0.92,
      "grad_norm": 5462.5771484375,
      "learning_rate": 9.834881320949433e-05,
      "loss": 22.2441,
      "step": 920
    },
    {
      "epoch": 0.92,
      "grad_norm": 9738.923828125,
      "learning_rate": 9.834365325077401e-05,
      "loss": 18.8419,
      "step": 921
    },
    {
      "epoch": 0.92,
      "grad_norm": 5133.0361328125,
      "learning_rate": 9.833849329205367e-05,
      "loss": 19.0392,
      "step": 922
    },
    {
      "epoch": 0.92,
      "grad_norm": 3903.98095703125,
      "learning_rate": 9.833333333333333e-05,
      "loss": 25.524,
      "step": 923
    },
    {
      "epoch": 0.92,
      "grad_norm": 2796.7919921875,
      "learning_rate": 9.8328173374613e-05,
      "loss": 17.87,
      "step": 924
    },
    {
      "epoch": 0.93,
      "grad_norm": 16295.4736328125,
      "learning_rate": 9.832301341589268e-05,
      "loss": 37.2373,
      "step": 925
    },
    {
      "epoch": 0.93,
      "grad_norm": 7673.767578125,
      "learning_rate": 9.831785345717235e-05,
      "loss": 18.1904,
      "step": 926
    },
    {
      "epoch": 0.93,
      "grad_norm": 8053.02783203125,
      "learning_rate": 9.831269349845201e-05,
      "loss": 27.2866,
      "step": 927
    },
    {
      "epoch": 0.93,
      "grad_norm": 16533.736328125,
      "learning_rate": 9.830753353973169e-05,
      "loss": 25.8362,
      "step": 928
    },
    {
      "epoch": 0.93,
      "grad_norm": 19709.109375,
      "learning_rate": 9.830237358101135e-05,
      "loss": 27.7727,
      "step": 929
    },
    {
      "epoch": 0.93,
      "grad_norm": 30914.900390625,
      "learning_rate": 9.829721362229102e-05,
      "loss": 27.9199,
      "step": 930
    },
    {
      "epoch": 0.93,
      "grad_norm": 3468.594970703125,
      "learning_rate": 9.82920536635707e-05,
      "loss": 22.2919,
      "step": 931
    },
    {
      "epoch": 0.93,
      "grad_norm": 23729.00390625,
      "learning_rate": 9.828689370485037e-05,
      "loss": 24.8568,
      "step": 932
    },
    {
      "epoch": 0.93,
      "grad_norm": 3407.22705078125,
      "learning_rate": 9.828173374613003e-05,
      "loss": 14.2435,
      "step": 933
    },
    {
      "epoch": 0.93,
      "grad_norm": 18685.4140625,
      "learning_rate": 9.827657378740971e-05,
      "loss": 18.1673,
      "step": 934
    },
    {
      "epoch": 0.94,
      "grad_norm": 3456.52392578125,
      "learning_rate": 9.827141382868937e-05,
      "loss": 15.0779,
      "step": 935
    },
    {
      "epoch": 0.94,
      "grad_norm": 18270.078125,
      "learning_rate": 9.826625386996904e-05,
      "loss": 21.5995,
      "step": 936
    },
    {
      "epoch": 0.94,
      "grad_norm": 15040.5849609375,
      "learning_rate": 9.826109391124872e-05,
      "loss": 14.6493,
      "step": 937
    },
    {
      "epoch": 0.94,
      "grad_norm": 87240.3203125,
      "learning_rate": 9.825593395252839e-05,
      "loss": 42.2923,
      "step": 938
    },
    {
      "epoch": 0.94,
      "grad_norm": 16148.9814453125,
      "learning_rate": 9.825077399380805e-05,
      "loss": 27.8736,
      "step": 939
    },
    {
      "epoch": 0.94,
      "grad_norm": 5151.5771484375,
      "learning_rate": 9.824561403508771e-05,
      "loss": 17.2312,
      "step": 940
    },
    {
      "epoch": 0.94,
      "grad_norm": 16270.689453125,
      "learning_rate": 9.824045407636739e-05,
      "loss": 28.4645,
      "step": 941
    },
    {
      "epoch": 0.94,
      "grad_norm": 2264.119384765625,
      "learning_rate": 9.823529411764706e-05,
      "loss": 21.8021,
      "step": 942
    },
    {
      "epoch": 0.94,
      "grad_norm": 2720.666015625,
      "learning_rate": 9.823013415892674e-05,
      "loss": 29.8281,
      "step": 943
    },
    {
      "epoch": 0.94,
      "grad_norm": 14481.1982421875,
      "learning_rate": 9.82249742002064e-05,
      "loss": 29.5627,
      "step": 944
    },
    {
      "epoch": 0.95,
      "grad_norm": 3272.640625,
      "learning_rate": 9.821981424148607e-05,
      "loss": 28.4363,
      "step": 945
    },
    {
      "epoch": 0.95,
      "grad_norm": 7116.52880859375,
      "learning_rate": 9.821465428276573e-05,
      "loss": 17.8568,
      "step": 946
    },
    {
      "epoch": 0.95,
      "grad_norm": 13381.42578125,
      "learning_rate": 9.820949432404541e-05,
      "loss": 23.894,
      "step": 947
    },
    {
      "epoch": 0.95,
      "grad_norm": 3306.352783203125,
      "learning_rate": 9.820433436532508e-05,
      "loss": 14.949,
      "step": 948
    },
    {
      "epoch": 0.95,
      "grad_norm": 10046.3251953125,
      "learning_rate": 9.819917440660476e-05,
      "loss": 29.4042,
      "step": 949
    },
    {
      "epoch": 0.95,
      "grad_norm": 19438.05859375,
      "learning_rate": 9.819401444788442e-05,
      "loss": 26.6986,
      "step": 950
    },
    {
      "epoch": 0.95,
      "grad_norm": 9787.6669921875,
      "learning_rate": 9.81888544891641e-05,
      "loss": 15.4786,
      "step": 951
    },
    {
      "epoch": 0.95,
      "grad_norm": 6581.19970703125,
      "learning_rate": 9.818369453044375e-05,
      "loss": 30.8861,
      "step": 952
    },
    {
      "epoch": 0.95,
      "grad_norm": 18545.96875,
      "learning_rate": 9.817853457172343e-05,
      "loss": 20.8525,
      "step": 953
    },
    {
      "epoch": 0.95,
      "grad_norm": 3467.965576171875,
      "learning_rate": 9.81733746130031e-05,
      "loss": 15.45,
      "step": 954
    },
    {
      "epoch": 0.96,
      "grad_norm": 3520.22509765625,
      "learning_rate": 9.816821465428278e-05,
      "loss": 18.8317,
      "step": 955
    },
    {
      "epoch": 0.96,
      "grad_norm": 4889.57177734375,
      "learning_rate": 9.816305469556244e-05,
      "loss": 13.4699,
      "step": 956
    },
    {
      "epoch": 0.96,
      "grad_norm": 809.0972290039062,
      "learning_rate": 9.815789473684211e-05,
      "loss": 16.2199,
      "step": 957
    },
    {
      "epoch": 0.96,
      "grad_norm": 4931.138671875,
      "learning_rate": 9.815273477812177e-05,
      "loss": 20.6298,
      "step": 958
    },
    {
      "epoch": 0.96,
      "grad_norm": 3751.509765625,
      "learning_rate": 9.814757481940145e-05,
      "loss": 14.6304,
      "step": 959
    },
    {
      "epoch": 0.96,
      "grad_norm": 14576.076171875,
      "learning_rate": 9.814241486068112e-05,
      "loss": 28.4414,
      "step": 960
    },
    {
      "epoch": 0.96,
      "grad_norm": 3017.410400390625,
      "learning_rate": 9.813725490196078e-05,
      "loss": 19.371,
      "step": 961
    },
    {
      "epoch": 0.96,
      "grad_norm": 26348.642578125,
      "learning_rate": 9.813209494324046e-05,
      "loss": 19.2273,
      "step": 962
    },
    {
      "epoch": 0.96,
      "grad_norm": 7038.2646484375,
      "learning_rate": 9.812693498452012e-05,
      "loss": 32.7256,
      "step": 963
    },
    {
      "epoch": 0.96,
      "grad_norm": 2853.08349609375,
      "learning_rate": 9.81217750257998e-05,
      "loss": 13.8182,
      "step": 964
    },
    {
      "epoch": 0.97,
      "grad_norm": 79345.609375,
      "learning_rate": 9.811661506707947e-05,
      "loss": 29.2501,
      "step": 965
    },
    {
      "epoch": 0.97,
      "grad_norm": 19705.6875,
      "learning_rate": 9.811145510835914e-05,
      "loss": 16.2986,
      "step": 966
    },
    {
      "epoch": 0.97,
      "grad_norm": 9738.580078125,
      "learning_rate": 9.81062951496388e-05,
      "loss": 24.5079,
      "step": 967
    },
    {
      "epoch": 0.97,
      "grad_norm": 9087.4912109375,
      "learning_rate": 9.810113519091848e-05,
      "loss": 18.188,
      "step": 968
    },
    {
      "epoch": 0.97,
      "grad_norm": 6365.45654296875,
      "learning_rate": 9.809597523219814e-05,
      "loss": 13.6198,
      "step": 969
    },
    {
      "epoch": 0.97,
      "grad_norm": 4288.62353515625,
      "learning_rate": 9.809081527347781e-05,
      "loss": 17.6998,
      "step": 970
    },
    {
      "epoch": 0.97,
      "grad_norm": 3117.494873046875,
      "learning_rate": 9.808565531475749e-05,
      "loss": 20.7269,
      "step": 971
    },
    {
      "epoch": 0.97,
      "grad_norm": 10734.2568359375,
      "learning_rate": 9.808049535603716e-05,
      "loss": 18.9059,
      "step": 972
    },
    {
      "epoch": 0.97,
      "grad_norm": 40214.00390625,
      "learning_rate": 9.807533539731682e-05,
      "loss": 32.9942,
      "step": 973
    },
    {
      "epoch": 0.97,
      "grad_norm": 28971.8828125,
      "learning_rate": 9.80701754385965e-05,
      "loss": 17.2227,
      "step": 974
    },
    {
      "epoch": 0.98,
      "grad_norm": 13594.9951171875,
      "learning_rate": 9.806501547987616e-05,
      "loss": 18.0886,
      "step": 975
    },
    {
      "epoch": 0.98,
      "grad_norm": 27182.353515625,
      "learning_rate": 9.805985552115583e-05,
      "loss": 34.7425,
      "step": 976
    },
    {
      "epoch": 0.98,
      "grad_norm": 14432.5712890625,
      "learning_rate": 9.805469556243551e-05,
      "loss": 17.9191,
      "step": 977
    },
    {
      "epoch": 0.98,
      "grad_norm": 9519.7607421875,
      "learning_rate": 9.804953560371517e-05,
      "loss": 29.4577,
      "step": 978
    },
    {
      "epoch": 0.98,
      "grad_norm": 12211.341796875,
      "learning_rate": 9.804437564499484e-05,
      "loss": 19.5051,
      "step": 979
    },
    {
      "epoch": 0.98,
      "grad_norm": 94452.78125,
      "learning_rate": 9.80392156862745e-05,
      "loss": 19.2985,
      "step": 980
    },
    {
      "epoch": 0.98,
      "grad_norm": 6724.77099609375,
      "learning_rate": 9.803405572755418e-05,
      "loss": 18.767,
      "step": 981
    },
    {
      "epoch": 0.98,
      "grad_norm": 21737.53125,
      "learning_rate": 9.802889576883385e-05,
      "loss": 17.277,
      "step": 982
    },
    {
      "epoch": 0.98,
      "grad_norm": 20355.455078125,
      "learning_rate": 9.802373581011353e-05,
      "loss": 26.6405,
      "step": 983
    },
    {
      "epoch": 0.98,
      "grad_norm": 3349.406494140625,
      "learning_rate": 9.801857585139319e-05,
      "loss": 18.0858,
      "step": 984
    },
    {
      "epoch": 0.99,
      "grad_norm": 14489.6787109375,
      "learning_rate": 9.801341589267286e-05,
      "loss": 36.226,
      "step": 985
    },
    {
      "epoch": 0.99,
      "grad_norm": 5825.650390625,
      "learning_rate": 9.800825593395253e-05,
      "loss": 17.2503,
      "step": 986
    },
    {
      "epoch": 0.99,
      "grad_norm": 17109.36328125,
      "learning_rate": 9.80030959752322e-05,
      "loss": 16.539,
      "step": 987
    },
    {
      "epoch": 0.99,
      "grad_norm": 55684.765625,
      "learning_rate": 9.799793601651187e-05,
      "loss": 33.5079,
      "step": 988
    },
    {
      "epoch": 0.99,
      "grad_norm": 6001.40234375,
      "learning_rate": 9.799277605779155e-05,
      "loss": 16.9292,
      "step": 989
    },
    {
      "epoch": 0.99,
      "grad_norm": 31917.353515625,
      "learning_rate": 9.798761609907121e-05,
      "loss": 14.7166,
      "step": 990
    },
    {
      "epoch": 0.99,
      "grad_norm": 5664.91064453125,
      "learning_rate": 9.798245614035088e-05,
      "loss": 21.8446,
      "step": 991
    },
    {
      "epoch": 0.99,
      "grad_norm": 9040.751953125,
      "learning_rate": 9.797729618163055e-05,
      "loss": 22.7917,
      "step": 992
    },
    {
      "epoch": 0.99,
      "grad_norm": 21024.140625,
      "learning_rate": 9.797213622291022e-05,
      "loss": 24.7245,
      "step": 993
    },
    {
      "epoch": 0.99,
      "grad_norm": 4496.7431640625,
      "learning_rate": 9.79669762641899e-05,
      "loss": 15.7037,
      "step": 994
    },
    {
      "epoch": 1.0,
      "grad_norm": 5436.76220703125,
      "learning_rate": 9.796181630546956e-05,
      "loss": 15.1426,
      "step": 995
    },
    {
      "epoch": 1.0,
      "grad_norm": 3497.8408203125,
      "learning_rate": 9.795665634674923e-05,
      "loss": 17.6775,
      "step": 996
    },
    {
      "epoch": 1.0,
      "grad_norm": 4147.88916015625,
      "learning_rate": 9.795149638802889e-05,
      "loss": 14.1333,
      "step": 997
    },
    {
      "epoch": 1.0,
      "grad_norm": 7443.3349609375,
      "learning_rate": 9.794633642930857e-05,
      "loss": 32.3439,
      "step": 998
    },
    {
      "epoch": 1.0,
      "grad_norm": 78711.765625,
      "learning_rate": 9.794117647058824e-05,
      "loss": 15.1018,
      "step": 999
    },
    {
      "epoch": 1.0,
      "grad_norm": 1463.2860107421875,
      "learning_rate": 9.793601651186791e-05,
      "loss": 24.6359,
      "step": 1000
    },
    {
      "epoch": 1.0,
      "grad_norm": 4466.87060546875,
      "learning_rate": 9.793085655314758e-05,
      "loss": 12.7677,
      "step": 1001
    },
    {
      "epoch": 1.0,
      "grad_norm": 4714.23681640625,
      "learning_rate": 9.792569659442725e-05,
      "loss": 26.1529,
      "step": 1002
    },
    {
      "epoch": 1.0,
      "grad_norm": 19761.716796875,
      "learning_rate": 9.792053663570691e-05,
      "loss": 23.8812,
      "step": 1003
    },
    {
      "epoch": 1.01,
      "grad_norm": 1319978.75,
      "learning_rate": 9.791537667698659e-05,
      "loss": 24.2244,
      "step": 1004
    },
    {
      "epoch": 1.01,
      "grad_norm": 6738.82666015625,
      "learning_rate": 9.791021671826626e-05,
      "loss": 15.6476,
      "step": 1005
    },
    {
      "epoch": 1.01,
      "grad_norm": 11405.373046875,
      "learning_rate": 9.790505675954593e-05,
      "loss": 24.9121,
      "step": 1006
    },
    {
      "epoch": 1.01,
      "grad_norm": 25997.82421875,
      "learning_rate": 9.78998968008256e-05,
      "loss": 19.5405,
      "step": 1007
    },
    {
      "epoch": 1.01,
      "grad_norm": 5812.53662109375,
      "learning_rate": 9.789473684210527e-05,
      "loss": 23.3603,
      "step": 1008
    },
    {
      "epoch": 1.01,
      "grad_norm": 49526.359375,
      "learning_rate": 9.788957688338493e-05,
      "loss": 22.5994,
      "step": 1009
    },
    {
      "epoch": 1.01,
      "grad_norm": 70256.578125,
      "learning_rate": 9.788441692466462e-05,
      "loss": 41.0237,
      "step": 1010
    },
    {
      "epoch": 1.01,
      "grad_norm": 11836.2119140625,
      "learning_rate": 9.787925696594428e-05,
      "loss": 19.0185,
      "step": 1011
    },
    {
      "epoch": 1.01,
      "grad_norm": 40429.43359375,
      "learning_rate": 9.787409700722394e-05,
      "loss": 28.5685,
      "step": 1012
    },
    {
      "epoch": 1.01,
      "grad_norm": 17338.298828125,
      "learning_rate": 9.786893704850362e-05,
      "loss": 15.1734,
      "step": 1013
    },
    {
      "epoch": 1.02,
      "grad_norm": 10009.4375,
      "learning_rate": 9.786377708978328e-05,
      "loss": 16.2134,
      "step": 1014
    },
    {
      "epoch": 1.02,
      "grad_norm": 8437.7607421875,
      "learning_rate": 9.785861713106295e-05,
      "loss": 27.6206,
      "step": 1015
    },
    {
      "epoch": 1.02,
      "grad_norm": 27174.642578125,
      "learning_rate": 9.785345717234263e-05,
      "loss": 20.5687,
      "step": 1016
    },
    {
      "epoch": 1.02,
      "grad_norm": 4624.28955078125,
      "learning_rate": 9.78482972136223e-05,
      "loss": 24.1781,
      "step": 1017
    },
    {
      "epoch": 1.02,
      "grad_norm": 4158.51708984375,
      "learning_rate": 9.784313725490196e-05,
      "loss": 23.053,
      "step": 1018
    },
    {
      "epoch": 1.02,
      "grad_norm": 2377.86279296875,
      "learning_rate": 9.783797729618164e-05,
      "loss": 12.0977,
      "step": 1019
    },
    {
      "epoch": 1.02,
      "grad_norm": 970.0427856445312,
      "learning_rate": 9.78328173374613e-05,
      "loss": 17.0982,
      "step": 1020
    },
    {
      "epoch": 1.02,
      "grad_norm": 13930.6875,
      "learning_rate": 9.782765737874097e-05,
      "loss": 21.6685,
      "step": 1021
    },
    {
      "epoch": 1.02,
      "grad_norm": 105362.796875,
      "learning_rate": 9.782249742002065e-05,
      "loss": 21.2788,
      "step": 1022
    },
    {
      "epoch": 1.02,
      "grad_norm": 11144.5556640625,
      "learning_rate": 9.781733746130032e-05,
      "loss": 19.051,
      "step": 1023
    },
    {
      "epoch": 1.03,
      "grad_norm": 3084.739501953125,
      "learning_rate": 9.781217750257998e-05,
      "loss": 16.9452,
      "step": 1024
    },
    {
      "epoch": 1.03,
      "grad_norm": 46867.46484375,
      "learning_rate": 9.780701754385966e-05,
      "loss": 33.4763,
      "step": 1025
    },
    {
      "epoch": 1.03,
      "grad_norm": 78112.46875,
      "learning_rate": 9.780185758513932e-05,
      "loss": 36.8021,
      "step": 1026
    },
    {
      "epoch": 1.03,
      "grad_norm": 4059.517333984375,
      "learning_rate": 9.7796697626419e-05,
      "loss": 30.0766,
      "step": 1027
    },
    {
      "epoch": 1.03,
      "grad_norm": 9320.5458984375,
      "learning_rate": 9.779153766769867e-05,
      "loss": 27.8524,
      "step": 1028
    },
    {
      "epoch": 1.03,
      "grad_norm": 1424.9437255859375,
      "learning_rate": 9.778637770897834e-05,
      "loss": 12.6569,
      "step": 1029
    },
    {
      "epoch": 1.03,
      "grad_norm": 17983.1953125,
      "learning_rate": 9.7781217750258e-05,
      "loss": 16.5308,
      "step": 1030
    },
    {
      "epoch": 1.03,
      "grad_norm": 15927.625,
      "learning_rate": 9.777605779153766e-05,
      "loss": 19.3724,
      "step": 1031
    },
    {
      "epoch": 1.03,
      "grad_norm": 44179.3125,
      "learning_rate": 9.777089783281734e-05,
      "loss": 45.2029,
      "step": 1032
    },
    {
      "epoch": 1.03,
      "grad_norm": 1871.7974853515625,
      "learning_rate": 9.776573787409701e-05,
      "loss": 12.8939,
      "step": 1033
    },
    {
      "epoch": 1.04,
      "grad_norm": 13554.841796875,
      "learning_rate": 9.776057791537669e-05,
      "loss": 29.6894,
      "step": 1034
    },
    {
      "epoch": 1.04,
      "grad_norm": 7902.97607421875,
      "learning_rate": 9.775541795665635e-05,
      "loss": 30.7019,
      "step": 1035
    },
    {
      "epoch": 1.04,
      "grad_norm": 5203.15576171875,
      "learning_rate": 9.775025799793602e-05,
      "loss": 19.9075,
      "step": 1036
    },
    {
      "epoch": 1.04,
      "grad_norm": 46464.43359375,
      "learning_rate": 9.774509803921568e-05,
      "loss": 28.9147,
      "step": 1037
    },
    {
      "epoch": 1.04,
      "grad_norm": 28763.841796875,
      "learning_rate": 9.773993808049536e-05,
      "loss": 27.0157,
      "step": 1038
    },
    {
      "epoch": 1.04,
      "grad_norm": 18479.275390625,
      "learning_rate": 9.773477812177503e-05,
      "loss": 17.4303,
      "step": 1039
    },
    {
      "epoch": 1.04,
      "grad_norm": 16951.78515625,
      "learning_rate": 9.77296181630547e-05,
      "loss": 27.5786,
      "step": 1040
    },
    {
      "epoch": 1.04,
      "grad_norm": 3197.9658203125,
      "learning_rate": 9.772445820433437e-05,
      "loss": 24.1598,
      "step": 1041
    },
    {
      "epoch": 1.04,
      "grad_norm": 20293.607421875,
      "learning_rate": 9.771929824561404e-05,
      "loss": 19.8264,
      "step": 1042
    },
    {
      "epoch": 1.04,
      "grad_norm": 4602.47021484375,
      "learning_rate": 9.77141382868937e-05,
      "loss": 12.1813,
      "step": 1043
    },
    {
      "epoch": 1.05,
      "grad_norm": 37257.54296875,
      "learning_rate": 9.770897832817339e-05,
      "loss": 46.3963,
      "step": 1044
    },
    {
      "epoch": 1.05,
      "grad_norm": 6477.52099609375,
      "learning_rate": 9.770381836945305e-05,
      "loss": 14.6502,
      "step": 1045
    },
    {
      "epoch": 1.05,
      "grad_norm": 13223.5048828125,
      "learning_rate": 9.769865841073273e-05,
      "loss": 16.7742,
      "step": 1046
    },
    {
      "epoch": 1.05,
      "grad_norm": 4217.69921875,
      "learning_rate": 9.769349845201239e-05,
      "loss": 21.9806,
      "step": 1047
    },
    {
      "epoch": 1.05,
      "grad_norm": 8639.51953125,
      "learning_rate": 9.768833849329205e-05,
      "loss": 40.8815,
      "step": 1048
    },
    {
      "epoch": 1.05,
      "grad_norm": 15636.4384765625,
      "learning_rate": 9.768317853457172e-05,
      "loss": 17.3839,
      "step": 1049
    },
    {
      "epoch": 1.05,
      "grad_norm": 118858.53125,
      "learning_rate": 9.76780185758514e-05,
      "loss": 15.7506,
      "step": 1050
    },
    {
      "epoch": 1.05,
      "grad_norm": 378658.6875,
      "learning_rate": 9.767285861713107e-05,
      "loss": 29.5985,
      "step": 1051
    },
    {
      "epoch": 1.05,
      "grad_norm": 16138.310546875,
      "learning_rate": 9.766769865841073e-05,
      "loss": 21.3208,
      "step": 1052
    },
    {
      "epoch": 1.05,
      "grad_norm": 43936.85546875,
      "learning_rate": 9.76625386996904e-05,
      "loss": 12.7808,
      "step": 1053
    },
    {
      "epoch": 1.06,
      "grad_norm": 5088.796875,
      "learning_rate": 9.765737874097007e-05,
      "loss": 14.4948,
      "step": 1054
    },
    {
      "epoch": 1.06,
      "grad_norm": 9516.193359375,
      "learning_rate": 9.765221878224976e-05,
      "loss": 20.2586,
      "step": 1055
    },
    {
      "epoch": 1.06,
      "grad_norm": 18909.529296875,
      "learning_rate": 9.764705882352942e-05,
      "loss": 30.7902,
      "step": 1056
    },
    {
      "epoch": 1.06,
      "grad_norm": 50738.140625,
      "learning_rate": 9.764189886480909e-05,
      "loss": 16.0036,
      "step": 1057
    },
    {
      "epoch": 1.06,
      "grad_norm": 24963.216796875,
      "learning_rate": 9.763673890608875e-05,
      "loss": 17.5167,
      "step": 1058
    },
    {
      "epoch": 1.06,
      "grad_norm": 36585.35546875,
      "learning_rate": 9.763157894736843e-05,
      "loss": 20.1092,
      "step": 1059
    },
    {
      "epoch": 1.06,
      "grad_norm": 8684.80859375,
      "learning_rate": 9.762641898864809e-05,
      "loss": 18.783,
      "step": 1060
    },
    {
      "epoch": 1.06,
      "grad_norm": 3760.946533203125,
      "learning_rate": 9.762125902992778e-05,
      "loss": 23.4351,
      "step": 1061
    },
    {
      "epoch": 1.06,
      "grad_norm": 6260.85888671875,
      "learning_rate": 9.761609907120744e-05,
      "loss": 23.4833,
      "step": 1062
    },
    {
      "epoch": 1.06,
      "grad_norm": 19394.91796875,
      "learning_rate": 9.761093911248711e-05,
      "loss": 15.5811,
      "step": 1063
    },
    {
      "epoch": 1.07,
      "grad_norm": 24633.529296875,
      "learning_rate": 9.760577915376677e-05,
      "loss": 28.0864,
      "step": 1064
    },
    {
      "epoch": 1.07,
      "grad_norm": 6965.54541015625,
      "learning_rate": 9.760061919504643e-05,
      "loss": 23.2129,
      "step": 1065
    },
    {
      "epoch": 1.07,
      "grad_norm": 1052.9761962890625,
      "learning_rate": 9.759545923632611e-05,
      "loss": 9.6351,
      "step": 1066
    },
    {
      "epoch": 1.07,
      "grad_norm": 21768.849609375,
      "learning_rate": 9.759029927760578e-05,
      "loss": 21.5532,
      "step": 1067
    },
    {
      "epoch": 1.07,
      "grad_norm": 38457.7734375,
      "learning_rate": 9.758513931888546e-05,
      "loss": 21.2512,
      "step": 1068
    },
    {
      "epoch": 1.07,
      "grad_norm": 32803.98046875,
      "learning_rate": 9.757997936016512e-05,
      "loss": 25.6941,
      "step": 1069
    },
    {
      "epoch": 1.07,
      "grad_norm": 52515.56640625,
      "learning_rate": 9.757481940144479e-05,
      "loss": 26.0577,
      "step": 1070
    },
    {
      "epoch": 1.07,
      "grad_norm": 55634.0703125,
      "learning_rate": 9.756965944272445e-05,
      "loss": 40.3532,
      "step": 1071
    },
    {
      "epoch": 1.07,
      "grad_norm": 17426.693359375,
      "learning_rate": 9.756449948400414e-05,
      "loss": 23.1525,
      "step": 1072
    },
    {
      "epoch": 1.07,
      "grad_norm": 13901.662109375,
      "learning_rate": 9.75593395252838e-05,
      "loss": 34.0255,
      "step": 1073
    },
    {
      "epoch": 1.08,
      "grad_norm": 15024.5556640625,
      "learning_rate": 9.755417956656348e-05,
      "loss": 31.6,
      "step": 1074
    },
    {
      "epoch": 1.08,
      "grad_norm": 116020.609375,
      "learning_rate": 9.754901960784314e-05,
      "loss": 30.0147,
      "step": 1075
    },
    {
      "epoch": 1.08,
      "grad_norm": 37284.0703125,
      "learning_rate": 9.754385964912281e-05,
      "loss": 26.2391,
      "step": 1076
    },
    {
      "epoch": 1.08,
      "grad_norm": 13461.3623046875,
      "learning_rate": 9.753869969040247e-05,
      "loss": 16.2409,
      "step": 1077
    },
    {
      "epoch": 1.08,
      "grad_norm": 524996.5,
      "learning_rate": 9.753353973168216e-05,
      "loss": 35.9828,
      "step": 1078
    },
    {
      "epoch": 1.08,
      "grad_norm": 7174.6689453125,
      "learning_rate": 9.752837977296182e-05,
      "loss": 20.7434,
      "step": 1079
    },
    {
      "epoch": 1.08,
      "grad_norm": 14449.4375,
      "learning_rate": 9.75232198142415e-05,
      "loss": 24.2879,
      "step": 1080
    },
    {
      "epoch": 1.08,
      "grad_norm": 4954.26806640625,
      "learning_rate": 9.751805985552116e-05,
      "loss": 16.5889,
      "step": 1081
    },
    {
      "epoch": 1.08,
      "grad_norm": 18397.509765625,
      "learning_rate": 9.751289989680083e-05,
      "loss": 30.345,
      "step": 1082
    },
    {
      "epoch": 1.08,
      "grad_norm": 44944.55859375,
      "learning_rate": 9.750773993808051e-05,
      "loss": 25.496,
      "step": 1083
    },
    {
      "epoch": 1.09,
      "grad_norm": 3454.854248046875,
      "learning_rate": 9.750257997936017e-05,
      "loss": 14.9205,
      "step": 1084
    },
    {
      "epoch": 1.09,
      "grad_norm": 6014.7236328125,
      "learning_rate": 9.749742002063984e-05,
      "loss": 21.9725,
      "step": 1085
    },
    {
      "epoch": 1.09,
      "grad_norm": 39295.12109375,
      "learning_rate": 9.74922600619195e-05,
      "loss": 38.3193,
      "step": 1086
    },
    {
      "epoch": 1.09,
      "grad_norm": 3523.368408203125,
      "learning_rate": 9.748710010319918e-05,
      "loss": 12.426,
      "step": 1087
    },
    {
      "epoch": 1.09,
      "grad_norm": 6406.390625,
      "learning_rate": 9.748194014447884e-05,
      "loss": 16.4983,
      "step": 1088
    },
    {
      "epoch": 1.09,
      "grad_norm": 2109.84228515625,
      "learning_rate": 9.747678018575853e-05,
      "loss": 20.5817,
      "step": 1089
    },
    {
      "epoch": 1.09,
      "grad_norm": 11240.859375,
      "learning_rate": 9.747162022703819e-05,
      "loss": 22.7618,
      "step": 1090
    },
    {
      "epoch": 1.09,
      "grad_norm": 59152.37890625,
      "learning_rate": 9.746646026831786e-05,
      "loss": 29.6024,
      "step": 1091
    },
    {
      "epoch": 1.09,
      "grad_norm": 354751.5,
      "learning_rate": 9.746130030959752e-05,
      "loss": 21.6852,
      "step": 1092
    },
    {
      "epoch": 1.09,
      "grad_norm": 7497.1318359375,
      "learning_rate": 9.74561403508772e-05,
      "loss": 31.2015,
      "step": 1093
    },
    {
      "epoch": 1.1,
      "grad_norm": 56172.0,
      "learning_rate": 9.745098039215686e-05,
      "loss": 19.3625,
      "step": 1094
    },
    {
      "epoch": 1.1,
      "grad_norm": 1139.2421875,
      "learning_rate": 9.744582043343655e-05,
      "loss": 11.9349,
      "step": 1095
    },
    {
      "epoch": 1.1,
      "grad_norm": 10242.33984375,
      "learning_rate": 9.744066047471621e-05,
      "loss": 20.357,
      "step": 1096
    },
    {
      "epoch": 1.1,
      "grad_norm": 29541.927734375,
      "learning_rate": 9.743550051599588e-05,
      "loss": 35.506,
      "step": 1097
    },
    {
      "epoch": 1.1,
      "grad_norm": 97799.2734375,
      "learning_rate": 9.743034055727554e-05,
      "loss": 33.0158,
      "step": 1098
    },
    {
      "epoch": 1.1,
      "grad_norm": 10947.8115234375,
      "learning_rate": 9.742518059855522e-05,
      "loss": 16.5742,
      "step": 1099
    },
    {
      "epoch": 1.1,
      "grad_norm": 19489.595703125,
      "learning_rate": 9.742002063983489e-05,
      "loss": 27.9646,
      "step": 1100
    },
    {
      "epoch": 1.1,
      "grad_norm": 6580.34375,
      "learning_rate": 9.741486068111455e-05,
      "loss": 16.0317,
      "step": 1101
    },
    {
      "epoch": 1.1,
      "grad_norm": 82767.53125,
      "learning_rate": 9.740970072239423e-05,
      "loss": 46.6467,
      "step": 1102
    },
    {
      "epoch": 1.1,
      "grad_norm": 4901.0791015625,
      "learning_rate": 9.740454076367389e-05,
      "loss": 22.643,
      "step": 1103
    },
    {
      "epoch": 1.11,
      "grad_norm": 21906.388671875,
      "learning_rate": 9.739938080495356e-05,
      "loss": 31.4544,
      "step": 1104
    },
    {
      "epoch": 1.11,
      "grad_norm": 11800.4580078125,
      "learning_rate": 9.739422084623322e-05,
      "loss": 35.4903,
      "step": 1105
    },
    {
      "epoch": 1.11,
      "grad_norm": 34907.73046875,
      "learning_rate": 9.738906088751291e-05,
      "loss": 38.4606,
      "step": 1106
    },
    {
      "epoch": 1.11,
      "grad_norm": 61619.05859375,
      "learning_rate": 9.738390092879257e-05,
      "loss": 22.3242,
      "step": 1107
    },
    {
      "epoch": 1.11,
      "grad_norm": 2838.951416015625,
      "learning_rate": 9.737874097007225e-05,
      "loss": 12.1579,
      "step": 1108
    },
    {
      "epoch": 1.11,
      "grad_norm": 13477.4404296875,
      "learning_rate": 9.737358101135191e-05,
      "loss": 16.2662,
      "step": 1109
    },
    {
      "epoch": 1.11,
      "grad_norm": 23638.3046875,
      "learning_rate": 9.736842105263158e-05,
      "loss": 17.4236,
      "step": 1110
    },
    {
      "epoch": 1.11,
      "grad_norm": 3394.408447265625,
      "learning_rate": 9.736326109391126e-05,
      "loss": 14.8847,
      "step": 1111
    },
    {
      "epoch": 1.11,
      "grad_norm": 16852.1015625,
      "learning_rate": 9.735810113519093e-05,
      "loss": 15.4385,
      "step": 1112
    },
    {
      "epoch": 1.11,
      "grad_norm": 2301.65625,
      "learning_rate": 9.73529411764706e-05,
      "loss": 13.9249,
      "step": 1113
    },
    {
      "epoch": 1.12,
      "grad_norm": 39787.0703125,
      "learning_rate": 9.734778121775027e-05,
      "loss": 33.5375,
      "step": 1114
    },
    {
      "epoch": 1.12,
      "grad_norm": 154299.359375,
      "learning_rate": 9.734262125902993e-05,
      "loss": 34.836,
      "step": 1115
    },
    {
      "epoch": 1.12,
      "grad_norm": 38472.58203125,
      "learning_rate": 9.73374613003096e-05,
      "loss": 45.1858,
      "step": 1116
    },
    {
      "epoch": 1.12,
      "grad_norm": 102844.140625,
      "learning_rate": 9.733230134158928e-05,
      "loss": 40.5001,
      "step": 1117
    },
    {
      "epoch": 1.12,
      "grad_norm": 3197.47265625,
      "learning_rate": 9.732714138286895e-05,
      "loss": 18.9285,
      "step": 1118
    },
    {
      "epoch": 1.12,
      "grad_norm": 8533.978515625,
      "learning_rate": 9.732198142414861e-05,
      "loss": 29.3427,
      "step": 1119
    },
    {
      "epoch": 1.12,
      "grad_norm": 7917.50048828125,
      "learning_rate": 9.731682146542827e-05,
      "loss": 18.3471,
      "step": 1120
    },
    {
      "epoch": 1.12,
      "grad_norm": 26422.140625,
      "learning_rate": 9.731166150670795e-05,
      "loss": 31.6762,
      "step": 1121
    },
    {
      "epoch": 1.12,
      "grad_norm": 22206.724609375,
      "learning_rate": 9.730650154798761e-05,
      "loss": 21.1673,
      "step": 1122
    },
    {
      "epoch": 1.12,
      "grad_norm": 8219.7119140625,
      "learning_rate": 9.73013415892673e-05,
      "loss": 18.4335,
      "step": 1123
    },
    {
      "epoch": 1.13,
      "grad_norm": 16592.701171875,
      "learning_rate": 9.729618163054696e-05,
      "loss": 42.1934,
      "step": 1124
    },
    {
      "epoch": 1.13,
      "grad_norm": 12191.041015625,
      "learning_rate": 9.729102167182663e-05,
      "loss": 34.8472,
      "step": 1125
    },
    {
      "epoch": 1.13,
      "grad_norm": 6899.6650390625,
      "learning_rate": 9.72858617131063e-05,
      "loss": 19.1986,
      "step": 1126
    },
    {
      "epoch": 1.13,
      "grad_norm": 16981.537109375,
      "learning_rate": 9.728070175438597e-05,
      "loss": 19.3574,
      "step": 1127
    },
    {
      "epoch": 1.13,
      "grad_norm": 6027.45361328125,
      "learning_rate": 9.727554179566564e-05,
      "loss": 27.2821,
      "step": 1128
    },
    {
      "epoch": 1.13,
      "grad_norm": 5487.85986328125,
      "learning_rate": 9.727038183694532e-05,
      "loss": 18.0641,
      "step": 1129
    },
    {
      "epoch": 1.13,
      "grad_norm": 10534.16796875,
      "learning_rate": 9.726522187822498e-05,
      "loss": 16.9172,
      "step": 1130
    },
    {
      "epoch": 1.13,
      "grad_norm": 1075.80126953125,
      "learning_rate": 9.726006191950465e-05,
      "loss": 12.1607,
      "step": 1131
    },
    {
      "epoch": 1.13,
      "grad_norm": 99813.7578125,
      "learning_rate": 9.725490196078431e-05,
      "loss": 34.5672,
      "step": 1132
    },
    {
      "epoch": 1.13,
      "grad_norm": 35281.1484375,
      "learning_rate": 9.724974200206399e-05,
      "loss": 39.2909,
      "step": 1133
    },
    {
      "epoch": 1.14,
      "grad_norm": 13836.2216796875,
      "learning_rate": 9.724458204334366e-05,
      "loss": 18.371,
      "step": 1134
    },
    {
      "epoch": 1.14,
      "grad_norm": 6408.32568359375,
      "learning_rate": 9.723942208462334e-05,
      "loss": 29.7731,
      "step": 1135
    },
    {
      "epoch": 1.14,
      "grad_norm": 15631.7998046875,
      "learning_rate": 9.7234262125903e-05,
      "loss": 49.2229,
      "step": 1136
    },
    {
      "epoch": 1.14,
      "grad_norm": 1522.5765380859375,
      "learning_rate": 9.722910216718266e-05,
      "loss": 13.2538,
      "step": 1137
    },
    {
      "epoch": 1.14,
      "grad_norm": 18164.5078125,
      "learning_rate": 9.722394220846233e-05,
      "loss": 27.4969,
      "step": 1138
    },
    {
      "epoch": 1.14,
      "grad_norm": 42929.2421875,
      "learning_rate": 9.721878224974201e-05,
      "loss": 13.7702,
      "step": 1139
    },
    {
      "epoch": 1.14,
      "grad_norm": 10028.5029296875,
      "learning_rate": 9.721362229102168e-05,
      "loss": 13.0847,
      "step": 1140
    },
    {
      "epoch": 1.14,
      "grad_norm": 7141.95458984375,
      "learning_rate": 9.720846233230134e-05,
      "loss": 26.9417,
      "step": 1141
    },
    {
      "epoch": 1.14,
      "grad_norm": 3347.390380859375,
      "learning_rate": 9.720330237358102e-05,
      "loss": 13.5909,
      "step": 1142
    },
    {
      "epoch": 1.14,
      "grad_norm": 34956.765625,
      "learning_rate": 9.719814241486068e-05,
      "loss": 34.3822,
      "step": 1143
    },
    {
      "epoch": 1.15,
      "grad_norm": 9566.4697265625,
      "learning_rate": 9.719298245614035e-05,
      "loss": 19.4864,
      "step": 1144
    },
    {
      "epoch": 1.15,
      "grad_norm": 8482.83984375,
      "learning_rate": 9.718782249742003e-05,
      "loss": 13.5548,
      "step": 1145
    },
    {
      "epoch": 1.15,
      "grad_norm": 15170.205078125,
      "learning_rate": 9.71826625386997e-05,
      "loss": 31.8441,
      "step": 1146
    },
    {
      "epoch": 1.15,
      "grad_norm": 21059.916015625,
      "learning_rate": 9.717750257997936e-05,
      "loss": 20.984,
      "step": 1147
    },
    {
      "epoch": 1.15,
      "grad_norm": 9412.9453125,
      "learning_rate": 9.717234262125904e-05,
      "loss": 18.8613,
      "step": 1148
    },
    {
      "epoch": 1.15,
      "grad_norm": 16517.5859375,
      "learning_rate": 9.71671826625387e-05,
      "loss": 33.4936,
      "step": 1149
    },
    {
      "epoch": 1.15,
      "grad_norm": 5883.77099609375,
      "learning_rate": 9.716202270381837e-05,
      "loss": 14.2678,
      "step": 1150
    },
    {
      "epoch": 1.15,
      "grad_norm": 6381.66455078125,
      "learning_rate": 9.715686274509805e-05,
      "loss": 20.2699,
      "step": 1151
    },
    {
      "epoch": 1.15,
      "grad_norm": 57608.2265625,
      "learning_rate": 9.715170278637772e-05,
      "loss": 29.8237,
      "step": 1152
    },
    {
      "epoch": 1.15,
      "grad_norm": 3122.760009765625,
      "learning_rate": 9.714654282765738e-05,
      "loss": 15.9727,
      "step": 1153
    },
    {
      "epoch": 1.16,
      "grad_norm": 9862.8916015625,
      "learning_rate": 9.714138286893706e-05,
      "loss": 24.769,
      "step": 1154
    },
    {
      "epoch": 1.16,
      "grad_norm": 3217.303955078125,
      "learning_rate": 9.713622291021672e-05,
      "loss": 20.6446,
      "step": 1155
    },
    {
      "epoch": 1.16,
      "grad_norm": 6207.396484375,
      "learning_rate": 9.71310629514964e-05,
      "loss": 15.3704,
      "step": 1156
    },
    {
      "epoch": 1.16,
      "grad_norm": 30577.033203125,
      "learning_rate": 9.712590299277607e-05,
      "loss": 27.0759,
      "step": 1157
    },
    {
      "epoch": 1.16,
      "grad_norm": 2614.098876953125,
      "learning_rate": 9.712074303405573e-05,
      "loss": 19.0057,
      "step": 1158
    },
    {
      "epoch": 1.16,
      "grad_norm": 32646.5625,
      "learning_rate": 9.71155830753354e-05,
      "loss": 28.2605,
      "step": 1159
    },
    {
      "epoch": 1.16,
      "grad_norm": 14238.7958984375,
      "learning_rate": 9.711042311661507e-05,
      "loss": 25.9368,
      "step": 1160
    },
    {
      "epoch": 1.16,
      "grad_norm": 5731.6015625,
      "learning_rate": 9.710526315789474e-05,
      "loss": 31.0653,
      "step": 1161
    },
    {
      "epoch": 1.16,
      "grad_norm": 2869.6474609375,
      "learning_rate": 9.710010319917441e-05,
      "loss": 14.0138,
      "step": 1162
    },
    {
      "epoch": 1.16,
      "grad_norm": 23805.109375,
      "learning_rate": 9.709494324045409e-05,
      "loss": 27.5077,
      "step": 1163
    },
    {
      "epoch": 1.17,
      "grad_norm": 2376.831787109375,
      "learning_rate": 9.708978328173375e-05,
      "loss": 14.436,
      "step": 1164
    },
    {
      "epoch": 1.17,
      "grad_norm": 29261.1171875,
      "learning_rate": 9.708462332301342e-05,
      "loss": 41.3865,
      "step": 1165
    },
    {
      "epoch": 1.17,
      "grad_norm": 3076.03466796875,
      "learning_rate": 9.707946336429309e-05,
      "loss": 17.6529,
      "step": 1166
    },
    {
      "epoch": 1.17,
      "grad_norm": 14850.22265625,
      "learning_rate": 9.707430340557276e-05,
      "loss": 13.4828,
      "step": 1167
    },
    {
      "epoch": 1.17,
      "grad_norm": 22615.505859375,
      "learning_rate": 9.706914344685243e-05,
      "loss": 35.4411,
      "step": 1168
    },
    {
      "epoch": 1.17,
      "grad_norm": 14133.08984375,
      "learning_rate": 9.706398348813211e-05,
      "loss": 14.204,
      "step": 1169
    },
    {
      "epoch": 1.17,
      "grad_norm": 7576.02783203125,
      "learning_rate": 9.705882352941177e-05,
      "loss": 14.0431,
      "step": 1170
    },
    {
      "epoch": 1.17,
      "grad_norm": 4962.74755859375,
      "learning_rate": 9.705366357069144e-05,
      "loss": 30.0167,
      "step": 1171
    },
    {
      "epoch": 1.17,
      "grad_norm": 12713.267578125,
      "learning_rate": 9.70485036119711e-05,
      "loss": 25.9006,
      "step": 1172
    },
    {
      "epoch": 1.17,
      "grad_norm": 143138.5625,
      "learning_rate": 9.704334365325078e-05,
      "loss": 32.1557,
      "step": 1173
    },
    {
      "epoch": 1.18,
      "grad_norm": 23206.60546875,
      "learning_rate": 9.703818369453045e-05,
      "loss": 38.6924,
      "step": 1174
    },
    {
      "epoch": 1.18,
      "grad_norm": 23699.775390625,
      "learning_rate": 9.703302373581012e-05,
      "loss": 44.8746,
      "step": 1175
    },
    {
      "epoch": 1.18,
      "grad_norm": 66164.921875,
      "learning_rate": 9.702786377708979e-05,
      "loss": 24.1785,
      "step": 1176
    },
    {
      "epoch": 1.18,
      "grad_norm": 21156.201171875,
      "learning_rate": 9.702270381836945e-05,
      "loss": 17.081,
      "step": 1177
    },
    {
      "epoch": 1.18,
      "grad_norm": 21810.158203125,
      "learning_rate": 9.701754385964913e-05,
      "loss": 22.026,
      "step": 1178
    },
    {
      "epoch": 1.18,
      "grad_norm": 13775.916015625,
      "learning_rate": 9.70123839009288e-05,
      "loss": 17.236,
      "step": 1179
    },
    {
      "epoch": 1.18,
      "grad_norm": 13723.5029296875,
      "learning_rate": 9.700722394220847e-05,
      "loss": 17.9973,
      "step": 1180
    },
    {
      "epoch": 1.18,
      "grad_norm": 1979.242919921875,
      "learning_rate": 9.700206398348814e-05,
      "loss": 18.8715,
      "step": 1181
    },
    {
      "epoch": 1.18,
      "grad_norm": 29209.072265625,
      "learning_rate": 9.699690402476781e-05,
      "loss": 22.107,
      "step": 1182
    },
    {
      "epoch": 1.18,
      "grad_norm": 13771.83984375,
      "learning_rate": 9.699174406604747e-05,
      "loss": 14.4269,
      "step": 1183
    },
    {
      "epoch": 1.19,
      "grad_norm": 4136.53759765625,
      "learning_rate": 9.698658410732715e-05,
      "loss": 16.1239,
      "step": 1184
    },
    {
      "epoch": 1.19,
      "grad_norm": 48154.20703125,
      "learning_rate": 9.698142414860682e-05,
      "loss": 21.8639,
      "step": 1185
    },
    {
      "epoch": 1.19,
      "grad_norm": 14469.2138671875,
      "learning_rate": 9.69762641898865e-05,
      "loss": 14.6613,
      "step": 1186
    },
    {
      "epoch": 1.19,
      "grad_norm": 73036.5546875,
      "learning_rate": 9.697110423116616e-05,
      "loss": 19.8882,
      "step": 1187
    },
    {
      "epoch": 1.19,
      "grad_norm": 9518.40625,
      "learning_rate": 9.696594427244583e-05,
      "loss": 18.4321,
      "step": 1188
    },
    {
      "epoch": 1.19,
      "grad_norm": 51343.62109375,
      "learning_rate": 9.696078431372549e-05,
      "loss": 28.9045,
      "step": 1189
    },
    {
      "epoch": 1.19,
      "grad_norm": 6128.3779296875,
      "learning_rate": 9.695562435500517e-05,
      "loss": 23.5281,
      "step": 1190
    },
    {
      "epoch": 1.19,
      "grad_norm": 51118.2265625,
      "learning_rate": 9.695046439628484e-05,
      "loss": 23.4385,
      "step": 1191
    },
    {
      "epoch": 1.19,
      "grad_norm": 7004.32470703125,
      "learning_rate": 9.69453044375645e-05,
      "loss": 21.8014,
      "step": 1192
    },
    {
      "epoch": 1.19,
      "grad_norm": 17758.40234375,
      "learning_rate": 9.694014447884418e-05,
      "loss": 14.9304,
      "step": 1193
    },
    {
      "epoch": 1.2,
      "grad_norm": 16213.1240234375,
      "learning_rate": 9.693498452012384e-05,
      "loss": 30.698,
      "step": 1194
    },
    {
      "epoch": 1.2,
      "grad_norm": 1321.8826904296875,
      "learning_rate": 9.692982456140351e-05,
      "loss": 14.5738,
      "step": 1195
    },
    {
      "epoch": 1.2,
      "grad_norm": 14143.494140625,
      "learning_rate": 9.692466460268319e-05,
      "loss": 17.0471,
      "step": 1196
    },
    {
      "epoch": 1.2,
      "grad_norm": 11064.6025390625,
      "learning_rate": 9.691950464396286e-05,
      "loss": 35.5459,
      "step": 1197
    },
    {
      "epoch": 1.2,
      "grad_norm": 6168.99169921875,
      "learning_rate": 9.691434468524252e-05,
      "loss": 26.0959,
      "step": 1198
    },
    {
      "epoch": 1.2,
      "grad_norm": 3129.036865234375,
      "learning_rate": 9.69091847265222e-05,
      "loss": 13.9502,
      "step": 1199
    },
    {
      "epoch": 1.2,
      "grad_norm": 35582.6484375,
      "learning_rate": 9.690402476780186e-05,
      "loss": 40.7635,
      "step": 1200
    },
    {
      "epoch": 1.2,
      "grad_norm": 7725.6123046875,
      "learning_rate": 9.689886480908153e-05,
      "loss": 29.6126,
      "step": 1201
    },
    {
      "epoch": 1.2,
      "grad_norm": 25486.564453125,
      "learning_rate": 9.68937048503612e-05,
      "loss": 22.8943,
      "step": 1202
    },
    {
      "epoch": 1.2,
      "grad_norm": 19356.755859375,
      "learning_rate": 9.688854489164088e-05,
      "loss": 24.7089,
      "step": 1203
    },
    {
      "epoch": 1.21,
      "grad_norm": 9270.0537109375,
      "learning_rate": 9.688338493292054e-05,
      "loss": 21.1307,
      "step": 1204
    },
    {
      "epoch": 1.21,
      "grad_norm": 123106.7265625,
      "learning_rate": 9.687822497420022e-05,
      "loss": 22.3047,
      "step": 1205
    },
    {
      "epoch": 1.21,
      "grad_norm": 7169.9814453125,
      "learning_rate": 9.687306501547988e-05,
      "loss": 15.6875,
      "step": 1206
    },
    {
      "epoch": 1.21,
      "grad_norm": 56275.3359375,
      "learning_rate": 9.686790505675955e-05,
      "loss": 51.8575,
      "step": 1207
    },
    {
      "epoch": 1.21,
      "grad_norm": 68907.703125,
      "learning_rate": 9.686274509803923e-05,
      "loss": 31.0661,
      "step": 1208
    },
    {
      "epoch": 1.21,
      "grad_norm": 86504.8828125,
      "learning_rate": 9.685758513931889e-05,
      "loss": 23.0176,
      "step": 1209
    },
    {
      "epoch": 1.21,
      "grad_norm": 5547.06396484375,
      "learning_rate": 9.685242518059856e-05,
      "loss": 23.4882,
      "step": 1210
    },
    {
      "epoch": 1.21,
      "grad_norm": 220615.609375,
      "learning_rate": 9.684726522187822e-05,
      "loss": 32.9499,
      "step": 1211
    },
    {
      "epoch": 1.21,
      "grad_norm": 6414.85009765625,
      "learning_rate": 9.68421052631579e-05,
      "loss": 29.0664,
      "step": 1212
    },
    {
      "epoch": 1.21,
      "grad_norm": 34406.5078125,
      "learning_rate": 9.683694530443757e-05,
      "loss": 32.5912,
      "step": 1213
    },
    {
      "epoch": 1.22,
      "grad_norm": 14544.9580078125,
      "learning_rate": 9.683178534571725e-05,
      "loss": 30.3895,
      "step": 1214
    },
    {
      "epoch": 1.22,
      "grad_norm": 48762.29296875,
      "learning_rate": 9.68266253869969e-05,
      "loss": 33.0627,
      "step": 1215
    },
    {
      "epoch": 1.22,
      "grad_norm": 83062.9609375,
      "learning_rate": 9.682146542827658e-05,
      "loss": 23.7595,
      "step": 1216
    },
    {
      "epoch": 1.22,
      "grad_norm": 16433.353515625,
      "learning_rate": 9.681630546955624e-05,
      "loss": 17.3021,
      "step": 1217
    },
    {
      "epoch": 1.22,
      "grad_norm": 14815.921875,
      "learning_rate": 9.681114551083592e-05,
      "loss": 23.885,
      "step": 1218
    },
    {
      "epoch": 1.22,
      "grad_norm": 28404.671875,
      "learning_rate": 9.680598555211559e-05,
      "loss": 22.1582,
      "step": 1219
    },
    {
      "epoch": 1.22,
      "grad_norm": 7750.99072265625,
      "learning_rate": 9.680082559339527e-05,
      "loss": 18.885,
      "step": 1220
    },
    {
      "epoch": 1.22,
      "grad_norm": 9232.984375,
      "learning_rate": 9.679566563467493e-05,
      "loss": 20.2872,
      "step": 1221
    },
    {
      "epoch": 1.22,
      "grad_norm": 39500.5625,
      "learning_rate": 9.67905056759546e-05,
      "loss": 22.6959,
      "step": 1222
    },
    {
      "epoch": 1.22,
      "grad_norm": 72592.5625,
      "learning_rate": 9.678534571723426e-05,
      "loss": 50.6602,
      "step": 1223
    },
    {
      "epoch": 1.23,
      "grad_norm": 22025.515625,
      "learning_rate": 9.678018575851394e-05,
      "loss": 26.9524,
      "step": 1224
    },
    {
      "epoch": 1.23,
      "grad_norm": 151104.015625,
      "learning_rate": 9.677502579979361e-05,
      "loss": 29.9377,
      "step": 1225
    },
    {
      "epoch": 1.23,
      "grad_norm": 27692.21484375,
      "learning_rate": 9.676986584107327e-05,
      "loss": 25.7844,
      "step": 1226
    },
    {
      "epoch": 1.23,
      "grad_norm": 20355.435546875,
      "learning_rate": 9.676470588235295e-05,
      "loss": 35.1663,
      "step": 1227
    },
    {
      "epoch": 1.23,
      "grad_norm": 6391.21533203125,
      "learning_rate": 9.675954592363261e-05,
      "loss": 17.3176,
      "step": 1228
    },
    {
      "epoch": 1.23,
      "grad_norm": 1523.714111328125,
      "learning_rate": 9.675438596491228e-05,
      "loss": 10.7795,
      "step": 1229
    },
    {
      "epoch": 1.23,
      "grad_norm": 17111.54296875,
      "learning_rate": 9.674922600619196e-05,
      "loss": 16.4344,
      "step": 1230
    },
    {
      "epoch": 1.23,
      "grad_norm": 19219.73828125,
      "learning_rate": 9.674406604747163e-05,
      "loss": 23.7029,
      "step": 1231
    },
    {
      "epoch": 1.23,
      "grad_norm": 13467.4248046875,
      "learning_rate": 9.673890608875129e-05,
      "loss": 33.5124,
      "step": 1232
    },
    {
      "epoch": 1.23,
      "grad_norm": 21463.359375,
      "learning_rate": 9.673374613003097e-05,
      "loss": 19.462,
      "step": 1233
    },
    {
      "epoch": 1.24,
      "grad_norm": 15176.7646484375,
      "learning_rate": 9.672858617131063e-05,
      "loss": 32.5452,
      "step": 1234
    },
    {
      "epoch": 1.24,
      "grad_norm": 7694.14453125,
      "learning_rate": 9.67234262125903e-05,
      "loss": 15.2367,
      "step": 1235
    },
    {
      "epoch": 1.24,
      "grad_norm": 39483.1015625,
      "learning_rate": 9.671826625386998e-05,
      "loss": 51.9234,
      "step": 1236
    },
    {
      "epoch": 1.24,
      "grad_norm": 27551.001953125,
      "learning_rate": 9.671310629514965e-05,
      "loss": 22.7773,
      "step": 1237
    },
    {
      "epoch": 1.24,
      "grad_norm": 33285.875,
      "learning_rate": 9.670794633642931e-05,
      "loss": 29.5029,
      "step": 1238
    },
    {
      "epoch": 1.24,
      "grad_norm": 59187.50390625,
      "learning_rate": 9.670278637770899e-05,
      "loss": 23.8133,
      "step": 1239
    },
    {
      "epoch": 1.24,
      "grad_norm": 10398.07421875,
      "learning_rate": 9.669762641898865e-05,
      "loss": 32.9264,
      "step": 1240
    },
    {
      "epoch": 1.24,
      "grad_norm": 55653.33203125,
      "learning_rate": 9.669246646026832e-05,
      "loss": 24.0502,
      "step": 1241
    },
    {
      "epoch": 1.24,
      "grad_norm": 6802.94091796875,
      "learning_rate": 9.6687306501548e-05,
      "loss": 35.4681,
      "step": 1242
    },
    {
      "epoch": 1.24,
      "grad_norm": 2484.986083984375,
      "learning_rate": 9.668214654282767e-05,
      "loss": 15.1503,
      "step": 1243
    },
    {
      "epoch": 1.25,
      "grad_norm": 181340.484375,
      "learning_rate": 9.667698658410733e-05,
      "loss": 22.0917,
      "step": 1244
    },
    {
      "epoch": 1.25,
      "grad_norm": 37248.5,
      "learning_rate": 9.667182662538699e-05,
      "loss": 16.404,
      "step": 1245
    },
    {
      "epoch": 1.25,
      "grad_norm": 15052.025390625,
      "learning_rate": 9.666666666666667e-05,
      "loss": 24.6773,
      "step": 1246
    },
    {
      "epoch": 1.25,
      "grad_norm": 10974.47265625,
      "learning_rate": 9.666150670794634e-05,
      "loss": 21.0414,
      "step": 1247
    },
    {
      "epoch": 1.25,
      "grad_norm": 10178.04296875,
      "learning_rate": 9.665634674922602e-05,
      "loss": 18.9871,
      "step": 1248
    },
    {
      "epoch": 1.25,
      "grad_norm": 35384.22265625,
      "learning_rate": 9.665118679050568e-05,
      "loss": 33.2994,
      "step": 1249
    },
    {
      "epoch": 1.25,
      "grad_norm": 8190.54296875,
      "learning_rate": 9.664602683178535e-05,
      "loss": 29.0285,
      "step": 1250
    },
    {
      "epoch": 1.25,
      "grad_norm": 130062.078125,
      "learning_rate": 9.664086687306501e-05,
      "loss": 56.6848,
      "step": 1251
    },
    {
      "epoch": 1.25,
      "grad_norm": 14311.9755859375,
      "learning_rate": 9.663570691434469e-05,
      "loss": 22.7794,
      "step": 1252
    },
    {
      "epoch": 1.25,
      "grad_norm": 6479.146484375,
      "learning_rate": 9.663054695562436e-05,
      "loss": 19.9415,
      "step": 1253
    },
    {
      "epoch": 1.26,
      "grad_norm": 62912.859375,
      "learning_rate": 9.662538699690404e-05,
      "loss": 24.0778,
      "step": 1254
    },
    {
      "epoch": 1.26,
      "grad_norm": 15941.96484375,
      "learning_rate": 9.66202270381837e-05,
      "loss": 20.1496,
      "step": 1255
    },
    {
      "epoch": 1.26,
      "grad_norm": 3943.200927734375,
      "learning_rate": 9.661506707946337e-05,
      "loss": 20.1948,
      "step": 1256
    },
    {
      "epoch": 1.26,
      "grad_norm": 42029.9140625,
      "learning_rate": 9.660990712074303e-05,
      "loss": 35.1911,
      "step": 1257
    },
    {
      "epoch": 1.26,
      "grad_norm": 9981.6591796875,
      "learning_rate": 9.660474716202271e-05,
      "loss": 14.0878,
      "step": 1258
    },
    {
      "epoch": 1.26,
      "grad_norm": 7020.04638671875,
      "learning_rate": 9.659958720330238e-05,
      "loss": 13.1995,
      "step": 1259
    },
    {
      "epoch": 1.26,
      "grad_norm": 19868.693359375,
      "learning_rate": 9.659442724458206e-05,
      "loss": 26.8692,
      "step": 1260
    },
    {
      "epoch": 1.26,
      "grad_norm": 9833.1669921875,
      "learning_rate": 9.658926728586172e-05,
      "loss": 34.4516,
      "step": 1261
    },
    {
      "epoch": 1.26,
      "grad_norm": 7501.08984375,
      "learning_rate": 9.658410732714138e-05,
      "loss": 24.4545,
      "step": 1262
    },
    {
      "epoch": 1.26,
      "grad_norm": 90810.5703125,
      "learning_rate": 9.657894736842105e-05,
      "loss": 15.5906,
      "step": 1263
    },
    {
      "epoch": 1.27,
      "grad_norm": 11582.76171875,
      "learning_rate": 9.657378740970073e-05,
      "loss": 18.7331,
      "step": 1264
    },
    {
      "epoch": 1.27,
      "grad_norm": 30253.732421875,
      "learning_rate": 9.65686274509804e-05,
      "loss": 41.6464,
      "step": 1265
    },
    {
      "epoch": 1.27,
      "grad_norm": 25228.521484375,
      "learning_rate": 9.656346749226006e-05,
      "loss": 21.6454,
      "step": 1266
    },
    {
      "epoch": 1.27,
      "grad_norm": 1639.9478759765625,
      "learning_rate": 9.655830753353974e-05,
      "loss": 13.2616,
      "step": 1267
    },
    {
      "epoch": 1.27,
      "grad_norm": 16598.017578125,
      "learning_rate": 9.65531475748194e-05,
      "loss": 27.8423,
      "step": 1268
    },
    {
      "epoch": 1.27,
      "grad_norm": 123297.765625,
      "learning_rate": 9.654798761609907e-05,
      "loss": 54.6537,
      "step": 1269
    },
    {
      "epoch": 1.27,
      "grad_norm": 52409.92578125,
      "learning_rate": 9.654282765737875e-05,
      "loss": 26.3192,
      "step": 1270
    },
    {
      "epoch": 1.27,
      "grad_norm": 8501.2158203125,
      "learning_rate": 9.653766769865842e-05,
      "loss": 25.1442,
      "step": 1271
    },
    {
      "epoch": 1.27,
      "grad_norm": 8285.716796875,
      "learning_rate": 9.653250773993808e-05,
      "loss": 27.3066,
      "step": 1272
    },
    {
      "epoch": 1.27,
      "grad_norm": 2047.767822265625,
      "learning_rate": 9.652734778121776e-05,
      "loss": 13.7628,
      "step": 1273
    },
    {
      "epoch": 1.28,
      "grad_norm": 40138.375,
      "learning_rate": 9.652218782249742e-05,
      "loss": 29.6054,
      "step": 1274
    },
    {
      "epoch": 1.28,
      "grad_norm": 36803.66796875,
      "learning_rate": 9.651702786377709e-05,
      "loss": 24.9507,
      "step": 1275
    },
    {
      "epoch": 1.28,
      "grad_norm": 8772.6103515625,
      "learning_rate": 9.651186790505677e-05,
      "loss": 23.7441,
      "step": 1276
    },
    {
      "epoch": 1.28,
      "grad_norm": 12573.4384765625,
      "learning_rate": 9.650670794633644e-05,
      "loss": 24.1235,
      "step": 1277
    },
    {
      "epoch": 1.28,
      "grad_norm": 22084.298828125,
      "learning_rate": 9.65015479876161e-05,
      "loss": 31.7034,
      "step": 1278
    },
    {
      "epoch": 1.28,
      "grad_norm": 37026.33984375,
      "learning_rate": 9.649638802889578e-05,
      "loss": 32.165,
      "step": 1279
    },
    {
      "epoch": 1.28,
      "grad_norm": 4726.73388671875,
      "learning_rate": 9.649122807017544e-05,
      "loss": 34.2771,
      "step": 1280
    },
    {
      "epoch": 1.28,
      "grad_norm": 16581.419921875,
      "learning_rate": 9.648606811145511e-05,
      "loss": 45.0651,
      "step": 1281
    },
    {
      "epoch": 1.28,
      "grad_norm": 23277.501953125,
      "learning_rate": 9.648090815273479e-05,
      "loss": 20.6592,
      "step": 1282
    },
    {
      "epoch": 1.28,
      "grad_norm": 15508.544921875,
      "learning_rate": 9.647574819401445e-05,
      "loss": 18.7555,
      "step": 1283
    },
    {
      "epoch": 1.29,
      "grad_norm": 15715.662109375,
      "learning_rate": 9.647058823529412e-05,
      "loss": 24.3805,
      "step": 1284
    },
    {
      "epoch": 1.29,
      "grad_norm": 14844.9140625,
      "learning_rate": 9.646542827657378e-05,
      "loss": 28.6295,
      "step": 1285
    },
    {
      "epoch": 1.29,
      "grad_norm": 6845.734375,
      "learning_rate": 9.646026831785346e-05,
      "loss": 18.978,
      "step": 1286
    },
    {
      "epoch": 1.29,
      "grad_norm": 4808.96142578125,
      "learning_rate": 9.645510835913313e-05,
      "loss": 14.0911,
      "step": 1287
    },
    {
      "epoch": 1.29,
      "grad_norm": 56510.92578125,
      "learning_rate": 9.644994840041281e-05,
      "loss": 42.7897,
      "step": 1288
    },
    {
      "epoch": 1.29,
      "grad_norm": 2673.855224609375,
      "learning_rate": 9.644478844169247e-05,
      "loss": 43.2156,
      "step": 1289
    },
    {
      "epoch": 1.29,
      "grad_norm": 19934.388671875,
      "learning_rate": 9.643962848297214e-05,
      "loss": 34.1378,
      "step": 1290
    },
    {
      "epoch": 1.29,
      "grad_norm": 3502.7294921875,
      "learning_rate": 9.64344685242518e-05,
      "loss": 19.9627,
      "step": 1291
    },
    {
      "epoch": 1.29,
      "grad_norm": 4345.8037109375,
      "learning_rate": 9.642930856553148e-05,
      "loss": 27.52,
      "step": 1292
    },
    {
      "epoch": 1.29,
      "grad_norm": 28448.005859375,
      "learning_rate": 9.642414860681115e-05,
      "loss": 32.7004,
      "step": 1293
    },
    {
      "epoch": 1.3,
      "grad_norm": 6289.69482421875,
      "learning_rate": 9.641898864809083e-05,
      "loss": 29.6904,
      "step": 1294
    },
    {
      "epoch": 1.3,
      "grad_norm": 19168.1640625,
      "learning_rate": 9.641382868937049e-05,
      "loss": 45.9996,
      "step": 1295
    },
    {
      "epoch": 1.3,
      "grad_norm": 24270.33984375,
      "learning_rate": 9.640866873065016e-05,
      "loss": 14.635,
      "step": 1296
    },
    {
      "epoch": 1.3,
      "grad_norm": 3519.005126953125,
      "learning_rate": 9.640350877192982e-05,
      "loss": 39.1227,
      "step": 1297
    },
    {
      "epoch": 1.3,
      "grad_norm": 23506.736328125,
      "learning_rate": 9.63983488132095e-05,
      "loss": 35.4533,
      "step": 1298
    },
    {
      "epoch": 1.3,
      "grad_norm": 2406.62451171875,
      "learning_rate": 9.639318885448917e-05,
      "loss": 13.285,
      "step": 1299
    },
    {
      "epoch": 1.3,
      "grad_norm": 12384.908203125,
      "learning_rate": 9.638802889576883e-05,
      "loss": 12.7733,
      "step": 1300
    },
    {
      "epoch": 1.3,
      "grad_norm": 16237.9599609375,
      "learning_rate": 9.638286893704851e-05,
      "loss": 15.6906,
      "step": 1301
    },
    {
      "epoch": 1.3,
      "grad_norm": 16338.1220703125,
      "learning_rate": 9.637770897832817e-05,
      "loss": 33.9949,
      "step": 1302
    },
    {
      "epoch": 1.3,
      "grad_norm": 1597.4349365234375,
      "learning_rate": 9.637254901960784e-05,
      "loss": 17.8168,
      "step": 1303
    },
    {
      "epoch": 1.31,
      "grad_norm": 6533.70166015625,
      "learning_rate": 9.636738906088752e-05,
      "loss": 23.3209,
      "step": 1304
    },
    {
      "epoch": 1.31,
      "grad_norm": 12244.5888671875,
      "learning_rate": 9.636222910216719e-05,
      "loss": 25.7404,
      "step": 1305
    },
    {
      "epoch": 1.31,
      "grad_norm": 93844.6875,
      "learning_rate": 9.635706914344685e-05,
      "loss": 37.8426,
      "step": 1306
    },
    {
      "epoch": 1.31,
      "grad_norm": 7065.44091796875,
      "learning_rate": 9.635190918472653e-05,
      "loss": 25.2777,
      "step": 1307
    },
    {
      "epoch": 1.31,
      "grad_norm": 31922.99609375,
      "learning_rate": 9.634674922600619e-05,
      "loss": 23.4945,
      "step": 1308
    },
    {
      "epoch": 1.31,
      "grad_norm": 1437218.375,
      "learning_rate": 9.634158926728586e-05,
      "loss": 38.9359,
      "step": 1309
    },
    {
      "epoch": 1.31,
      "grad_norm": 27182.888671875,
      "learning_rate": 9.633642930856554e-05,
      "loss": 20.3286,
      "step": 1310
    },
    {
      "epoch": 1.31,
      "grad_norm": 8551.927734375,
      "learning_rate": 9.633126934984521e-05,
      "loss": 17.4018,
      "step": 1311
    },
    {
      "epoch": 1.31,
      "grad_norm": 9577.73046875,
      "learning_rate": 9.632610939112487e-05,
      "loss": 25.995,
      "step": 1312
    },
    {
      "epoch": 1.31,
      "grad_norm": 4403.51025390625,
      "learning_rate": 9.632094943240455e-05,
      "loss": 15.9949,
      "step": 1313
    },
    {
      "epoch": 1.32,
      "grad_norm": 24996.03515625,
      "learning_rate": 9.631578947368421e-05,
      "loss": 26.244,
      "step": 1314
    },
    {
      "epoch": 1.32,
      "grad_norm": 3470.547119140625,
      "learning_rate": 9.631062951496388e-05,
      "loss": 31.3988,
      "step": 1315
    },
    {
      "epoch": 1.32,
      "grad_norm": 34356.48046875,
      "learning_rate": 9.630546955624356e-05,
      "loss": 24.5958,
      "step": 1316
    },
    {
      "epoch": 1.32,
      "grad_norm": 6142.6865234375,
      "learning_rate": 9.630030959752322e-05,
      "loss": 14.1799,
      "step": 1317
    },
    {
      "epoch": 1.32,
      "grad_norm": 24122.2109375,
      "learning_rate": 9.62951496388029e-05,
      "loss": 25.2554,
      "step": 1318
    },
    {
      "epoch": 1.32,
      "grad_norm": 5167.32080078125,
      "learning_rate": 9.628998968008255e-05,
      "loss": 15.2646,
      "step": 1319
    },
    {
      "epoch": 1.32,
      "grad_norm": 8623.3115234375,
      "learning_rate": 9.628482972136223e-05,
      "loss": 21.8261,
      "step": 1320
    },
    {
      "epoch": 1.32,
      "grad_norm": 6736.93359375,
      "learning_rate": 9.62796697626419e-05,
      "loss": 38.8135,
      "step": 1321
    },
    {
      "epoch": 1.32,
      "grad_norm": 3182.626953125,
      "learning_rate": 9.627450980392158e-05,
      "loss": 26.0615,
      "step": 1322
    },
    {
      "epoch": 1.32,
      "grad_norm": 14694.58203125,
      "learning_rate": 9.626934984520124e-05,
      "loss": 20.0606,
      "step": 1323
    },
    {
      "epoch": 1.33,
      "grad_norm": 28229.099609375,
      "learning_rate": 9.626418988648091e-05,
      "loss": 31.174,
      "step": 1324
    },
    {
      "epoch": 1.33,
      "grad_norm": 339481.0,
      "learning_rate": 9.625902992776057e-05,
      "loss": 39.9732,
      "step": 1325
    },
    {
      "epoch": 1.33,
      "grad_norm": 5250.1484375,
      "learning_rate": 9.625386996904025e-05,
      "loss": 24.6455,
      "step": 1326
    },
    {
      "epoch": 1.33,
      "grad_norm": 4235.95556640625,
      "learning_rate": 9.624871001031992e-05,
      "loss": 14.9137,
      "step": 1327
    },
    {
      "epoch": 1.33,
      "grad_norm": 7138.52490234375,
      "learning_rate": 9.62435500515996e-05,
      "loss": 15.5739,
      "step": 1328
    },
    {
      "epoch": 1.33,
      "grad_norm": 14620.455078125,
      "learning_rate": 9.623839009287926e-05,
      "loss": 41.4474,
      "step": 1329
    },
    {
      "epoch": 1.33,
      "grad_norm": 6802.603515625,
      "learning_rate": 9.623323013415893e-05,
      "loss": 21.4478,
      "step": 1330
    },
    {
      "epoch": 1.33,
      "grad_norm": 17403.71875,
      "learning_rate": 9.62280701754386e-05,
      "loss": 30.6826,
      "step": 1331
    },
    {
      "epoch": 1.33,
      "grad_norm": 34046.8203125,
      "learning_rate": 9.622291021671828e-05,
      "loss": 22.8586,
      "step": 1332
    },
    {
      "epoch": 1.33,
      "grad_norm": 1316.581298828125,
      "learning_rate": 9.621775025799794e-05,
      "loss": 20.618,
      "step": 1333
    },
    {
      "epoch": 1.34,
      "grad_norm": 23432.56640625,
      "learning_rate": 9.62125902992776e-05,
      "loss": 25.5407,
      "step": 1334
    },
    {
      "epoch": 1.34,
      "grad_norm": 40313.171875,
      "learning_rate": 9.620743034055728e-05,
      "loss": 28.5122,
      "step": 1335
    },
    {
      "epoch": 1.34,
      "grad_norm": 7863.83740234375,
      "learning_rate": 9.620227038183694e-05,
      "loss": 16.8115,
      "step": 1336
    },
    {
      "epoch": 1.34,
      "grad_norm": 2072.705078125,
      "learning_rate": 9.619711042311661e-05,
      "loss": 11.6061,
      "step": 1337
    },
    {
      "epoch": 1.34,
      "grad_norm": 40739.49609375,
      "learning_rate": 9.619195046439629e-05,
      "loss": 48.6657,
      "step": 1338
    },
    {
      "epoch": 1.34,
      "grad_norm": 8008.90673828125,
      "learning_rate": 9.618679050567596e-05,
      "loss": 42.2495,
      "step": 1339
    },
    {
      "epoch": 1.34,
      "grad_norm": 20786.359375,
      "learning_rate": 9.618163054695562e-05,
      "loss": 28.8642,
      "step": 1340
    },
    {
      "epoch": 1.34,
      "grad_norm": 14043.7119140625,
      "learning_rate": 9.61764705882353e-05,
      "loss": 37.6484,
      "step": 1341
    },
    {
      "epoch": 1.34,
      "grad_norm": 21555.3984375,
      "learning_rate": 9.617131062951496e-05,
      "loss": 17.503,
      "step": 1342
    },
    {
      "epoch": 1.34,
      "grad_norm": 15744.029296875,
      "learning_rate": 9.616615067079463e-05,
      "loss": 17.5393,
      "step": 1343
    },
    {
      "epoch": 1.35,
      "grad_norm": 14325.1083984375,
      "learning_rate": 9.616099071207431e-05,
      "loss": 25.4463,
      "step": 1344
    },
    {
      "epoch": 1.35,
      "grad_norm": 23409.009765625,
      "learning_rate": 9.615583075335398e-05,
      "loss": 27.4196,
      "step": 1345
    },
    {
      "epoch": 1.35,
      "grad_norm": 15485.615234375,
      "learning_rate": 9.615067079463364e-05,
      "loss": 12.2671,
      "step": 1346
    },
    {
      "epoch": 1.35,
      "grad_norm": 7424.2744140625,
      "learning_rate": 9.614551083591332e-05,
      "loss": 36.1285,
      "step": 1347
    },
    {
      "epoch": 1.35,
      "grad_norm": 120627.140625,
      "learning_rate": 9.614035087719298e-05,
      "loss": 41.5437,
      "step": 1348
    },
    {
      "epoch": 1.35,
      "grad_norm": 10414.970703125,
      "learning_rate": 9.613519091847267e-05,
      "loss": 25.4078,
      "step": 1349
    },
    {
      "epoch": 1.35,
      "grad_norm": 85652.109375,
      "learning_rate": 9.613003095975233e-05,
      "loss": 33.834,
      "step": 1350
    },
    {
      "epoch": 1.35,
      "grad_norm": 50891.5703125,
      "learning_rate": 9.612487100103199e-05,
      "loss": 33.7735,
      "step": 1351
    },
    {
      "epoch": 1.35,
      "grad_norm": 30282.9921875,
      "learning_rate": 9.611971104231166e-05,
      "loss": 40.2911,
      "step": 1352
    },
    {
      "epoch": 1.35,
      "grad_norm": 739.3343505859375,
      "learning_rate": 9.611455108359133e-05,
      "loss": 12.6242,
      "step": 1353
    },
    {
      "epoch": 1.36,
      "grad_norm": 5569.19970703125,
      "learning_rate": 9.6109391124871e-05,
      "loss": 26.1181,
      "step": 1354
    },
    {
      "epoch": 1.36,
      "grad_norm": 4969.77587890625,
      "learning_rate": 9.610423116615067e-05,
      "loss": 16.1925,
      "step": 1355
    },
    {
      "epoch": 1.36,
      "grad_norm": 6524.97119140625,
      "learning_rate": 9.609907120743035e-05,
      "loss": 26.3707,
      "step": 1356
    },
    {
      "epoch": 1.36,
      "grad_norm": 15168.9482421875,
      "learning_rate": 9.609391124871001e-05,
      "loss": 22.6149,
      "step": 1357
    },
    {
      "epoch": 1.36,
      "grad_norm": 5839.09228515625,
      "learning_rate": 9.608875128998968e-05,
      "loss": 20.3991,
      "step": 1358
    },
    {
      "epoch": 1.36,
      "grad_norm": 22165.423828125,
      "learning_rate": 9.608359133126935e-05,
      "loss": 30.6515,
      "step": 1359
    },
    {
      "epoch": 1.36,
      "grad_norm": 5802.64306640625,
      "learning_rate": 9.607843137254903e-05,
      "loss": 19.9361,
      "step": 1360
    },
    {
      "epoch": 1.36,
      "grad_norm": 3745.48974609375,
      "learning_rate": 9.60732714138287e-05,
      "loss": 22.914,
      "step": 1361
    },
    {
      "epoch": 1.36,
      "grad_norm": 13515.5380859375,
      "learning_rate": 9.606811145510837e-05,
      "loss": 14.338,
      "step": 1362
    },
    {
      "epoch": 1.36,
      "grad_norm": 14309.6337890625,
      "learning_rate": 9.606295149638803e-05,
      "loss": 20.5133,
      "step": 1363
    },
    {
      "epoch": 1.37,
      "grad_norm": 47093.8359375,
      "learning_rate": 9.60577915376677e-05,
      "loss": 29.3552,
      "step": 1364
    },
    {
      "epoch": 1.37,
      "grad_norm": 9758.76171875,
      "learning_rate": 9.605263157894737e-05,
      "loss": 30.4522,
      "step": 1365
    },
    {
      "epoch": 1.37,
      "grad_norm": 13353.3125,
      "learning_rate": 9.604747162022705e-05,
      "loss": 17.1201,
      "step": 1366
    },
    {
      "epoch": 1.37,
      "grad_norm": 17596.212890625,
      "learning_rate": 9.604231166150671e-05,
      "loss": 12.1138,
      "step": 1367
    },
    {
      "epoch": 1.37,
      "grad_norm": 19456.37890625,
      "learning_rate": 9.603715170278639e-05,
      "loss": 36.0281,
      "step": 1368
    },
    {
      "epoch": 1.37,
      "grad_norm": 4706.36572265625,
      "learning_rate": 9.603199174406605e-05,
      "loss": 12.6023,
      "step": 1369
    },
    {
      "epoch": 1.37,
      "grad_norm": 25001.435546875,
      "learning_rate": 9.602683178534571e-05,
      "loss": 18.4217,
      "step": 1370
    },
    {
      "epoch": 1.37,
      "grad_norm": 20830.9609375,
      "learning_rate": 9.602167182662539e-05,
      "loss": 47.8271,
      "step": 1371
    },
    {
      "epoch": 1.37,
      "grad_norm": 2059.875732421875,
      "learning_rate": 9.601651186790506e-05,
      "loss": 24.4852,
      "step": 1372
    },
    {
      "epoch": 1.37,
      "grad_norm": 26690.2265625,
      "learning_rate": 9.601135190918473e-05,
      "loss": 11.9548,
      "step": 1373
    },
    {
      "epoch": 1.38,
      "grad_norm": 3625.787841796875,
      "learning_rate": 9.60061919504644e-05,
      "loss": 37.1393,
      "step": 1374
    },
    {
      "epoch": 1.38,
      "grad_norm": 15298.591796875,
      "learning_rate": 9.600103199174407e-05,
      "loss": 25.425,
      "step": 1375
    },
    {
      "epoch": 1.38,
      "grad_norm": 6464.1044921875,
      "learning_rate": 9.599587203302373e-05,
      "loss": 44.3951,
      "step": 1376
    },
    {
      "epoch": 1.38,
      "grad_norm": 2623.924072265625,
      "learning_rate": 9.599071207430342e-05,
      "loss": 11.965,
      "step": 1377
    },
    {
      "epoch": 1.38,
      "grad_norm": 17742.87109375,
      "learning_rate": 9.598555211558308e-05,
      "loss": 23.3695,
      "step": 1378
    },
    {
      "epoch": 1.38,
      "grad_norm": 6121.994140625,
      "learning_rate": 9.598039215686275e-05,
      "loss": 12.3743,
      "step": 1379
    },
    {
      "epoch": 1.38,
      "grad_norm": 3306.170654296875,
      "learning_rate": 9.597523219814242e-05,
      "loss": 17.1148,
      "step": 1380
    },
    {
      "epoch": 1.38,
      "grad_norm": 15213.822265625,
      "learning_rate": 9.597007223942209e-05,
      "loss": 25.4914,
      "step": 1381
    },
    {
      "epoch": 1.38,
      "grad_norm": 26957.185546875,
      "learning_rate": 9.596491228070175e-05,
      "loss": 18.6731,
      "step": 1382
    },
    {
      "epoch": 1.38,
      "grad_norm": 17048.025390625,
      "learning_rate": 9.595975232198144e-05,
      "loss": 21.0794,
      "step": 1383
    },
    {
      "epoch": 1.39,
      "grad_norm": 11103.611328125,
      "learning_rate": 9.59545923632611e-05,
      "loss": 15.4243,
      "step": 1384
    },
    {
      "epoch": 1.39,
      "grad_norm": 3546.411376953125,
      "learning_rate": 9.594943240454077e-05,
      "loss": 20.6945,
      "step": 1385
    },
    {
      "epoch": 1.39,
      "grad_norm": 124098.296875,
      "learning_rate": 9.594427244582044e-05,
      "loss": 32.0381,
      "step": 1386
    },
    {
      "epoch": 1.39,
      "grad_norm": 58646.0859375,
      "learning_rate": 9.59391124871001e-05,
      "loss": 22.161,
      "step": 1387
    },
    {
      "epoch": 1.39,
      "grad_norm": 20699.27734375,
      "learning_rate": 9.593395252837977e-05,
      "loss": 53.6872,
      "step": 1388
    },
    {
      "epoch": 1.39,
      "grad_norm": 4321.818359375,
      "learning_rate": 9.592879256965945e-05,
      "loss": 33.5818,
      "step": 1389
    },
    {
      "epoch": 1.39,
      "grad_norm": 16487.787109375,
      "learning_rate": 9.592363261093912e-05,
      "loss": 31.3134,
      "step": 1390
    },
    {
      "epoch": 1.39,
      "grad_norm": 13684.44140625,
      "learning_rate": 9.591847265221878e-05,
      "loss": 17.9851,
      "step": 1391
    },
    {
      "epoch": 1.39,
      "grad_norm": 611217.5,
      "learning_rate": 9.591331269349846e-05,
      "loss": 34.0865,
      "step": 1392
    },
    {
      "epoch": 1.39,
      "grad_norm": 24299.95703125,
      "learning_rate": 9.590815273477812e-05,
      "loss": 19.9139,
      "step": 1393
    },
    {
      "epoch": 1.4,
      "grad_norm": 10389.6572265625,
      "learning_rate": 9.59029927760578e-05,
      "loss": 27.72,
      "step": 1394
    },
    {
      "epoch": 1.4,
      "grad_norm": 408327.28125,
      "learning_rate": 9.589783281733747e-05,
      "loss": 32.6568,
      "step": 1395
    },
    {
      "epoch": 1.4,
      "grad_norm": 5644.1875,
      "learning_rate": 9.589267285861714e-05,
      "loss": 20.7727,
      "step": 1396
    },
    {
      "epoch": 1.4,
      "grad_norm": 2914.995849609375,
      "learning_rate": 9.58875128998968e-05,
      "loss": 14.438,
      "step": 1397
    },
    {
      "epoch": 1.4,
      "grad_norm": 5390.14990234375,
      "learning_rate": 9.588235294117648e-05,
      "loss": 21.2218,
      "step": 1398
    },
    {
      "epoch": 1.4,
      "grad_norm": 15559.2158203125,
      "learning_rate": 9.587719298245614e-05,
      "loss": 23.6065,
      "step": 1399
    },
    {
      "epoch": 1.4,
      "grad_norm": 1899.4390869140625,
      "learning_rate": 9.587203302373582e-05,
      "loss": 13.5933,
      "step": 1400
    },
    {
      "epoch": 1.4,
      "grad_norm": 10691.443359375,
      "learning_rate": 9.586687306501549e-05,
      "loss": 32.2524,
      "step": 1401
    },
    {
      "epoch": 1.4,
      "grad_norm": 75740.234375,
      "learning_rate": 9.586171310629516e-05,
      "loss": 21.3751,
      "step": 1402
    },
    {
      "epoch": 1.4,
      "grad_norm": 10422.421875,
      "learning_rate": 9.585655314757482e-05,
      "loss": 13.2664,
      "step": 1403
    },
    {
      "epoch": 1.41,
      "grad_norm": 8703.994140625,
      "learning_rate": 9.58513931888545e-05,
      "loss": 12.8283,
      "step": 1404
    },
    {
      "epoch": 1.41,
      "grad_norm": 23373.0546875,
      "learning_rate": 9.584623323013417e-05,
      "loss": 29.1819,
      "step": 1405
    },
    {
      "epoch": 1.41,
      "grad_norm": 4690.14208984375,
      "learning_rate": 9.584107327141383e-05,
      "loss": 17.3448,
      "step": 1406
    },
    {
      "epoch": 1.41,
      "grad_norm": 3032.072021484375,
      "learning_rate": 9.58359133126935e-05,
      "loss": 19.9613,
      "step": 1407
    },
    {
      "epoch": 1.41,
      "grad_norm": 19087.291015625,
      "learning_rate": 9.583075335397317e-05,
      "loss": 15.1935,
      "step": 1408
    },
    {
      "epoch": 1.41,
      "grad_norm": 26948.515625,
      "learning_rate": 9.582559339525284e-05,
      "loss": 21.9345,
      "step": 1409
    },
    {
      "epoch": 1.41,
      "grad_norm": 5070.7470703125,
      "learning_rate": 9.58204334365325e-05,
      "loss": 17.2849,
      "step": 1410
    },
    {
      "epoch": 1.41,
      "grad_norm": 35946.55078125,
      "learning_rate": 9.581527347781219e-05,
      "loss": 36.7826,
      "step": 1411
    },
    {
      "epoch": 1.41,
      "grad_norm": 46770.703125,
      "learning_rate": 9.581011351909185e-05,
      "loss": 31.1883,
      "step": 1412
    },
    {
      "epoch": 1.41,
      "grad_norm": 1952.7406005859375,
      "learning_rate": 9.580495356037153e-05,
      "loss": 14.8173,
      "step": 1413
    },
    {
      "epoch": 1.42,
      "grad_norm": 14984.7587890625,
      "learning_rate": 9.579979360165119e-05,
      "loss": 32.4785,
      "step": 1414
    },
    {
      "epoch": 1.42,
      "grad_norm": 16893.138671875,
      "learning_rate": 9.579463364293086e-05,
      "loss": 20.5236,
      "step": 1415
    },
    {
      "epoch": 1.42,
      "grad_norm": 7475.64990234375,
      "learning_rate": 9.578947368421052e-05,
      "loss": 18.2922,
      "step": 1416
    },
    {
      "epoch": 1.42,
      "grad_norm": 16506.767578125,
      "learning_rate": 9.578431372549021e-05,
      "loss": 34.6797,
      "step": 1417
    },
    {
      "epoch": 1.42,
      "grad_norm": 13355.17578125,
      "learning_rate": 9.577915376676987e-05,
      "loss": 21.4873,
      "step": 1418
    },
    {
      "epoch": 1.42,
      "grad_norm": 35205.86328125,
      "learning_rate": 9.577399380804955e-05,
      "loss": 15.8595,
      "step": 1419
    },
    {
      "epoch": 1.42,
      "grad_norm": 3517.316650390625,
      "learning_rate": 9.57688338493292e-05,
      "loss": 16.5118,
      "step": 1420
    },
    {
      "epoch": 1.42,
      "grad_norm": 10832.46875,
      "learning_rate": 9.576367389060888e-05,
      "loss": 23.0062,
      "step": 1421
    },
    {
      "epoch": 1.42,
      "grad_norm": 5382.25244140625,
      "learning_rate": 9.575851393188856e-05,
      "loss": 16.9483,
      "step": 1422
    },
    {
      "epoch": 1.42,
      "grad_norm": 2596.298583984375,
      "learning_rate": 9.575335397316822e-05,
      "loss": 27.3575,
      "step": 1423
    },
    {
      "epoch": 1.43,
      "grad_norm": 6809.9921875,
      "learning_rate": 9.574819401444789e-05,
      "loss": 12.6119,
      "step": 1424
    },
    {
      "epoch": 1.43,
      "grad_norm": 37616.0,
      "learning_rate": 9.574303405572755e-05,
      "loss": 34.0689,
      "step": 1425
    },
    {
      "epoch": 1.43,
      "grad_norm": 6035.458984375,
      "learning_rate": 9.573787409700723e-05,
      "loss": 18.3717,
      "step": 1426
    },
    {
      "epoch": 1.43,
      "grad_norm": 5768.98486328125,
      "learning_rate": 9.573271413828689e-05,
      "loss": 13.0575,
      "step": 1427
    },
    {
      "epoch": 1.43,
      "grad_norm": 28482.998046875,
      "learning_rate": 9.572755417956658e-05,
      "loss": 21.5498,
      "step": 1428
    },
    {
      "epoch": 1.43,
      "grad_norm": 5201.6708984375,
      "learning_rate": 9.572239422084624e-05,
      "loss": 15.4435,
      "step": 1429
    },
    {
      "epoch": 1.43,
      "grad_norm": 15658.0654296875,
      "learning_rate": 9.571723426212591e-05,
      "loss": 17.0869,
      "step": 1430
    },
    {
      "epoch": 1.43,
      "grad_norm": 14252.021484375,
      "learning_rate": 9.571207430340557e-05,
      "loss": 20.0221,
      "step": 1431
    },
    {
      "epoch": 1.43,
      "grad_norm": 12121.8798828125,
      "learning_rate": 9.570691434468525e-05,
      "loss": 13.1147,
      "step": 1432
    },
    {
      "epoch": 1.43,
      "grad_norm": 5834.97509765625,
      "learning_rate": 9.570175438596492e-05,
      "loss": 18.804,
      "step": 1433
    },
    {
      "epoch": 1.44,
      "grad_norm": 9807.3115234375,
      "learning_rate": 9.56965944272446e-05,
      "loss": 18.9864,
      "step": 1434
    },
    {
      "epoch": 1.44,
      "grad_norm": 7582.88671875,
      "learning_rate": 9.569143446852426e-05,
      "loss": 13.3852,
      "step": 1435
    },
    {
      "epoch": 1.44,
      "grad_norm": 24611.419921875,
      "learning_rate": 9.568627450980393e-05,
      "loss": 31.2193,
      "step": 1436
    },
    {
      "epoch": 1.44,
      "grad_norm": 1527.0472412109375,
      "learning_rate": 9.568111455108359e-05,
      "loss": 16.7736,
      "step": 1437
    },
    {
      "epoch": 1.44,
      "grad_norm": 20182.06640625,
      "learning_rate": 9.567595459236327e-05,
      "loss": 25.3581,
      "step": 1438
    },
    {
      "epoch": 1.44,
      "grad_norm": 18913.328125,
      "learning_rate": 9.567079463364294e-05,
      "loss": 17.8042,
      "step": 1439
    },
    {
      "epoch": 1.44,
      "grad_norm": 27571.21875,
      "learning_rate": 9.566563467492262e-05,
      "loss": 22.2377,
      "step": 1440
    },
    {
      "epoch": 1.44,
      "grad_norm": 21140.291015625,
      "learning_rate": 9.566047471620228e-05,
      "loss": 18.5508,
      "step": 1441
    },
    {
      "epoch": 1.44,
      "grad_norm": 710.7388916015625,
      "learning_rate": 9.565531475748194e-05,
      "loss": 9.9722,
      "step": 1442
    },
    {
      "epoch": 1.44,
      "grad_norm": 4336.65283203125,
      "learning_rate": 9.565015479876161e-05,
      "loss": 13.6781,
      "step": 1443
    },
    {
      "epoch": 1.45,
      "grad_norm": 4186.71142578125,
      "learning_rate": 9.564499484004127e-05,
      "loss": 16.4657,
      "step": 1444
    },
    {
      "epoch": 1.45,
      "grad_norm": 3121.00830078125,
      "learning_rate": 9.563983488132096e-05,
      "loss": 21.1633,
      "step": 1445
    },
    {
      "epoch": 1.45,
      "grad_norm": 9927.3955078125,
      "learning_rate": 9.563467492260062e-05,
      "loss": 20.3026,
      "step": 1446
    },
    {
      "epoch": 1.45,
      "grad_norm": 4862.8037109375,
      "learning_rate": 9.56295149638803e-05,
      "loss": 22.2746,
      "step": 1447
    },
    {
      "epoch": 1.45,
      "grad_norm": 66401.6171875,
      "learning_rate": 9.562435500515996e-05,
      "loss": 19.5156,
      "step": 1448
    },
    {
      "epoch": 1.45,
      "grad_norm": 16147.703125,
      "learning_rate": 9.561919504643963e-05,
      "loss": 19.0027,
      "step": 1449
    },
    {
      "epoch": 1.45,
      "grad_norm": 1775.5306396484375,
      "learning_rate": 9.56140350877193e-05,
      "loss": 12.1655,
      "step": 1450
    },
    {
      "epoch": 1.45,
      "grad_norm": 9577.115234375,
      "learning_rate": 9.560887512899898e-05,
      "loss": 23.1383,
      "step": 1451
    },
    {
      "epoch": 1.45,
      "grad_norm": 16885.107421875,
      "learning_rate": 9.560371517027864e-05,
      "loss": 31.6781,
      "step": 1452
    },
    {
      "epoch": 1.45,
      "grad_norm": 16995.201171875,
      "learning_rate": 9.559855521155832e-05,
      "loss": 37.0294,
      "step": 1453
    },
    {
      "epoch": 1.46,
      "grad_norm": 35003.32421875,
      "learning_rate": 9.559339525283798e-05,
      "loss": 16.5738,
      "step": 1454
    },
    {
      "epoch": 1.46,
      "grad_norm": 3690.313720703125,
      "learning_rate": 9.558823529411765e-05,
      "loss": 13.6837,
      "step": 1455
    },
    {
      "epoch": 1.46,
      "grad_norm": 3077.8056640625,
      "learning_rate": 9.558307533539733e-05,
      "loss": 16.6204,
      "step": 1456
    },
    {
      "epoch": 1.46,
      "grad_norm": 9749.0830078125,
      "learning_rate": 9.5577915376677e-05,
      "loss": 23.9183,
      "step": 1457
    },
    {
      "epoch": 1.46,
      "grad_norm": 5754.0634765625,
      "learning_rate": 9.557275541795666e-05,
      "loss": 22.1969,
      "step": 1458
    },
    {
      "epoch": 1.46,
      "grad_norm": 14852.267578125,
      "learning_rate": 9.556759545923632e-05,
      "loss": 23.8988,
      "step": 1459
    },
    {
      "epoch": 1.46,
      "grad_norm": 12644.3642578125,
      "learning_rate": 9.5562435500516e-05,
      "loss": 31.3255,
      "step": 1460
    },
    {
      "epoch": 1.46,
      "grad_norm": 4373.38037109375,
      "learning_rate": 9.555727554179567e-05,
      "loss": 23.5642,
      "step": 1461
    },
    {
      "epoch": 1.46,
      "grad_norm": 9659.7451171875,
      "learning_rate": 9.555211558307535e-05,
      "loss": 19.8401,
      "step": 1462
    },
    {
      "epoch": 1.46,
      "grad_norm": 68980.7890625,
      "learning_rate": 9.554695562435501e-05,
      "loss": 32.7046,
      "step": 1463
    },
    {
      "epoch": 1.47,
      "grad_norm": 35045.39453125,
      "learning_rate": 9.554179566563468e-05,
      "loss": 14.075,
      "step": 1464
    },
    {
      "epoch": 1.47,
      "grad_norm": 3371.099365234375,
      "learning_rate": 9.553663570691434e-05,
      "loss": 18.9755,
      "step": 1465
    },
    {
      "epoch": 1.47,
      "grad_norm": 11122.5517578125,
      "learning_rate": 9.553147574819402e-05,
      "loss": 16.7915,
      "step": 1466
    },
    {
      "epoch": 1.47,
      "grad_norm": 24198.173828125,
      "learning_rate": 9.552631578947369e-05,
      "loss": 14.3438,
      "step": 1467
    },
    {
      "epoch": 1.47,
      "grad_norm": 2670.099365234375,
      "learning_rate": 9.552115583075337e-05,
      "loss": 10.9621,
      "step": 1468
    },
    {
      "epoch": 1.47,
      "grad_norm": 2909.276611328125,
      "learning_rate": 9.551599587203303e-05,
      "loss": 26.6327,
      "step": 1469
    },
    {
      "epoch": 1.47,
      "grad_norm": 2086.26611328125,
      "learning_rate": 9.55108359133127e-05,
      "loss": 19.7849,
      "step": 1470
    },
    {
      "epoch": 1.47,
      "grad_norm": 12063.7041015625,
      "learning_rate": 9.550567595459236e-05,
      "loss": 34.2871,
      "step": 1471
    },
    {
      "epoch": 1.47,
      "grad_norm": 17661.486328125,
      "learning_rate": 9.550051599587204e-05,
      "loss": 20.8712,
      "step": 1472
    },
    {
      "epoch": 1.47,
      "grad_norm": 14688.73828125,
      "learning_rate": 9.549535603715171e-05,
      "loss": 18.2659,
      "step": 1473
    },
    {
      "epoch": 1.48,
      "grad_norm": 29985.7421875,
      "learning_rate": 9.549019607843139e-05,
      "loss": 33.7406,
      "step": 1474
    },
    {
      "epoch": 1.48,
      "grad_norm": 4051.128662109375,
      "learning_rate": 9.548503611971105e-05,
      "loss": 13.3376,
      "step": 1475
    },
    {
      "epoch": 1.48,
      "grad_norm": 9259.216796875,
      "learning_rate": 9.547987616099072e-05,
      "loss": 19.6966,
      "step": 1476
    },
    {
      "epoch": 1.48,
      "grad_norm": 4524.3984375,
      "learning_rate": 9.547471620227038e-05,
      "loss": 16.2662,
      "step": 1477
    },
    {
      "epoch": 1.48,
      "grad_norm": 4153.19384765625,
      "learning_rate": 9.546955624355006e-05,
      "loss": 28.3594,
      "step": 1478
    },
    {
      "epoch": 1.48,
      "grad_norm": 31812.876953125,
      "learning_rate": 9.546439628482973e-05,
      "loss": 33.5411,
      "step": 1479
    },
    {
      "epoch": 1.48,
      "grad_norm": 8327.5615234375,
      "learning_rate": 9.54592363261094e-05,
      "loss": 25.2135,
      "step": 1480
    },
    {
      "epoch": 1.48,
      "grad_norm": 9211.36328125,
      "learning_rate": 9.545407636738907e-05,
      "loss": 21.218,
      "step": 1481
    },
    {
      "epoch": 1.48,
      "grad_norm": 11667.4443359375,
      "learning_rate": 9.544891640866873e-05,
      "loss": 21.0037,
      "step": 1482
    },
    {
      "epoch": 1.48,
      "grad_norm": 3837.482177734375,
      "learning_rate": 9.54437564499484e-05,
      "loss": 13.1742,
      "step": 1483
    },
    {
      "epoch": 1.49,
      "grad_norm": 8181.353515625,
      "learning_rate": 9.543859649122808e-05,
      "loss": 27.2067,
      "step": 1484
    },
    {
      "epoch": 1.49,
      "grad_norm": 14285.783203125,
      "learning_rate": 9.543343653250775e-05,
      "loss": 24.307,
      "step": 1485
    },
    {
      "epoch": 1.49,
      "grad_norm": 2423.17041015625,
      "learning_rate": 9.542827657378741e-05,
      "loss": 14.9423,
      "step": 1486
    },
    {
      "epoch": 1.49,
      "grad_norm": 5734.9375,
      "learning_rate": 9.542311661506709e-05,
      "loss": 16.7905,
      "step": 1487
    },
    {
      "epoch": 1.49,
      "grad_norm": 5196.07958984375,
      "learning_rate": 9.541795665634675e-05,
      "loss": 20.5244,
      "step": 1488
    },
    {
      "epoch": 1.49,
      "grad_norm": 7192.80859375,
      "learning_rate": 9.541279669762642e-05,
      "loss": 20.4981,
      "step": 1489
    },
    {
      "epoch": 1.49,
      "grad_norm": 10873.482421875,
      "learning_rate": 9.54076367389061e-05,
      "loss": 17.207,
      "step": 1490
    },
    {
      "epoch": 1.49,
      "grad_norm": 12445.5302734375,
      "learning_rate": 9.540247678018577e-05,
      "loss": 33.7503,
      "step": 1491
    },
    {
      "epoch": 1.49,
      "grad_norm": 9739.1162109375,
      "learning_rate": 9.539731682146543e-05,
      "loss": 33.4832,
      "step": 1492
    },
    {
      "epoch": 1.49,
      "grad_norm": 46810.0859375,
      "learning_rate": 9.539215686274511e-05,
      "loss": 18.3778,
      "step": 1493
    },
    {
      "epoch": 1.5,
      "grad_norm": 327577.9375,
      "learning_rate": 9.538699690402477e-05,
      "loss": 17.318,
      "step": 1494
    },
    {
      "epoch": 1.5,
      "grad_norm": 27477.67578125,
      "learning_rate": 9.538183694530444e-05,
      "loss": 27.9079,
      "step": 1495
    },
    {
      "epoch": 1.5,
      "grad_norm": 3629.217041015625,
      "learning_rate": 9.537667698658412e-05,
      "loss": 19.416,
      "step": 1496
    },
    {
      "epoch": 1.5,
      "grad_norm": 2617.90478515625,
      "learning_rate": 9.537151702786378e-05,
      "loss": 13.9577,
      "step": 1497
    },
    {
      "epoch": 1.5,
      "grad_norm": 21188.59765625,
      "learning_rate": 9.536635706914345e-05,
      "loss": 22.248,
      "step": 1498
    },
    {
      "epoch": 1.5,
      "grad_norm": 12124.7099609375,
      "learning_rate": 9.536119711042311e-05,
      "loss": 12.29,
      "step": 1499
    },
    {
      "epoch": 1.5,
      "grad_norm": 6680.025390625,
      "learning_rate": 9.535603715170279e-05,
      "loss": 14.1071,
      "step": 1500
    },
    {
      "epoch": 1.5,
      "grad_norm": 7470.80029296875,
      "learning_rate": 9.535087719298246e-05,
      "loss": 20.0464,
      "step": 1501
    },
    {
      "epoch": 1.5,
      "grad_norm": 8333.7548828125,
      "learning_rate": 9.534571723426214e-05,
      "loss": 13.6055,
      "step": 1502
    },
    {
      "epoch": 1.5,
      "grad_norm": 23049.220703125,
      "learning_rate": 9.53405572755418e-05,
      "loss": 19.0828,
      "step": 1503
    },
    {
      "epoch": 1.51,
      "grad_norm": 5570.72265625,
      "learning_rate": 9.533539731682147e-05,
      "loss": 23.6023,
      "step": 1504
    },
    {
      "epoch": 1.51,
      "grad_norm": 11214.51953125,
      "learning_rate": 9.533023735810113e-05,
      "loss": 20.0798,
      "step": 1505
    },
    {
      "epoch": 1.51,
      "grad_norm": 2688.620849609375,
      "learning_rate": 9.532507739938081e-05,
      "loss": 15.0574,
      "step": 1506
    },
    {
      "epoch": 1.51,
      "grad_norm": 12480.4638671875,
      "learning_rate": 9.531991744066048e-05,
      "loss": 27.1888,
      "step": 1507
    },
    {
      "epoch": 1.51,
      "grad_norm": 21871.2421875,
      "learning_rate": 9.531475748194016e-05,
      "loss": 15.8552,
      "step": 1508
    },
    {
      "epoch": 1.51,
      "grad_norm": 13126.064453125,
      "learning_rate": 9.530959752321982e-05,
      "loss": 30.9244,
      "step": 1509
    },
    {
      "epoch": 1.51,
      "grad_norm": 3621.44482421875,
      "learning_rate": 9.53044375644995e-05,
      "loss": 12.7512,
      "step": 1510
    },
    {
      "epoch": 1.51,
      "grad_norm": 5442.39013671875,
      "learning_rate": 9.529927760577915e-05,
      "loss": 15.1832,
      "step": 1511
    },
    {
      "epoch": 1.51,
      "grad_norm": 10300.7177734375,
      "learning_rate": 9.529411764705883e-05,
      "loss": 13.7667,
      "step": 1512
    },
    {
      "epoch": 1.51,
      "grad_norm": 25143.4609375,
      "learning_rate": 9.52889576883385e-05,
      "loss": 37.4904,
      "step": 1513
    },
    {
      "epoch": 1.52,
      "grad_norm": 56416.6796875,
      "learning_rate": 9.528379772961816e-05,
      "loss": 45.982,
      "step": 1514
    },
    {
      "epoch": 1.52,
      "grad_norm": 14755.912109375,
      "learning_rate": 9.527863777089784e-05,
      "loss": 17.807,
      "step": 1515
    },
    {
      "epoch": 1.52,
      "grad_norm": 27864.20703125,
      "learning_rate": 9.52734778121775e-05,
      "loss": 25.0217,
      "step": 1516
    },
    {
      "epoch": 1.52,
      "grad_norm": 19906.587890625,
      "learning_rate": 9.526831785345717e-05,
      "loss": 20.3702,
      "step": 1517
    },
    {
      "epoch": 1.52,
      "grad_norm": 1798.947509765625,
      "learning_rate": 9.526315789473685e-05,
      "loss": 12.5266,
      "step": 1518
    },
    {
      "epoch": 1.52,
      "grad_norm": 24654.482421875,
      "learning_rate": 9.525799793601652e-05,
      "loss": 20.5266,
      "step": 1519
    },
    {
      "epoch": 1.52,
      "grad_norm": 105442.4921875,
      "learning_rate": 9.525283797729618e-05,
      "loss": 29.9187,
      "step": 1520
    },
    {
      "epoch": 1.52,
      "grad_norm": 5294.82275390625,
      "learning_rate": 9.524767801857586e-05,
      "loss": 19.0397,
      "step": 1521
    },
    {
      "epoch": 1.52,
      "grad_norm": 22843.888671875,
      "learning_rate": 9.524251805985552e-05,
      "loss": 21.3019,
      "step": 1522
    },
    {
      "epoch": 1.52,
      "grad_norm": 6423.94677734375,
      "learning_rate": 9.52373581011352e-05,
      "loss": 20.5568,
      "step": 1523
    },
    {
      "epoch": 1.53,
      "grad_norm": 6298.78955078125,
      "learning_rate": 9.523219814241487e-05,
      "loss": 17.6452,
      "step": 1524
    },
    {
      "epoch": 1.53,
      "grad_norm": 5325.67236328125,
      "learning_rate": 9.522703818369454e-05,
      "loss": 25.1074,
      "step": 1525
    },
    {
      "epoch": 1.53,
      "grad_norm": 11374.283203125,
      "learning_rate": 9.52218782249742e-05,
      "loss": 24.322,
      "step": 1526
    },
    {
      "epoch": 1.53,
      "grad_norm": 8351.7861328125,
      "learning_rate": 9.521671826625388e-05,
      "loss": 24.0008,
      "step": 1527
    },
    {
      "epoch": 1.53,
      "grad_norm": 18620.7265625,
      "learning_rate": 9.521155830753354e-05,
      "loss": 16.2279,
      "step": 1528
    },
    {
      "epoch": 1.53,
      "grad_norm": 12630.423828125,
      "learning_rate": 9.520639834881321e-05,
      "loss": 34.4396,
      "step": 1529
    },
    {
      "epoch": 1.53,
      "grad_norm": 9874.892578125,
      "learning_rate": 9.520123839009289e-05,
      "loss": 17.4686,
      "step": 1530
    },
    {
      "epoch": 1.53,
      "grad_norm": 144418.140625,
      "learning_rate": 9.519607843137255e-05,
      "loss": 30.0282,
      "step": 1531
    },
    {
      "epoch": 1.53,
      "grad_norm": 3055.147705078125,
      "learning_rate": 9.519091847265222e-05,
      "loss": 12.991,
      "step": 1532
    },
    {
      "epoch": 1.53,
      "grad_norm": 5446.69580078125,
      "learning_rate": 9.518575851393189e-05,
      "loss": 14.0921,
      "step": 1533
    },
    {
      "epoch": 1.54,
      "grad_norm": 34628.94921875,
      "learning_rate": 9.518059855521156e-05,
      "loss": 24.3776,
      "step": 1534
    },
    {
      "epoch": 1.54,
      "grad_norm": 14384.01171875,
      "learning_rate": 9.517543859649123e-05,
      "loss": 18.2277,
      "step": 1535
    },
    {
      "epoch": 1.54,
      "grad_norm": 5627.67529296875,
      "learning_rate": 9.517027863777091e-05,
      "loss": 21.7844,
      "step": 1536
    },
    {
      "epoch": 1.54,
      "grad_norm": 20679.859375,
      "learning_rate": 9.516511867905057e-05,
      "loss": 26.1565,
      "step": 1537
    },
    {
      "epoch": 1.54,
      "grad_norm": 8181.49169921875,
      "learning_rate": 9.515995872033024e-05,
      "loss": 18.4187,
      "step": 1538
    },
    {
      "epoch": 1.54,
      "grad_norm": 9137.8408203125,
      "learning_rate": 9.51547987616099e-05,
      "loss": 27.8533,
      "step": 1539
    },
    {
      "epoch": 1.54,
      "grad_norm": 85557.1953125,
      "learning_rate": 9.514963880288958e-05,
      "loss": 25.5505,
      "step": 1540
    },
    {
      "epoch": 1.54,
      "grad_norm": 21085.904296875,
      "learning_rate": 9.514447884416925e-05,
      "loss": 26.7082,
      "step": 1541
    },
    {
      "epoch": 1.54,
      "grad_norm": 4991.98193359375,
      "learning_rate": 9.513931888544893e-05,
      "loss": 29.4433,
      "step": 1542
    },
    {
      "epoch": 1.54,
      "grad_norm": 6507.86669921875,
      "learning_rate": 9.513415892672859e-05,
      "loss": 30.6585,
      "step": 1543
    },
    {
      "epoch": 1.55,
      "grad_norm": 4251.10888671875,
      "learning_rate": 9.512899896800826e-05,
      "loss": 16.3254,
      "step": 1544
    },
    {
      "epoch": 1.55,
      "grad_norm": 3517.905517578125,
      "learning_rate": 9.512383900928793e-05,
      "loss": 26.1372,
      "step": 1545
    },
    {
      "epoch": 1.55,
      "grad_norm": 19118.751953125,
      "learning_rate": 9.51186790505676e-05,
      "loss": 21.7536,
      "step": 1546
    },
    {
      "epoch": 1.55,
      "grad_norm": 50194.64453125,
      "learning_rate": 9.511351909184727e-05,
      "loss": 23.5138,
      "step": 1547
    },
    {
      "epoch": 1.55,
      "grad_norm": 769.2783203125,
      "learning_rate": 9.510835913312694e-05,
      "loss": 9.5723,
      "step": 1548
    },
    {
      "epoch": 1.55,
      "grad_norm": 28495.28515625,
      "learning_rate": 9.510319917440661e-05,
      "loss": 25.5967,
      "step": 1549
    },
    {
      "epoch": 1.55,
      "grad_norm": 48903.55859375,
      "learning_rate": 9.509803921568627e-05,
      "loss": 16.89,
      "step": 1550
    },
    {
      "epoch": 1.55,
      "grad_norm": 29003.1875,
      "learning_rate": 9.509287925696595e-05,
      "loss": 36.0721,
      "step": 1551
    },
    {
      "epoch": 1.55,
      "grad_norm": 6118.32373046875,
      "learning_rate": 9.508771929824562e-05,
      "loss": 19.7966,
      "step": 1552
    },
    {
      "epoch": 1.55,
      "grad_norm": 19343.0234375,
      "learning_rate": 9.50825593395253e-05,
      "loss": 25.2122,
      "step": 1553
    },
    {
      "epoch": 1.56,
      "grad_norm": 10497.892578125,
      "learning_rate": 9.507739938080496e-05,
      "loss": 16.2702,
      "step": 1554
    },
    {
      "epoch": 1.56,
      "grad_norm": 5064.25146484375,
      "learning_rate": 9.507223942208463e-05,
      "loss": 22.5173,
      "step": 1555
    },
    {
      "epoch": 1.56,
      "grad_norm": 17709.517578125,
      "learning_rate": 9.506707946336429e-05,
      "loss": 27.63,
      "step": 1556
    },
    {
      "epoch": 1.56,
      "grad_norm": 6811.775390625,
      "learning_rate": 9.506191950464397e-05,
      "loss": 30.1957,
      "step": 1557
    },
    {
      "epoch": 1.56,
      "grad_norm": 9550.609375,
      "learning_rate": 9.505675954592364e-05,
      "loss": 18.3637,
      "step": 1558
    },
    {
      "epoch": 1.56,
      "grad_norm": 14832.0478515625,
      "learning_rate": 9.505159958720331e-05,
      "loss": 30.1394,
      "step": 1559
    },
    {
      "epoch": 1.56,
      "grad_norm": 3718.60693359375,
      "learning_rate": 9.504643962848298e-05,
      "loss": 14.6403,
      "step": 1560
    },
    {
      "epoch": 1.56,
      "grad_norm": 5264.560546875,
      "learning_rate": 9.504127966976265e-05,
      "loss": 12.8047,
      "step": 1561
    },
    {
      "epoch": 1.56,
      "grad_norm": 4429.220703125,
      "learning_rate": 9.503611971104231e-05,
      "loss": 31.8387,
      "step": 1562
    },
    {
      "epoch": 1.56,
      "grad_norm": 4671.8349609375,
      "learning_rate": 9.503095975232199e-05,
      "loss": 28.4838,
      "step": 1563
    },
    {
      "epoch": 1.57,
      "grad_norm": 23964.171875,
      "learning_rate": 9.502579979360166e-05,
      "loss": 26.7902,
      "step": 1564
    },
    {
      "epoch": 1.57,
      "grad_norm": 8549.08203125,
      "learning_rate": 9.502063983488133e-05,
      "loss": 21.7826,
      "step": 1565
    },
    {
      "epoch": 1.57,
      "grad_norm": 12771.466796875,
      "learning_rate": 9.5015479876161e-05,
      "loss": 28.7804,
      "step": 1566
    },
    {
      "epoch": 1.57,
      "grad_norm": 14698.400390625,
      "learning_rate": 9.501031991744066e-05,
      "loss": 16.5306,
      "step": 1567
    },
    {
      "epoch": 1.57,
      "grad_norm": 8957.931640625,
      "learning_rate": 9.500515995872033e-05,
      "loss": 24.7924,
      "step": 1568
    },
    {
      "epoch": 1.57,
      "grad_norm": 35263.57421875,
      "learning_rate": 9.5e-05,
      "loss": 18.5096,
      "step": 1569
    },
    {
      "epoch": 1.57,
      "grad_norm": 59421.62890625,
      "learning_rate": 9.499484004127968e-05,
      "loss": 26.0652,
      "step": 1570
    },
    {
      "epoch": 1.57,
      "grad_norm": 19277.69140625,
      "learning_rate": 9.498968008255934e-05,
      "loss": 22.4645,
      "step": 1571
    },
    {
      "epoch": 1.57,
      "grad_norm": 2728.24462890625,
      "learning_rate": 9.498452012383902e-05,
      "loss": 13.1771,
      "step": 1572
    },
    {
      "epoch": 1.57,
      "grad_norm": 6431.8720703125,
      "learning_rate": 9.497936016511868e-05,
      "loss": 31.1674,
      "step": 1573
    },
    {
      "epoch": 1.58,
      "grad_norm": 9927.8984375,
      "learning_rate": 9.497420020639835e-05,
      "loss": 21.8252,
      "step": 1574
    },
    {
      "epoch": 1.58,
      "grad_norm": 9516.5419921875,
      "learning_rate": 9.496904024767803e-05,
      "loss": 25.8281,
      "step": 1575
    },
    {
      "epoch": 1.58,
      "grad_norm": 6471.7373046875,
      "learning_rate": 9.49638802889577e-05,
      "loss": 21.3841,
      "step": 1576
    },
    {
      "epoch": 1.58,
      "grad_norm": 25718.11328125,
      "learning_rate": 9.495872033023736e-05,
      "loss": 23.848,
      "step": 1577
    },
    {
      "epoch": 1.58,
      "grad_norm": 25637.48828125,
      "learning_rate": 9.495356037151704e-05,
      "loss": 22.8779,
      "step": 1578
    },
    {
      "epoch": 1.58,
      "grad_norm": 11800.9716796875,
      "learning_rate": 9.49484004127967e-05,
      "loss": 21.819,
      "step": 1579
    },
    {
      "epoch": 1.58,
      "grad_norm": 23461.31640625,
      "learning_rate": 9.494324045407637e-05,
      "loss": 23.0255,
      "step": 1580
    },
    {
      "epoch": 1.58,
      "grad_norm": 8910.279296875,
      "learning_rate": 9.493808049535605e-05,
      "loss": 28.8893,
      "step": 1581
    },
    {
      "epoch": 1.58,
      "grad_norm": 29738.681640625,
      "learning_rate": 9.493292053663572e-05,
      "loss": 40.9557,
      "step": 1582
    },
    {
      "epoch": 1.58,
      "grad_norm": 7678.14892578125,
      "learning_rate": 9.492776057791538e-05,
      "loss": 14.6804,
      "step": 1583
    },
    {
      "epoch": 1.59,
      "grad_norm": 5840.5029296875,
      "learning_rate": 9.492260061919504e-05,
      "loss": 12.0081,
      "step": 1584
    },
    {
      "epoch": 1.59,
      "grad_norm": 3929.01171875,
      "learning_rate": 9.491744066047472e-05,
      "loss": 11.2461,
      "step": 1585
    },
    {
      "epoch": 1.59,
      "grad_norm": 11074.595703125,
      "learning_rate": 9.491228070175439e-05,
      "loss": 27.4702,
      "step": 1586
    },
    {
      "epoch": 1.59,
      "grad_norm": 8574.9453125,
      "learning_rate": 9.490712074303407e-05,
      "loss": 14.336,
      "step": 1587
    },
    {
      "epoch": 1.59,
      "grad_norm": 9742.8271484375,
      "learning_rate": 9.490196078431373e-05,
      "loss": 21.1385,
      "step": 1588
    },
    {
      "epoch": 1.59,
      "grad_norm": 3774.347412109375,
      "learning_rate": 9.48968008255934e-05,
      "loss": 14.4107,
      "step": 1589
    },
    {
      "epoch": 1.59,
      "grad_norm": 38531.44140625,
      "learning_rate": 9.489164086687306e-05,
      "loss": 24.2144,
      "step": 1590
    },
    {
      "epoch": 1.59,
      "grad_norm": 1929.0001220703125,
      "learning_rate": 9.488648090815274e-05,
      "loss": 16.3477,
      "step": 1591
    },
    {
      "epoch": 1.59,
      "grad_norm": 5606.197265625,
      "learning_rate": 9.488132094943241e-05,
      "loss": 20.7329,
      "step": 1592
    },
    {
      "epoch": 1.59,
      "grad_norm": 11509.388671875,
      "learning_rate": 9.487616099071209e-05,
      "loss": 36.2983,
      "step": 1593
    },
    {
      "epoch": 1.6,
      "grad_norm": 5484.34423828125,
      "learning_rate": 9.487100103199175e-05,
      "loss": 35.2518,
      "step": 1594
    },
    {
      "epoch": 1.6,
      "grad_norm": 6746.67626953125,
      "learning_rate": 9.486584107327142e-05,
      "loss": 25.7581,
      "step": 1595
    },
    {
      "epoch": 1.6,
      "grad_norm": 3753.5078125,
      "learning_rate": 9.486068111455108e-05,
      "loss": 22.0801,
      "step": 1596
    },
    {
      "epoch": 1.6,
      "grad_norm": 17324.982421875,
      "learning_rate": 9.485552115583076e-05,
      "loss": 37.2995,
      "step": 1597
    },
    {
      "epoch": 1.6,
      "grad_norm": 9625.33203125,
      "learning_rate": 9.485036119711043e-05,
      "loss": 19.0268,
      "step": 1598
    },
    {
      "epoch": 1.6,
      "grad_norm": 4582.39306640625,
      "learning_rate": 9.48452012383901e-05,
      "loss": 13.8737,
      "step": 1599
    },
    {
      "epoch": 1.6,
      "grad_norm": 8538.130859375,
      "learning_rate": 9.484004127966977e-05,
      "loss": 21.4504,
      "step": 1600
    },
    {
      "epoch": 1.6,
      "grad_norm": 6688.74609375,
      "learning_rate": 9.483488132094944e-05,
      "loss": 15.0279,
      "step": 1601
    },
    {
      "epoch": 1.6,
      "grad_norm": 52879.83203125,
      "learning_rate": 9.48297213622291e-05,
      "loss": 29.2863,
      "step": 1602
    },
    {
      "epoch": 1.6,
      "grad_norm": 3623.87890625,
      "learning_rate": 9.482456140350878e-05,
      "loss": 16.6914,
      "step": 1603
    },
    {
      "epoch": 1.61,
      "grad_norm": 6476.82568359375,
      "learning_rate": 9.481940144478845e-05,
      "loss": 15.5003,
      "step": 1604
    },
    {
      "epoch": 1.61,
      "grad_norm": 15393.40625,
      "learning_rate": 9.481424148606811e-05,
      "loss": 28.4389,
      "step": 1605
    },
    {
      "epoch": 1.61,
      "grad_norm": 5125.7998046875,
      "learning_rate": 9.480908152734779e-05,
      "loss": 13.3308,
      "step": 1606
    },
    {
      "epoch": 1.61,
      "grad_norm": 12404.9091796875,
      "learning_rate": 9.480392156862745e-05,
      "loss": 24.2906,
      "step": 1607
    },
    {
      "epoch": 1.61,
      "grad_norm": 57594.515625,
      "learning_rate": 9.479876160990712e-05,
      "loss": 28.4236,
      "step": 1608
    },
    {
      "epoch": 1.61,
      "grad_norm": 14705.9990234375,
      "learning_rate": 9.47936016511868e-05,
      "loss": 21.3482,
      "step": 1609
    },
    {
      "epoch": 1.61,
      "grad_norm": 6107.99169921875,
      "learning_rate": 9.478844169246647e-05,
      "loss": 25.0172,
      "step": 1610
    },
    {
      "epoch": 1.61,
      "grad_norm": 16324.0986328125,
      "learning_rate": 9.478328173374613e-05,
      "loss": 40.5671,
      "step": 1611
    },
    {
      "epoch": 1.61,
      "grad_norm": 46760.9140625,
      "learning_rate": 9.47781217750258e-05,
      "loss": 28.1929,
      "step": 1612
    },
    {
      "epoch": 1.61,
      "grad_norm": 175549.71875,
      "learning_rate": 9.477296181630547e-05,
      "loss": 37.4384,
      "step": 1613
    },
    {
      "epoch": 1.62,
      "grad_norm": 90881.234375,
      "learning_rate": 9.476780185758514e-05,
      "loss": 34.1579,
      "step": 1614
    },
    {
      "epoch": 1.62,
      "grad_norm": 18427.458984375,
      "learning_rate": 9.476264189886482e-05,
      "loss": 35.8353,
      "step": 1615
    },
    {
      "epoch": 1.62,
      "grad_norm": 27459.744140625,
      "learning_rate": 9.475748194014449e-05,
      "loss": 27.2289,
      "step": 1616
    },
    {
      "epoch": 1.62,
      "grad_norm": 6251.88232421875,
      "learning_rate": 9.475232198142415e-05,
      "loss": 20.4156,
      "step": 1617
    },
    {
      "epoch": 1.62,
      "grad_norm": 68111.75,
      "learning_rate": 9.474716202270383e-05,
      "loss": 41.3136,
      "step": 1618
    },
    {
      "epoch": 1.62,
      "grad_norm": 5992.89111328125,
      "learning_rate": 9.474200206398349e-05,
      "loss": 18.9696,
      "step": 1619
    },
    {
      "epoch": 1.62,
      "grad_norm": 13628.4365234375,
      "learning_rate": 9.473684210526316e-05,
      "loss": 20.7691,
      "step": 1620
    },
    {
      "epoch": 1.62,
      "grad_norm": 21096.669921875,
      "learning_rate": 9.473168214654284e-05,
      "loss": 22.5254,
      "step": 1621
    },
    {
      "epoch": 1.62,
      "grad_norm": 58914.671875,
      "learning_rate": 9.47265221878225e-05,
      "loss": 20.609,
      "step": 1622
    },
    {
      "epoch": 1.62,
      "grad_norm": 22454.068359375,
      "learning_rate": 9.472136222910217e-05,
      "loss": 38.4667,
      "step": 1623
    },
    {
      "epoch": 1.63,
      "grad_norm": 120196.7734375,
      "learning_rate": 9.471620227038183e-05,
      "loss": 32.8527,
      "step": 1624
    },
    {
      "epoch": 1.63,
      "grad_norm": 11888.79296875,
      "learning_rate": 9.471104231166151e-05,
      "loss": 15.4714,
      "step": 1625
    },
    {
      "epoch": 1.63,
      "grad_norm": 9337.7275390625,
      "learning_rate": 9.470588235294118e-05,
      "loss": 27.6206,
      "step": 1626
    },
    {
      "epoch": 1.63,
      "grad_norm": 6132.49560546875,
      "learning_rate": 9.470072239422086e-05,
      "loss": 12.9337,
      "step": 1627
    },
    {
      "epoch": 1.63,
      "grad_norm": 22634.21875,
      "learning_rate": 9.469556243550052e-05,
      "loss": 25.3693,
      "step": 1628
    },
    {
      "epoch": 1.63,
      "grad_norm": 4656.66455078125,
      "learning_rate": 9.469040247678019e-05,
      "loss": 21.1585,
      "step": 1629
    },
    {
      "epoch": 1.63,
      "grad_norm": 3415.726806640625,
      "learning_rate": 9.468524251805985e-05,
      "loss": 30.0913,
      "step": 1630
    },
    {
      "epoch": 1.63,
      "grad_norm": 43863.9296875,
      "learning_rate": 9.468008255933953e-05,
      "loss": 21.3648,
      "step": 1631
    },
    {
      "epoch": 1.63,
      "grad_norm": 12062.810546875,
      "learning_rate": 9.46749226006192e-05,
      "loss": 16.0035,
      "step": 1632
    },
    {
      "epoch": 1.63,
      "grad_norm": 3302.196044921875,
      "learning_rate": 9.466976264189888e-05,
      "loss": 15.9173,
      "step": 1633
    },
    {
      "epoch": 1.64,
      "grad_norm": 63974.48828125,
      "learning_rate": 9.466460268317854e-05,
      "loss": 17.5002,
      "step": 1634
    },
    {
      "epoch": 1.64,
      "grad_norm": 9876.1171875,
      "learning_rate": 9.465944272445821e-05,
      "loss": 37.8662,
      "step": 1635
    },
    {
      "epoch": 1.64,
      "grad_norm": 10905.9951171875,
      "learning_rate": 9.465428276573787e-05,
      "loss": 28.4242,
      "step": 1636
    },
    {
      "epoch": 1.64,
      "grad_norm": 6691.81103515625,
      "learning_rate": 9.464912280701755e-05,
      "loss": 23.3343,
      "step": 1637
    },
    {
      "epoch": 1.64,
      "grad_norm": 10862.859375,
      "learning_rate": 9.464396284829722e-05,
      "loss": 17.8153,
      "step": 1638
    },
    {
      "epoch": 1.64,
      "grad_norm": 12450.728515625,
      "learning_rate": 9.463880288957688e-05,
      "loss": 25.5918,
      "step": 1639
    },
    {
      "epoch": 1.64,
      "grad_norm": 6297.634765625,
      "learning_rate": 9.463364293085656e-05,
      "loss": 18.1164,
      "step": 1640
    },
    {
      "epoch": 1.64,
      "grad_norm": 53082.4140625,
      "learning_rate": 9.462848297213622e-05,
      "loss": 35.247,
      "step": 1641
    },
    {
      "epoch": 1.64,
      "grad_norm": 13677.771484375,
      "learning_rate": 9.462332301341589e-05,
      "loss": 18.4006,
      "step": 1642
    },
    {
      "epoch": 1.64,
      "grad_norm": 13731.017578125,
      "learning_rate": 9.461816305469557e-05,
      "loss": 20.2265,
      "step": 1643
    },
    {
      "epoch": 1.65,
      "grad_norm": 5531.9951171875,
      "learning_rate": 9.461300309597524e-05,
      "loss": 17.9697,
      "step": 1644
    },
    {
      "epoch": 1.65,
      "grad_norm": 43250.07421875,
      "learning_rate": 9.46078431372549e-05,
      "loss": 21.3365,
      "step": 1645
    },
    {
      "epoch": 1.65,
      "grad_norm": 21694.458984375,
      "learning_rate": 9.460268317853458e-05,
      "loss": 35.7604,
      "step": 1646
    },
    {
      "epoch": 1.65,
      "grad_norm": 405725.5625,
      "learning_rate": 9.459752321981424e-05,
      "loss": 38.0233,
      "step": 1647
    },
    {
      "epoch": 1.65,
      "grad_norm": 3764.156494140625,
      "learning_rate": 9.459236326109391e-05,
      "loss": 15.1794,
      "step": 1648
    },
    {
      "epoch": 1.65,
      "grad_norm": 4427.125,
      "learning_rate": 9.458720330237359e-05,
      "loss": 14.1851,
      "step": 1649
    },
    {
      "epoch": 1.65,
      "grad_norm": 7794.55419921875,
      "learning_rate": 9.458204334365326e-05,
      "loss": 38.7849,
      "step": 1650
    },
    {
      "epoch": 1.65,
      "grad_norm": 26064.263671875,
      "learning_rate": 9.457688338493292e-05,
      "loss": 21.9216,
      "step": 1651
    },
    {
      "epoch": 1.65,
      "grad_norm": 4307.99755859375,
      "learning_rate": 9.45717234262126e-05,
      "loss": 14.2991,
      "step": 1652
    },
    {
      "epoch": 1.65,
      "grad_norm": 12760.1318359375,
      "learning_rate": 9.456656346749226e-05,
      "loss": 33.2984,
      "step": 1653
    },
    {
      "epoch": 1.66,
      "grad_norm": 8021.36181640625,
      "learning_rate": 9.456140350877195e-05,
      "loss": 12.0265,
      "step": 1654
    },
    {
      "epoch": 1.66,
      "grad_norm": 39646.56640625,
      "learning_rate": 9.455624355005161e-05,
      "loss": 28.2588,
      "step": 1655
    },
    {
      "epoch": 1.66,
      "grad_norm": 5401.85107421875,
      "learning_rate": 9.455108359133127e-05,
      "loss": 18.1148,
      "step": 1656
    },
    {
      "epoch": 1.66,
      "grad_norm": 4361.4365234375,
      "learning_rate": 9.454592363261094e-05,
      "loss": 16.0835,
      "step": 1657
    },
    {
      "epoch": 1.66,
      "grad_norm": 2860.120361328125,
      "learning_rate": 9.45407636738906e-05,
      "loss": 25.0733,
      "step": 1658
    },
    {
      "epoch": 1.66,
      "grad_norm": 9021.1787109375,
      "learning_rate": 9.453560371517028e-05,
      "loss": 23.2861,
      "step": 1659
    },
    {
      "epoch": 1.66,
      "grad_norm": 14896.623046875,
      "learning_rate": 9.453044375644995e-05,
      "loss": 34.9936,
      "step": 1660
    },
    {
      "epoch": 1.66,
      "grad_norm": 19686.298828125,
      "learning_rate": 9.452528379772963e-05,
      "loss": 25.1317,
      "step": 1661
    },
    {
      "epoch": 1.66,
      "grad_norm": 2125.57177734375,
      "learning_rate": 9.452012383900929e-05,
      "loss": 13.2593,
      "step": 1662
    },
    {
      "epoch": 1.66,
      "grad_norm": 9653.5234375,
      "learning_rate": 9.451496388028896e-05,
      "loss": 23.1584,
      "step": 1663
    },
    {
      "epoch": 1.67,
      "grad_norm": 7495.412109375,
      "learning_rate": 9.450980392156862e-05,
      "loss": 15.5405,
      "step": 1664
    },
    {
      "epoch": 1.67,
      "grad_norm": 8624.4443359375,
      "learning_rate": 9.45046439628483e-05,
      "loss": 17.9655,
      "step": 1665
    },
    {
      "epoch": 1.67,
      "grad_norm": 5935.10693359375,
      "learning_rate": 9.449948400412797e-05,
      "loss": 20.5429,
      "step": 1666
    },
    {
      "epoch": 1.67,
      "grad_norm": 11124.5146484375,
      "learning_rate": 9.449432404540765e-05,
      "loss": 19.0607,
      "step": 1667
    },
    {
      "epoch": 1.67,
      "grad_norm": 2693.15087890625,
      "learning_rate": 9.448916408668731e-05,
      "loss": 13.2806,
      "step": 1668
    },
    {
      "epoch": 1.67,
      "grad_norm": 8018.2158203125,
      "learning_rate": 9.448400412796698e-05,
      "loss": 25.9771,
      "step": 1669
    },
    {
      "epoch": 1.67,
      "grad_norm": 16249.03515625,
      "learning_rate": 9.447884416924664e-05,
      "loss": 25.7049,
      "step": 1670
    },
    {
      "epoch": 1.67,
      "grad_norm": 12417.9638671875,
      "learning_rate": 9.447368421052633e-05,
      "loss": 20.4468,
      "step": 1671
    },
    {
      "epoch": 1.67,
      "grad_norm": 5838.14599609375,
      "learning_rate": 9.446852425180599e-05,
      "loss": 25.451,
      "step": 1672
    },
    {
      "epoch": 1.67,
      "grad_norm": 6532.25244140625,
      "learning_rate": 9.446336429308565e-05,
      "loss": 37.9018,
      "step": 1673
    },
    {
      "epoch": 1.68,
      "grad_norm": 2485.35888671875,
      "learning_rate": 9.445820433436533e-05,
      "loss": 24.826,
      "step": 1674
    },
    {
      "epoch": 1.68,
      "grad_norm": 1902.748779296875,
      "learning_rate": 9.445304437564499e-05,
      "loss": 12.0242,
      "step": 1675
    },
    {
      "epoch": 1.68,
      "grad_norm": 10953.8916015625,
      "learning_rate": 9.444788441692466e-05,
      "loss": 22.2434,
      "step": 1676
    },
    {
      "epoch": 1.68,
      "grad_norm": 4023.830078125,
      "learning_rate": 9.444272445820434e-05,
      "loss": 25.7209,
      "step": 1677
    },
    {
      "epoch": 1.68,
      "grad_norm": 27699.078125,
      "learning_rate": 9.443756449948401e-05,
      "loss": 23.8713,
      "step": 1678
    },
    {
      "epoch": 1.68,
      "grad_norm": 10522.5703125,
      "learning_rate": 9.443240454076367e-05,
      "loss": 21.0272,
      "step": 1679
    },
    {
      "epoch": 1.68,
      "grad_norm": 1961.5614013671875,
      "learning_rate": 9.442724458204335e-05,
      "loss": 16.2031,
      "step": 1680
    },
    {
      "epoch": 1.68,
      "grad_norm": 5110.56494140625,
      "learning_rate": 9.442208462332301e-05,
      "loss": 11.7613,
      "step": 1681
    },
    {
      "epoch": 1.68,
      "grad_norm": 34261.93359375,
      "learning_rate": 9.44169246646027e-05,
      "loss": 27.0182,
      "step": 1682
    },
    {
      "epoch": 1.68,
      "grad_norm": 31521.482421875,
      "learning_rate": 9.441176470588236e-05,
      "loss": 18.803,
      "step": 1683
    },
    {
      "epoch": 1.69,
      "grad_norm": 3985.91650390625,
      "learning_rate": 9.440660474716203e-05,
      "loss": 15.2702,
      "step": 1684
    },
    {
      "epoch": 1.69,
      "grad_norm": 4941.57763671875,
      "learning_rate": 9.44014447884417e-05,
      "loss": 13.0811,
      "step": 1685
    },
    {
      "epoch": 1.69,
      "grad_norm": 29256.3515625,
      "learning_rate": 9.439628482972137e-05,
      "loss": 29.5561,
      "step": 1686
    },
    {
      "epoch": 1.69,
      "grad_norm": 111378.8984375,
      "learning_rate": 9.439112487100103e-05,
      "loss": 25.8175,
      "step": 1687
    },
    {
      "epoch": 1.69,
      "grad_norm": 15905.01953125,
      "learning_rate": 9.438596491228072e-05,
      "loss": 15.4978,
      "step": 1688
    },
    {
      "epoch": 1.69,
      "grad_norm": 6023.3154296875,
      "learning_rate": 9.438080495356038e-05,
      "loss": 21.1799,
      "step": 1689
    },
    {
      "epoch": 1.69,
      "grad_norm": 9606.3564453125,
      "learning_rate": 9.437564499484005e-05,
      "loss": 27.6083,
      "step": 1690
    },
    {
      "epoch": 1.69,
      "grad_norm": 27848.4765625,
      "learning_rate": 9.437048503611971e-05,
      "loss": 25.1268,
      "step": 1691
    },
    {
      "epoch": 1.69,
      "grad_norm": 5631.7919921875,
      "learning_rate": 9.436532507739937e-05,
      "loss": 19.4289,
      "step": 1692
    },
    {
      "epoch": 1.69,
      "grad_norm": 17069.6484375,
      "learning_rate": 9.436016511867905e-05,
      "loss": 30.8024,
      "step": 1693
    },
    {
      "epoch": 1.7,
      "grad_norm": 77477.0625,
      "learning_rate": 9.435500515995872e-05,
      "loss": 34.8096,
      "step": 1694
    },
    {
      "epoch": 1.7,
      "grad_norm": 13139.4482421875,
      "learning_rate": 9.43498452012384e-05,
      "loss": 25.1844,
      "step": 1695
    },
    {
      "epoch": 1.7,
      "grad_norm": 5806.6513671875,
      "learning_rate": 9.434468524251806e-05,
      "loss": 37.1038,
      "step": 1696
    },
    {
      "epoch": 1.7,
      "grad_norm": 87818.5703125,
      "learning_rate": 9.433952528379773e-05,
      "loss": 27.8204,
      "step": 1697
    },
    {
      "epoch": 1.7,
      "grad_norm": 5329.908203125,
      "learning_rate": 9.43343653250774e-05,
      "loss": 17.9117,
      "step": 1698
    },
    {
      "epoch": 1.7,
      "grad_norm": 11289.513671875,
      "learning_rate": 9.432920536635708e-05,
      "loss": 15.9548,
      "step": 1699
    },
    {
      "epoch": 1.7,
      "grad_norm": 3270.518310546875,
      "learning_rate": 9.432404540763674e-05,
      "loss": 16.4776,
      "step": 1700
    },
    {
      "epoch": 1.7,
      "grad_norm": 11145.322265625,
      "learning_rate": 9.431888544891642e-05,
      "loss": 17.1681,
      "step": 1701
    },
    {
      "epoch": 1.7,
      "grad_norm": 12363.6474609375,
      "learning_rate": 9.431372549019608e-05,
      "loss": 20.8991,
      "step": 1702
    },
    {
      "epoch": 1.7,
      "grad_norm": 16099.0859375,
      "learning_rate": 9.430856553147575e-05,
      "loss": 39.1694,
      "step": 1703
    },
    {
      "epoch": 1.71,
      "grad_norm": 8485.16015625,
      "learning_rate": 9.430340557275541e-05,
      "loss": 33.4298,
      "step": 1704
    },
    {
      "epoch": 1.71,
      "grad_norm": 11541.109375,
      "learning_rate": 9.42982456140351e-05,
      "loss": 15.6428,
      "step": 1705
    },
    {
      "epoch": 1.71,
      "grad_norm": 6790.130859375,
      "learning_rate": 9.429308565531476e-05,
      "loss": 18.6314,
      "step": 1706
    },
    {
      "epoch": 1.71,
      "grad_norm": 6153.04296875,
      "learning_rate": 9.428792569659444e-05,
      "loss": 19.128,
      "step": 1707
    },
    {
      "epoch": 1.71,
      "grad_norm": 3516.767333984375,
      "learning_rate": 9.42827657378741e-05,
      "loss": 12.8624,
      "step": 1708
    },
    {
      "epoch": 1.71,
      "grad_norm": 10487.83984375,
      "learning_rate": 9.427760577915376e-05,
      "loss": 35.0137,
      "step": 1709
    },
    {
      "epoch": 1.71,
      "grad_norm": 9499.8740234375,
      "learning_rate": 9.427244582043345e-05,
      "loss": 24.4511,
      "step": 1710
    },
    {
      "epoch": 1.71,
      "grad_norm": 8713.0224609375,
      "learning_rate": 9.426728586171311e-05,
      "loss": 19.7854,
      "step": 1711
    },
    {
      "epoch": 1.71,
      "grad_norm": 7491.75439453125,
      "learning_rate": 9.426212590299278e-05,
      "loss": 17.7134,
      "step": 1712
    },
    {
      "epoch": 1.71,
      "grad_norm": 24660.0390625,
      "learning_rate": 9.425696594427244e-05,
      "loss": 30.2709,
      "step": 1713
    },
    {
      "epoch": 1.72,
      "grad_norm": 10819.09375,
      "learning_rate": 9.425180598555212e-05,
      "loss": 16.1434,
      "step": 1714
    },
    {
      "epoch": 1.72,
      "grad_norm": 5346.36328125,
      "learning_rate": 9.424664602683178e-05,
      "loss": 15.6259,
      "step": 1715
    },
    {
      "epoch": 1.72,
      "grad_norm": 6424.2529296875,
      "learning_rate": 9.424148606811147e-05,
      "loss": 24.9076,
      "step": 1716
    },
    {
      "epoch": 1.72,
      "grad_norm": 18281.775390625,
      "learning_rate": 9.423632610939113e-05,
      "loss": 21.5837,
      "step": 1717
    },
    {
      "epoch": 1.72,
      "grad_norm": 7627.673828125,
      "learning_rate": 9.42311661506708e-05,
      "loss": 28.9594,
      "step": 1718
    },
    {
      "epoch": 1.72,
      "grad_norm": 40754.2890625,
      "learning_rate": 9.422600619195046e-05,
      "loss": 20.6882,
      "step": 1719
    },
    {
      "epoch": 1.72,
      "grad_norm": 2745.489990234375,
      "learning_rate": 9.422084623323014e-05,
      "loss": 14.1803,
      "step": 1720
    },
    {
      "epoch": 1.72,
      "grad_norm": 6806.20263671875,
      "learning_rate": 9.42156862745098e-05,
      "loss": 31.5859,
      "step": 1721
    },
    {
      "epoch": 1.72,
      "grad_norm": 17053.041015625,
      "learning_rate": 9.421052631578949e-05,
      "loss": 19.5144,
      "step": 1722
    },
    {
      "epoch": 1.72,
      "grad_norm": 3617.32666015625,
      "learning_rate": 9.420536635706915e-05,
      "loss": 17.9913,
      "step": 1723
    },
    {
      "epoch": 1.73,
      "grad_norm": 3369.964599609375,
      "learning_rate": 9.420020639834882e-05,
      "loss": 11.5025,
      "step": 1724
    },
    {
      "epoch": 1.73,
      "grad_norm": 35956.0078125,
      "learning_rate": 9.419504643962848e-05,
      "loss": 26.8912,
      "step": 1725
    },
    {
      "epoch": 1.73,
      "grad_norm": 9828.162109375,
      "learning_rate": 9.418988648090816e-05,
      "loss": 21.2479,
      "step": 1726
    },
    {
      "epoch": 1.73,
      "grad_norm": 6503.2646484375,
      "learning_rate": 9.418472652218783e-05,
      "loss": 19.3375,
      "step": 1727
    },
    {
      "epoch": 1.73,
      "grad_norm": 11823.353515625,
      "learning_rate": 9.41795665634675e-05,
      "loss": 22.96,
      "step": 1728
    },
    {
      "epoch": 1.73,
      "grad_norm": 10333.73828125,
      "learning_rate": 9.417440660474717e-05,
      "loss": 19.2462,
      "step": 1729
    },
    {
      "epoch": 1.73,
      "grad_norm": 16909.599609375,
      "learning_rate": 9.416924664602683e-05,
      "loss": 32.4645,
      "step": 1730
    },
    {
      "epoch": 1.73,
      "grad_norm": 5070.7412109375,
      "learning_rate": 9.41640866873065e-05,
      "loss": 19.4134,
      "step": 1731
    },
    {
      "epoch": 1.73,
      "grad_norm": 9244.470703125,
      "learning_rate": 9.415892672858617e-05,
      "loss": 18.2708,
      "step": 1732
    },
    {
      "epoch": 1.73,
      "grad_norm": 7648.20361328125,
      "learning_rate": 9.415376676986585e-05,
      "loss": 18.8692,
      "step": 1733
    },
    {
      "epoch": 1.74,
      "grad_norm": 9683.625,
      "learning_rate": 9.414860681114551e-05,
      "loss": 18.5384,
      "step": 1734
    },
    {
      "epoch": 1.74,
      "grad_norm": 21227.95703125,
      "learning_rate": 9.414344685242519e-05,
      "loss": 20.8224,
      "step": 1735
    },
    {
      "epoch": 1.74,
      "grad_norm": 7651.02587890625,
      "learning_rate": 9.413828689370485e-05,
      "loss": 20.9576,
      "step": 1736
    },
    {
      "epoch": 1.74,
      "grad_norm": 5063.43701171875,
      "learning_rate": 9.413312693498452e-05,
      "loss": 21.9207,
      "step": 1737
    },
    {
      "epoch": 1.74,
      "grad_norm": 17529.265625,
      "learning_rate": 9.41279669762642e-05,
      "loss": 25.3584,
      "step": 1738
    },
    {
      "epoch": 1.74,
      "grad_norm": 18662.591796875,
      "learning_rate": 9.412280701754387e-05,
      "loss": 13.9485,
      "step": 1739
    },
    {
      "epoch": 1.74,
      "grad_norm": 23661.875,
      "learning_rate": 9.411764705882353e-05,
      "loss": 26.6345,
      "step": 1740
    },
    {
      "epoch": 1.74,
      "grad_norm": 35186.02734375,
      "learning_rate": 9.411248710010321e-05,
      "loss": 15.3045,
      "step": 1741
    },
    {
      "epoch": 1.74,
      "grad_norm": 8878.1142578125,
      "learning_rate": 9.410732714138287e-05,
      "loss": 22.7667,
      "step": 1742
    },
    {
      "epoch": 1.74,
      "grad_norm": 4300.14697265625,
      "learning_rate": 9.410216718266254e-05,
      "loss": 19.4278,
      "step": 1743
    },
    {
      "epoch": 1.75,
      "grad_norm": 5761.54638671875,
      "learning_rate": 9.409700722394222e-05,
      "loss": 21.8206,
      "step": 1744
    },
    {
      "epoch": 1.75,
      "grad_norm": 10139.4697265625,
      "learning_rate": 9.409184726522188e-05,
      "loss": 18.2809,
      "step": 1745
    },
    {
      "epoch": 1.75,
      "grad_norm": 2048.211669921875,
      "learning_rate": 9.408668730650155e-05,
      "loss": 14.9004,
      "step": 1746
    },
    {
      "epoch": 1.75,
      "grad_norm": 9099.341796875,
      "learning_rate": 9.408152734778122e-05,
      "loss": 24.4753,
      "step": 1747
    },
    {
      "epoch": 1.75,
      "grad_norm": 3661.260009765625,
      "learning_rate": 9.407636738906089e-05,
      "loss": 11.4021,
      "step": 1748
    },
    {
      "epoch": 1.75,
      "grad_norm": 15183.77734375,
      "learning_rate": 9.407120743034055e-05,
      "loss": 33.7601,
      "step": 1749
    },
    {
      "epoch": 1.75,
      "grad_norm": 33218.51953125,
      "learning_rate": 9.406604747162024e-05,
      "loss": 22.2353,
      "step": 1750
    },
    {
      "epoch": 1.75,
      "grad_norm": 34402.25,
      "learning_rate": 9.40608875128999e-05,
      "loss": 34.9708,
      "step": 1751
    },
    {
      "epoch": 1.75,
      "grad_norm": 7522.72216796875,
      "learning_rate": 9.405572755417957e-05,
      "loss": 24.4065,
      "step": 1752
    },
    {
      "epoch": 1.75,
      "grad_norm": 2910.5859375,
      "learning_rate": 9.405056759545924e-05,
      "loss": 26.9278,
      "step": 1753
    },
    {
      "epoch": 1.76,
      "grad_norm": 20722.474609375,
      "learning_rate": 9.404540763673891e-05,
      "loss": 21.1495,
      "step": 1754
    },
    {
      "epoch": 1.76,
      "grad_norm": 24842.71875,
      "learning_rate": 9.404024767801858e-05,
      "loss": 18.1669,
      "step": 1755
    },
    {
      "epoch": 1.76,
      "grad_norm": 14471.5048828125,
      "learning_rate": 9.403508771929826e-05,
      "loss": 15.4344,
      "step": 1756
    },
    {
      "epoch": 1.76,
      "grad_norm": 48133.36328125,
      "learning_rate": 9.402992776057792e-05,
      "loss": 18.3023,
      "step": 1757
    },
    {
      "epoch": 1.76,
      "grad_norm": 4781.267578125,
      "learning_rate": 9.40247678018576e-05,
      "loss": 16.0904,
      "step": 1758
    },
    {
      "epoch": 1.76,
      "grad_norm": 6724.9638671875,
      "learning_rate": 9.401960784313726e-05,
      "loss": 17.7231,
      "step": 1759
    },
    {
      "epoch": 1.76,
      "grad_norm": 39282.26953125,
      "learning_rate": 9.401444788441693e-05,
      "loss": 33.6238,
      "step": 1760
    },
    {
      "epoch": 1.76,
      "grad_norm": 9251.4111328125,
      "learning_rate": 9.40092879256966e-05,
      "loss": 16.3879,
      "step": 1761
    },
    {
      "epoch": 1.76,
      "grad_norm": 102371.6640625,
      "learning_rate": 9.400412796697628e-05,
      "loss": 22.6543,
      "step": 1762
    },
    {
      "epoch": 1.76,
      "grad_norm": 16616.38671875,
      "learning_rate": 9.399896800825594e-05,
      "loss": 24.3816,
      "step": 1763
    },
    {
      "epoch": 1.77,
      "grad_norm": 7147.7685546875,
      "learning_rate": 9.39938080495356e-05,
      "loss": 22.2282,
      "step": 1764
    },
    {
      "epoch": 1.77,
      "grad_norm": 49306.2734375,
      "learning_rate": 9.398864809081528e-05,
      "loss": 18.6361,
      "step": 1765
    },
    {
      "epoch": 1.77,
      "grad_norm": 1970.32470703125,
      "learning_rate": 9.398348813209494e-05,
      "loss": 25.9773,
      "step": 1766
    },
    {
      "epoch": 1.77,
      "grad_norm": 67042.125,
      "learning_rate": 9.397832817337462e-05,
      "loss": 19.3182,
      "step": 1767
    },
    {
      "epoch": 1.77,
      "grad_norm": 19160.13671875,
      "learning_rate": 9.397316821465429e-05,
      "loss": 21.6397,
      "step": 1768
    },
    {
      "epoch": 1.77,
      "grad_norm": 17327.45703125,
      "learning_rate": 9.396800825593396e-05,
      "loss": 16.2958,
      "step": 1769
    },
    {
      "epoch": 1.77,
      "grad_norm": 14018.4296875,
      "learning_rate": 9.396284829721362e-05,
      "loss": 22.745,
      "step": 1770
    },
    {
      "epoch": 1.77,
      "grad_norm": 10211.412109375,
      "learning_rate": 9.39576883384933e-05,
      "loss": 25.6631,
      "step": 1771
    },
    {
      "epoch": 1.77,
      "grad_norm": 28695.455078125,
      "learning_rate": 9.395252837977297e-05,
      "loss": 18.7358,
      "step": 1772
    },
    {
      "epoch": 1.77,
      "grad_norm": 4372.0029296875,
      "learning_rate": 9.394736842105264e-05,
      "loss": 17.15,
      "step": 1773
    },
    {
      "epoch": 1.78,
      "grad_norm": 2468.404541015625,
      "learning_rate": 9.39422084623323e-05,
      "loss": 12.308,
      "step": 1774
    },
    {
      "epoch": 1.78,
      "grad_norm": 5297.04248046875,
      "learning_rate": 9.393704850361198e-05,
      "loss": 16.8836,
      "step": 1775
    },
    {
      "epoch": 1.78,
      "grad_norm": 75528.3125,
      "learning_rate": 9.393188854489164e-05,
      "loss": 18.9905,
      "step": 1776
    },
    {
      "epoch": 1.78,
      "grad_norm": 9671.10546875,
      "learning_rate": 9.392672858617132e-05,
      "loss": 19.8435,
      "step": 1777
    },
    {
      "epoch": 1.78,
      "grad_norm": 11732.6611328125,
      "learning_rate": 9.392156862745099e-05,
      "loss": 14.4342,
      "step": 1778
    },
    {
      "epoch": 1.78,
      "grad_norm": 2513.82763671875,
      "learning_rate": 9.391640866873066e-05,
      "loss": 13.6419,
      "step": 1779
    },
    {
      "epoch": 1.78,
      "grad_norm": 93312.8046875,
      "learning_rate": 9.391124871001033e-05,
      "loss": 22.0872,
      "step": 1780
    },
    {
      "epoch": 1.78,
      "grad_norm": 7510.2255859375,
      "learning_rate": 9.390608875128999e-05,
      "loss": 14.2624,
      "step": 1781
    },
    {
      "epoch": 1.78,
      "grad_norm": 9147.333984375,
      "learning_rate": 9.390092879256966e-05,
      "loss": 23.8412,
      "step": 1782
    },
    {
      "epoch": 1.78,
      "grad_norm": 12430.169921875,
      "learning_rate": 9.389576883384934e-05,
      "loss": 21.5301,
      "step": 1783
    },
    {
      "epoch": 1.79,
      "grad_norm": 44069.25,
      "learning_rate": 9.389060887512901e-05,
      "loss": 25.7969,
      "step": 1784
    },
    {
      "epoch": 1.79,
      "grad_norm": 3076.68408203125,
      "learning_rate": 9.388544891640867e-05,
      "loss": 16.4167,
      "step": 1785
    },
    {
      "epoch": 1.79,
      "grad_norm": 29479.93359375,
      "learning_rate": 9.388028895768835e-05,
      "loss": 19.4385,
      "step": 1786
    },
    {
      "epoch": 1.79,
      "grad_norm": 4845.1572265625,
      "learning_rate": 9.3875128998968e-05,
      "loss": 29.9935,
      "step": 1787
    },
    {
      "epoch": 1.79,
      "grad_norm": 18654.435546875,
      "learning_rate": 9.386996904024768e-05,
      "loss": 16.1489,
      "step": 1788
    },
    {
      "epoch": 1.79,
      "grad_norm": 3944.641357421875,
      "learning_rate": 9.386480908152736e-05,
      "loss": 16.1629,
      "step": 1789
    },
    {
      "epoch": 1.79,
      "grad_norm": 5661.30712890625,
      "learning_rate": 9.385964912280703e-05,
      "loss": 14.8975,
      "step": 1790
    },
    {
      "epoch": 1.79,
      "grad_norm": 4813.16455078125,
      "learning_rate": 9.385448916408669e-05,
      "loss": 11.4106,
      "step": 1791
    },
    {
      "epoch": 1.79,
      "grad_norm": 4119.673828125,
      "learning_rate": 9.384932920536637e-05,
      "loss": 13.4564,
      "step": 1792
    },
    {
      "epoch": 1.79,
      "grad_norm": 6160.515625,
      "learning_rate": 9.384416924664603e-05,
      "loss": 18.3129,
      "step": 1793
    },
    {
      "epoch": 1.8,
      "grad_norm": 7985.62060546875,
      "learning_rate": 9.38390092879257e-05,
      "loss": 17.9218,
      "step": 1794
    },
    {
      "epoch": 1.8,
      "grad_norm": 18604.080078125,
      "learning_rate": 9.383384932920538e-05,
      "loss": 23.8096,
      "step": 1795
    },
    {
      "epoch": 1.8,
      "grad_norm": 8241.8115234375,
      "learning_rate": 9.382868937048505e-05,
      "loss": 16.8665,
      "step": 1796
    },
    {
      "epoch": 1.8,
      "grad_norm": 14942.6259765625,
      "learning_rate": 9.382352941176471e-05,
      "loss": 14.8781,
      "step": 1797
    },
    {
      "epoch": 1.8,
      "grad_norm": 4934.67041015625,
      "learning_rate": 9.381836945304437e-05,
      "loss": 20.9352,
      "step": 1798
    },
    {
      "epoch": 1.8,
      "grad_norm": 8386.5,
      "learning_rate": 9.381320949432405e-05,
      "loss": 15.3896,
      "step": 1799
    },
    {
      "epoch": 1.8,
      "grad_norm": 1051.5697021484375,
      "learning_rate": 9.380804953560372e-05,
      "loss": 22.8343,
      "step": 1800
    },
    {
      "epoch": 1.8,
      "grad_norm": 3688.431396484375,
      "learning_rate": 9.38028895768834e-05,
      "loss": 20.2443,
      "step": 1801
    },
    {
      "epoch": 1.8,
      "grad_norm": 6266.4658203125,
      "learning_rate": 9.379772961816306e-05,
      "loss": 40.3935,
      "step": 1802
    },
    {
      "epoch": 1.8,
      "grad_norm": 8075.63134765625,
      "learning_rate": 9.379256965944273e-05,
      "loss": 18.127,
      "step": 1803
    },
    {
      "epoch": 1.81,
      "grad_norm": 10939.8642578125,
      "learning_rate": 9.378740970072239e-05,
      "loss": 26.4951,
      "step": 1804
    },
    {
      "epoch": 1.81,
      "grad_norm": 3068.2109375,
      "learning_rate": 9.378224974200207e-05,
      "loss": 15.7257,
      "step": 1805
    },
    {
      "epoch": 1.81,
      "grad_norm": 2405.177001953125,
      "learning_rate": 9.377708978328174e-05,
      "loss": 15.031,
      "step": 1806
    },
    {
      "epoch": 1.81,
      "grad_norm": 10676.806640625,
      "learning_rate": 9.377192982456142e-05,
      "loss": 21.1832,
      "step": 1807
    },
    {
      "epoch": 1.81,
      "grad_norm": 14698.2939453125,
      "learning_rate": 9.376676986584108e-05,
      "loss": 30.8151,
      "step": 1808
    },
    {
      "epoch": 1.81,
      "grad_norm": 15368.9794921875,
      "learning_rate": 9.376160990712075e-05,
      "loss": 20.9141,
      "step": 1809
    },
    {
      "epoch": 1.81,
      "grad_norm": 678783.25,
      "learning_rate": 9.375644994840041e-05,
      "loss": 22.3758,
      "step": 1810
    },
    {
      "epoch": 1.81,
      "grad_norm": 1097.522705078125,
      "learning_rate": 9.375128998968009e-05,
      "loss": 25.6083,
      "step": 1811
    },
    {
      "epoch": 1.81,
      "grad_norm": 9764.296875,
      "learning_rate": 9.374613003095976e-05,
      "loss": 12.4713,
      "step": 1812
    },
    {
      "epoch": 1.81,
      "grad_norm": 16544.884765625,
      "learning_rate": 9.374097007223944e-05,
      "loss": 20.5447,
      "step": 1813
    },
    {
      "epoch": 1.82,
      "grad_norm": 8038.900390625,
      "learning_rate": 9.37358101135191e-05,
      "loss": 15.7313,
      "step": 1814
    },
    {
      "epoch": 1.82,
      "grad_norm": 15020.431640625,
      "learning_rate": 9.373065015479877e-05,
      "loss": 20.5672,
      "step": 1815
    },
    {
      "epoch": 1.82,
      "grad_norm": 5430.8857421875,
      "learning_rate": 9.372549019607843e-05,
      "loss": 19.6705,
      "step": 1816
    },
    {
      "epoch": 1.82,
      "grad_norm": 4481.21240234375,
      "learning_rate": 9.37203302373581e-05,
      "loss": 22.2324,
      "step": 1817
    },
    {
      "epoch": 1.82,
      "grad_norm": 27471.30078125,
      "learning_rate": 9.371517027863778e-05,
      "loss": 23.0532,
      "step": 1818
    },
    {
      "epoch": 1.82,
      "grad_norm": 9834.970703125,
      "learning_rate": 9.371001031991744e-05,
      "loss": 19.9471,
      "step": 1819
    },
    {
      "epoch": 1.82,
      "grad_norm": 14313.17578125,
      "learning_rate": 9.370485036119712e-05,
      "loss": 28.5476,
      "step": 1820
    },
    {
      "epoch": 1.82,
      "grad_norm": 2550.275146484375,
      "learning_rate": 9.369969040247678e-05,
      "loss": 24.2459,
      "step": 1821
    },
    {
      "epoch": 1.82,
      "grad_norm": 3744.018310546875,
      "learning_rate": 9.369453044375645e-05,
      "loss": 17.1687,
      "step": 1822
    },
    {
      "epoch": 1.82,
      "grad_norm": 15178.365234375,
      "learning_rate": 9.368937048503613e-05,
      "loss": 15.0975,
      "step": 1823
    },
    {
      "epoch": 1.83,
      "grad_norm": 5011.96923828125,
      "learning_rate": 9.36842105263158e-05,
      "loss": 22.2825,
      "step": 1824
    },
    {
      "epoch": 1.83,
      "grad_norm": 20621.11328125,
      "learning_rate": 9.367905056759546e-05,
      "loss": 13.1253,
      "step": 1825
    },
    {
      "epoch": 1.83,
      "grad_norm": 113412.96875,
      "learning_rate": 9.367389060887514e-05,
      "loss": 24.4596,
      "step": 1826
    },
    {
      "epoch": 1.83,
      "grad_norm": 15026.0244140625,
      "learning_rate": 9.36687306501548e-05,
      "loss": 33.7421,
      "step": 1827
    },
    {
      "epoch": 1.83,
      "grad_norm": 2925.54638671875,
      "learning_rate": 9.366357069143447e-05,
      "loss": 13.846,
      "step": 1828
    },
    {
      "epoch": 1.83,
      "grad_norm": 7364.14111328125,
      "learning_rate": 9.365841073271415e-05,
      "loss": 14.5758,
      "step": 1829
    },
    {
      "epoch": 1.83,
      "grad_norm": 10939.755859375,
      "learning_rate": 9.365325077399382e-05,
      "loss": 18.1077,
      "step": 1830
    },
    {
      "epoch": 1.83,
      "grad_norm": 24725.3984375,
      "learning_rate": 9.364809081527348e-05,
      "loss": 21.7156,
      "step": 1831
    },
    {
      "epoch": 1.83,
      "grad_norm": 29056.466796875,
      "learning_rate": 9.364293085655316e-05,
      "loss": 37.4829,
      "step": 1832
    },
    {
      "epoch": 1.83,
      "grad_norm": 5280.201171875,
      "learning_rate": 9.363777089783282e-05,
      "loss": 29.3738,
      "step": 1833
    },
    {
      "epoch": 1.84,
      "grad_norm": 4769.32763671875,
      "learning_rate": 9.363261093911249e-05,
      "loss": 17.9024,
      "step": 1834
    },
    {
      "epoch": 1.84,
      "grad_norm": 8228.068359375,
      "learning_rate": 9.362745098039217e-05,
      "loss": 17.6836,
      "step": 1835
    },
    {
      "epoch": 1.84,
      "grad_norm": 16683.53515625,
      "learning_rate": 9.362229102167183e-05,
      "loss": 20.1862,
      "step": 1836
    },
    {
      "epoch": 1.84,
      "grad_norm": 79631.84375,
      "learning_rate": 9.36171310629515e-05,
      "loss": 23.643,
      "step": 1837
    },
    {
      "epoch": 1.84,
      "grad_norm": 11817.8486328125,
      "learning_rate": 9.361197110423116e-05,
      "loss": 20.103,
      "step": 1838
    },
    {
      "epoch": 1.84,
      "grad_norm": 66967.71875,
      "learning_rate": 9.360681114551084e-05,
      "loss": 23.4629,
      "step": 1839
    },
    {
      "epoch": 1.84,
      "grad_norm": 12511.615234375,
      "learning_rate": 9.360165118679051e-05,
      "loss": 20.1896,
      "step": 1840
    },
    {
      "epoch": 1.84,
      "grad_norm": 6381.97412109375,
      "learning_rate": 9.359649122807019e-05,
      "loss": 28.6216,
      "step": 1841
    },
    {
      "epoch": 1.84,
      "grad_norm": 4546.5615234375,
      "learning_rate": 9.359133126934985e-05,
      "loss": 12.6899,
      "step": 1842
    },
    {
      "epoch": 1.84,
      "grad_norm": 9861.8935546875,
      "learning_rate": 9.358617131062952e-05,
      "loss": 20.7153,
      "step": 1843
    },
    {
      "epoch": 1.85,
      "grad_norm": 3254.062744140625,
      "learning_rate": 9.358101135190918e-05,
      "loss": 17.116,
      "step": 1844
    },
    {
      "epoch": 1.85,
      "grad_norm": 9380.6640625,
      "learning_rate": 9.357585139318886e-05,
      "loss": 15.6796,
      "step": 1845
    },
    {
      "epoch": 1.85,
      "grad_norm": 11798.2841796875,
      "learning_rate": 9.357069143446853e-05,
      "loss": 15.7737,
      "step": 1846
    },
    {
      "epoch": 1.85,
      "grad_norm": 18798.177734375,
      "learning_rate": 9.35655314757482e-05,
      "loss": 15.6426,
      "step": 1847
    },
    {
      "epoch": 1.85,
      "grad_norm": 8401.693359375,
      "learning_rate": 9.356037151702787e-05,
      "loss": 20.4284,
      "step": 1848
    },
    {
      "epoch": 1.85,
      "grad_norm": 90052.765625,
      "learning_rate": 9.355521155830754e-05,
      "loss": 23.7462,
      "step": 1849
    },
    {
      "epoch": 1.85,
      "grad_norm": 9977.6103515625,
      "learning_rate": 9.35500515995872e-05,
      "loss": 16.9008,
      "step": 1850
    },
    {
      "epoch": 1.85,
      "grad_norm": 9979.45703125,
      "learning_rate": 9.354489164086688e-05,
      "loss": 23.0704,
      "step": 1851
    },
    {
      "epoch": 1.85,
      "grad_norm": 2875.010986328125,
      "learning_rate": 9.353973168214655e-05,
      "loss": 16.5366,
      "step": 1852
    },
    {
      "epoch": 1.85,
      "grad_norm": 3106.75341796875,
      "learning_rate": 9.353457172342621e-05,
      "loss": 15.1561,
      "step": 1853
    },
    {
      "epoch": 1.86,
      "grad_norm": 10674.6328125,
      "learning_rate": 9.352941176470589e-05,
      "loss": 16.3626,
      "step": 1854
    },
    {
      "epoch": 1.86,
      "grad_norm": 23684.01171875,
      "learning_rate": 9.352425180598555e-05,
      "loss": 29.7968,
      "step": 1855
    },
    {
      "epoch": 1.86,
      "grad_norm": 4889.98828125,
      "learning_rate": 9.351909184726522e-05,
      "loss": 18.0944,
      "step": 1856
    },
    {
      "epoch": 1.86,
      "grad_norm": 8383.6455078125,
      "learning_rate": 9.35139318885449e-05,
      "loss": 16.6268,
      "step": 1857
    },
    {
      "epoch": 1.86,
      "grad_norm": 18309.130859375,
      "learning_rate": 9.350877192982457e-05,
      "loss": 26.5759,
      "step": 1858
    },
    {
      "epoch": 1.86,
      "grad_norm": 38936.3671875,
      "learning_rate": 9.350361197110423e-05,
      "loss": 28.4841,
      "step": 1859
    },
    {
      "epoch": 1.86,
      "grad_norm": 10152.81640625,
      "learning_rate": 9.349845201238391e-05,
      "loss": 20.1973,
      "step": 1860
    },
    {
      "epoch": 1.86,
      "grad_norm": 3766.561279296875,
      "learning_rate": 9.349329205366357e-05,
      "loss": 13.7328,
      "step": 1861
    },
    {
      "epoch": 1.86,
      "grad_norm": 11612.9970703125,
      "learning_rate": 9.348813209494324e-05,
      "loss": 29.2907,
      "step": 1862
    },
    {
      "epoch": 1.86,
      "grad_norm": 3763.538818359375,
      "learning_rate": 9.348297213622292e-05,
      "loss": 17.3195,
      "step": 1863
    },
    {
      "epoch": 1.87,
      "grad_norm": 73745.984375,
      "learning_rate": 9.347781217750259e-05,
      "loss": 17.9733,
      "step": 1864
    },
    {
      "epoch": 1.87,
      "grad_norm": 5698.51220703125,
      "learning_rate": 9.347265221878225e-05,
      "loss": 13.2349,
      "step": 1865
    },
    {
      "epoch": 1.87,
      "grad_norm": 7648.810546875,
      "learning_rate": 9.346749226006193e-05,
      "loss": 17.3113,
      "step": 1866
    },
    {
      "epoch": 1.87,
      "grad_norm": 4559.0390625,
      "learning_rate": 9.346233230134159e-05,
      "loss": 14.7641,
      "step": 1867
    },
    {
      "epoch": 1.87,
      "grad_norm": 28376.56640625,
      "learning_rate": 9.345717234262126e-05,
      "loss": 20.1853,
      "step": 1868
    },
    {
      "epoch": 1.87,
      "grad_norm": 5240.68994140625,
      "learning_rate": 9.345201238390094e-05,
      "loss": 13.6872,
      "step": 1869
    },
    {
      "epoch": 1.87,
      "grad_norm": 5170.5595703125,
      "learning_rate": 9.34468524251806e-05,
      "loss": 14.348,
      "step": 1870
    },
    {
      "epoch": 1.87,
      "grad_norm": 97838.359375,
      "learning_rate": 9.344169246646027e-05,
      "loss": 34.7026,
      "step": 1871
    },
    {
      "epoch": 1.87,
      "grad_norm": 2772.513427734375,
      "learning_rate": 9.343653250773993e-05,
      "loss": 12.4594,
      "step": 1872
    },
    {
      "epoch": 1.87,
      "grad_norm": 7552.8466796875,
      "learning_rate": 9.343137254901961e-05,
      "loss": 17.5965,
      "step": 1873
    },
    {
      "epoch": 1.88,
      "grad_norm": 19540.67578125,
      "learning_rate": 9.342621259029928e-05,
      "loss": 26.8464,
      "step": 1874
    },
    {
      "epoch": 1.88,
      "grad_norm": 194898.921875,
      "learning_rate": 9.342105263157896e-05,
      "loss": 18.5965,
      "step": 1875
    },
    {
      "epoch": 1.88,
      "grad_norm": 8547.6826171875,
      "learning_rate": 9.341589267285862e-05,
      "loss": 16.1266,
      "step": 1876
    },
    {
      "epoch": 1.88,
      "grad_norm": 7768.75830078125,
      "learning_rate": 9.341073271413829e-05,
      "loss": 15.8533,
      "step": 1877
    },
    {
      "epoch": 1.88,
      "grad_norm": 9176.7275390625,
      "learning_rate": 9.340557275541795e-05,
      "loss": 15.865,
      "step": 1878
    },
    {
      "epoch": 1.88,
      "grad_norm": 20541.390625,
      "learning_rate": 9.340041279669763e-05,
      "loss": 24.1827,
      "step": 1879
    },
    {
      "epoch": 1.88,
      "grad_norm": 13788.00390625,
      "learning_rate": 9.33952528379773e-05,
      "loss": 28.0699,
      "step": 1880
    },
    {
      "epoch": 1.88,
      "grad_norm": 2200.177978515625,
      "learning_rate": 9.339009287925698e-05,
      "loss": 16.4065,
      "step": 1881
    },
    {
      "epoch": 1.88,
      "grad_norm": 3108.6962890625,
      "learning_rate": 9.338493292053664e-05,
      "loss": 14.9302,
      "step": 1882
    },
    {
      "epoch": 1.88,
      "grad_norm": 37667.23046875,
      "learning_rate": 9.337977296181631e-05,
      "loss": 22.4682,
      "step": 1883
    },
    {
      "epoch": 1.89,
      "grad_norm": 5993.064453125,
      "learning_rate": 9.337461300309597e-05,
      "loss": 21.4314,
      "step": 1884
    },
    {
      "epoch": 1.89,
      "grad_norm": 28925.224609375,
      "learning_rate": 9.336945304437565e-05,
      "loss": 17.1987,
      "step": 1885
    },
    {
      "epoch": 1.89,
      "grad_norm": 34291.453125,
      "learning_rate": 9.336429308565532e-05,
      "loss": 15.5305,
      "step": 1886
    },
    {
      "epoch": 1.89,
      "grad_norm": 3601.816162109375,
      "learning_rate": 9.3359133126935e-05,
      "loss": 16.9077,
      "step": 1887
    },
    {
      "epoch": 1.89,
      "grad_norm": 5110.6494140625,
      "learning_rate": 9.335397316821466e-05,
      "loss": 21.1694,
      "step": 1888
    },
    {
      "epoch": 1.89,
      "grad_norm": 4597.20263671875,
      "learning_rate": 9.334881320949432e-05,
      "loss": 18.5328,
      "step": 1889
    },
    {
      "epoch": 1.89,
      "grad_norm": 16129.0078125,
      "learning_rate": 9.3343653250774e-05,
      "loss": 26.3738,
      "step": 1890
    },
    {
      "epoch": 1.89,
      "grad_norm": 2352.113037109375,
      "learning_rate": 9.333849329205367e-05,
      "loss": 16.2854,
      "step": 1891
    },
    {
      "epoch": 1.89,
      "grad_norm": 4241.01611328125,
      "learning_rate": 9.333333333333334e-05,
      "loss": 14.1923,
      "step": 1892
    },
    {
      "epoch": 1.89,
      "grad_norm": 2900.4033203125,
      "learning_rate": 9.3328173374613e-05,
      "loss": 19.6589,
      "step": 1893
    },
    {
      "epoch": 1.9,
      "grad_norm": 15643.1767578125,
      "learning_rate": 9.332301341589268e-05,
      "loss": 19.32,
      "step": 1894
    },
    {
      "epoch": 1.9,
      "grad_norm": 9274.345703125,
      "learning_rate": 9.331785345717234e-05,
      "loss": 16.2045,
      "step": 1895
    },
    {
      "epoch": 1.9,
      "grad_norm": 3299.7197265625,
      "learning_rate": 9.331269349845201e-05,
      "loss": 19.1284,
      "step": 1896
    },
    {
      "epoch": 1.9,
      "grad_norm": 4459.95849609375,
      "learning_rate": 9.330753353973169e-05,
      "loss": 32.067,
      "step": 1897
    },
    {
      "epoch": 1.9,
      "grad_norm": 18048.8828125,
      "learning_rate": 9.330237358101136e-05,
      "loss": 22.1204,
      "step": 1898
    },
    {
      "epoch": 1.9,
      "grad_norm": 16141.716796875,
      "learning_rate": 9.329721362229102e-05,
      "loss": 16.9731,
      "step": 1899
    },
    {
      "epoch": 1.9,
      "grad_norm": 10028.94921875,
      "learning_rate": 9.32920536635707e-05,
      "loss": 19.7262,
      "step": 1900
    },
    {
      "epoch": 1.9,
      "grad_norm": 18614.47265625,
      "learning_rate": 9.328689370485036e-05,
      "loss": 18.7771,
      "step": 1901
    },
    {
      "epoch": 1.9,
      "grad_norm": 1543.9039306640625,
      "learning_rate": 9.328173374613003e-05,
      "loss": 14.4775,
      "step": 1902
    },
    {
      "epoch": 1.9,
      "grad_norm": 3446.89111328125,
      "learning_rate": 9.327657378740971e-05,
      "loss": 22.8265,
      "step": 1903
    },
    {
      "epoch": 1.91,
      "grad_norm": 13897.09765625,
      "learning_rate": 9.327141382868938e-05,
      "loss": 23.0708,
      "step": 1904
    },
    {
      "epoch": 1.91,
      "grad_norm": 1622.30810546875,
      "learning_rate": 9.326625386996904e-05,
      "loss": 23.6307,
      "step": 1905
    },
    {
      "epoch": 1.91,
      "grad_norm": 4791.77490234375,
      "learning_rate": 9.32610939112487e-05,
      "loss": 12.7451,
      "step": 1906
    },
    {
      "epoch": 1.91,
      "grad_norm": 3406.989990234375,
      "learning_rate": 9.325593395252838e-05,
      "loss": 25.0174,
      "step": 1907
    },
    {
      "epoch": 1.91,
      "grad_norm": 15924.4833984375,
      "learning_rate": 9.325077399380805e-05,
      "loss": 31.3089,
      "step": 1908
    },
    {
      "epoch": 1.91,
      "grad_norm": 12124.6875,
      "learning_rate": 9.324561403508773e-05,
      "loss": 21.2596,
      "step": 1909
    },
    {
      "epoch": 1.91,
      "grad_norm": 8795.0869140625,
      "learning_rate": 9.324045407636739e-05,
      "loss": 14.9826,
      "step": 1910
    },
    {
      "epoch": 1.91,
      "grad_norm": 3886.183837890625,
      "learning_rate": 9.323529411764706e-05,
      "loss": 14.0908,
      "step": 1911
    },
    {
      "epoch": 1.91,
      "grad_norm": 26861.369140625,
      "learning_rate": 9.323013415892673e-05,
      "loss": 16.983,
      "step": 1912
    },
    {
      "epoch": 1.91,
      "grad_norm": 2792.1064453125,
      "learning_rate": 9.32249742002064e-05,
      "loss": 22.2173,
      "step": 1913
    },
    {
      "epoch": 1.92,
      "grad_norm": 2401.124755859375,
      "learning_rate": 9.321981424148607e-05,
      "loss": 15.8371,
      "step": 1914
    },
    {
      "epoch": 1.92,
      "grad_norm": 9442.916015625,
      "learning_rate": 9.321465428276575e-05,
      "loss": 24.3081,
      "step": 1915
    },
    {
      "epoch": 1.92,
      "grad_norm": 2234.91015625,
      "learning_rate": 9.320949432404541e-05,
      "loss": 11.1494,
      "step": 1916
    },
    {
      "epoch": 1.92,
      "grad_norm": 18901.134765625,
      "learning_rate": 9.320433436532508e-05,
      "loss": 17.613,
      "step": 1917
    },
    {
      "epoch": 1.92,
      "grad_norm": 10723.19140625,
      "learning_rate": 9.319917440660475e-05,
      "loss": 36.3963,
      "step": 1918
    },
    {
      "epoch": 1.92,
      "grad_norm": 13772.4794921875,
      "learning_rate": 9.319401444788442e-05,
      "loss": 25.9236,
      "step": 1919
    },
    {
      "epoch": 1.92,
      "grad_norm": 39593.3125,
      "learning_rate": 9.31888544891641e-05,
      "loss": 19.4146,
      "step": 1920
    },
    {
      "epoch": 1.92,
      "grad_norm": 1901.916259765625,
      "learning_rate": 9.318369453044377e-05,
      "loss": 11.5458,
      "step": 1921
    },
    {
      "epoch": 1.92,
      "grad_norm": 5911.85107421875,
      "learning_rate": 9.317853457172343e-05,
      "loss": 13.839,
      "step": 1922
    },
    {
      "epoch": 1.92,
      "grad_norm": 4502.55859375,
      "learning_rate": 9.317337461300309e-05,
      "loss": 30.3155,
      "step": 1923
    },
    {
      "epoch": 1.93,
      "grad_norm": 9873.6787109375,
      "learning_rate": 9.316821465428277e-05,
      "loss": 13.9275,
      "step": 1924
    },
    {
      "epoch": 1.93,
      "grad_norm": 11905.806640625,
      "learning_rate": 9.316305469556244e-05,
      "loss": 17.1696,
      "step": 1925
    },
    {
      "epoch": 1.93,
      "grad_norm": 147072.03125,
      "learning_rate": 9.315789473684211e-05,
      "loss": 28.1639,
      "step": 1926
    },
    {
      "epoch": 1.93,
      "grad_norm": 6794.6142578125,
      "learning_rate": 9.315273477812178e-05,
      "loss": 17.8912,
      "step": 1927
    },
    {
      "epoch": 1.93,
      "grad_norm": 17516.947265625,
      "learning_rate": 9.314757481940145e-05,
      "loss": 25.7388,
      "step": 1928
    },
    {
      "epoch": 1.93,
      "grad_norm": 7125.98876953125,
      "learning_rate": 9.314241486068111e-05,
      "loss": 25.8002,
      "step": 1929
    },
    {
      "epoch": 1.93,
      "grad_norm": 22845.94140625,
      "learning_rate": 9.313725490196079e-05,
      "loss": 20.0287,
      "step": 1930
    },
    {
      "epoch": 1.93,
      "grad_norm": 23238.513671875,
      "learning_rate": 9.313209494324046e-05,
      "loss": 24.6504,
      "step": 1931
    },
    {
      "epoch": 1.93,
      "grad_norm": 6200.5361328125,
      "learning_rate": 9.312693498452013e-05,
      "loss": 17.9931,
      "step": 1932
    },
    {
      "epoch": 1.93,
      "grad_norm": 6278.626953125,
      "learning_rate": 9.31217750257998e-05,
      "loss": 12.9518,
      "step": 1933
    },
    {
      "epoch": 1.94,
      "grad_norm": 3252.7314453125,
      "learning_rate": 9.311661506707947e-05,
      "loss": 15.8766,
      "step": 1934
    },
    {
      "epoch": 1.94,
      "grad_norm": 4656.31201171875,
      "learning_rate": 9.311145510835913e-05,
      "loss": 15.7935,
      "step": 1935
    },
    {
      "epoch": 1.94,
      "grad_norm": 15962.267578125,
      "learning_rate": 9.31062951496388e-05,
      "loss": 17.0471,
      "step": 1936
    },
    {
      "epoch": 1.94,
      "grad_norm": 32951.6796875,
      "learning_rate": 9.310113519091848e-05,
      "loss": 27.0984,
      "step": 1937
    },
    {
      "epoch": 1.94,
      "grad_norm": 3971.885986328125,
      "learning_rate": 9.309597523219815e-05,
      "loss": 12.4468,
      "step": 1938
    },
    {
      "epoch": 1.94,
      "grad_norm": 924.2315673828125,
      "learning_rate": 9.309081527347782e-05,
      "loss": 18.4476,
      "step": 1939
    },
    {
      "epoch": 1.94,
      "grad_norm": 2172.82373046875,
      "learning_rate": 9.308565531475749e-05,
      "loss": 13.3491,
      "step": 1940
    },
    {
      "epoch": 1.94,
      "grad_norm": 7296.6748046875,
      "learning_rate": 9.308049535603715e-05,
      "loss": 21.1564,
      "step": 1941
    },
    {
      "epoch": 1.94,
      "grad_norm": 6725.98828125,
      "learning_rate": 9.307533539731683e-05,
      "loss": 21.2358,
      "step": 1942
    },
    {
      "epoch": 1.94,
      "grad_norm": 9981.826171875,
      "learning_rate": 9.30701754385965e-05,
      "loss": 16.0019,
      "step": 1943
    },
    {
      "epoch": 1.95,
      "grad_norm": 3908.9833984375,
      "learning_rate": 9.306501547987616e-05,
      "loss": 31.5414,
      "step": 1944
    },
    {
      "epoch": 1.95,
      "grad_norm": 8542.2060546875,
      "learning_rate": 9.305985552115584e-05,
      "loss": 20.1707,
      "step": 1945
    },
    {
      "epoch": 1.95,
      "grad_norm": 12329.619140625,
      "learning_rate": 9.30546955624355e-05,
      "loss": 24.9716,
      "step": 1946
    },
    {
      "epoch": 1.95,
      "grad_norm": 5246.5966796875,
      "learning_rate": 9.304953560371517e-05,
      "loss": 19.1016,
      "step": 1947
    },
    {
      "epoch": 1.95,
      "grad_norm": 4978.75048828125,
      "learning_rate": 9.304437564499485e-05,
      "loss": 19.4167,
      "step": 1948
    },
    {
      "epoch": 1.95,
      "grad_norm": 5637.69140625,
      "learning_rate": 9.303921568627452e-05,
      "loss": 13.5116,
      "step": 1949
    },
    {
      "epoch": 1.95,
      "grad_norm": 32704.908203125,
      "learning_rate": 9.303405572755418e-05,
      "loss": 30.8835,
      "step": 1950
    },
    {
      "epoch": 1.95,
      "grad_norm": 1489.819091796875,
      "learning_rate": 9.302889576883386e-05,
      "loss": 15.7559,
      "step": 1951
    },
    {
      "epoch": 1.95,
      "grad_norm": 5863.697265625,
      "learning_rate": 9.302373581011352e-05,
      "loss": 20.6247,
      "step": 1952
    },
    {
      "epoch": 1.95,
      "grad_norm": 2717.8837890625,
      "learning_rate": 9.301857585139319e-05,
      "loss": 14.0747,
      "step": 1953
    },
    {
      "epoch": 1.96,
      "grad_norm": 6197.529296875,
      "learning_rate": 9.301341589267287e-05,
      "loss": 17.6495,
      "step": 1954
    },
    {
      "epoch": 1.96,
      "grad_norm": 42854.375,
      "learning_rate": 9.300825593395254e-05,
      "loss": 17.5591,
      "step": 1955
    },
    {
      "epoch": 1.96,
      "grad_norm": 14366.62109375,
      "learning_rate": 9.30030959752322e-05,
      "loss": 20.0317,
      "step": 1956
    },
    {
      "epoch": 1.96,
      "grad_norm": 3030.33740234375,
      "learning_rate": 9.299793601651188e-05,
      "loss": 19.5712,
      "step": 1957
    },
    {
      "epoch": 1.96,
      "grad_norm": 5978.875,
      "learning_rate": 9.299277605779154e-05,
      "loss": 13.7818,
      "step": 1958
    },
    {
      "epoch": 1.96,
      "grad_norm": 3822.2470703125,
      "learning_rate": 9.298761609907121e-05,
      "loss": 12.5144,
      "step": 1959
    },
    {
      "epoch": 1.96,
      "grad_norm": 42052.70703125,
      "learning_rate": 9.298245614035089e-05,
      "loss": 18.4138,
      "step": 1960
    },
    {
      "epoch": 1.96,
      "grad_norm": 22390.583984375,
      "learning_rate": 9.297729618163055e-05,
      "loss": 27.4297,
      "step": 1961
    },
    {
      "epoch": 1.96,
      "grad_norm": 23090.966796875,
      "learning_rate": 9.297213622291022e-05,
      "loss": 13.5424,
      "step": 1962
    },
    {
      "epoch": 1.96,
      "grad_norm": 12710.779296875,
      "learning_rate": 9.296697626418988e-05,
      "loss": 30.944,
      "step": 1963
    },
    {
      "epoch": 1.97,
      "grad_norm": 1593.5872802734375,
      "learning_rate": 9.296181630546956e-05,
      "loss": 14.7309,
      "step": 1964
    },
    {
      "epoch": 1.97,
      "grad_norm": 2849.70361328125,
      "learning_rate": 9.295665634674923e-05,
      "loss": 26.3934,
      "step": 1965
    },
    {
      "epoch": 1.97,
      "grad_norm": 973.747802734375,
      "learning_rate": 9.29514963880289e-05,
      "loss": 22.3174,
      "step": 1966
    },
    {
      "epoch": 1.97,
      "grad_norm": 21178.12890625,
      "learning_rate": 9.294633642930857e-05,
      "loss": 17.9356,
      "step": 1967
    },
    {
      "epoch": 1.97,
      "grad_norm": 8106.0693359375,
      "learning_rate": 9.294117647058824e-05,
      "loss": 18.3728,
      "step": 1968
    },
    {
      "epoch": 1.97,
      "grad_norm": 21920.525390625,
      "learning_rate": 9.29360165118679e-05,
      "loss": 21.5165,
      "step": 1969
    },
    {
      "epoch": 1.97,
      "grad_norm": 4699.46826171875,
      "learning_rate": 9.293085655314758e-05,
      "loss": 18.5632,
      "step": 1970
    },
    {
      "epoch": 1.97,
      "grad_norm": 4145.47802734375,
      "learning_rate": 9.292569659442725e-05,
      "loss": 20.9392,
      "step": 1971
    },
    {
      "epoch": 1.97,
      "grad_norm": 9650.6455078125,
      "learning_rate": 9.292053663570693e-05,
      "loss": 21.7333,
      "step": 1972
    },
    {
      "epoch": 1.97,
      "grad_norm": 19510.455078125,
      "learning_rate": 9.291537667698659e-05,
      "loss": 30.5097,
      "step": 1973
    },
    {
      "epoch": 1.98,
      "grad_norm": 5640.48046875,
      "learning_rate": 9.291021671826626e-05,
      "loss": 12.2435,
      "step": 1974
    },
    {
      "epoch": 1.98,
      "grad_norm": 8896.55859375,
      "learning_rate": 9.290505675954592e-05,
      "loss": 21.5022,
      "step": 1975
    },
    {
      "epoch": 1.98,
      "grad_norm": 12593.76953125,
      "learning_rate": 9.289989680082561e-05,
      "loss": 14.5141,
      "step": 1976
    },
    {
      "epoch": 1.98,
      "grad_norm": 5187.8349609375,
      "learning_rate": 9.289473684210527e-05,
      "loss": 24.1302,
      "step": 1977
    },
    {
      "epoch": 1.98,
      "grad_norm": 34978.08984375,
      "learning_rate": 9.288957688338493e-05,
      "loss": 23.0538,
      "step": 1978
    },
    {
      "epoch": 1.98,
      "grad_norm": 9866.9521484375,
      "learning_rate": 9.28844169246646e-05,
      "loss": 15.6957,
      "step": 1979
    },
    {
      "epoch": 1.98,
      "grad_norm": 112716.5546875,
      "learning_rate": 9.287925696594427e-05,
      "loss": 19.2843,
      "step": 1980
    },
    {
      "epoch": 1.98,
      "grad_norm": 7550.00439453125,
      "learning_rate": 9.287409700722394e-05,
      "loss": 20.0305,
      "step": 1981
    },
    {
      "epoch": 1.98,
      "grad_norm": 13788.4091796875,
      "learning_rate": 9.286893704850362e-05,
      "loss": 22.2772,
      "step": 1982
    },
    {
      "epoch": 1.98,
      "grad_norm": 3842.8935546875,
      "learning_rate": 9.286377708978329e-05,
      "loss": 12.1404,
      "step": 1983
    },
    {
      "epoch": 1.99,
      "grad_norm": 7796.6298828125,
      "learning_rate": 9.285861713106295e-05,
      "loss": 11.8042,
      "step": 1984
    },
    {
      "epoch": 1.99,
      "grad_norm": 1928.806640625,
      "learning_rate": 9.285345717234263e-05,
      "loss": 16.5353,
      "step": 1985
    },
    {
      "epoch": 1.99,
      "grad_norm": 11171.83984375,
      "learning_rate": 9.284829721362229e-05,
      "loss": 22.857,
      "step": 1986
    },
    {
      "epoch": 1.99,
      "grad_norm": 7545.8984375,
      "learning_rate": 9.284313725490196e-05,
      "loss": 26.8759,
      "step": 1987
    },
    {
      "epoch": 1.99,
      "grad_norm": 9675.3330078125,
      "learning_rate": 9.283797729618164e-05,
      "loss": 15.9938,
      "step": 1988
    },
    {
      "epoch": 1.99,
      "grad_norm": 3860.559814453125,
      "learning_rate": 9.283281733746131e-05,
      "loss": 10.308,
      "step": 1989
    },
    {
      "epoch": 1.99,
      "grad_norm": 64964.28125,
      "learning_rate": 9.282765737874097e-05,
      "loss": 23.976,
      "step": 1990
    },
    {
      "epoch": 1.99,
      "grad_norm": 5985.794921875,
      "learning_rate": 9.282249742002065e-05,
      "loss": 27.6397,
      "step": 1991
    },
    {
      "epoch": 1.99,
      "grad_norm": 4237.556640625,
      "learning_rate": 9.281733746130031e-05,
      "loss": 18.586,
      "step": 1992
    },
    {
      "epoch": 1.99,
      "grad_norm": 1793.5164794921875,
      "learning_rate": 9.281217750258e-05,
      "loss": 13.7148,
      "step": 1993
    },
    {
      "epoch": 2.0,
      "grad_norm": 3639.954833984375,
      "learning_rate": 9.280701754385966e-05,
      "loss": 12.819,
      "step": 1994
    },
    {
      "epoch": 2.0,
      "grad_norm": 29912.068359375,
      "learning_rate": 9.280185758513932e-05,
      "loss": 17.9558,
      "step": 1995
    },
    {
      "epoch": 2.0,
      "grad_norm": 77636.15625,
      "learning_rate": 9.279669762641899e-05,
      "loss": 18.3013,
      "step": 1996
    },
    {
      "epoch": 2.0,
      "grad_norm": 3686.362548828125,
      "learning_rate": 9.279153766769865e-05,
      "loss": 20.7779,
      "step": 1997
    },
    {
      "epoch": 2.0,
      "grad_norm": 88166.9609375,
      "learning_rate": 9.278637770897833e-05,
      "loss": 30.1849,
      "step": 1998
    },
    {
      "epoch": 2.0,
      "grad_norm": 17006.064453125,
      "learning_rate": 9.2781217750258e-05,
      "loss": 18.6757,
      "step": 1999
    },
    {
      "epoch": 2.0,
      "grad_norm": 19678.697265625,
      "learning_rate": 9.277605779153768e-05,
      "loss": 19.3651,
      "step": 2000
    },
    {
      "epoch": 2.0,
      "grad_norm": 15672.9931640625,
      "learning_rate": 9.277089783281734e-05,
      "loss": 34.4378,
      "step": 2001
    },
    {
      "epoch": 2.0,
      "grad_norm": 119936.125,
      "learning_rate": 9.276573787409701e-05,
      "loss": 23.8768,
      "step": 2002
    },
    {
      "epoch": 2.01,
      "grad_norm": 4856.98974609375,
      "learning_rate": 9.276057791537667e-05,
      "loss": 17.4462,
      "step": 2003
    },
    {
      "epoch": 2.01,
      "grad_norm": 11221.3486328125,
      "learning_rate": 9.275541795665636e-05,
      "loss": 16.1103,
      "step": 2004
    },
    {
      "epoch": 2.01,
      "grad_norm": 76585.5859375,
      "learning_rate": 9.275025799793602e-05,
      "loss": 15.2119,
      "step": 2005
    },
    {
      "epoch": 2.01,
      "grad_norm": 13277.056640625,
      "learning_rate": 9.27450980392157e-05,
      "loss": 22.6862,
      "step": 2006
    },
    {
      "epoch": 2.01,
      "grad_norm": 4394.05908203125,
      "learning_rate": 9.273993808049536e-05,
      "loss": 18.5812,
      "step": 2007
    },
    {
      "epoch": 2.01,
      "grad_norm": 7317.3173828125,
      "learning_rate": 9.273477812177503e-05,
      "loss": 19.7382,
      "step": 2008
    },
    {
      "epoch": 2.01,
      "grad_norm": 6400.75439453125,
      "learning_rate": 9.272961816305469e-05,
      "loss": 13.9686,
      "step": 2009
    },
    {
      "epoch": 2.01,
      "grad_norm": 4068.072998046875,
      "learning_rate": 9.272445820433438e-05,
      "loss": 12.4872,
      "step": 2010
    },
    {
      "epoch": 2.01,
      "grad_norm": 3178.4541015625,
      "learning_rate": 9.271929824561404e-05,
      "loss": 15.8843,
      "step": 2011
    },
    {
      "epoch": 2.01,
      "grad_norm": 5104.31640625,
      "learning_rate": 9.271413828689372e-05,
      "loss": 26.8319,
      "step": 2012
    },
    {
      "epoch": 2.02,
      "grad_norm": 9207.7685546875,
      "learning_rate": 9.270897832817338e-05,
      "loss": 21.6887,
      "step": 2013
    },
    {
      "epoch": 2.02,
      "grad_norm": 42077.4921875,
      "learning_rate": 9.270381836945304e-05,
      "loss": 26.9887,
      "step": 2014
    },
    {
      "epoch": 2.02,
      "grad_norm": 18631.552734375,
      "learning_rate": 9.269865841073271e-05,
      "loss": 22.8822,
      "step": 2015
    },
    {
      "epoch": 2.02,
      "grad_norm": 3634.034423828125,
      "learning_rate": 9.269349845201239e-05,
      "loss": 15.5769,
      "step": 2016
    },
    {
      "epoch": 2.02,
      "grad_norm": 1936.7921142578125,
      "learning_rate": 9.268833849329206e-05,
      "loss": 13.9164,
      "step": 2017
    },
    {
      "epoch": 2.02,
      "grad_norm": 4853.82958984375,
      "learning_rate": 9.268317853457172e-05,
      "loss": 28.0509,
      "step": 2018
    },
    {
      "epoch": 2.02,
      "grad_norm": 1623.581298828125,
      "learning_rate": 9.26780185758514e-05,
      "loss": 11.3786,
      "step": 2019
    },
    {
      "epoch": 2.02,
      "grad_norm": 3624.98193359375,
      "learning_rate": 9.267285861713106e-05,
      "loss": 11.0377,
      "step": 2020
    },
    {
      "epoch": 2.02,
      "grad_norm": 2653.707275390625,
      "learning_rate": 9.266769865841075e-05,
      "loss": 15.397,
      "step": 2021
    },
    {
      "epoch": 2.02,
      "grad_norm": 2678.666015625,
      "learning_rate": 9.266253869969041e-05,
      "loss": 15.6818,
      "step": 2022
    },
    {
      "epoch": 2.03,
      "grad_norm": 1198.0555419921875,
      "learning_rate": 9.265737874097008e-05,
      "loss": 10.9979,
      "step": 2023
    },
    {
      "epoch": 2.03,
      "grad_norm": 5035.8525390625,
      "learning_rate": 9.265221878224974e-05,
      "loss": 15.7308,
      "step": 2024
    },
    {
      "epoch": 2.03,
      "grad_norm": 21082.892578125,
      "learning_rate": 9.264705882352942e-05,
      "loss": 28.2971,
      "step": 2025
    },
    {
      "epoch": 2.03,
      "grad_norm": 41151.75,
      "learning_rate": 9.264189886480908e-05,
      "loss": 17.5091,
      "step": 2026
    },
    {
      "epoch": 2.03,
      "grad_norm": 2585.656982421875,
      "learning_rate": 9.263673890608877e-05,
      "loss": 12.041,
      "step": 2027
    },
    {
      "epoch": 2.03,
      "grad_norm": 4724.6220703125,
      "learning_rate": 9.263157894736843e-05,
      "loss": 17.7101,
      "step": 2028
    },
    {
      "epoch": 2.03,
      "grad_norm": 1231.267333984375,
      "learning_rate": 9.26264189886481e-05,
      "loss": 24.4405,
      "step": 2029
    },
    {
      "epoch": 2.03,
      "grad_norm": 5945.52001953125,
      "learning_rate": 9.262125902992776e-05,
      "loss": 18.5,
      "step": 2030
    },
    {
      "epoch": 2.03,
      "grad_norm": 111737.8125,
      "learning_rate": 9.261609907120742e-05,
      "loss": 24.8604,
      "step": 2031
    },
    {
      "epoch": 2.03,
      "grad_norm": 22342.052734375,
      "learning_rate": 9.261093911248711e-05,
      "loss": 21.333,
      "step": 2032
    },
    {
      "epoch": 2.04,
      "grad_norm": 6754.576171875,
      "learning_rate": 9.260577915376677e-05,
      "loss": 22.0622,
      "step": 2033
    },
    {
      "epoch": 2.04,
      "grad_norm": 3898.772705078125,
      "learning_rate": 9.260061919504645e-05,
      "loss": 16.1141,
      "step": 2034
    },
    {
      "epoch": 2.04,
      "grad_norm": 8056.55029296875,
      "learning_rate": 9.259545923632611e-05,
      "loss": 21.7752,
      "step": 2035
    },
    {
      "epoch": 2.04,
      "grad_norm": 25404.85546875,
      "learning_rate": 9.259029927760578e-05,
      "loss": 19.2757,
      "step": 2036
    },
    {
      "epoch": 2.04,
      "grad_norm": 4530.7392578125,
      "learning_rate": 9.258513931888544e-05,
      "loss": 31.6419,
      "step": 2037
    },
    {
      "epoch": 2.04,
      "grad_norm": 53237.38671875,
      "learning_rate": 9.257997936016513e-05,
      "loss": 27.094,
      "step": 2038
    },
    {
      "epoch": 2.04,
      "grad_norm": 7620.62451171875,
      "learning_rate": 9.257481940144479e-05,
      "loss": 23.9678,
      "step": 2039
    },
    {
      "epoch": 2.04,
      "grad_norm": 7670.5439453125,
      "learning_rate": 9.256965944272447e-05,
      "loss": 18.1457,
      "step": 2040
    },
    {
      "epoch": 2.04,
      "grad_norm": 9149.1328125,
      "learning_rate": 9.256449948400413e-05,
      "loss": 24.2815,
      "step": 2041
    },
    {
      "epoch": 2.04,
      "grad_norm": 67466.7109375,
      "learning_rate": 9.25593395252838e-05,
      "loss": 25.3815,
      "step": 2042
    },
    {
      "epoch": 2.05,
      "grad_norm": 20596.693359375,
      "learning_rate": 9.255417956656346e-05,
      "loss": 19.3666,
      "step": 2043
    },
    {
      "epoch": 2.05,
      "grad_norm": 19025.34765625,
      "learning_rate": 9.254901960784315e-05,
      "loss": 26.8011,
      "step": 2044
    },
    {
      "epoch": 2.05,
      "grad_norm": 4138.1328125,
      "learning_rate": 9.254385964912281e-05,
      "loss": 16.8035,
      "step": 2045
    },
    {
      "epoch": 2.05,
      "grad_norm": 17034.416015625,
      "learning_rate": 9.253869969040249e-05,
      "loss": 19.1991,
      "step": 2046
    },
    {
      "epoch": 2.05,
      "grad_norm": 1177.1290283203125,
      "learning_rate": 9.253353973168215e-05,
      "loss": 15.7169,
      "step": 2047
    },
    {
      "epoch": 2.05,
      "grad_norm": 9330.1123046875,
      "learning_rate": 9.252837977296182e-05,
      "loss": 15.9826,
      "step": 2048
    },
    {
      "epoch": 2.05,
      "grad_norm": 2043.671875,
      "learning_rate": 9.25232198142415e-05,
      "loss": 14.9687,
      "step": 2049
    },
    {
      "epoch": 2.05,
      "grad_norm": 6469.15283203125,
      "learning_rate": 9.251805985552116e-05,
      "loss": 24.6069,
      "step": 2050
    },
    {
      "epoch": 2.05,
      "grad_norm": 12293.8447265625,
      "learning_rate": 9.251289989680083e-05,
      "loss": 21.698,
      "step": 2051
    },
    {
      "epoch": 2.05,
      "grad_norm": 62718.44140625,
      "learning_rate": 9.25077399380805e-05,
      "loss": 16.8741,
      "step": 2052
    },
    {
      "epoch": 2.06,
      "grad_norm": 9228.2197265625,
      "learning_rate": 9.250257997936017e-05,
      "loss": 15.254,
      "step": 2053
    },
    {
      "epoch": 2.06,
      "grad_norm": 14082.794921875,
      "learning_rate": 9.249742002063983e-05,
      "loss": 19.1611,
      "step": 2054
    },
    {
      "epoch": 2.06,
      "grad_norm": 13212.921875,
      "learning_rate": 9.249226006191952e-05,
      "loss": 18.0397,
      "step": 2055
    },
    {
      "epoch": 2.06,
      "grad_norm": 6797.45703125,
      "learning_rate": 9.248710010319918e-05,
      "loss": 14.2352,
      "step": 2056
    },
    {
      "epoch": 2.06,
      "grad_norm": 2588.146484375,
      "learning_rate": 9.248194014447885e-05,
      "loss": 16.8372,
      "step": 2057
    },
    {
      "epoch": 2.06,
      "grad_norm": 16591.734375,
      "learning_rate": 9.247678018575851e-05,
      "loss": 19.73,
      "step": 2058
    },
    {
      "epoch": 2.06,
      "grad_norm": 30278.724609375,
      "learning_rate": 9.247162022703819e-05,
      "loss": 22.2444,
      "step": 2059
    },
    {
      "epoch": 2.06,
      "grad_norm": 3759.9013671875,
      "learning_rate": 9.246646026831786e-05,
      "loss": 12.7407,
      "step": 2060
    },
    {
      "epoch": 2.06,
      "grad_norm": 11495.1416015625,
      "learning_rate": 9.246130030959754e-05,
      "loss": 14.2606,
      "step": 2061
    },
    {
      "epoch": 2.06,
      "grad_norm": 5012.4990234375,
      "learning_rate": 9.24561403508772e-05,
      "loss": 16.7679,
      "step": 2062
    },
    {
      "epoch": 2.07,
      "grad_norm": 6109.2001953125,
      "learning_rate": 9.245098039215687e-05,
      "loss": 17.0016,
      "step": 2063
    },
    {
      "epoch": 2.07,
      "grad_norm": 8625.7919921875,
      "learning_rate": 9.244582043343653e-05,
      "loss": 12.8889,
      "step": 2064
    },
    {
      "epoch": 2.07,
      "grad_norm": 12071.8955078125,
      "learning_rate": 9.244066047471621e-05,
      "loss": 20.833,
      "step": 2065
    },
    {
      "epoch": 2.07,
      "grad_norm": 7945.48681640625,
      "learning_rate": 9.243550051599588e-05,
      "loss": 18.3162,
      "step": 2066
    },
    {
      "epoch": 2.07,
      "grad_norm": 8903.30078125,
      "learning_rate": 9.243034055727554e-05,
      "loss": 16.4325,
      "step": 2067
    },
    {
      "epoch": 2.07,
      "grad_norm": 8087.986328125,
      "learning_rate": 9.242518059855522e-05,
      "loss": 12.8813,
      "step": 2068
    },
    {
      "epoch": 2.07,
      "grad_norm": 7837.21484375,
      "learning_rate": 9.242002063983488e-05,
      "loss": 19.8296,
      "step": 2069
    },
    {
      "epoch": 2.07,
      "grad_norm": 6981.52490234375,
      "learning_rate": 9.241486068111455e-05,
      "loss": 19.8823,
      "step": 2070
    },
    {
      "epoch": 2.07,
      "grad_norm": 2571.969482421875,
      "learning_rate": 9.240970072239421e-05,
      "loss": 18.3535,
      "step": 2071
    },
    {
      "epoch": 2.07,
      "grad_norm": 6025.65673828125,
      "learning_rate": 9.24045407636739e-05,
      "loss": 26.5073,
      "step": 2072
    },
    {
      "epoch": 2.08,
      "grad_norm": 20602.671875,
      "learning_rate": 9.239938080495356e-05,
      "loss": 29.0486,
      "step": 2073
    },
    {
      "epoch": 2.08,
      "grad_norm": 15435.001953125,
      "learning_rate": 9.239422084623324e-05,
      "loss": 18.6559,
      "step": 2074
    },
    {
      "epoch": 2.08,
      "grad_norm": 38135.29296875,
      "learning_rate": 9.23890608875129e-05,
      "loss": 15.0811,
      "step": 2075
    },
    {
      "epoch": 2.08,
      "grad_norm": 6073.61865234375,
      "learning_rate": 9.238390092879257e-05,
      "loss": 19.1349,
      "step": 2076
    },
    {
      "epoch": 2.08,
      "grad_norm": 6592.5029296875,
      "learning_rate": 9.237874097007225e-05,
      "loss": 28.1067,
      "step": 2077
    },
    {
      "epoch": 2.08,
      "grad_norm": 7428.830078125,
      "learning_rate": 9.237358101135192e-05,
      "loss": 16.5837,
      "step": 2078
    },
    {
      "epoch": 2.08,
      "grad_norm": 15464.9892578125,
      "learning_rate": 9.236842105263158e-05,
      "loss": 16.7082,
      "step": 2079
    },
    {
      "epoch": 2.08,
      "grad_norm": 13053.1494140625,
      "learning_rate": 9.236326109391126e-05,
      "loss": 28.4956,
      "step": 2080
    },
    {
      "epoch": 2.08,
      "grad_norm": 4463.4521484375,
      "learning_rate": 9.235810113519092e-05,
      "loss": 18.4994,
      "step": 2081
    },
    {
      "epoch": 2.08,
      "grad_norm": 1470.7840576171875,
      "learning_rate": 9.23529411764706e-05,
      "loss": 12.5834,
      "step": 2082
    },
    {
      "epoch": 2.09,
      "grad_norm": 15638.4677734375,
      "learning_rate": 9.234778121775027e-05,
      "loss": 14.9886,
      "step": 2083
    },
    {
      "epoch": 2.09,
      "grad_norm": 16882.265625,
      "learning_rate": 9.234262125902993e-05,
      "loss": 26.8163,
      "step": 2084
    },
    {
      "epoch": 2.09,
      "grad_norm": 5161.60107421875,
      "learning_rate": 9.23374613003096e-05,
      "loss": 21.2715,
      "step": 2085
    },
    {
      "epoch": 2.09,
      "grad_norm": 2043.9593505859375,
      "learning_rate": 9.233230134158926e-05,
      "loss": 14.4644,
      "step": 2086
    },
    {
      "epoch": 2.09,
      "grad_norm": 4118.12158203125,
      "learning_rate": 9.232714138286894e-05,
      "loss": 22.2651,
      "step": 2087
    },
    {
      "epoch": 2.09,
      "grad_norm": 6501.29052734375,
      "learning_rate": 9.232198142414861e-05,
      "loss": 21.9897,
      "step": 2088
    },
    {
      "epoch": 2.09,
      "grad_norm": 4401.484375,
      "learning_rate": 9.231682146542829e-05,
      "loss": 14.5142,
      "step": 2089
    },
    {
      "epoch": 2.09,
      "grad_norm": 4997.0458984375,
      "learning_rate": 9.231166150670795e-05,
      "loss": 19.5494,
      "step": 2090
    },
    {
      "epoch": 2.09,
      "grad_norm": 19639.03125,
      "learning_rate": 9.230650154798762e-05,
      "loss": 16.9809,
      "step": 2091
    },
    {
      "epoch": 2.09,
      "grad_norm": 8979.3994140625,
      "learning_rate": 9.230134158926728e-05,
      "loss": 16.5498,
      "step": 2092
    },
    {
      "epoch": 2.1,
      "grad_norm": 27131.181640625,
      "learning_rate": 9.229618163054696e-05,
      "loss": 28.7597,
      "step": 2093
    },
    {
      "epoch": 2.1,
      "grad_norm": 25906.001953125,
      "learning_rate": 9.229102167182663e-05,
      "loss": 23.5199,
      "step": 2094
    },
    {
      "epoch": 2.1,
      "grad_norm": 3625.749755859375,
      "learning_rate": 9.228586171310631e-05,
      "loss": 14.2633,
      "step": 2095
    },
    {
      "epoch": 2.1,
      "grad_norm": 20125.6015625,
      "learning_rate": 9.228070175438597e-05,
      "loss": 17.8673,
      "step": 2096
    },
    {
      "epoch": 2.1,
      "grad_norm": 225472.40625,
      "learning_rate": 9.227554179566564e-05,
      "loss": 23.2432,
      "step": 2097
    },
    {
      "epoch": 2.1,
      "grad_norm": 4612.97119140625,
      "learning_rate": 9.22703818369453e-05,
      "loss": 15.8826,
      "step": 2098
    },
    {
      "epoch": 2.1,
      "grad_norm": 12282.4267578125,
      "learning_rate": 9.226522187822498e-05,
      "loss": 16.0821,
      "step": 2099
    },
    {
      "epoch": 2.1,
      "grad_norm": 14526.505859375,
      "learning_rate": 9.226006191950465e-05,
      "loss": 16.7339,
      "step": 2100
    },
    {
      "epoch": 2.1,
      "grad_norm": 10164.3388671875,
      "learning_rate": 9.225490196078433e-05,
      "loss": 24.9361,
      "step": 2101
    },
    {
      "epoch": 2.1,
      "grad_norm": 9338.7705078125,
      "learning_rate": 9.224974200206399e-05,
      "loss": 17.2624,
      "step": 2102
    },
    {
      "epoch": 2.11,
      "grad_norm": 6343.7763671875,
      "learning_rate": 9.224458204334365e-05,
      "loss": 15.7952,
      "step": 2103
    },
    {
      "epoch": 2.11,
      "grad_norm": 31439.154296875,
      "learning_rate": 9.223942208462332e-05,
      "loss": 19.4731,
      "step": 2104
    },
    {
      "epoch": 2.11,
      "grad_norm": 9296.4814453125,
      "learning_rate": 9.2234262125903e-05,
      "loss": 22.1123,
      "step": 2105
    },
    {
      "epoch": 2.11,
      "grad_norm": 4379.916015625,
      "learning_rate": 9.222910216718267e-05,
      "loss": 22.9154,
      "step": 2106
    },
    {
      "epoch": 2.11,
      "grad_norm": 23121.048828125,
      "learning_rate": 9.222394220846233e-05,
      "loss": 16.7844,
      "step": 2107
    },
    {
      "epoch": 2.11,
      "grad_norm": 10371.7705078125,
      "learning_rate": 9.221878224974201e-05,
      "loss": 16.5855,
      "step": 2108
    },
    {
      "epoch": 2.11,
      "grad_norm": 2124.02685546875,
      "learning_rate": 9.221362229102167e-05,
      "loss": 14.3818,
      "step": 2109
    },
    {
      "epoch": 2.11,
      "grad_norm": 3230.8134765625,
      "learning_rate": 9.220846233230134e-05,
      "loss": 17.4405,
      "step": 2110
    },
    {
      "epoch": 2.11,
      "grad_norm": 1741.7122802734375,
      "learning_rate": 9.220330237358102e-05,
      "loss": 21.1275,
      "step": 2111
    },
    {
      "epoch": 2.11,
      "grad_norm": 5738.833984375,
      "learning_rate": 9.21981424148607e-05,
      "loss": 14.2011,
      "step": 2112
    },
    {
      "epoch": 2.12,
      "grad_norm": 26699.123046875,
      "learning_rate": 9.219298245614035e-05,
      "loss": 19.1997,
      "step": 2113
    },
    {
      "epoch": 2.12,
      "grad_norm": 33366.63671875,
      "learning_rate": 9.218782249742003e-05,
      "loss": 22.5382,
      "step": 2114
    },
    {
      "epoch": 2.12,
      "grad_norm": 4508.5849609375,
      "learning_rate": 9.218266253869969e-05,
      "loss": 13.7408,
      "step": 2115
    },
    {
      "epoch": 2.12,
      "grad_norm": 3042.4970703125,
      "learning_rate": 9.217750257997936e-05,
      "loss": 20.8031,
      "step": 2116
    },
    {
      "epoch": 2.12,
      "grad_norm": 9207.8740234375,
      "learning_rate": 9.217234262125904e-05,
      "loss": 19.4544,
      "step": 2117
    },
    {
      "epoch": 2.12,
      "grad_norm": 15257.7158203125,
      "learning_rate": 9.216718266253871e-05,
      "loss": 24.1965,
      "step": 2118
    },
    {
      "epoch": 2.12,
      "grad_norm": 6494.44140625,
      "learning_rate": 9.216202270381837e-05,
      "loss": 23.681,
      "step": 2119
    },
    {
      "epoch": 2.12,
      "grad_norm": 3762.545654296875,
      "learning_rate": 9.215686274509804e-05,
      "loss": 17.3142,
      "step": 2120
    },
    {
      "epoch": 2.12,
      "grad_norm": 9930.265625,
      "learning_rate": 9.215170278637771e-05,
      "loss": 14.8686,
      "step": 2121
    },
    {
      "epoch": 2.12,
      "grad_norm": 4650.181640625,
      "learning_rate": 9.214654282765738e-05,
      "loss": 29.8554,
      "step": 2122
    },
    {
      "epoch": 2.13,
      "grad_norm": 9825.1142578125,
      "learning_rate": 9.214138286893706e-05,
      "loss": 17.0134,
      "step": 2123
    },
    {
      "epoch": 2.13,
      "grad_norm": 15371.744140625,
      "learning_rate": 9.213622291021672e-05,
      "loss": 16.7678,
      "step": 2124
    },
    {
      "epoch": 2.13,
      "grad_norm": 26849.201171875,
      "learning_rate": 9.21310629514964e-05,
      "loss": 18.5218,
      "step": 2125
    },
    {
      "epoch": 2.13,
      "grad_norm": 19369.63671875,
      "learning_rate": 9.212590299277606e-05,
      "loss": 15.5171,
      "step": 2126
    },
    {
      "epoch": 2.13,
      "grad_norm": 2601.858642578125,
      "learning_rate": 9.212074303405573e-05,
      "loss": 12.0931,
      "step": 2127
    },
    {
      "epoch": 2.13,
      "grad_norm": 22518.51171875,
      "learning_rate": 9.21155830753354e-05,
      "loss": 17.4333,
      "step": 2128
    },
    {
      "epoch": 2.13,
      "grad_norm": 79607.1640625,
      "learning_rate": 9.211042311661508e-05,
      "loss": 19.8103,
      "step": 2129
    },
    {
      "epoch": 2.13,
      "grad_norm": 16280.93359375,
      "learning_rate": 9.210526315789474e-05,
      "loss": 17.9998,
      "step": 2130
    },
    {
      "epoch": 2.13,
      "grad_norm": 7341.55029296875,
      "learning_rate": 9.210010319917441e-05,
      "loss": 31.2462,
      "step": 2131
    },
    {
      "epoch": 2.13,
      "grad_norm": 3328.062255859375,
      "learning_rate": 9.209494324045408e-05,
      "loss": 13.7223,
      "step": 2132
    },
    {
      "epoch": 2.14,
      "grad_norm": 2014.8253173828125,
      "learning_rate": 9.208978328173375e-05,
      "loss": 21.7859,
      "step": 2133
    },
    {
      "epoch": 2.14,
      "grad_norm": 6295.94921875,
      "learning_rate": 9.208462332301342e-05,
      "loss": 12.1196,
      "step": 2134
    },
    {
      "epoch": 2.14,
      "grad_norm": 9787.83203125,
      "learning_rate": 9.20794633642931e-05,
      "loss": 19.0664,
      "step": 2135
    },
    {
      "epoch": 2.14,
      "grad_norm": 10382.412109375,
      "learning_rate": 9.207430340557276e-05,
      "loss": 16.2474,
      "step": 2136
    },
    {
      "epoch": 2.14,
      "grad_norm": 10370.87890625,
      "learning_rate": 9.206914344685243e-05,
      "loss": 19.0958,
      "step": 2137
    },
    {
      "epoch": 2.14,
      "grad_norm": 4586.6220703125,
      "learning_rate": 9.20639834881321e-05,
      "loss": 16.5165,
      "step": 2138
    },
    {
      "epoch": 2.14,
      "grad_norm": 13419.4072265625,
      "learning_rate": 9.205882352941177e-05,
      "loss": 21.377,
      "step": 2139
    },
    {
      "epoch": 2.14,
      "grad_norm": 4482.21875,
      "learning_rate": 9.205366357069144e-05,
      "loss": 11.5581,
      "step": 2140
    },
    {
      "epoch": 2.14,
      "grad_norm": 14860.029296875,
      "learning_rate": 9.20485036119711e-05,
      "loss": 23.2145,
      "step": 2141
    },
    {
      "epoch": 2.14,
      "grad_norm": 1544.14697265625,
      "learning_rate": 9.204334365325078e-05,
      "loss": 14.623,
      "step": 2142
    },
    {
      "epoch": 2.15,
      "grad_norm": 2678.552490234375,
      "learning_rate": 9.203818369453044e-05,
      "loss": 14.194,
      "step": 2143
    },
    {
      "epoch": 2.15,
      "grad_norm": 37197.3359375,
      "learning_rate": 9.203302373581012e-05,
      "loss": 15.0505,
      "step": 2144
    },
    {
      "epoch": 2.15,
      "grad_norm": 3726.4033203125,
      "learning_rate": 9.202786377708979e-05,
      "loss": 21.2737,
      "step": 2145
    },
    {
      "epoch": 2.15,
      "grad_norm": 7076.17822265625,
      "learning_rate": 9.202270381836946e-05,
      "loss": 17.0995,
      "step": 2146
    },
    {
      "epoch": 2.15,
      "grad_norm": 6641.28173828125,
      "learning_rate": 9.201754385964913e-05,
      "loss": 16.2705,
      "step": 2147
    },
    {
      "epoch": 2.15,
      "grad_norm": 13943.009765625,
      "learning_rate": 9.20123839009288e-05,
      "loss": 23.3465,
      "step": 2148
    },
    {
      "epoch": 2.15,
      "grad_norm": 5327.287109375,
      "learning_rate": 9.200722394220846e-05,
      "loss": 17.4383,
      "step": 2149
    },
    {
      "epoch": 2.15,
      "grad_norm": 15461.4814453125,
      "learning_rate": 9.200206398348814e-05,
      "loss": 14.4298,
      "step": 2150
    },
    {
      "epoch": 2.15,
      "grad_norm": 36542.66796875,
      "learning_rate": 9.199690402476781e-05,
      "loss": 20.9387,
      "step": 2151
    },
    {
      "epoch": 2.15,
      "grad_norm": 7701.1240234375,
      "learning_rate": 9.199174406604748e-05,
      "loss": 20.6243,
      "step": 2152
    },
    {
      "epoch": 2.16,
      "grad_norm": 6282.9072265625,
      "learning_rate": 9.198658410732715e-05,
      "loss": 20.8457,
      "step": 2153
    },
    {
      "epoch": 2.16,
      "grad_norm": 19537.080078125,
      "learning_rate": 9.198142414860682e-05,
      "loss": 13.7283,
      "step": 2154
    },
    {
      "epoch": 2.16,
      "grad_norm": 14321.466796875,
      "learning_rate": 9.197626418988648e-05,
      "loss": 28.0058,
      "step": 2155
    },
    {
      "epoch": 2.16,
      "grad_norm": 10696.01953125,
      "learning_rate": 9.197110423116616e-05,
      "loss": 22.7001,
      "step": 2156
    },
    {
      "epoch": 2.16,
      "grad_norm": 50334.3515625,
      "learning_rate": 9.196594427244583e-05,
      "loss": 23.6724,
      "step": 2157
    },
    {
      "epoch": 2.16,
      "grad_norm": 2472.007568359375,
      "learning_rate": 9.196078431372549e-05,
      "loss": 20.3969,
      "step": 2158
    },
    {
      "epoch": 2.16,
      "grad_norm": 4414.54638671875,
      "learning_rate": 9.195562435500517e-05,
      "loss": 17.3919,
      "step": 2159
    },
    {
      "epoch": 2.16,
      "grad_norm": 14447.3154296875,
      "learning_rate": 9.195046439628483e-05,
      "loss": 20.2211,
      "step": 2160
    },
    {
      "epoch": 2.16,
      "grad_norm": 19102.69921875,
      "learning_rate": 9.19453044375645e-05,
      "loss": 17.0826,
      "step": 2161
    },
    {
      "epoch": 2.16,
      "grad_norm": 5707.978515625,
      "learning_rate": 9.194014447884418e-05,
      "loss": 17.8363,
      "step": 2162
    },
    {
      "epoch": 2.17,
      "grad_norm": 21807.478515625,
      "learning_rate": 9.193498452012385e-05,
      "loss": 18.7674,
      "step": 2163
    },
    {
      "epoch": 2.17,
      "grad_norm": 21744.283203125,
      "learning_rate": 9.192982456140351e-05,
      "loss": 15.1923,
      "step": 2164
    },
    {
      "epoch": 2.17,
      "grad_norm": 5975.43212890625,
      "learning_rate": 9.192466460268319e-05,
      "loss": 21.3548,
      "step": 2165
    },
    {
      "epoch": 2.17,
      "grad_norm": 3517.09033203125,
      "learning_rate": 9.191950464396285e-05,
      "loss": 14.1727,
      "step": 2166
    },
    {
      "epoch": 2.17,
      "grad_norm": 5223.2216796875,
      "learning_rate": 9.191434468524252e-05,
      "loss": 19.3501,
      "step": 2167
    },
    {
      "epoch": 2.17,
      "grad_norm": 31715.798828125,
      "learning_rate": 9.19091847265222e-05,
      "loss": 22.2868,
      "step": 2168
    },
    {
      "epoch": 2.17,
      "grad_norm": 16328.3740234375,
      "learning_rate": 9.190402476780187e-05,
      "loss": 19.5589,
      "step": 2169
    },
    {
      "epoch": 2.17,
      "grad_norm": 4812.9169921875,
      "learning_rate": 9.189886480908153e-05,
      "loss": 22.083,
      "step": 2170
    },
    {
      "epoch": 2.17,
      "grad_norm": 22680.896484375,
      "learning_rate": 9.18937048503612e-05,
      "loss": 17.1033,
      "step": 2171
    },
    {
      "epoch": 2.17,
      "grad_norm": 28204.61328125,
      "learning_rate": 9.188854489164087e-05,
      "loss": 18.0951,
      "step": 2172
    },
    {
      "epoch": 2.18,
      "grad_norm": 4507.89990234375,
      "learning_rate": 9.188338493292054e-05,
      "loss": 13.8378,
      "step": 2173
    },
    {
      "epoch": 2.18,
      "grad_norm": 2710.344970703125,
      "learning_rate": 9.187822497420022e-05,
      "loss": 19.2074,
      "step": 2174
    },
    {
      "epoch": 2.18,
      "grad_norm": 10551.716796875,
      "learning_rate": 9.187306501547988e-05,
      "loss": 11.4165,
      "step": 2175
    },
    {
      "epoch": 2.18,
      "grad_norm": 8510.138671875,
      "learning_rate": 9.186790505675955e-05,
      "loss": 21.8326,
      "step": 2176
    },
    {
      "epoch": 2.18,
      "grad_norm": 6878.76171875,
      "learning_rate": 9.186274509803921e-05,
      "loss": 15.8269,
      "step": 2177
    },
    {
      "epoch": 2.18,
      "grad_norm": 5508.81689453125,
      "learning_rate": 9.185758513931889e-05,
      "loss": 13.5978,
      "step": 2178
    },
    {
      "epoch": 2.18,
      "grad_norm": 6177.3095703125,
      "learning_rate": 9.185242518059856e-05,
      "loss": 13.0767,
      "step": 2179
    },
    {
      "epoch": 2.18,
      "grad_norm": 24518.291015625,
      "learning_rate": 9.184726522187824e-05,
      "loss": 26.9247,
      "step": 2180
    },
    {
      "epoch": 2.18,
      "grad_norm": 3341.848388671875,
      "learning_rate": 9.18421052631579e-05,
      "loss": 26.9277,
      "step": 2181
    },
    {
      "epoch": 2.18,
      "grad_norm": 12678.1826171875,
      "learning_rate": 9.183694530443757e-05,
      "loss": 19.7306,
      "step": 2182
    },
    {
      "epoch": 2.19,
      "grad_norm": 7491.986328125,
      "learning_rate": 9.183178534571723e-05,
      "loss": 12.6349,
      "step": 2183
    },
    {
      "epoch": 2.19,
      "grad_norm": 11145.427734375,
      "learning_rate": 9.18266253869969e-05,
      "loss": 21.5386,
      "step": 2184
    },
    {
      "epoch": 2.19,
      "grad_norm": 3699.104736328125,
      "learning_rate": 9.182146542827658e-05,
      "loss": 15.8818,
      "step": 2185
    },
    {
      "epoch": 2.19,
      "grad_norm": 13694.298828125,
      "learning_rate": 9.181630546955626e-05,
      "loss": 25.8664,
      "step": 2186
    },
    {
      "epoch": 2.19,
      "grad_norm": 11326.5947265625,
      "learning_rate": 9.181114551083592e-05,
      "loss": 14.7981,
      "step": 2187
    },
    {
      "epoch": 2.19,
      "grad_norm": 2546.705078125,
      "learning_rate": 9.180598555211559e-05,
      "loss": 14.5715,
      "step": 2188
    },
    {
      "epoch": 2.19,
      "grad_norm": 3918.915771484375,
      "learning_rate": 9.180082559339525e-05,
      "loss": 26.7902,
      "step": 2189
    },
    {
      "epoch": 2.19,
      "grad_norm": 2580.341064453125,
      "learning_rate": 9.179566563467493e-05,
      "loss": 12.248,
      "step": 2190
    },
    {
      "epoch": 2.19,
      "grad_norm": 12640.169921875,
      "learning_rate": 9.17905056759546e-05,
      "loss": 18.781,
      "step": 2191
    },
    {
      "epoch": 2.19,
      "grad_norm": 182718.921875,
      "learning_rate": 9.178534571723426e-05,
      "loss": 20.1184,
      "step": 2192
    },
    {
      "epoch": 2.2,
      "grad_norm": 6611.265625,
      "learning_rate": 9.178018575851394e-05,
      "loss": 15.4766,
      "step": 2193
    },
    {
      "epoch": 2.2,
      "grad_norm": 21298.82421875,
      "learning_rate": 9.17750257997936e-05,
      "loss": 30.3394,
      "step": 2194
    },
    {
      "epoch": 2.2,
      "grad_norm": 12425.1064453125,
      "learning_rate": 9.176986584107327e-05,
      "loss": 20.1506,
      "step": 2195
    },
    {
      "epoch": 2.2,
      "grad_norm": 5375.1767578125,
      "learning_rate": 9.176470588235295e-05,
      "loss": 29.4291,
      "step": 2196
    },
    {
      "epoch": 2.2,
      "grad_norm": 8248.5556640625,
      "learning_rate": 9.175954592363262e-05,
      "loss": 13.0937,
      "step": 2197
    },
    {
      "epoch": 2.2,
      "grad_norm": 36566.1484375,
      "learning_rate": 9.175438596491228e-05,
      "loss": 26.203,
      "step": 2198
    },
    {
      "epoch": 2.2,
      "grad_norm": 4907.12744140625,
      "learning_rate": 9.174922600619196e-05,
      "loss": 16.3108,
      "step": 2199
    },
    {
      "epoch": 2.2,
      "grad_norm": 24709.4140625,
      "learning_rate": 9.174406604747162e-05,
      "loss": 19.7558,
      "step": 2200
    },
    {
      "epoch": 2.2,
      "grad_norm": 4284.49951171875,
      "learning_rate": 9.173890608875129e-05,
      "loss": 20.3603,
      "step": 2201
    },
    {
      "epoch": 2.2,
      "grad_norm": 9832.064453125,
      "learning_rate": 9.173374613003097e-05,
      "loss": 21.2846,
      "step": 2202
    },
    {
      "epoch": 2.21,
      "grad_norm": 5848.845703125,
      "learning_rate": 9.172858617131064e-05,
      "loss": 14.4727,
      "step": 2203
    },
    {
      "epoch": 2.21,
      "grad_norm": 21088.072265625,
      "learning_rate": 9.17234262125903e-05,
      "loss": 28.5907,
      "step": 2204
    },
    {
      "epoch": 2.21,
      "grad_norm": 9228.2392578125,
      "learning_rate": 9.171826625386998e-05,
      "loss": 18.2785,
      "step": 2205
    },
    {
      "epoch": 2.21,
      "grad_norm": 9660.3740234375,
      "learning_rate": 9.171310629514964e-05,
      "loss": 29.6864,
      "step": 2206
    },
    {
      "epoch": 2.21,
      "grad_norm": 7625.34716796875,
      "learning_rate": 9.170794633642931e-05,
      "loss": 11.9278,
      "step": 2207
    },
    {
      "epoch": 2.21,
      "grad_norm": 18067.470703125,
      "learning_rate": 9.170278637770899e-05,
      "loss": 19.3515,
      "step": 2208
    },
    {
      "epoch": 2.21,
      "grad_norm": 8553.3798828125,
      "learning_rate": 9.169762641898866e-05,
      "loss": 18.2168,
      "step": 2209
    },
    {
      "epoch": 2.21,
      "grad_norm": 4886.34326171875,
      "learning_rate": 9.169246646026832e-05,
      "loss": 21.2745,
      "step": 2210
    },
    {
      "epoch": 2.21,
      "grad_norm": 3422.4912109375,
      "learning_rate": 9.168730650154798e-05,
      "loss": 15.1574,
      "step": 2211
    },
    {
      "epoch": 2.21,
      "grad_norm": 4196.16748046875,
      "learning_rate": 9.168214654282766e-05,
      "loss": 19.0155,
      "step": 2212
    },
    {
      "epoch": 2.22,
      "grad_norm": 4662.25830078125,
      "learning_rate": 9.167698658410733e-05,
      "loss": 15.233,
      "step": 2213
    },
    {
      "epoch": 2.22,
      "grad_norm": 4280.267578125,
      "learning_rate": 9.1671826625387e-05,
      "loss": 25.7493,
      "step": 2214
    },
    {
      "epoch": 2.22,
      "grad_norm": 962.2245483398438,
      "learning_rate": 9.166666666666667e-05,
      "loss": 11.7079,
      "step": 2215
    },
    {
      "epoch": 2.22,
      "grad_norm": 8072.416015625,
      "learning_rate": 9.166150670794634e-05,
      "loss": 25.0617,
      "step": 2216
    },
    {
      "epoch": 2.22,
      "grad_norm": 2481.460693359375,
      "learning_rate": 9.1656346749226e-05,
      "loss": 14.5458,
      "step": 2217
    },
    {
      "epoch": 2.22,
      "grad_norm": 3327.04248046875,
      "learning_rate": 9.165118679050568e-05,
      "loss": 16.3151,
      "step": 2218
    },
    {
      "epoch": 2.22,
      "grad_norm": 3651.6865234375,
      "learning_rate": 9.164602683178535e-05,
      "loss": 27.6667,
      "step": 2219
    },
    {
      "epoch": 2.22,
      "grad_norm": 8479.1376953125,
      "learning_rate": 9.164086687306503e-05,
      "loss": 23.0347,
      "step": 2220
    },
    {
      "epoch": 2.22,
      "grad_norm": 100488.4453125,
      "learning_rate": 9.163570691434469e-05,
      "loss": 28.0162,
      "step": 2221
    },
    {
      "epoch": 2.22,
      "grad_norm": 29924.05078125,
      "learning_rate": 9.163054695562436e-05,
      "loss": 15.6019,
      "step": 2222
    },
    {
      "epoch": 2.23,
      "grad_norm": 5133.9208984375,
      "learning_rate": 9.162538699690402e-05,
      "loss": 14.5135,
      "step": 2223
    },
    {
      "epoch": 2.23,
      "grad_norm": 5346.57080078125,
      "learning_rate": 9.16202270381837e-05,
      "loss": 14.6228,
      "step": 2224
    },
    {
      "epoch": 2.23,
      "grad_norm": 2142.904052734375,
      "learning_rate": 9.161506707946337e-05,
      "loss": 14.6925,
      "step": 2225
    },
    {
      "epoch": 2.23,
      "grad_norm": 8594.5263671875,
      "learning_rate": 9.160990712074305e-05,
      "loss": 16.0704,
      "step": 2226
    },
    {
      "epoch": 2.23,
      "grad_norm": 27424.611328125,
      "learning_rate": 9.160474716202271e-05,
      "loss": 17.7997,
      "step": 2227
    },
    {
      "epoch": 2.23,
      "grad_norm": 6201.20263671875,
      "learning_rate": 9.159958720330237e-05,
      "loss": 25.9664,
      "step": 2228
    },
    {
      "epoch": 2.23,
      "grad_norm": 2103.3369140625,
      "learning_rate": 9.159442724458204e-05,
      "loss": 16.8411,
      "step": 2229
    },
    {
      "epoch": 2.23,
      "grad_norm": 7112.04736328125,
      "learning_rate": 9.158926728586172e-05,
      "loss": 15.0791,
      "step": 2230
    },
    {
      "epoch": 2.23,
      "grad_norm": 16919.890625,
      "learning_rate": 9.158410732714139e-05,
      "loss": 17.1298,
      "step": 2231
    },
    {
      "epoch": 2.23,
      "grad_norm": 17832.4921875,
      "learning_rate": 9.157894736842105e-05,
      "loss": 20.4848,
      "step": 2232
    },
    {
      "epoch": 2.24,
      "grad_norm": 12626.3701171875,
      "learning_rate": 9.157378740970073e-05,
      "loss": 17.412,
      "step": 2233
    },
    {
      "epoch": 2.24,
      "grad_norm": 7753.38134765625,
      "learning_rate": 9.156862745098039e-05,
      "loss": 17.7977,
      "step": 2234
    },
    {
      "epoch": 2.24,
      "grad_norm": 9181.1123046875,
      "learning_rate": 9.156346749226006e-05,
      "loss": 14.8738,
      "step": 2235
    },
    {
      "epoch": 2.24,
      "grad_norm": 8261.8671875,
      "learning_rate": 9.155830753353974e-05,
      "loss": 19.9364,
      "step": 2236
    },
    {
      "epoch": 2.24,
      "grad_norm": 2658.700927734375,
      "learning_rate": 9.155314757481941e-05,
      "loss": 17.0872,
      "step": 2237
    },
    {
      "epoch": 2.24,
      "grad_norm": 2149.9619140625,
      "learning_rate": 9.154798761609907e-05,
      "loss": 13.8677,
      "step": 2238
    },
    {
      "epoch": 2.24,
      "grad_norm": 3199.755859375,
      "learning_rate": 9.154282765737875e-05,
      "loss": 17.2716,
      "step": 2239
    },
    {
      "epoch": 2.24,
      "grad_norm": 6295.431640625,
      "learning_rate": 9.153766769865841e-05,
      "loss": 15.836,
      "step": 2240
    },
    {
      "epoch": 2.24,
      "grad_norm": 7200.265625,
      "learning_rate": 9.153250773993808e-05,
      "loss": 12.2148,
      "step": 2241
    },
    {
      "epoch": 2.24,
      "grad_norm": 5772.775390625,
      "learning_rate": 9.152734778121776e-05,
      "loss": 19.1876,
      "step": 2242
    },
    {
      "epoch": 2.25,
      "grad_norm": 22447.580078125,
      "learning_rate": 9.152218782249743e-05,
      "loss": 13.3625,
      "step": 2243
    },
    {
      "epoch": 2.25,
      "grad_norm": 39293.94140625,
      "learning_rate": 9.151702786377709e-05,
      "loss": 16.6434,
      "step": 2244
    },
    {
      "epoch": 2.25,
      "grad_norm": 9406.314453125,
      "learning_rate": 9.151186790505675e-05,
      "loss": 20.5793,
      "step": 2245
    },
    {
      "epoch": 2.25,
      "grad_norm": 19738.908203125,
      "learning_rate": 9.150670794633643e-05,
      "loss": 19.4785,
      "step": 2246
    },
    {
      "epoch": 2.25,
      "grad_norm": 8344.25,
      "learning_rate": 9.15015479876161e-05,
      "loss": 17.8018,
      "step": 2247
    },
    {
      "epoch": 2.25,
      "grad_norm": 25653.564453125,
      "learning_rate": 9.149638802889578e-05,
      "loss": 15.4644,
      "step": 2248
    },
    {
      "epoch": 2.25,
      "grad_norm": 2501.03173828125,
      "learning_rate": 9.149122807017544e-05,
      "loss": 14.1346,
      "step": 2249
    },
    {
      "epoch": 2.25,
      "grad_norm": 2249.91064453125,
      "learning_rate": 9.148606811145511e-05,
      "loss": 14.4742,
      "step": 2250
    },
    {
      "epoch": 2.25,
      "grad_norm": 4582.67431640625,
      "learning_rate": 9.148090815273477e-05,
      "loss": 19.5153,
      "step": 2251
    },
    {
      "epoch": 2.25,
      "grad_norm": 2562.154296875,
      "learning_rate": 9.147574819401445e-05,
      "loss": 18.0975,
      "step": 2252
    },
    {
      "epoch": 2.26,
      "grad_norm": 8904.4794921875,
      "learning_rate": 9.147058823529412e-05,
      "loss": 23.8819,
      "step": 2253
    },
    {
      "epoch": 2.26,
      "grad_norm": 3615.48828125,
      "learning_rate": 9.14654282765738e-05,
      "loss": 17.1173,
      "step": 2254
    },
    {
      "epoch": 2.26,
      "grad_norm": 9828.650390625,
      "learning_rate": 9.146026831785346e-05,
      "loss": 21.0645,
      "step": 2255
    },
    {
      "epoch": 2.26,
      "grad_norm": 11135.4384765625,
      "learning_rate": 9.145510835913313e-05,
      "loss": 17.4661,
      "step": 2256
    },
    {
      "epoch": 2.26,
      "grad_norm": 30322.748046875,
      "learning_rate": 9.14499484004128e-05,
      "loss": 19.7753,
      "step": 2257
    },
    {
      "epoch": 2.26,
      "grad_norm": 13306.3798828125,
      "learning_rate": 9.144478844169247e-05,
      "loss": 20.0319,
      "step": 2258
    },
    {
      "epoch": 2.26,
      "grad_norm": 20595.19921875,
      "learning_rate": 9.143962848297214e-05,
      "loss": 15.1743,
      "step": 2259
    },
    {
      "epoch": 2.26,
      "grad_norm": 8980.1337890625,
      "learning_rate": 9.143446852425182e-05,
      "loss": 16.6243,
      "step": 2260
    },
    {
      "epoch": 2.26,
      "grad_norm": 19719.65234375,
      "learning_rate": 9.142930856553148e-05,
      "loss": 23.9726,
      "step": 2261
    },
    {
      "epoch": 2.26,
      "grad_norm": 9194.3759765625,
      "learning_rate": 9.142414860681115e-05,
      "loss": 11.4992,
      "step": 2262
    },
    {
      "epoch": 2.27,
      "grad_norm": 23839.46484375,
      "learning_rate": 9.141898864809081e-05,
      "loss": 20.8128,
      "step": 2263
    },
    {
      "epoch": 2.27,
      "grad_norm": 15081.328125,
      "learning_rate": 9.141382868937049e-05,
      "loss": 17.5004,
      "step": 2264
    },
    {
      "epoch": 2.27,
      "grad_norm": 5229.861328125,
      "learning_rate": 9.140866873065016e-05,
      "loss": 12.1747,
      "step": 2265
    },
    {
      "epoch": 2.27,
      "grad_norm": 2436.014404296875,
      "learning_rate": 9.140350877192982e-05,
      "loss": 23.997,
      "step": 2266
    },
    {
      "epoch": 2.27,
      "grad_norm": 7838.96533203125,
      "learning_rate": 9.13983488132095e-05,
      "loss": 17.7937,
      "step": 2267
    },
    {
      "epoch": 2.27,
      "grad_norm": 8981.498046875,
      "learning_rate": 9.139318885448916e-05,
      "loss": 19.21,
      "step": 2268
    },
    {
      "epoch": 2.27,
      "grad_norm": 71999.1171875,
      "learning_rate": 9.138802889576883e-05,
      "loss": 24.9863,
      "step": 2269
    },
    {
      "epoch": 2.27,
      "grad_norm": 17170.544921875,
      "learning_rate": 9.138286893704851e-05,
      "loss": 15.0218,
      "step": 2270
    },
    {
      "epoch": 2.27,
      "grad_norm": 15901.5595703125,
      "learning_rate": 9.137770897832818e-05,
      "loss": 26.9329,
      "step": 2271
    },
    {
      "epoch": 2.27,
      "grad_norm": 6821.05029296875,
      "learning_rate": 9.137254901960784e-05,
      "loss": 17.4035,
      "step": 2272
    },
    {
      "epoch": 2.28,
      "grad_norm": 14194.4736328125,
      "learning_rate": 9.136738906088752e-05,
      "loss": 17.2443,
      "step": 2273
    },
    {
      "epoch": 2.28,
      "grad_norm": 19638.0625,
      "learning_rate": 9.136222910216718e-05,
      "loss": 21.68,
      "step": 2274
    },
    {
      "epoch": 2.28,
      "grad_norm": 14613.4736328125,
      "learning_rate": 9.135706914344685e-05,
      "loss": 27.1936,
      "step": 2275
    },
    {
      "epoch": 2.28,
      "grad_norm": 1769.201904296875,
      "learning_rate": 9.135190918472653e-05,
      "loss": 13.8001,
      "step": 2276
    },
    {
      "epoch": 2.28,
      "grad_norm": 6456.11181640625,
      "learning_rate": 9.13467492260062e-05,
      "loss": 18.7014,
      "step": 2277
    },
    {
      "epoch": 2.28,
      "grad_norm": 49638.2421875,
      "learning_rate": 9.134158926728586e-05,
      "loss": 20.3628,
      "step": 2278
    },
    {
      "epoch": 2.28,
      "grad_norm": 16696.548828125,
      "learning_rate": 9.133642930856554e-05,
      "loss": 23.7398,
      "step": 2279
    },
    {
      "epoch": 2.28,
      "grad_norm": 15374.90625,
      "learning_rate": 9.13312693498452e-05,
      "loss": 28.3985,
      "step": 2280
    },
    {
      "epoch": 2.28,
      "grad_norm": 30591.072265625,
      "learning_rate": 9.132610939112487e-05,
      "loss": 20.4039,
      "step": 2281
    },
    {
      "epoch": 2.28,
      "grad_norm": 16337.5029296875,
      "learning_rate": 9.132094943240455e-05,
      "loss": 13.6933,
      "step": 2282
    },
    {
      "epoch": 2.29,
      "grad_norm": 18331.3515625,
      "learning_rate": 9.131578947368421e-05,
      "loss": 27.5174,
      "step": 2283
    },
    {
      "epoch": 2.29,
      "grad_norm": 5506.29736328125,
      "learning_rate": 9.131062951496388e-05,
      "loss": 13.8759,
      "step": 2284
    },
    {
      "epoch": 2.29,
      "grad_norm": 6643.314453125,
      "learning_rate": 9.130546955624355e-05,
      "loss": 17.3393,
      "step": 2285
    },
    {
      "epoch": 2.29,
      "grad_norm": 5116.93701171875,
      "learning_rate": 9.130030959752322e-05,
      "loss": 12.5935,
      "step": 2286
    },
    {
      "epoch": 2.29,
      "grad_norm": 26271.13671875,
      "learning_rate": 9.12951496388029e-05,
      "loss": 12.9037,
      "step": 2287
    },
    {
      "epoch": 2.29,
      "grad_norm": 3581.744873046875,
      "learning_rate": 9.128998968008257e-05,
      "loss": 14.1201,
      "step": 2288
    },
    {
      "epoch": 2.29,
      "grad_norm": 12881.427734375,
      "learning_rate": 9.128482972136223e-05,
      "loss": 16.2144,
      "step": 2289
    },
    {
      "epoch": 2.29,
      "grad_norm": 5310.16650390625,
      "learning_rate": 9.12796697626419e-05,
      "loss": 15.9595,
      "step": 2290
    },
    {
      "epoch": 2.29,
      "grad_norm": 7703.8525390625,
      "learning_rate": 9.127450980392157e-05,
      "loss": 24.6831,
      "step": 2291
    },
    {
      "epoch": 2.29,
      "grad_norm": 2363.271728515625,
      "learning_rate": 9.126934984520124e-05,
      "loss": 17.6566,
      "step": 2292
    },
    {
      "epoch": 2.3,
      "grad_norm": 43007.765625,
      "learning_rate": 9.126418988648091e-05,
      "loss": 22.5226,
      "step": 2293
    },
    {
      "epoch": 2.3,
      "grad_norm": 3835.732421875,
      "learning_rate": 9.125902992776059e-05,
      "loss": 19.2207,
      "step": 2294
    },
    {
      "epoch": 2.3,
      "grad_norm": 8275.4423828125,
      "learning_rate": 9.125386996904025e-05,
      "loss": 15.3733,
      "step": 2295
    },
    {
      "epoch": 2.3,
      "grad_norm": 9689.958984375,
      "learning_rate": 9.124871001031992e-05,
      "loss": 15.1619,
      "step": 2296
    },
    {
      "epoch": 2.3,
      "grad_norm": 10318.677734375,
      "learning_rate": 9.124355005159959e-05,
      "loss": 19.5907,
      "step": 2297
    },
    {
      "epoch": 2.3,
      "grad_norm": 12674.0458984375,
      "learning_rate": 9.123839009287927e-05,
      "loss": 20.0488,
      "step": 2298
    },
    {
      "epoch": 2.3,
      "grad_norm": 12642.1044921875,
      "learning_rate": 9.123323013415893e-05,
      "loss": 20.1203,
      "step": 2299
    },
    {
      "epoch": 2.3,
      "grad_norm": 3107.27880859375,
      "learning_rate": 9.12280701754386e-05,
      "loss": 14.152,
      "step": 2300
    },
    {
      "epoch": 2.3,
      "grad_norm": 7703.2021484375,
      "learning_rate": 9.122291021671827e-05,
      "loss": 11.9781,
      "step": 2301
    },
    {
      "epoch": 2.3,
      "grad_norm": 6818.427734375,
      "learning_rate": 9.121775025799793e-05,
      "loss": 16.7743,
      "step": 2302
    },
    {
      "epoch": 2.31,
      "grad_norm": 1367.96044921875,
      "learning_rate": 9.12125902992776e-05,
      "loss": 14.2892,
      "step": 2303
    },
    {
      "epoch": 2.31,
      "grad_norm": 3933.483642578125,
      "learning_rate": 9.120743034055728e-05,
      "loss": 18.0852,
      "step": 2304
    },
    {
      "epoch": 2.31,
      "grad_norm": 16568.70703125,
      "learning_rate": 9.120227038183695e-05,
      "loss": 16.819,
      "step": 2305
    },
    {
      "epoch": 2.31,
      "grad_norm": 1781105.375,
      "learning_rate": 9.119711042311662e-05,
      "loss": 24.0138,
      "step": 2306
    },
    {
      "epoch": 2.31,
      "grad_norm": 25101.583984375,
      "learning_rate": 9.119195046439629e-05,
      "loss": 29.5364,
      "step": 2307
    },
    {
      "epoch": 2.31,
      "grad_norm": 13600.126953125,
      "learning_rate": 9.118679050567595e-05,
      "loss": 13.181,
      "step": 2308
    },
    {
      "epoch": 2.31,
      "grad_norm": 4661.12646484375,
      "learning_rate": 9.118163054695564e-05,
      "loss": 16.828,
      "step": 2309
    },
    {
      "epoch": 2.31,
      "grad_norm": 44538.984375,
      "learning_rate": 9.11764705882353e-05,
      "loss": 32.7392,
      "step": 2310
    },
    {
      "epoch": 2.31,
      "grad_norm": 3077.57275390625,
      "learning_rate": 9.117131062951497e-05,
      "loss": 14.6269,
      "step": 2311
    },
    {
      "epoch": 2.31,
      "grad_norm": 8504.349609375,
      "learning_rate": 9.116615067079464e-05,
      "loss": 21.1566,
      "step": 2312
    },
    {
      "epoch": 2.32,
      "grad_norm": 3636.643798828125,
      "learning_rate": 9.116099071207431e-05,
      "loss": 17.3408,
      "step": 2313
    },
    {
      "epoch": 2.32,
      "grad_norm": 7282.86376953125,
      "learning_rate": 9.115583075335397e-05,
      "loss": 18.8165,
      "step": 2314
    },
    {
      "epoch": 2.32,
      "grad_norm": 59143.1171875,
      "learning_rate": 9.115067079463366e-05,
      "loss": 17.3268,
      "step": 2315
    },
    {
      "epoch": 2.32,
      "grad_norm": 3138.56201171875,
      "learning_rate": 9.114551083591332e-05,
      "loss": 18.319,
      "step": 2316
    },
    {
      "epoch": 2.32,
      "grad_norm": 52219.54296875,
      "learning_rate": 9.114035087719298e-05,
      "loss": 12.1836,
      "step": 2317
    },
    {
      "epoch": 2.32,
      "grad_norm": 11245.4951171875,
      "learning_rate": 9.113519091847266e-05,
      "loss": 15.8025,
      "step": 2318
    },
    {
      "epoch": 2.32,
      "grad_norm": 8130.21337890625,
      "learning_rate": 9.113003095975232e-05,
      "loss": 23.2836,
      "step": 2319
    },
    {
      "epoch": 2.32,
      "grad_norm": 6428.361328125,
      "learning_rate": 9.112487100103199e-05,
      "loss": 15.4158,
      "step": 2320
    },
    {
      "epoch": 2.32,
      "grad_norm": 18089.44921875,
      "learning_rate": 9.111971104231167e-05,
      "loss": 17.8062,
      "step": 2321
    },
    {
      "epoch": 2.32,
      "grad_norm": 6566.93701171875,
      "learning_rate": 9.111455108359134e-05,
      "loss": 16.877,
      "step": 2322
    },
    {
      "epoch": 2.33,
      "grad_norm": 2332.41455078125,
      "learning_rate": 9.1109391124871e-05,
      "loss": 20.4045,
      "step": 2323
    },
    {
      "epoch": 2.33,
      "grad_norm": 16708.4765625,
      "learning_rate": 9.110423116615068e-05,
      "loss": 23.5183,
      "step": 2324
    },
    {
      "epoch": 2.33,
      "grad_norm": 37904.5625,
      "learning_rate": 9.109907120743034e-05,
      "loss": 16.2366,
      "step": 2325
    },
    {
      "epoch": 2.33,
      "grad_norm": 21643.7421875,
      "learning_rate": 9.109391124871002e-05,
      "loss": 15.5606,
      "step": 2326
    },
    {
      "epoch": 2.33,
      "grad_norm": 7215.87646484375,
      "learning_rate": 9.108875128998969e-05,
      "loss": 23.6392,
      "step": 2327
    },
    {
      "epoch": 2.33,
      "grad_norm": 5725.41015625,
      "learning_rate": 9.108359133126936e-05,
      "loss": 21.6099,
      "step": 2328
    },
    {
      "epoch": 2.33,
      "grad_norm": 3874.168701171875,
      "learning_rate": 9.107843137254902e-05,
      "loss": 24.4226,
      "step": 2329
    },
    {
      "epoch": 2.33,
      "grad_norm": 1383.832763671875,
      "learning_rate": 9.10732714138287e-05,
      "loss": 13.5417,
      "step": 2330
    },
    {
      "epoch": 2.33,
      "grad_norm": 1689.0057373046875,
      "learning_rate": 9.106811145510836e-05,
      "loss": 14.1015,
      "step": 2331
    },
    {
      "epoch": 2.33,
      "grad_norm": 2434.579833984375,
      "learning_rate": 9.106295149638804e-05,
      "loss": 15.0438,
      "step": 2332
    },
    {
      "epoch": 2.34,
      "grad_norm": 18352.0078125,
      "learning_rate": 9.10577915376677e-05,
      "loss": 19.8084,
      "step": 2333
    },
    {
      "epoch": 2.34,
      "grad_norm": 17012.14453125,
      "learning_rate": 9.105263157894738e-05,
      "loss": 20.8009,
      "step": 2334
    },
    {
      "epoch": 2.34,
      "grad_norm": 29916.720703125,
      "learning_rate": 9.104747162022704e-05,
      "loss": 18.8287,
      "step": 2335
    },
    {
      "epoch": 2.34,
      "grad_norm": 16027.5966796875,
      "learning_rate": 9.10423116615067e-05,
      "loss": 21.5862,
      "step": 2336
    },
    {
      "epoch": 2.34,
      "grad_norm": 31567.7734375,
      "learning_rate": 9.103715170278639e-05,
      "loss": 20.7292,
      "step": 2337
    },
    {
      "epoch": 2.34,
      "grad_norm": 11977.8271484375,
      "learning_rate": 9.103199174406605e-05,
      "loss": 34.6912,
      "step": 2338
    },
    {
      "epoch": 2.34,
      "grad_norm": 1997.7418212890625,
      "learning_rate": 9.102683178534573e-05,
      "loss": 16.2557,
      "step": 2339
    },
    {
      "epoch": 2.34,
      "grad_norm": 13049.65234375,
      "learning_rate": 9.102167182662539e-05,
      "loss": 23.5922,
      "step": 2340
    },
    {
      "epoch": 2.34,
      "grad_norm": 3798.02685546875,
      "learning_rate": 9.101651186790506e-05,
      "loss": 22.3801,
      "step": 2341
    },
    {
      "epoch": 2.34,
      "grad_norm": 3138.9619140625,
      "learning_rate": 9.101135190918472e-05,
      "loss": 12.0999,
      "step": 2342
    },
    {
      "epoch": 2.35,
      "grad_norm": 4098.74755859375,
      "learning_rate": 9.100619195046441e-05,
      "loss": 14.1331,
      "step": 2343
    },
    {
      "epoch": 2.35,
      "grad_norm": 2939.802978515625,
      "learning_rate": 9.100103199174407e-05,
      "loss": 13.4848,
      "step": 2344
    },
    {
      "epoch": 2.35,
      "grad_norm": 9482.728515625,
      "learning_rate": 9.099587203302375e-05,
      "loss": 20.4912,
      "step": 2345
    },
    {
      "epoch": 2.35,
      "grad_norm": 2405.966552734375,
      "learning_rate": 9.09907120743034e-05,
      "loss": 13.3307,
      "step": 2346
    },
    {
      "epoch": 2.35,
      "grad_norm": 8064.14404296875,
      "learning_rate": 9.098555211558308e-05,
      "loss": 18.4571,
      "step": 2347
    },
    {
      "epoch": 2.35,
      "grad_norm": 937.0836791992188,
      "learning_rate": 9.098039215686274e-05,
      "loss": 10.8021,
      "step": 2348
    },
    {
      "epoch": 2.35,
      "grad_norm": 3588.228271484375,
      "learning_rate": 9.097523219814243e-05,
      "loss": 12.6818,
      "step": 2349
    },
    {
      "epoch": 2.35,
      "grad_norm": 2001.19970703125,
      "learning_rate": 9.097007223942209e-05,
      "loss": 23.4562,
      "step": 2350
    },
    {
      "epoch": 2.35,
      "grad_norm": 4699.26416015625,
      "learning_rate": 9.096491228070177e-05,
      "loss": 16.7744,
      "step": 2351
    },
    {
      "epoch": 2.35,
      "grad_norm": 13190.9736328125,
      "learning_rate": 9.095975232198143e-05,
      "loss": 16.4977,
      "step": 2352
    },
    {
      "epoch": 2.36,
      "grad_norm": 21227.65234375,
      "learning_rate": 9.095459236326109e-05,
      "loss": 21.0801,
      "step": 2353
    },
    {
      "epoch": 2.36,
      "grad_norm": 5045.74755859375,
      "learning_rate": 9.094943240454078e-05,
      "loss": 20.547,
      "step": 2354
    },
    {
      "epoch": 2.36,
      "grad_norm": 7058.82177734375,
      "learning_rate": 9.094427244582044e-05,
      "loss": 27.5263,
      "step": 2355
    },
    {
      "epoch": 2.36,
      "grad_norm": 6732.11962890625,
      "learning_rate": 9.093911248710011e-05,
      "loss": 16.4118,
      "step": 2356
    },
    {
      "epoch": 2.36,
      "grad_norm": 68071.046875,
      "learning_rate": 9.093395252837977e-05,
      "loss": 17.7054,
      "step": 2357
    },
    {
      "epoch": 2.36,
      "grad_norm": 10972.6904296875,
      "learning_rate": 9.092879256965945e-05,
      "loss": 16.4139,
      "step": 2358
    },
    {
      "epoch": 2.36,
      "grad_norm": 12414.083984375,
      "learning_rate": 9.092363261093911e-05,
      "loss": 16.14,
      "step": 2359
    },
    {
      "epoch": 2.36,
      "grad_norm": 8532.046875,
      "learning_rate": 9.09184726522188e-05,
      "loss": 18.2445,
      "step": 2360
    },
    {
      "epoch": 2.36,
      "grad_norm": 10134.505859375,
      "learning_rate": 9.091331269349846e-05,
      "loss": 15.6444,
      "step": 2361
    },
    {
      "epoch": 2.36,
      "grad_norm": 1986.6033935546875,
      "learning_rate": 9.090815273477813e-05,
      "loss": 12.9901,
      "step": 2362
    },
    {
      "epoch": 2.37,
      "grad_norm": 631.9110717773438,
      "learning_rate": 9.090299277605779e-05,
      "loss": 11.4913,
      "step": 2363
    },
    {
      "epoch": 2.37,
      "grad_norm": 107749.890625,
      "learning_rate": 9.089783281733747e-05,
      "loss": 30.7924,
      "step": 2364
    },
    {
      "epoch": 2.37,
      "grad_norm": 854.7945556640625,
      "learning_rate": 9.089267285861713e-05,
      "loss": 14.4853,
      "step": 2365
    },
    {
      "epoch": 2.37,
      "grad_norm": 6481.62939453125,
      "learning_rate": 9.088751289989682e-05,
      "loss": 22.5703,
      "step": 2366
    },
    {
      "epoch": 2.37,
      "grad_norm": 5980.07373046875,
      "learning_rate": 9.088235294117648e-05,
      "loss": 18.4096,
      "step": 2367
    },
    {
      "epoch": 2.37,
      "grad_norm": 5855.248046875,
      "learning_rate": 9.087719298245615e-05,
      "loss": 27.6064,
      "step": 2368
    },
    {
      "epoch": 2.37,
      "grad_norm": 5562.26513671875,
      "learning_rate": 9.087203302373581e-05,
      "loss": 14.6581,
      "step": 2369
    },
    {
      "epoch": 2.37,
      "grad_norm": 4284.81396484375,
      "learning_rate": 9.086687306501547e-05,
      "loss": 23.4483,
      "step": 2370
    },
    {
      "epoch": 2.37,
      "grad_norm": 15832.880859375,
      "learning_rate": 9.086171310629516e-05,
      "loss": 22.4511,
      "step": 2371
    },
    {
      "epoch": 2.37,
      "grad_norm": 23741.87109375,
      "learning_rate": 9.085655314757482e-05,
      "loss": 20.6277,
      "step": 2372
    },
    {
      "epoch": 2.38,
      "grad_norm": 31040.115234375,
      "learning_rate": 9.08513931888545e-05,
      "loss": 16.8562,
      "step": 2373
    },
    {
      "epoch": 2.38,
      "grad_norm": 9719.51953125,
      "learning_rate": 9.084623323013416e-05,
      "loss": 19.7682,
      "step": 2374
    },
    {
      "epoch": 2.38,
      "grad_norm": 1985.393310546875,
      "learning_rate": 9.084107327141383e-05,
      "loss": 32.8488,
      "step": 2375
    },
    {
      "epoch": 2.38,
      "grad_norm": 19611.880859375,
      "learning_rate": 9.083591331269349e-05,
      "loss": 49.1724,
      "step": 2376
    },
    {
      "epoch": 2.38,
      "grad_norm": 2126.6044921875,
      "learning_rate": 9.083075335397318e-05,
      "loss": 20.8307,
      "step": 2377
    },
    {
      "epoch": 2.38,
      "grad_norm": 31844.71484375,
      "learning_rate": 9.082559339525284e-05,
      "loss": 24.5871,
      "step": 2378
    },
    {
      "epoch": 2.38,
      "grad_norm": 6122.9697265625,
      "learning_rate": 9.082043343653252e-05,
      "loss": 22.4046,
      "step": 2379
    },
    {
      "epoch": 2.38,
      "grad_norm": 12675.0,
      "learning_rate": 9.081527347781218e-05,
      "loss": 19.1915,
      "step": 2380
    },
    {
      "epoch": 2.38,
      "grad_norm": 45909.7265625,
      "learning_rate": 9.081011351909185e-05,
      "loss": 29.7544,
      "step": 2381
    },
    {
      "epoch": 2.38,
      "grad_norm": 6028.23388671875,
      "learning_rate": 9.080495356037153e-05,
      "loss": 15.2484,
      "step": 2382
    },
    {
      "epoch": 2.39,
      "grad_norm": 10215.6025390625,
      "learning_rate": 9.07997936016512e-05,
      "loss": 28.094,
      "step": 2383
    },
    {
      "epoch": 2.39,
      "grad_norm": 12742.2001953125,
      "learning_rate": 9.079463364293086e-05,
      "loss": 14.0973,
      "step": 2384
    },
    {
      "epoch": 2.39,
      "grad_norm": 26449.96484375,
      "learning_rate": 9.078947368421054e-05,
      "loss": 22.7432,
      "step": 2385
    },
    {
      "epoch": 2.39,
      "grad_norm": 4487.8095703125,
      "learning_rate": 9.07843137254902e-05,
      "loss": 18.4668,
      "step": 2386
    },
    {
      "epoch": 2.39,
      "grad_norm": 10254.73046875,
      "learning_rate": 9.077915376676987e-05,
      "loss": 22.933,
      "step": 2387
    },
    {
      "epoch": 2.39,
      "grad_norm": 2080.34423828125,
      "learning_rate": 9.077399380804955e-05,
      "loss": 11.2312,
      "step": 2388
    },
    {
      "epoch": 2.39,
      "grad_norm": 7806.89794921875,
      "learning_rate": 9.076883384932921e-05,
      "loss": 15.4911,
      "step": 2389
    },
    {
      "epoch": 2.39,
      "grad_norm": 4641.38330078125,
      "learning_rate": 9.076367389060888e-05,
      "loss": 16.0505,
      "step": 2390
    },
    {
      "epoch": 2.39,
      "grad_norm": 1817.382568359375,
      "learning_rate": 9.075851393188854e-05,
      "loss": 13.0376,
      "step": 2391
    },
    {
      "epoch": 2.39,
      "grad_norm": 11959.9306640625,
      "learning_rate": 9.075335397316822e-05,
      "loss": 17.3222,
      "step": 2392
    },
    {
      "epoch": 2.4,
      "grad_norm": 12100.701171875,
      "learning_rate": 9.074819401444788e-05,
      "loss": 23.2408,
      "step": 2393
    },
    {
      "epoch": 2.4,
      "grad_norm": 16630.353515625,
      "learning_rate": 9.074303405572757e-05,
      "loss": 18.9968,
      "step": 2394
    },
    {
      "epoch": 2.4,
      "grad_norm": 6815.037109375,
      "learning_rate": 9.073787409700723e-05,
      "loss": 14.1882,
      "step": 2395
    },
    {
      "epoch": 2.4,
      "grad_norm": 2246.04150390625,
      "learning_rate": 9.07327141382869e-05,
      "loss": 16.128,
      "step": 2396
    },
    {
      "epoch": 2.4,
      "grad_norm": 6737.00927734375,
      "learning_rate": 9.072755417956656e-05,
      "loss": 19.3182,
      "step": 2397
    },
    {
      "epoch": 2.4,
      "grad_norm": 12081.5302734375,
      "learning_rate": 9.072239422084624e-05,
      "loss": 23.9552,
      "step": 2398
    },
    {
      "epoch": 2.4,
      "grad_norm": 4219.00390625,
      "learning_rate": 9.071723426212591e-05,
      "loss": 27.5472,
      "step": 2399
    },
    {
      "epoch": 2.4,
      "grad_norm": 4062.5302734375,
      "learning_rate": 9.071207430340559e-05,
      "loss": 13.3555,
      "step": 2400
    },
    {
      "epoch": 2.4,
      "grad_norm": 20718.794921875,
      "learning_rate": 9.070691434468525e-05,
      "loss": 15.7237,
      "step": 2401
    },
    {
      "epoch": 2.4,
      "grad_norm": 15346.8837890625,
      "learning_rate": 9.070175438596492e-05,
      "loss": 20.7179,
      "step": 2402
    },
    {
      "epoch": 2.41,
      "grad_norm": 5937.94140625,
      "learning_rate": 9.069659442724458e-05,
      "loss": 15.8222,
      "step": 2403
    },
    {
      "epoch": 2.41,
      "grad_norm": 10143.083984375,
      "learning_rate": 9.069143446852426e-05,
      "loss": 15.7363,
      "step": 2404
    },
    {
      "epoch": 2.41,
      "grad_norm": 6758.61083984375,
      "learning_rate": 9.068627450980393e-05,
      "loss": 12.0117,
      "step": 2405
    },
    {
      "epoch": 2.41,
      "grad_norm": 27191.1640625,
      "learning_rate": 9.068111455108359e-05,
      "loss": 14.2306,
      "step": 2406
    },
    {
      "epoch": 2.41,
      "grad_norm": 18006.578125,
      "learning_rate": 9.067595459236327e-05,
      "loss": 17.5392,
      "step": 2407
    },
    {
      "epoch": 2.41,
      "grad_norm": 3354.322265625,
      "learning_rate": 9.067079463364293e-05,
      "loss": 19.3193,
      "step": 2408
    },
    {
      "epoch": 2.41,
      "grad_norm": 2030.7352294921875,
      "learning_rate": 9.06656346749226e-05,
      "loss": 15.3579,
      "step": 2409
    },
    {
      "epoch": 2.41,
      "grad_norm": 7149.875,
      "learning_rate": 9.066047471620228e-05,
      "loss": 18.8023,
      "step": 2410
    },
    {
      "epoch": 2.41,
      "grad_norm": 1549.5029296875,
      "learning_rate": 9.065531475748195e-05,
      "loss": 16.6016,
      "step": 2411
    },
    {
      "epoch": 2.41,
      "grad_norm": 1134.1632080078125,
      "learning_rate": 9.065015479876161e-05,
      "loss": 14.9908,
      "step": 2412
    },
    {
      "epoch": 2.42,
      "grad_norm": 10946.1875,
      "learning_rate": 9.064499484004129e-05,
      "loss": 22.8993,
      "step": 2413
    },
    {
      "epoch": 2.42,
      "grad_norm": 55845.140625,
      "learning_rate": 9.063983488132095e-05,
      "loss": 26.0062,
      "step": 2414
    },
    {
      "epoch": 2.42,
      "grad_norm": 27004.220703125,
      "learning_rate": 9.063467492260062e-05,
      "loss": 20.763,
      "step": 2415
    },
    {
      "epoch": 2.42,
      "grad_norm": 21648.81640625,
      "learning_rate": 9.06295149638803e-05,
      "loss": 16.6671,
      "step": 2416
    },
    {
      "epoch": 2.42,
      "grad_norm": 8123.138671875,
      "learning_rate": 9.062435500515997e-05,
      "loss": 15.5087,
      "step": 2417
    },
    {
      "epoch": 2.42,
      "grad_norm": 21672.6328125,
      "learning_rate": 9.061919504643963e-05,
      "loss": 33.6014,
      "step": 2418
    },
    {
      "epoch": 2.42,
      "grad_norm": 5877.44775390625,
      "learning_rate": 9.061403508771931e-05,
      "loss": 12.5267,
      "step": 2419
    },
    {
      "epoch": 2.42,
      "grad_norm": 2832.746826171875,
      "learning_rate": 9.060887512899897e-05,
      "loss": 13.6511,
      "step": 2420
    },
    {
      "epoch": 2.42,
      "grad_norm": 21895.78515625,
      "learning_rate": 9.060371517027864e-05,
      "loss": 21.2983,
      "step": 2421
    },
    {
      "epoch": 2.42,
      "grad_norm": 11400.076171875,
      "learning_rate": 9.059855521155832e-05,
      "loss": 23.1379,
      "step": 2422
    },
    {
      "epoch": 2.43,
      "grad_norm": 1998.948486328125,
      "learning_rate": 9.059339525283799e-05,
      "loss": 14.03,
      "step": 2423
    },
    {
      "epoch": 2.43,
      "grad_norm": 2949.968994140625,
      "learning_rate": 9.058823529411765e-05,
      "loss": 20.242,
      "step": 2424
    },
    {
      "epoch": 2.43,
      "grad_norm": 2722.808837890625,
      "learning_rate": 9.058307533539731e-05,
      "loss": 24.8113,
      "step": 2425
    },
    {
      "epoch": 2.43,
      "grad_norm": 2667.8818359375,
      "learning_rate": 9.057791537667699e-05,
      "loss": 15.8753,
      "step": 2426
    },
    {
      "epoch": 2.43,
      "grad_norm": 27588.947265625,
      "learning_rate": 9.057275541795666e-05,
      "loss": 19.755,
      "step": 2427
    },
    {
      "epoch": 2.43,
      "grad_norm": 8985.322265625,
      "learning_rate": 9.056759545923634e-05,
      "loss": 22.4994,
      "step": 2428
    },
    {
      "epoch": 2.43,
      "grad_norm": 4870.8125,
      "learning_rate": 9.0562435500516e-05,
      "loss": 16.0831,
      "step": 2429
    },
    {
      "epoch": 2.43,
      "grad_norm": 10953.2099609375,
      "learning_rate": 9.055727554179567e-05,
      "loss": 16.4365,
      "step": 2430
    },
    {
      "epoch": 2.43,
      "grad_norm": 15287.32421875,
      "learning_rate": 9.055211558307533e-05,
      "loss": 20.4083,
      "step": 2431
    },
    {
      "epoch": 2.43,
      "grad_norm": 6959.11181640625,
      "learning_rate": 9.054695562435501e-05,
      "loss": 11.9319,
      "step": 2432
    },
    {
      "epoch": 2.44,
      "grad_norm": 5964.27294921875,
      "learning_rate": 9.054179566563468e-05,
      "loss": 21.2321,
      "step": 2433
    },
    {
      "epoch": 2.44,
      "grad_norm": 24165.9765625,
      "learning_rate": 9.053663570691436e-05,
      "loss": 20.8261,
      "step": 2434
    },
    {
      "epoch": 2.44,
      "grad_norm": 5323.4208984375,
      "learning_rate": 9.053147574819402e-05,
      "loss": 15.5656,
      "step": 2435
    },
    {
      "epoch": 2.44,
      "grad_norm": 5591.92041015625,
      "learning_rate": 9.052631578947369e-05,
      "loss": 38.636,
      "step": 2436
    },
    {
      "epoch": 2.44,
      "grad_norm": 5058.20703125,
      "learning_rate": 9.052115583075335e-05,
      "loss": 18.1892,
      "step": 2437
    },
    {
      "epoch": 2.44,
      "grad_norm": 5872.23388671875,
      "learning_rate": 9.051599587203303e-05,
      "loss": 13.2003,
      "step": 2438
    },
    {
      "epoch": 2.44,
      "grad_norm": 14379.1865234375,
      "learning_rate": 9.05108359133127e-05,
      "loss": 15.0668,
      "step": 2439
    },
    {
      "epoch": 2.44,
      "grad_norm": 3275.05029296875,
      "learning_rate": 9.050567595459238e-05,
      "loss": 14.1833,
      "step": 2440
    },
    {
      "epoch": 2.44,
      "grad_norm": 3621.877685546875,
      "learning_rate": 9.050051599587204e-05,
      "loss": 18.0441,
      "step": 2441
    },
    {
      "epoch": 2.44,
      "grad_norm": 5436.041015625,
      "learning_rate": 9.04953560371517e-05,
      "loss": 11.7289,
      "step": 2442
    },
    {
      "epoch": 2.45,
      "grad_norm": 4233.83544921875,
      "learning_rate": 9.049019607843137e-05,
      "loss": 16.6447,
      "step": 2443
    },
    {
      "epoch": 2.45,
      "grad_norm": 5624.0693359375,
      "learning_rate": 9.048503611971105e-05,
      "loss": 25.4397,
      "step": 2444
    },
    {
      "epoch": 2.45,
      "grad_norm": 2185.18505859375,
      "learning_rate": 9.047987616099072e-05,
      "loss": 12.8614,
      "step": 2445
    },
    {
      "epoch": 2.45,
      "grad_norm": 2019.2852783203125,
      "learning_rate": 9.047471620227038e-05,
      "loss": 12.7891,
      "step": 2446
    },
    {
      "epoch": 2.45,
      "grad_norm": 46878.453125,
      "learning_rate": 9.046955624355006e-05,
      "loss": 17.0298,
      "step": 2447
    },
    {
      "epoch": 2.45,
      "grad_norm": 8009.4619140625,
      "learning_rate": 9.046439628482972e-05,
      "loss": 22.1442,
      "step": 2448
    },
    {
      "epoch": 2.45,
      "grad_norm": 4831.84716796875,
      "learning_rate": 9.04592363261094e-05,
      "loss": 12.8875,
      "step": 2449
    },
    {
      "epoch": 2.45,
      "grad_norm": 6961.35009765625,
      "learning_rate": 9.045407636738907e-05,
      "loss": 19.635,
      "step": 2450
    },
    {
      "epoch": 2.45,
      "grad_norm": 94922.7265625,
      "learning_rate": 9.044891640866874e-05,
      "loss": 13.5988,
      "step": 2451
    },
    {
      "epoch": 2.45,
      "grad_norm": 10062.818359375,
      "learning_rate": 9.04437564499484e-05,
      "loss": 13.9225,
      "step": 2452
    },
    {
      "epoch": 2.46,
      "grad_norm": 79448.546875,
      "learning_rate": 9.043859649122808e-05,
      "loss": 19.5128,
      "step": 2453
    },
    {
      "epoch": 2.46,
      "grad_norm": 1919.393310546875,
      "learning_rate": 9.043343653250774e-05,
      "loss": 12.4791,
      "step": 2454
    },
    {
      "epoch": 2.46,
      "grad_norm": 15385.044921875,
      "learning_rate": 9.042827657378741e-05,
      "loss": 31.4266,
      "step": 2455
    },
    {
      "epoch": 2.46,
      "grad_norm": 20004.375,
      "learning_rate": 9.042311661506709e-05,
      "loss": 15.158,
      "step": 2456
    },
    {
      "epoch": 2.46,
      "grad_norm": 6359.19482421875,
      "learning_rate": 9.041795665634676e-05,
      "loss": 12.0694,
      "step": 2457
    },
    {
      "epoch": 2.46,
      "grad_norm": 7775.11279296875,
      "learning_rate": 9.041279669762642e-05,
      "loss": 16.1191,
      "step": 2458
    },
    {
      "epoch": 2.46,
      "grad_norm": 14899.8095703125,
      "learning_rate": 9.04076367389061e-05,
      "loss": 24.7348,
      "step": 2459
    },
    {
      "epoch": 2.46,
      "grad_norm": 2695.658447265625,
      "learning_rate": 9.040247678018576e-05,
      "loss": 11.5694,
      "step": 2460
    },
    {
      "epoch": 2.46,
      "grad_norm": 17919.595703125,
      "learning_rate": 9.039731682146543e-05,
      "loss": 32.8344,
      "step": 2461
    },
    {
      "epoch": 2.46,
      "grad_norm": 15804.3232421875,
      "learning_rate": 9.039215686274511e-05,
      "loss": 14.8416,
      "step": 2462
    },
    {
      "epoch": 2.47,
      "grad_norm": 8117.876953125,
      "learning_rate": 9.038699690402477e-05,
      "loss": 15.5656,
      "step": 2463
    },
    {
      "epoch": 2.47,
      "grad_norm": 6430.8359375,
      "learning_rate": 9.038183694530444e-05,
      "loss": 18.5556,
      "step": 2464
    },
    {
      "epoch": 2.47,
      "grad_norm": 22391.876953125,
      "learning_rate": 9.03766769865841e-05,
      "loss": 21.0695,
      "step": 2465
    },
    {
      "epoch": 2.47,
      "grad_norm": 15219.7119140625,
      "learning_rate": 9.037151702786378e-05,
      "loss": 19.5894,
      "step": 2466
    },
    {
      "epoch": 2.47,
      "grad_norm": 39299.234375,
      "learning_rate": 9.036635706914345e-05,
      "loss": 18.8537,
      "step": 2467
    },
    {
      "epoch": 2.47,
      "grad_norm": 11325.55078125,
      "learning_rate": 9.036119711042313e-05,
      "loss": 23.1806,
      "step": 2468
    },
    {
      "epoch": 2.47,
      "grad_norm": 13850.7939453125,
      "learning_rate": 9.035603715170279e-05,
      "loss": 25.0011,
      "step": 2469
    },
    {
      "epoch": 2.47,
      "grad_norm": 5186.4267578125,
      "learning_rate": 9.035087719298246e-05,
      "loss": 11.5007,
      "step": 2470
    },
    {
      "epoch": 2.47,
      "grad_norm": 191496.125,
      "learning_rate": 9.034571723426212e-05,
      "loss": 15.0999,
      "step": 2471
    },
    {
      "epoch": 2.47,
      "grad_norm": 6171.1796875,
      "learning_rate": 9.03405572755418e-05,
      "loss": 21.1008,
      "step": 2472
    },
    {
      "epoch": 2.48,
      "grad_norm": 34441.7421875,
      "learning_rate": 9.033539731682147e-05,
      "loss": 21.0043,
      "step": 2473
    },
    {
      "epoch": 2.48,
      "grad_norm": 3126.234619140625,
      "learning_rate": 9.033023735810115e-05,
      "loss": 10.7674,
      "step": 2474
    },
    {
      "epoch": 2.48,
      "grad_norm": 3931.303955078125,
      "learning_rate": 9.032507739938081e-05,
      "loss": 12.8593,
      "step": 2475
    },
    {
      "epoch": 2.48,
      "grad_norm": 324258.375,
      "learning_rate": 9.031991744066048e-05,
      "loss": 23.1881,
      "step": 2476
    },
    {
      "epoch": 2.48,
      "grad_norm": 7861.9482421875,
      "learning_rate": 9.031475748194014e-05,
      "loss": 15.649,
      "step": 2477
    },
    {
      "epoch": 2.48,
      "grad_norm": 18711.791015625,
      "learning_rate": 9.030959752321982e-05,
      "loss": 25.8054,
      "step": 2478
    },
    {
      "epoch": 2.48,
      "grad_norm": 5231.4462890625,
      "learning_rate": 9.03044375644995e-05,
      "loss": 11.2734,
      "step": 2479
    },
    {
      "epoch": 2.48,
      "grad_norm": 7095.962890625,
      "learning_rate": 9.029927760577915e-05,
      "loss": 14.0352,
      "step": 2480
    },
    {
      "epoch": 2.48,
      "grad_norm": 5813.78564453125,
      "learning_rate": 9.029411764705883e-05,
      "loss": 14.7261,
      "step": 2481
    },
    {
      "epoch": 2.48,
      "grad_norm": 36759.11328125,
      "learning_rate": 9.028895768833849e-05,
      "loss": 25.7044,
      "step": 2482
    },
    {
      "epoch": 2.49,
      "grad_norm": 4099.3642578125,
      "learning_rate": 9.028379772961816e-05,
      "loss": 14.5549,
      "step": 2483
    },
    {
      "epoch": 2.49,
      "grad_norm": 17588.806640625,
      "learning_rate": 9.027863777089784e-05,
      "loss": 14.5708,
      "step": 2484
    },
    {
      "epoch": 2.49,
      "grad_norm": 1366.249755859375,
      "learning_rate": 9.027347781217751e-05,
      "loss": 17.1208,
      "step": 2485
    },
    {
      "epoch": 2.49,
      "grad_norm": 8214.0732421875,
      "learning_rate": 9.026831785345717e-05,
      "loss": 20.1369,
      "step": 2486
    },
    {
      "epoch": 2.49,
      "grad_norm": 12644.6533203125,
      "learning_rate": 9.026315789473685e-05,
      "loss": 14.4532,
      "step": 2487
    },
    {
      "epoch": 2.49,
      "grad_norm": 17781.146484375,
      "learning_rate": 9.025799793601651e-05,
      "loss": 11.5077,
      "step": 2488
    },
    {
      "epoch": 2.49,
      "grad_norm": 5344.36767578125,
      "learning_rate": 9.025283797729618e-05,
      "loss": 21.7759,
      "step": 2489
    },
    {
      "epoch": 2.49,
      "grad_norm": 14301.861328125,
      "learning_rate": 9.024767801857586e-05,
      "loss": 26.0003,
      "step": 2490
    },
    {
      "epoch": 2.49,
      "grad_norm": 8798.3076171875,
      "learning_rate": 9.024251805985553e-05,
      "loss": 17.3499,
      "step": 2491
    },
    {
      "epoch": 2.49,
      "grad_norm": 8936.3486328125,
      "learning_rate": 9.02373581011352e-05,
      "loss": 27.5077,
      "step": 2492
    },
    {
      "epoch": 2.5,
      "grad_norm": 8120.93603515625,
      "learning_rate": 9.023219814241487e-05,
      "loss": 15.4441,
      "step": 2493
    },
    {
      "epoch": 2.5,
      "grad_norm": 21900.775390625,
      "learning_rate": 9.022703818369453e-05,
      "loss": 13.0366,
      "step": 2494
    },
    {
      "epoch": 2.5,
      "grad_norm": 2902.185302734375,
      "learning_rate": 9.02218782249742e-05,
      "loss": 20.6684,
      "step": 2495
    },
    {
      "epoch": 2.5,
      "grad_norm": 53479.95703125,
      "learning_rate": 9.021671826625388e-05,
      "loss": 14.5024,
      "step": 2496
    },
    {
      "epoch": 2.5,
      "grad_norm": 8253.6904296875,
      "learning_rate": 9.021155830753354e-05,
      "loss": 15.6967,
      "step": 2497
    },
    {
      "epoch": 2.5,
      "grad_norm": 11299.09765625,
      "learning_rate": 9.020639834881321e-05,
      "loss": 22.8074,
      "step": 2498
    },
    {
      "epoch": 2.5,
      "grad_norm": 4043.822509765625,
      "learning_rate": 9.020123839009288e-05,
      "loss": 15.8103,
      "step": 2499
    },
    {
      "epoch": 2.5,
      "grad_norm": 16209.16796875,
      "learning_rate": 9.019607843137255e-05,
      "loss": 28.1307,
      "step": 2500
    },
    {
      "epoch": 2.5,
      "grad_norm": 3183.1953125,
      "learning_rate": 9.019091847265222e-05,
      "loss": 12.1836,
      "step": 2501
    },
    {
      "epoch": 2.5,
      "grad_norm": 14035.361328125,
      "learning_rate": 9.01857585139319e-05,
      "loss": 19.3418,
      "step": 2502
    },
    {
      "epoch": 2.51,
      "grad_norm": 41571.0234375,
      "learning_rate": 9.018059855521156e-05,
      "loss": 25.3957,
      "step": 2503
    },
    {
      "epoch": 2.51,
      "grad_norm": 13122.6669921875,
      "learning_rate": 9.017543859649123e-05,
      "loss": 24.1005,
      "step": 2504
    },
    {
      "epoch": 2.51,
      "grad_norm": 5272.513671875,
      "learning_rate": 9.01702786377709e-05,
      "loss": 19.6151,
      "step": 2505
    },
    {
      "epoch": 2.51,
      "grad_norm": 9031.501953125,
      "learning_rate": 9.016511867905057e-05,
      "loss": 16.7352,
      "step": 2506
    },
    {
      "epoch": 2.51,
      "grad_norm": 10290.7900390625,
      "learning_rate": 9.015995872033024e-05,
      "loss": 12.6235,
      "step": 2507
    },
    {
      "epoch": 2.51,
      "grad_norm": 3843.394775390625,
      "learning_rate": 9.015479876160992e-05,
      "loss": 15.7385,
      "step": 2508
    },
    {
      "epoch": 2.51,
      "grad_norm": 5948.0615234375,
      "learning_rate": 9.014963880288958e-05,
      "loss": 13.4566,
      "step": 2509
    },
    {
      "epoch": 2.51,
      "grad_norm": 39653.27734375,
      "learning_rate": 9.014447884416925e-05,
      "loss": 14.4684,
      "step": 2510
    },
    {
      "epoch": 2.51,
      "grad_norm": 4678.86181640625,
      "learning_rate": 9.013931888544892e-05,
      "loss": 14.2835,
      "step": 2511
    },
    {
      "epoch": 2.51,
      "grad_norm": 2398.906982421875,
      "learning_rate": 9.013415892672859e-05,
      "loss": 12.6915,
      "step": 2512
    },
    {
      "epoch": 2.52,
      "grad_norm": 2682.730224609375,
      "learning_rate": 9.012899896800826e-05,
      "loss": 12.3936,
      "step": 2513
    },
    {
      "epoch": 2.52,
      "grad_norm": 2534.150146484375,
      "learning_rate": 9.012383900928793e-05,
      "loss": 13.8135,
      "step": 2514
    },
    {
      "epoch": 2.52,
      "grad_norm": 3808.560546875,
      "learning_rate": 9.01186790505676e-05,
      "loss": 14.7853,
      "step": 2515
    },
    {
      "epoch": 2.52,
      "grad_norm": 8294.47265625,
      "learning_rate": 9.011351909184726e-05,
      "loss": 15.1651,
      "step": 2516
    },
    {
      "epoch": 2.52,
      "grad_norm": 3623.157958984375,
      "learning_rate": 9.010835913312694e-05,
      "loss": 21.7497,
      "step": 2517
    },
    {
      "epoch": 2.52,
      "grad_norm": 5770.83251953125,
      "learning_rate": 9.010319917440661e-05,
      "loss": 17.5437,
      "step": 2518
    },
    {
      "epoch": 2.52,
      "grad_norm": 15229.447265625,
      "learning_rate": 9.009803921568628e-05,
      "loss": 17.2879,
      "step": 2519
    },
    {
      "epoch": 2.52,
      "grad_norm": 4226.51025390625,
      "learning_rate": 9.009287925696595e-05,
      "loss": 28.5722,
      "step": 2520
    },
    {
      "epoch": 2.52,
      "grad_norm": 59679.1796875,
      "learning_rate": 9.008771929824562e-05,
      "loss": 28.4151,
      "step": 2521
    },
    {
      "epoch": 2.52,
      "grad_norm": 4697.95458984375,
      "learning_rate": 9.008255933952528e-05,
      "loss": 15.7361,
      "step": 2522
    },
    {
      "epoch": 2.53,
      "grad_norm": 9906.53515625,
      "learning_rate": 9.007739938080496e-05,
      "loss": 22.3902,
      "step": 2523
    },
    {
      "epoch": 2.53,
      "grad_norm": 4039.4765625,
      "learning_rate": 9.007223942208463e-05,
      "loss": 18.8883,
      "step": 2524
    },
    {
      "epoch": 2.53,
      "grad_norm": 8051.3671875,
      "learning_rate": 9.00670794633643e-05,
      "loss": 16.2962,
      "step": 2525
    },
    {
      "epoch": 2.53,
      "grad_norm": 4708.1552734375,
      "learning_rate": 9.006191950464397e-05,
      "loss": 13.521,
      "step": 2526
    },
    {
      "epoch": 2.53,
      "grad_norm": 6626.0927734375,
      "learning_rate": 9.005675954592364e-05,
      "loss": 25.3351,
      "step": 2527
    },
    {
      "epoch": 2.53,
      "grad_norm": 3113.678955078125,
      "learning_rate": 9.00515995872033e-05,
      "loss": 30.597,
      "step": 2528
    },
    {
      "epoch": 2.53,
      "grad_norm": 3763.6875,
      "learning_rate": 9.004643962848298e-05,
      "loss": 13.5383,
      "step": 2529
    },
    {
      "epoch": 2.53,
      "grad_norm": 3723.36767578125,
      "learning_rate": 9.004127966976265e-05,
      "loss": 16.6352,
      "step": 2530
    },
    {
      "epoch": 2.53,
      "grad_norm": 7403.96630859375,
      "learning_rate": 9.003611971104231e-05,
      "loss": 16.2199,
      "step": 2531
    },
    {
      "epoch": 2.53,
      "grad_norm": 61468.5,
      "learning_rate": 9.003095975232199e-05,
      "loss": 23.9324,
      "step": 2532
    },
    {
      "epoch": 2.54,
      "grad_norm": 1374.66796875,
      "learning_rate": 9.002579979360165e-05,
      "loss": 23.8498,
      "step": 2533
    },
    {
      "epoch": 2.54,
      "grad_norm": 10314.0654296875,
      "learning_rate": 9.002063983488132e-05,
      "loss": 22.6438,
      "step": 2534
    },
    {
      "epoch": 2.54,
      "grad_norm": 17820.3828125,
      "learning_rate": 9.0015479876161e-05,
      "loss": 17.0655,
      "step": 2535
    },
    {
      "epoch": 2.54,
      "grad_norm": 9498.787109375,
      "learning_rate": 9.001031991744067e-05,
      "loss": 14.2101,
      "step": 2536
    },
    {
      "epoch": 2.54,
      "grad_norm": 16234.6328125,
      "learning_rate": 9.000515995872033e-05,
      "loss": 18.275,
      "step": 2537
    },
    {
      "epoch": 2.54,
      "grad_norm": 3056.861572265625,
      "learning_rate": 9e-05,
      "loss": 17.8106,
      "step": 2538
    },
    {
      "epoch": 2.54,
      "grad_norm": 7374.728515625,
      "learning_rate": 8.999484004127967e-05,
      "loss": 21.1703,
      "step": 2539
    },
    {
      "epoch": 2.54,
      "grad_norm": 7388.267578125,
      "learning_rate": 8.998968008255934e-05,
      "loss": 20.1517,
      "step": 2540
    },
    {
      "epoch": 2.54,
      "grad_norm": 2989.88671875,
      "learning_rate": 8.998452012383902e-05,
      "loss": 19.3126,
      "step": 2541
    },
    {
      "epoch": 2.54,
      "grad_norm": 3647.256103515625,
      "learning_rate": 8.997936016511869e-05,
      "loss": 16.2518,
      "step": 2542
    },
    {
      "epoch": 2.55,
      "grad_norm": 5997.18359375,
      "learning_rate": 8.997420020639835e-05,
      "loss": 14.0084,
      "step": 2543
    },
    {
      "epoch": 2.55,
      "grad_norm": 4373.7783203125,
      "learning_rate": 8.996904024767803e-05,
      "loss": 15.6082,
      "step": 2544
    },
    {
      "epoch": 2.55,
      "grad_norm": 70974.15625,
      "learning_rate": 8.996388028895769e-05,
      "loss": 21.6938,
      "step": 2545
    },
    {
      "epoch": 2.55,
      "grad_norm": 3858.138916015625,
      "learning_rate": 8.995872033023736e-05,
      "loss": 18.7883,
      "step": 2546
    },
    {
      "epoch": 2.55,
      "grad_norm": 888.1036987304688,
      "learning_rate": 8.995356037151704e-05,
      "loss": 13.9425,
      "step": 2547
    },
    {
      "epoch": 2.55,
      "grad_norm": 8076.30126953125,
      "learning_rate": 8.994840041279671e-05,
      "loss": 11.9995,
      "step": 2548
    },
    {
      "epoch": 2.55,
      "grad_norm": 7305.59521484375,
      "learning_rate": 8.994324045407637e-05,
      "loss": 18.5464,
      "step": 2549
    },
    {
      "epoch": 2.55,
      "grad_norm": 25019.12890625,
      "learning_rate": 8.993808049535603e-05,
      "loss": 16.3768,
      "step": 2550
    },
    {
      "epoch": 2.55,
      "grad_norm": 3591.45654296875,
      "learning_rate": 8.99329205366357e-05,
      "loss": 12.963,
      "step": 2551
    },
    {
      "epoch": 2.55,
      "grad_norm": 4751.26904296875,
      "learning_rate": 8.992776057791538e-05,
      "loss": 21.9903,
      "step": 2552
    },
    {
      "epoch": 2.56,
      "grad_norm": 9898.513671875,
      "learning_rate": 8.992260061919506e-05,
      "loss": 19.6263,
      "step": 2553
    },
    {
      "epoch": 2.56,
      "grad_norm": 3883.9951171875,
      "learning_rate": 8.991744066047472e-05,
      "loss": 15.3383,
      "step": 2554
    },
    {
      "epoch": 2.56,
      "grad_norm": 6913.17041015625,
      "learning_rate": 8.991228070175439e-05,
      "loss": 14.3684,
      "step": 2555
    },
    {
      "epoch": 2.56,
      "grad_norm": 6810.4208984375,
      "learning_rate": 8.990712074303405e-05,
      "loss": 18.6335,
      "step": 2556
    },
    {
      "epoch": 2.56,
      "grad_norm": 4090.875244140625,
      "learning_rate": 8.990196078431373e-05,
      "loss": 14.1861,
      "step": 2557
    },
    {
      "epoch": 2.56,
      "grad_norm": 1098.89990234375,
      "learning_rate": 8.98968008255934e-05,
      "loss": 13.9943,
      "step": 2558
    },
    {
      "epoch": 2.56,
      "grad_norm": 24049.91796875,
      "learning_rate": 8.989164086687308e-05,
      "loss": 15.2112,
      "step": 2559
    },
    {
      "epoch": 2.56,
      "grad_norm": 9430.404296875,
      "learning_rate": 8.988648090815274e-05,
      "loss": 14.386,
      "step": 2560
    },
    {
      "epoch": 2.56,
      "grad_norm": 9432.048828125,
      "learning_rate": 8.988132094943241e-05,
      "loss": 12.9583,
      "step": 2561
    },
    {
      "epoch": 2.56,
      "grad_norm": 16000.400390625,
      "learning_rate": 8.987616099071207e-05,
      "loss": 21.6451,
      "step": 2562
    },
    {
      "epoch": 2.57,
      "grad_norm": 8617.166015625,
      "learning_rate": 8.987100103199175e-05,
      "loss": 19.185,
      "step": 2563
    },
    {
      "epoch": 2.57,
      "grad_norm": 86444.859375,
      "learning_rate": 8.986584107327142e-05,
      "loss": 25.293,
      "step": 2564
    },
    {
      "epoch": 2.57,
      "grad_norm": 3995.291259765625,
      "learning_rate": 8.98606811145511e-05,
      "loss": 14.8625,
      "step": 2565
    },
    {
      "epoch": 2.57,
      "grad_norm": 10665.615234375,
      "learning_rate": 8.985552115583076e-05,
      "loss": 18.2202,
      "step": 2566
    },
    {
      "epoch": 2.57,
      "grad_norm": 8671.818359375,
      "learning_rate": 8.985036119711042e-05,
      "loss": 14.2774,
      "step": 2567
    },
    {
      "epoch": 2.57,
      "grad_norm": 5177.28515625,
      "learning_rate": 8.984520123839009e-05,
      "loss": 14.2448,
      "step": 2568
    },
    {
      "epoch": 2.57,
      "grad_norm": 12684.1640625,
      "learning_rate": 8.984004127966977e-05,
      "loss": 22.2204,
      "step": 2569
    },
    {
      "epoch": 2.57,
      "grad_norm": 5916.35546875,
      "learning_rate": 8.983488132094944e-05,
      "loss": 13.9513,
      "step": 2570
    },
    {
      "epoch": 2.57,
      "grad_norm": 36532.7890625,
      "learning_rate": 8.98297213622291e-05,
      "loss": 17.86,
      "step": 2571
    },
    {
      "epoch": 2.57,
      "grad_norm": 21325.259765625,
      "learning_rate": 8.982456140350878e-05,
      "loss": 18.9872,
      "step": 2572
    },
    {
      "epoch": 2.58,
      "grad_norm": 18448.42578125,
      "learning_rate": 8.981940144478844e-05,
      "loss": 17.3532,
      "step": 2573
    },
    {
      "epoch": 2.58,
      "grad_norm": 5036.6103515625,
      "learning_rate": 8.981424148606811e-05,
      "loss": 12.9141,
      "step": 2574
    },
    {
      "epoch": 2.58,
      "grad_norm": 6703.9521484375,
      "learning_rate": 8.980908152734779e-05,
      "loss": 13.5533,
      "step": 2575
    },
    {
      "epoch": 2.58,
      "grad_norm": 7353.06884765625,
      "learning_rate": 8.980392156862746e-05,
      "loss": 15.7795,
      "step": 2576
    },
    {
      "epoch": 2.58,
      "grad_norm": 10112.46484375,
      "learning_rate": 8.979876160990712e-05,
      "loss": 14.1522,
      "step": 2577
    },
    {
      "epoch": 2.58,
      "grad_norm": 2069.84521484375,
      "learning_rate": 8.97936016511868e-05,
      "loss": 13.6513,
      "step": 2578
    },
    {
      "epoch": 2.58,
      "grad_norm": 4533.4033203125,
      "learning_rate": 8.978844169246646e-05,
      "loss": 15.1238,
      "step": 2579
    },
    {
      "epoch": 2.58,
      "grad_norm": 7745.58837890625,
      "learning_rate": 8.978328173374613e-05,
      "loss": 19.7699,
      "step": 2580
    },
    {
      "epoch": 2.58,
      "grad_norm": 41954.10546875,
      "learning_rate": 8.97781217750258e-05,
      "loss": 23.7894,
      "step": 2581
    },
    {
      "epoch": 2.58,
      "grad_norm": 4953.28076171875,
      "learning_rate": 8.977296181630548e-05,
      "loss": 12.9907,
      "step": 2582
    },
    {
      "epoch": 2.59,
      "grad_norm": 14655.849609375,
      "learning_rate": 8.976780185758514e-05,
      "loss": 16.3445,
      "step": 2583
    },
    {
      "epoch": 2.59,
      "grad_norm": 5405.67529296875,
      "learning_rate": 8.976264189886482e-05,
      "loss": 15.6813,
      "step": 2584
    },
    {
      "epoch": 2.59,
      "grad_norm": 3983.68603515625,
      "learning_rate": 8.975748194014448e-05,
      "loss": 27.1334,
      "step": 2585
    },
    {
      "epoch": 2.59,
      "grad_norm": 21760.306640625,
      "learning_rate": 8.975232198142415e-05,
      "loss": 33.7537,
      "step": 2586
    },
    {
      "epoch": 2.59,
      "grad_norm": 6712.60107421875,
      "learning_rate": 8.974716202270383e-05,
      "loss": 23.7716,
      "step": 2587
    },
    {
      "epoch": 2.59,
      "grad_norm": 21347.109375,
      "learning_rate": 8.974200206398349e-05,
      "loss": 26.4545,
      "step": 2588
    },
    {
      "epoch": 2.59,
      "grad_norm": 2744.718994140625,
      "learning_rate": 8.973684210526316e-05,
      "loss": 15.2309,
      "step": 2589
    },
    {
      "epoch": 2.59,
      "grad_norm": 28145.962890625,
      "learning_rate": 8.973168214654282e-05,
      "loss": 22.8184,
      "step": 2590
    },
    {
      "epoch": 2.59,
      "grad_norm": 7384.9599609375,
      "learning_rate": 8.97265221878225e-05,
      "loss": 23.2664,
      "step": 2591
    },
    {
      "epoch": 2.59,
      "grad_norm": 19128.330078125,
      "learning_rate": 8.972136222910217e-05,
      "loss": 18.3851,
      "step": 2592
    },
    {
      "epoch": 2.6,
      "grad_norm": 8289.447265625,
      "learning_rate": 8.971620227038185e-05,
      "loss": 13.4358,
      "step": 2593
    },
    {
      "epoch": 2.6,
      "grad_norm": 3812.460205078125,
      "learning_rate": 8.971104231166151e-05,
      "loss": 16.673,
      "step": 2594
    },
    {
      "epoch": 2.6,
      "grad_norm": 24748.470703125,
      "learning_rate": 8.970588235294118e-05,
      "loss": 13.0585,
      "step": 2595
    },
    {
      "epoch": 2.6,
      "grad_norm": 46331.375,
      "learning_rate": 8.970072239422084e-05,
      "loss": 16.8653,
      "step": 2596
    },
    {
      "epoch": 2.6,
      "grad_norm": 904.8001098632812,
      "learning_rate": 8.969556243550052e-05,
      "loss": 11.3313,
      "step": 2597
    },
    {
      "epoch": 2.6,
      "grad_norm": 27077.888671875,
      "learning_rate": 8.969040247678019e-05,
      "loss": 19.7225,
      "step": 2598
    },
    {
      "epoch": 2.6,
      "grad_norm": 1274.8514404296875,
      "learning_rate": 8.968524251805987e-05,
      "loss": 11.6811,
      "step": 2599
    },
    {
      "epoch": 2.6,
      "grad_norm": 10828.4599609375,
      "learning_rate": 8.968008255933953e-05,
      "loss": 20.7933,
      "step": 2600
    },
    {
      "epoch": 2.6,
      "grad_norm": 2045.408203125,
      "learning_rate": 8.96749226006192e-05,
      "loss": 17.3417,
      "step": 2601
    },
    {
      "epoch": 2.6,
      "grad_norm": 3644.159912109375,
      "learning_rate": 8.966976264189886e-05,
      "loss": 30.3499,
      "step": 2602
    },
    {
      "epoch": 2.61,
      "grad_norm": 5801.15771484375,
      "learning_rate": 8.966460268317854e-05,
      "loss": 23.181,
      "step": 2603
    },
    {
      "epoch": 2.61,
      "grad_norm": 18733.49609375,
      "learning_rate": 8.965944272445821e-05,
      "loss": 26.8716,
      "step": 2604
    },
    {
      "epoch": 2.61,
      "grad_norm": 12116.970703125,
      "learning_rate": 8.965428276573787e-05,
      "loss": 18.4749,
      "step": 2605
    },
    {
      "epoch": 2.61,
      "grad_norm": 19787.6640625,
      "learning_rate": 8.964912280701755e-05,
      "loss": 17.0318,
      "step": 2606
    },
    {
      "epoch": 2.61,
      "grad_norm": 18871.267578125,
      "learning_rate": 8.964396284829721e-05,
      "loss": 16.0381,
      "step": 2607
    },
    {
      "epoch": 2.61,
      "grad_norm": 19784.69140625,
      "learning_rate": 8.963880288957688e-05,
      "loss": 17.1287,
      "step": 2608
    },
    {
      "epoch": 2.61,
      "grad_norm": 9213.4716796875,
      "learning_rate": 8.963364293085656e-05,
      "loss": 15.3279,
      "step": 2609
    },
    {
      "epoch": 2.61,
      "grad_norm": 40829.6328125,
      "learning_rate": 8.962848297213623e-05,
      "loss": 19.7391,
      "step": 2610
    },
    {
      "epoch": 2.61,
      "grad_norm": 8523.833984375,
      "learning_rate": 8.962332301341589e-05,
      "loss": 18.8711,
      "step": 2611
    },
    {
      "epoch": 2.61,
      "grad_norm": 10313.4609375,
      "learning_rate": 8.961816305469557e-05,
      "loss": 13.7666,
      "step": 2612
    },
    {
      "epoch": 2.62,
      "grad_norm": 4272.2783203125,
      "learning_rate": 8.961300309597523e-05,
      "loss": 20.1016,
      "step": 2613
    },
    {
      "epoch": 2.62,
      "grad_norm": 133061.640625,
      "learning_rate": 8.96078431372549e-05,
      "loss": 22.2383,
      "step": 2614
    },
    {
      "epoch": 2.62,
      "grad_norm": 4302.56298828125,
      "learning_rate": 8.960268317853458e-05,
      "loss": 13.9696,
      "step": 2615
    },
    {
      "epoch": 2.62,
      "grad_norm": 9889.318359375,
      "learning_rate": 8.959752321981425e-05,
      "loss": 26.7882,
      "step": 2616
    },
    {
      "epoch": 2.62,
      "grad_norm": 12753.5849609375,
      "learning_rate": 8.959236326109391e-05,
      "loss": 17.0841,
      "step": 2617
    },
    {
      "epoch": 2.62,
      "grad_norm": 30227.78515625,
      "learning_rate": 8.958720330237359e-05,
      "loss": 14.6579,
      "step": 2618
    },
    {
      "epoch": 2.62,
      "grad_norm": 15885.1904296875,
      "learning_rate": 8.958204334365325e-05,
      "loss": 24.3156,
      "step": 2619
    },
    {
      "epoch": 2.62,
      "grad_norm": 12219.3798828125,
      "learning_rate": 8.957688338493294e-05,
      "loss": 24.5533,
      "step": 2620
    },
    {
      "epoch": 2.62,
      "grad_norm": 39241.18359375,
      "learning_rate": 8.95717234262126e-05,
      "loss": 26.6161,
      "step": 2621
    },
    {
      "epoch": 2.62,
      "grad_norm": 10808.306640625,
      "learning_rate": 8.956656346749226e-05,
      "loss": 13.2483,
      "step": 2622
    },
    {
      "epoch": 2.63,
      "grad_norm": 10157.42578125,
      "learning_rate": 8.956140350877193e-05,
      "loss": 21.1323,
      "step": 2623
    },
    {
      "epoch": 2.63,
      "grad_norm": 9530.03515625,
      "learning_rate": 8.95562435500516e-05,
      "loss": 23.0647,
      "step": 2624
    },
    {
      "epoch": 2.63,
      "grad_norm": 6269.4716796875,
      "learning_rate": 8.955108359133127e-05,
      "loss": 19.3939,
      "step": 2625
    },
    {
      "epoch": 2.63,
      "grad_norm": 5368.9296875,
      "learning_rate": 8.954592363261094e-05,
      "loss": 15.6256,
      "step": 2626
    },
    {
      "epoch": 2.63,
      "grad_norm": 6106.09033203125,
      "learning_rate": 8.954076367389062e-05,
      "loss": 21.9871,
      "step": 2627
    },
    {
      "epoch": 2.63,
      "grad_norm": 8461.150390625,
      "learning_rate": 8.953560371517028e-05,
      "loss": 18.3902,
      "step": 2628
    },
    {
      "epoch": 2.63,
      "grad_norm": 4813.67724609375,
      "learning_rate": 8.953044375644995e-05,
      "loss": 20.3716,
      "step": 2629
    },
    {
      "epoch": 2.63,
      "grad_norm": 4151.69482421875,
      "learning_rate": 8.952528379772961e-05,
      "loss": 17.963,
      "step": 2630
    },
    {
      "epoch": 2.63,
      "grad_norm": 8262.880859375,
      "learning_rate": 8.95201238390093e-05,
      "loss": 19.2466,
      "step": 2631
    },
    {
      "epoch": 2.63,
      "grad_norm": 12649.9052734375,
      "learning_rate": 8.951496388028896e-05,
      "loss": 21.8902,
      "step": 2632
    },
    {
      "epoch": 2.64,
      "grad_norm": 7830.87060546875,
      "learning_rate": 8.950980392156864e-05,
      "loss": 12.8022,
      "step": 2633
    },
    {
      "epoch": 2.64,
      "grad_norm": 1826.510009765625,
      "learning_rate": 8.95046439628483e-05,
      "loss": 13.153,
      "step": 2634
    },
    {
      "epoch": 2.64,
      "grad_norm": 7395.19580078125,
      "learning_rate": 8.949948400412797e-05,
      "loss": 14.5026,
      "step": 2635
    },
    {
      "epoch": 2.64,
      "grad_norm": 1233.0919189453125,
      "learning_rate": 8.949432404540763e-05,
      "loss": 13.1582,
      "step": 2636
    },
    {
      "epoch": 2.64,
      "grad_norm": 5407.68017578125,
      "learning_rate": 8.948916408668732e-05,
      "loss": 13.0436,
      "step": 2637
    },
    {
      "epoch": 2.64,
      "grad_norm": 7500.6083984375,
      "learning_rate": 8.948400412796698e-05,
      "loss": 14.2094,
      "step": 2638
    },
    {
      "epoch": 2.64,
      "grad_norm": 12451.5947265625,
      "learning_rate": 8.947884416924664e-05,
      "loss": 11.645,
      "step": 2639
    },
    {
      "epoch": 2.64,
      "grad_norm": 2619.55908203125,
      "learning_rate": 8.947368421052632e-05,
      "loss": 15.3569,
      "step": 2640
    },
    {
      "epoch": 2.64,
      "grad_norm": 1004.2664184570312,
      "learning_rate": 8.946852425180598e-05,
      "loss": 10.1643,
      "step": 2641
    },
    {
      "epoch": 2.64,
      "grad_norm": 6454.32958984375,
      "learning_rate": 8.946336429308565e-05,
      "loss": 19.4006,
      "step": 2642
    },
    {
      "epoch": 2.65,
      "grad_norm": 11998.6787109375,
      "learning_rate": 8.945820433436533e-05,
      "loss": 15.3733,
      "step": 2643
    },
    {
      "epoch": 2.65,
      "grad_norm": 2045.6693115234375,
      "learning_rate": 8.9453044375645e-05,
      "loss": 14.9775,
      "step": 2644
    },
    {
      "epoch": 2.65,
      "grad_norm": 110866.1640625,
      "learning_rate": 8.944788441692466e-05,
      "loss": 14.3682,
      "step": 2645
    },
    {
      "epoch": 2.65,
      "grad_norm": 3164.710693359375,
      "learning_rate": 8.944272445820434e-05,
      "loss": 12.6245,
      "step": 2646
    },
    {
      "epoch": 2.65,
      "grad_norm": 5466.6826171875,
      "learning_rate": 8.9437564499484e-05,
      "loss": 15.3502,
      "step": 2647
    },
    {
      "epoch": 2.65,
      "grad_norm": 2751.821044921875,
      "learning_rate": 8.943240454076369e-05,
      "loss": 13.4534,
      "step": 2648
    },
    {
      "epoch": 2.65,
      "grad_norm": 8322.701171875,
      "learning_rate": 8.942724458204335e-05,
      "loss": 24.1634,
      "step": 2649
    },
    {
      "epoch": 2.65,
      "grad_norm": 2744.79736328125,
      "learning_rate": 8.942208462332302e-05,
      "loss": 13.6349,
      "step": 2650
    },
    {
      "epoch": 2.65,
      "grad_norm": 3298.1640625,
      "learning_rate": 8.941692466460268e-05,
      "loss": 13.6356,
      "step": 2651
    },
    {
      "epoch": 2.65,
      "grad_norm": 7828.890625,
      "learning_rate": 8.941176470588236e-05,
      "loss": 14.5827,
      "step": 2652
    },
    {
      "epoch": 2.66,
      "grad_norm": 11608.380859375,
      "learning_rate": 8.940660474716202e-05,
      "loss": 14.7316,
      "step": 2653
    },
    {
      "epoch": 2.66,
      "grad_norm": 42541.18359375,
      "learning_rate": 8.940144478844171e-05,
      "loss": 18.3684,
      "step": 2654
    },
    {
      "epoch": 2.66,
      "grad_norm": 5867.5322265625,
      "learning_rate": 8.939628482972137e-05,
      "loss": 19.6725,
      "step": 2655
    },
    {
      "epoch": 2.66,
      "grad_norm": 18543.189453125,
      "learning_rate": 8.939112487100103e-05,
      "loss": 19.3142,
      "step": 2656
    },
    {
      "epoch": 2.66,
      "grad_norm": 8760.484375,
      "learning_rate": 8.93859649122807e-05,
      "loss": 10.5865,
      "step": 2657
    },
    {
      "epoch": 2.66,
      "grad_norm": 2842.635498046875,
      "learning_rate": 8.938080495356037e-05,
      "loss": 16.8255,
      "step": 2658
    },
    {
      "epoch": 2.66,
      "grad_norm": 3896.853271484375,
      "learning_rate": 8.937564499484005e-05,
      "loss": 12.7568,
      "step": 2659
    },
    {
      "epoch": 2.66,
      "grad_norm": 13720.4404296875,
      "learning_rate": 8.937048503611971e-05,
      "loss": 27.481,
      "step": 2660
    },
    {
      "epoch": 2.66,
      "grad_norm": 4985.01171875,
      "learning_rate": 8.936532507739939e-05,
      "loss": 13.1124,
      "step": 2661
    },
    {
      "epoch": 2.66,
      "grad_norm": 6035.20751953125,
      "learning_rate": 8.936016511867905e-05,
      "loss": 15.3852,
      "step": 2662
    },
    {
      "epoch": 2.67,
      "grad_norm": 7241.3310546875,
      "learning_rate": 8.935500515995872e-05,
      "loss": 17.8694,
      "step": 2663
    },
    {
      "epoch": 2.67,
      "grad_norm": 1622.2789306640625,
      "learning_rate": 8.934984520123839e-05,
      "loss": 12.1067,
      "step": 2664
    },
    {
      "epoch": 2.67,
      "grad_norm": 7757.06591796875,
      "learning_rate": 8.934468524251807e-05,
      "loss": 19.1069,
      "step": 2665
    },
    {
      "epoch": 2.67,
      "grad_norm": 3676.983154296875,
      "learning_rate": 8.933952528379773e-05,
      "loss": 13.1291,
      "step": 2666
    },
    {
      "epoch": 2.67,
      "grad_norm": 3684.84814453125,
      "learning_rate": 8.933436532507741e-05,
      "loss": 13.0257,
      "step": 2667
    },
    {
      "epoch": 2.67,
      "grad_norm": 49132.98046875,
      "learning_rate": 8.932920536635707e-05,
      "loss": 22.7435,
      "step": 2668
    },
    {
      "epoch": 2.67,
      "grad_norm": 7912.7060546875,
      "learning_rate": 8.932404540763674e-05,
      "loss": 19.3006,
      "step": 2669
    },
    {
      "epoch": 2.67,
      "grad_norm": 7562.705078125,
      "learning_rate": 8.93188854489164e-05,
      "loss": 13.8017,
      "step": 2670
    },
    {
      "epoch": 2.67,
      "grad_norm": 8749.80078125,
      "learning_rate": 8.931372549019609e-05,
      "loss": 15.0095,
      "step": 2671
    },
    {
      "epoch": 2.67,
      "grad_norm": 41034.3125,
      "learning_rate": 8.930856553147575e-05,
      "loss": 14.8311,
      "step": 2672
    },
    {
      "epoch": 2.68,
      "grad_norm": 12514.2314453125,
      "learning_rate": 8.930340557275543e-05,
      "loss": 14.1174,
      "step": 2673
    },
    {
      "epoch": 2.68,
      "grad_norm": 3945.371826171875,
      "learning_rate": 8.929824561403509e-05,
      "loss": 19.229,
      "step": 2674
    },
    {
      "epoch": 2.68,
      "grad_norm": 23199.564453125,
      "learning_rate": 8.929308565531475e-05,
      "loss": 29.5114,
      "step": 2675
    },
    {
      "epoch": 2.68,
      "grad_norm": 29091.20703125,
      "learning_rate": 8.928792569659444e-05,
      "loss": 12.3983,
      "step": 2676
    },
    {
      "epoch": 2.68,
      "grad_norm": 3889.9873046875,
      "learning_rate": 8.92827657378741e-05,
      "loss": 20.931,
      "step": 2677
    },
    {
      "epoch": 2.68,
      "grad_norm": 102374.640625,
      "learning_rate": 8.927760577915377e-05,
      "loss": 17.9014,
      "step": 2678
    },
    {
      "epoch": 2.68,
      "grad_norm": 5741.365234375,
      "learning_rate": 8.927244582043344e-05,
      "loss": 12.4555,
      "step": 2679
    },
    {
      "epoch": 2.68,
      "grad_norm": 18902.31640625,
      "learning_rate": 8.926728586171311e-05,
      "loss": 24.8757,
      "step": 2680
    },
    {
      "epoch": 2.68,
      "grad_norm": 6912.07861328125,
      "learning_rate": 8.926212590299277e-05,
      "loss": 12.6949,
      "step": 2681
    },
    {
      "epoch": 2.68,
      "grad_norm": 1391.93994140625,
      "learning_rate": 8.925696594427246e-05,
      "loss": 19.8167,
      "step": 2682
    },
    {
      "epoch": 2.69,
      "grad_norm": 18289.802734375,
      "learning_rate": 8.925180598555212e-05,
      "loss": 20.2427,
      "step": 2683
    },
    {
      "epoch": 2.69,
      "grad_norm": 42094.63671875,
      "learning_rate": 8.92466460268318e-05,
      "loss": 22.2033,
      "step": 2684
    },
    {
      "epoch": 2.69,
      "grad_norm": 3490.93408203125,
      "learning_rate": 8.924148606811146e-05,
      "loss": 12.9707,
      "step": 2685
    },
    {
      "epoch": 2.69,
      "grad_norm": 2953.522705078125,
      "learning_rate": 8.923632610939113e-05,
      "loss": 17.8884,
      "step": 2686
    },
    {
      "epoch": 2.69,
      "grad_norm": 1710.32666015625,
      "learning_rate": 8.92311661506708e-05,
      "loss": 10.5758,
      "step": 2687
    },
    {
      "epoch": 2.69,
      "grad_norm": 13511.8115234375,
      "learning_rate": 8.922600619195048e-05,
      "loss": 21.905,
      "step": 2688
    },
    {
      "epoch": 2.69,
      "grad_norm": 5660.82080078125,
      "learning_rate": 8.922084623323014e-05,
      "loss": 15.0003,
      "step": 2689
    },
    {
      "epoch": 2.69,
      "grad_norm": 2578.20703125,
      "learning_rate": 8.921568627450981e-05,
      "loss": 19.3603,
      "step": 2690
    },
    {
      "epoch": 2.69,
      "grad_norm": 17768.9609375,
      "learning_rate": 8.921052631578948e-05,
      "loss": 21.6651,
      "step": 2691
    },
    {
      "epoch": 2.69,
      "grad_norm": 12976.5751953125,
      "learning_rate": 8.920536635706914e-05,
      "loss": 18.3613,
      "step": 2692
    },
    {
      "epoch": 2.7,
      "grad_norm": 4127.19921875,
      "learning_rate": 8.920020639834882e-05,
      "loss": 20.9615,
      "step": 2693
    },
    {
      "epoch": 2.7,
      "grad_norm": 4914.0654296875,
      "learning_rate": 8.919504643962849e-05,
      "loss": 14.9001,
      "step": 2694
    },
    {
      "epoch": 2.7,
      "grad_norm": 5547.07373046875,
      "learning_rate": 8.918988648090816e-05,
      "loss": 19.5506,
      "step": 2695
    },
    {
      "epoch": 2.7,
      "grad_norm": 9118.8994140625,
      "learning_rate": 8.918472652218782e-05,
      "loss": 28.1868,
      "step": 2696
    },
    {
      "epoch": 2.7,
      "grad_norm": 26440.765625,
      "learning_rate": 8.91795665634675e-05,
      "loss": 21.079,
      "step": 2697
    },
    {
      "epoch": 2.7,
      "grad_norm": 2861.519775390625,
      "learning_rate": 8.917440660474716e-05,
      "loss": 11.7488,
      "step": 2698
    },
    {
      "epoch": 2.7,
      "grad_norm": 8073.212890625,
      "learning_rate": 8.916924664602684e-05,
      "loss": 15.3285,
      "step": 2699
    },
    {
      "epoch": 2.7,
      "grad_norm": 2706.342529296875,
      "learning_rate": 8.91640866873065e-05,
      "loss": 13.5656,
      "step": 2700
    },
    {
      "epoch": 2.7,
      "grad_norm": 12690.244140625,
      "learning_rate": 8.915892672858618e-05,
      "loss": 18.6554,
      "step": 2701
    },
    {
      "epoch": 2.7,
      "grad_norm": 1961.6495361328125,
      "learning_rate": 8.915376676986584e-05,
      "loss": 15.2044,
      "step": 2702
    },
    {
      "epoch": 2.71,
      "grad_norm": 15851.8828125,
      "learning_rate": 8.914860681114552e-05,
      "loss": 22.3045,
      "step": 2703
    },
    {
      "epoch": 2.71,
      "grad_norm": 10292.6953125,
      "learning_rate": 8.914344685242519e-05,
      "loss": 15.8222,
      "step": 2704
    },
    {
      "epoch": 2.71,
      "grad_norm": 12702.548828125,
      "learning_rate": 8.913828689370486e-05,
      "loss": 18.6962,
      "step": 2705
    },
    {
      "epoch": 2.71,
      "grad_norm": 32603.095703125,
      "learning_rate": 8.913312693498453e-05,
      "loss": 19.1595,
      "step": 2706
    },
    {
      "epoch": 2.71,
      "grad_norm": 12501.9306640625,
      "learning_rate": 8.91279669762642e-05,
      "loss": 21.3424,
      "step": 2707
    },
    {
      "epoch": 2.71,
      "grad_norm": 3337.52734375,
      "learning_rate": 8.912280701754386e-05,
      "loss": 12.449,
      "step": 2708
    },
    {
      "epoch": 2.71,
      "grad_norm": 6893.6240234375,
      "learning_rate": 8.911764705882354e-05,
      "loss": 25.3352,
      "step": 2709
    },
    {
      "epoch": 2.71,
      "grad_norm": 5541.25439453125,
      "learning_rate": 8.911248710010321e-05,
      "loss": 32.0032,
      "step": 2710
    },
    {
      "epoch": 2.71,
      "grad_norm": 3385.799072265625,
      "learning_rate": 8.910732714138287e-05,
      "loss": 13.8589,
      "step": 2711
    },
    {
      "epoch": 2.71,
      "grad_norm": 2404.249267578125,
      "learning_rate": 8.910216718266255e-05,
      "loss": 13.6509,
      "step": 2712
    },
    {
      "epoch": 2.72,
      "grad_norm": 2783.34716796875,
      "learning_rate": 8.90970072239422e-05,
      "loss": 18.6224,
      "step": 2713
    },
    {
      "epoch": 2.72,
      "grad_norm": 1274.8349609375,
      "learning_rate": 8.909184726522188e-05,
      "loss": 27.9354,
      "step": 2714
    },
    {
      "epoch": 2.72,
      "grad_norm": 23075.322265625,
      "learning_rate": 8.908668730650154e-05,
      "loss": 28.9961,
      "step": 2715
    },
    {
      "epoch": 2.72,
      "grad_norm": 107487.015625,
      "learning_rate": 8.908152734778123e-05,
      "loss": 24.8096,
      "step": 2716
    },
    {
      "epoch": 2.72,
      "grad_norm": 20601.396484375,
      "learning_rate": 8.907636738906089e-05,
      "loss": 25.3135,
      "step": 2717
    },
    {
      "epoch": 2.72,
      "grad_norm": 40476.3671875,
      "learning_rate": 8.907120743034057e-05,
      "loss": 15.4836,
      "step": 2718
    },
    {
      "epoch": 2.72,
      "grad_norm": 2808.779052734375,
      "learning_rate": 8.906604747162023e-05,
      "loss": 19.514,
      "step": 2719
    },
    {
      "epoch": 2.72,
      "grad_norm": 5659.78125,
      "learning_rate": 8.90608875128999e-05,
      "loss": 12.0474,
      "step": 2720
    },
    {
      "epoch": 2.72,
      "grad_norm": 3908.85498046875,
      "learning_rate": 8.905572755417958e-05,
      "loss": 12.0,
      "step": 2721
    },
    {
      "epoch": 2.72,
      "grad_norm": 3854.503662109375,
      "learning_rate": 8.905056759545925e-05,
      "loss": 13.7091,
      "step": 2722
    },
    {
      "epoch": 2.73,
      "grad_norm": 3305.467529296875,
      "learning_rate": 8.904540763673891e-05,
      "loss": 12.3928,
      "step": 2723
    },
    {
      "epoch": 2.73,
      "grad_norm": 8885.974609375,
      "learning_rate": 8.904024767801859e-05,
      "loss": 14.2163,
      "step": 2724
    },
    {
      "epoch": 2.73,
      "grad_norm": 11275.560546875,
      "learning_rate": 8.903508771929825e-05,
      "loss": 31.7284,
      "step": 2725
    },
    {
      "epoch": 2.73,
      "grad_norm": 21585.08984375,
      "learning_rate": 8.902992776057792e-05,
      "loss": 25.8185,
      "step": 2726
    },
    {
      "epoch": 2.73,
      "grad_norm": 3504.885986328125,
      "learning_rate": 8.90247678018576e-05,
      "loss": 17.857,
      "step": 2727
    },
    {
      "epoch": 2.73,
      "grad_norm": 74649.59375,
      "learning_rate": 8.901960784313726e-05,
      "loss": 24.1387,
      "step": 2728
    },
    {
      "epoch": 2.73,
      "grad_norm": 3768.10546875,
      "learning_rate": 8.901444788441693e-05,
      "loss": 13.0579,
      "step": 2729
    },
    {
      "epoch": 2.73,
      "grad_norm": 3001.2373046875,
      "learning_rate": 8.900928792569659e-05,
      "loss": 14.0467,
      "step": 2730
    },
    {
      "epoch": 2.73,
      "grad_norm": 21780.59765625,
      "learning_rate": 8.900412796697627e-05,
      "loss": 18.4702,
      "step": 2731
    },
    {
      "epoch": 2.73,
      "grad_norm": 26893.423828125,
      "learning_rate": 8.899896800825594e-05,
      "loss": 21.1203,
      "step": 2732
    },
    {
      "epoch": 2.74,
      "grad_norm": 29110.40625,
      "learning_rate": 8.899380804953562e-05,
      "loss": 18.6341,
      "step": 2733
    },
    {
      "epoch": 2.74,
      "grad_norm": 7879.77197265625,
      "learning_rate": 8.898864809081528e-05,
      "loss": 21.6873,
      "step": 2734
    },
    {
      "epoch": 2.74,
      "grad_norm": 45780.1171875,
      "learning_rate": 8.898348813209495e-05,
      "loss": 23.6541,
      "step": 2735
    },
    {
      "epoch": 2.74,
      "grad_norm": 3640.905029296875,
      "learning_rate": 8.897832817337461e-05,
      "loss": 11.2928,
      "step": 2736
    },
    {
      "epoch": 2.74,
      "grad_norm": 3662.251708984375,
      "learning_rate": 8.897316821465429e-05,
      "loss": 19.7732,
      "step": 2737
    },
    {
      "epoch": 2.74,
      "grad_norm": 14906.158203125,
      "learning_rate": 8.896800825593396e-05,
      "loss": 46.0928,
      "step": 2738
    },
    {
      "epoch": 2.74,
      "grad_norm": 22813.490234375,
      "learning_rate": 8.896284829721364e-05,
      "loss": 15.6551,
      "step": 2739
    },
    {
      "epoch": 2.74,
      "grad_norm": 5348.6171875,
      "learning_rate": 8.89576883384933e-05,
      "loss": 20.7916,
      "step": 2740
    },
    {
      "epoch": 2.74,
      "grad_norm": 32511.564453125,
      "learning_rate": 8.895252837977297e-05,
      "loss": 16.9402,
      "step": 2741
    },
    {
      "epoch": 2.74,
      "grad_norm": 7809.0205078125,
      "learning_rate": 8.894736842105263e-05,
      "loss": 26.1415,
      "step": 2742
    },
    {
      "epoch": 2.75,
      "grad_norm": 1322.8404541015625,
      "learning_rate": 8.89422084623323e-05,
      "loss": 20.4046,
      "step": 2743
    },
    {
      "epoch": 2.75,
      "grad_norm": 6801.35009765625,
      "learning_rate": 8.893704850361198e-05,
      "loss": 26.1691,
      "step": 2744
    },
    {
      "epoch": 2.75,
      "grad_norm": 2615.928955078125,
      "learning_rate": 8.893188854489166e-05,
      "loss": 15.4131,
      "step": 2745
    },
    {
      "epoch": 2.75,
      "grad_norm": 15195.1845703125,
      "learning_rate": 8.892672858617132e-05,
      "loss": 34.7373,
      "step": 2746
    },
    {
      "epoch": 2.75,
      "grad_norm": 15432.55078125,
      "learning_rate": 8.892156862745098e-05,
      "loss": 16.2793,
      "step": 2747
    },
    {
      "epoch": 2.75,
      "grad_norm": 11929.615234375,
      "learning_rate": 8.891640866873065e-05,
      "loss": 15.1611,
      "step": 2748
    },
    {
      "epoch": 2.75,
      "grad_norm": 4205.556640625,
      "learning_rate": 8.891124871001033e-05,
      "loss": 11.9352,
      "step": 2749
    },
    {
      "epoch": 2.75,
      "grad_norm": 4947.61865234375,
      "learning_rate": 8.890608875129e-05,
      "loss": 12.7547,
      "step": 2750
    },
    {
      "epoch": 2.75,
      "grad_norm": 1740.5513916015625,
      "learning_rate": 8.890092879256966e-05,
      "loss": 12.2359,
      "step": 2751
    },
    {
      "epoch": 2.75,
      "grad_norm": 5630.021484375,
      "learning_rate": 8.889576883384934e-05,
      "loss": 12.2227,
      "step": 2752
    },
    {
      "epoch": 2.76,
      "grad_norm": 5298.1533203125,
      "learning_rate": 8.8890608875129e-05,
      "loss": 24.4444,
      "step": 2753
    },
    {
      "epoch": 2.76,
      "grad_norm": 4726.06640625,
      "learning_rate": 8.888544891640867e-05,
      "loss": 21.0117,
      "step": 2754
    },
    {
      "epoch": 2.76,
      "grad_norm": 24336.904296875,
      "learning_rate": 8.888028895768835e-05,
      "loss": 19.876,
      "step": 2755
    },
    {
      "epoch": 2.76,
      "grad_norm": 48129.234375,
      "learning_rate": 8.887512899896802e-05,
      "loss": 14.0029,
      "step": 2756
    },
    {
      "epoch": 2.76,
      "grad_norm": 6467.7900390625,
      "learning_rate": 8.886996904024768e-05,
      "loss": 24.2745,
      "step": 2757
    },
    {
      "epoch": 2.76,
      "grad_norm": 9890.7060546875,
      "learning_rate": 8.886480908152736e-05,
      "loss": 26.1833,
      "step": 2758
    },
    {
      "epoch": 2.76,
      "grad_norm": 3226.051025390625,
      "learning_rate": 8.885964912280702e-05,
      "loss": 13.7918,
      "step": 2759
    },
    {
      "epoch": 2.76,
      "grad_norm": 8960.0546875,
      "learning_rate": 8.885448916408669e-05,
      "loss": 16.0231,
      "step": 2760
    },
    {
      "epoch": 2.76,
      "grad_norm": 71292.578125,
      "learning_rate": 8.884932920536637e-05,
      "loss": 19.3188,
      "step": 2761
    },
    {
      "epoch": 2.76,
      "grad_norm": 1720.587158203125,
      "learning_rate": 8.884416924664604e-05,
      "loss": 11.9093,
      "step": 2762
    },
    {
      "epoch": 2.77,
      "grad_norm": 15114.380859375,
      "learning_rate": 8.88390092879257e-05,
      "loss": 30.5348,
      "step": 2763
    },
    {
      "epoch": 2.77,
      "grad_norm": 21629.712890625,
      "learning_rate": 8.883384932920536e-05,
      "loss": 17.3631,
      "step": 2764
    },
    {
      "epoch": 2.77,
      "grad_norm": 3129.597412109375,
      "learning_rate": 8.882868937048504e-05,
      "loss": 21.8342,
      "step": 2765
    },
    {
      "epoch": 2.77,
      "grad_norm": 1798.18115234375,
      "learning_rate": 8.882352941176471e-05,
      "loss": 13.6464,
      "step": 2766
    },
    {
      "epoch": 2.77,
      "grad_norm": 4622.98583984375,
      "learning_rate": 8.881836945304439e-05,
      "loss": 19.2375,
      "step": 2767
    },
    {
      "epoch": 2.77,
      "grad_norm": 8201.18359375,
      "learning_rate": 8.881320949432405e-05,
      "loss": 10.7123,
      "step": 2768
    },
    {
      "epoch": 2.77,
      "grad_norm": 6695.89111328125,
      "learning_rate": 8.880804953560372e-05,
      "loss": 21.8813,
      "step": 2769
    },
    {
      "epoch": 2.77,
      "grad_norm": 2529.781494140625,
      "learning_rate": 8.880288957688338e-05,
      "loss": 16.4945,
      "step": 2770
    },
    {
      "epoch": 2.77,
      "grad_norm": 6058.77490234375,
      "learning_rate": 8.879772961816306e-05,
      "loss": 16.5882,
      "step": 2771
    },
    {
      "epoch": 2.77,
      "grad_norm": 9227.486328125,
      "learning_rate": 8.879256965944273e-05,
      "loss": 22.3633,
      "step": 2772
    },
    {
      "epoch": 2.78,
      "grad_norm": 4827.88232421875,
      "learning_rate": 8.87874097007224e-05,
      "loss": 16.397,
      "step": 2773
    },
    {
      "epoch": 2.78,
      "grad_norm": 2970.391845703125,
      "learning_rate": 8.878224974200207e-05,
      "loss": 12.8746,
      "step": 2774
    },
    {
      "epoch": 2.78,
      "grad_norm": 6634.1064453125,
      "learning_rate": 8.877708978328174e-05,
      "loss": 19.0395,
      "step": 2775
    },
    {
      "epoch": 2.78,
      "grad_norm": 16769.599609375,
      "learning_rate": 8.87719298245614e-05,
      "loss": 23.9133,
      "step": 2776
    },
    {
      "epoch": 2.78,
      "grad_norm": 31266.08203125,
      "learning_rate": 8.876676986584108e-05,
      "loss": 22.3146,
      "step": 2777
    },
    {
      "epoch": 2.78,
      "grad_norm": 3265.353515625,
      "learning_rate": 8.876160990712075e-05,
      "loss": 14.8944,
      "step": 2778
    },
    {
      "epoch": 2.78,
      "grad_norm": 21601.587890625,
      "learning_rate": 8.875644994840043e-05,
      "loss": 20.6177,
      "step": 2779
    },
    {
      "epoch": 2.78,
      "grad_norm": 3275.032470703125,
      "learning_rate": 8.875128998968009e-05,
      "loss": 26.6488,
      "step": 2780
    },
    {
      "epoch": 2.78,
      "grad_norm": 7066.07177734375,
      "learning_rate": 8.874613003095976e-05,
      "loss": 19.1631,
      "step": 2781
    },
    {
      "epoch": 2.78,
      "grad_norm": 14026.8916015625,
      "learning_rate": 8.874097007223942e-05,
      "loss": 29.1116,
      "step": 2782
    },
    {
      "epoch": 2.79,
      "grad_norm": 1452.012451171875,
      "learning_rate": 8.87358101135191e-05,
      "loss": 19.2125,
      "step": 2783
    },
    {
      "epoch": 2.79,
      "grad_norm": 16040.6083984375,
      "learning_rate": 8.873065015479877e-05,
      "loss": 12.6555,
      "step": 2784
    },
    {
      "epoch": 2.79,
      "grad_norm": 14782.8720703125,
      "learning_rate": 8.872549019607843e-05,
      "loss": 15.1301,
      "step": 2785
    },
    {
      "epoch": 2.79,
      "grad_norm": 5501.498046875,
      "learning_rate": 8.872033023735811e-05,
      "loss": 13.7356,
      "step": 2786
    },
    {
      "epoch": 2.79,
      "grad_norm": 41041.1328125,
      "learning_rate": 8.871517027863777e-05,
      "loss": 16.8259,
      "step": 2787
    },
    {
      "epoch": 2.79,
      "grad_norm": 1582.63525390625,
      "learning_rate": 8.871001031991744e-05,
      "loss": 10.9492,
      "step": 2788
    },
    {
      "epoch": 2.79,
      "grad_norm": 11295.9052734375,
      "learning_rate": 8.870485036119712e-05,
      "loss": 15.6058,
      "step": 2789
    },
    {
      "epoch": 2.79,
      "grad_norm": 7956.3623046875,
      "learning_rate": 8.869969040247679e-05,
      "loss": 13.3503,
      "step": 2790
    },
    {
      "epoch": 2.79,
      "grad_norm": 1525.3172607421875,
      "learning_rate": 8.869453044375645e-05,
      "loss": 11.7444,
      "step": 2791
    },
    {
      "epoch": 2.79,
      "grad_norm": 10486.0703125,
      "learning_rate": 8.868937048503613e-05,
      "loss": 21.1809,
      "step": 2792
    },
    {
      "epoch": 2.8,
      "grad_norm": 15941.03125,
      "learning_rate": 8.868421052631579e-05,
      "loss": 21.9114,
      "step": 2793
    },
    {
      "epoch": 2.8,
      "grad_norm": 3617.47998046875,
      "learning_rate": 8.867905056759546e-05,
      "loss": 21.2949,
      "step": 2794
    },
    {
      "epoch": 2.8,
      "grad_norm": 2726.748046875,
      "learning_rate": 8.867389060887514e-05,
      "loss": 18.9634,
      "step": 2795
    },
    {
      "epoch": 2.8,
      "grad_norm": 4733.48486328125,
      "learning_rate": 8.866873065015481e-05,
      "loss": 15.2894,
      "step": 2796
    },
    {
      "epoch": 2.8,
      "grad_norm": 6269.13134765625,
      "learning_rate": 8.866357069143447e-05,
      "loss": 23.2131,
      "step": 2797
    },
    {
      "epoch": 2.8,
      "grad_norm": 2290.596435546875,
      "learning_rate": 8.865841073271415e-05,
      "loss": 12.6748,
      "step": 2798
    },
    {
      "epoch": 2.8,
      "grad_norm": 8153.68603515625,
      "learning_rate": 8.865325077399381e-05,
      "loss": 25.9833,
      "step": 2799
    },
    {
      "epoch": 2.8,
      "grad_norm": 13303.4443359375,
      "learning_rate": 8.864809081527348e-05,
      "loss": 23.1666,
      "step": 2800
    },
    {
      "epoch": 2.8,
      "grad_norm": 11090.1708984375,
      "learning_rate": 8.864293085655316e-05,
      "loss": 21.9,
      "step": 2801
    },
    {
      "epoch": 2.8,
      "grad_norm": 16364.4140625,
      "learning_rate": 8.863777089783282e-05,
      "loss": 15.6387,
      "step": 2802
    },
    {
      "epoch": 2.81,
      "grad_norm": 3709.385986328125,
      "learning_rate": 8.863261093911249e-05,
      "loss": 12.0847,
      "step": 2803
    },
    {
      "epoch": 2.81,
      "grad_norm": 17403.162109375,
      "learning_rate": 8.862745098039215e-05,
      "loss": 14.0161,
      "step": 2804
    },
    {
      "epoch": 2.81,
      "grad_norm": 4699.5498046875,
      "learning_rate": 8.862229102167183e-05,
      "loss": 18.5744,
      "step": 2805
    },
    {
      "epoch": 2.81,
      "grad_norm": 14302.828125,
      "learning_rate": 8.86171310629515e-05,
      "loss": 24.9261,
      "step": 2806
    },
    {
      "epoch": 2.81,
      "grad_norm": 3409.19921875,
      "learning_rate": 8.861197110423118e-05,
      "loss": 18.0096,
      "step": 2807
    },
    {
      "epoch": 2.81,
      "grad_norm": 16237.7177734375,
      "learning_rate": 8.860681114551084e-05,
      "loss": 14.0223,
      "step": 2808
    },
    {
      "epoch": 2.81,
      "grad_norm": 3949.826904296875,
      "learning_rate": 8.860165118679051e-05,
      "loss": 23.1148,
      "step": 2809
    },
    {
      "epoch": 2.81,
      "grad_norm": 2206.08740234375,
      "learning_rate": 8.859649122807017e-05,
      "loss": 20.5718,
      "step": 2810
    },
    {
      "epoch": 2.81,
      "grad_norm": 4772.50048828125,
      "learning_rate": 8.859133126934985e-05,
      "loss": 20.685,
      "step": 2811
    },
    {
      "epoch": 2.81,
      "grad_norm": 10580.5908203125,
      "learning_rate": 8.858617131062952e-05,
      "loss": 15.534,
      "step": 2812
    },
    {
      "epoch": 2.82,
      "grad_norm": 7967.00927734375,
      "learning_rate": 8.85810113519092e-05,
      "loss": 18.0676,
      "step": 2813
    },
    {
      "epoch": 2.82,
      "grad_norm": 6886.77783203125,
      "learning_rate": 8.857585139318886e-05,
      "loss": 19.2834,
      "step": 2814
    },
    {
      "epoch": 2.82,
      "grad_norm": 4684.314453125,
      "learning_rate": 8.857069143446853e-05,
      "loss": 20.2649,
      "step": 2815
    },
    {
      "epoch": 2.82,
      "grad_norm": 5235.03369140625,
      "learning_rate": 8.85655314757482e-05,
      "loss": 14.4173,
      "step": 2816
    },
    {
      "epoch": 2.82,
      "grad_norm": 1202.3126220703125,
      "learning_rate": 8.856037151702787e-05,
      "loss": 14.9022,
      "step": 2817
    },
    {
      "epoch": 2.82,
      "grad_norm": 3008.304931640625,
      "learning_rate": 8.855521155830754e-05,
      "loss": 14.2761,
      "step": 2818
    },
    {
      "epoch": 2.82,
      "grad_norm": 8495.6669921875,
      "learning_rate": 8.85500515995872e-05,
      "loss": 21.676,
      "step": 2819
    },
    {
      "epoch": 2.82,
      "grad_norm": 3032.7109375,
      "learning_rate": 8.854489164086688e-05,
      "loss": 21.5922,
      "step": 2820
    },
    {
      "epoch": 2.82,
      "grad_norm": 19239.421875,
      "learning_rate": 8.853973168214654e-05,
      "loss": 16.0476,
      "step": 2821
    },
    {
      "epoch": 2.82,
      "grad_norm": 3636.76123046875,
      "learning_rate": 8.853457172342621e-05,
      "loss": 14.1158,
      "step": 2822
    },
    {
      "epoch": 2.83,
      "grad_norm": 3605.439453125,
      "learning_rate": 8.852941176470589e-05,
      "loss": 15.8627,
      "step": 2823
    },
    {
      "epoch": 2.83,
      "grad_norm": 96915.3671875,
      "learning_rate": 8.852425180598556e-05,
      "loss": 16.8428,
      "step": 2824
    },
    {
      "epoch": 2.83,
      "grad_norm": 55452.02734375,
      "learning_rate": 8.851909184726522e-05,
      "loss": 22.4199,
      "step": 2825
    },
    {
      "epoch": 2.83,
      "grad_norm": 1984.9736328125,
      "learning_rate": 8.85139318885449e-05,
      "loss": 12.2216,
      "step": 2826
    },
    {
      "epoch": 2.83,
      "grad_norm": 22358.705078125,
      "learning_rate": 8.850877192982456e-05,
      "loss": 20.6383,
      "step": 2827
    },
    {
      "epoch": 2.83,
      "grad_norm": 6610.02099609375,
      "learning_rate": 8.850361197110423e-05,
      "loss": 24.3004,
      "step": 2828
    },
    {
      "epoch": 2.83,
      "grad_norm": 16459.392578125,
      "learning_rate": 8.849845201238391e-05,
      "loss": 17.4256,
      "step": 2829
    },
    {
      "epoch": 2.83,
      "grad_norm": 11854.0927734375,
      "learning_rate": 8.849329205366358e-05,
      "loss": 23.9063,
      "step": 2830
    },
    {
      "epoch": 2.83,
      "grad_norm": 36778.05078125,
      "learning_rate": 8.848813209494324e-05,
      "loss": 24.1413,
      "step": 2831
    },
    {
      "epoch": 2.83,
      "grad_norm": 17092.41796875,
      "learning_rate": 8.848297213622292e-05,
      "loss": 13.1288,
      "step": 2832
    },
    {
      "epoch": 2.84,
      "grad_norm": 2933.946533203125,
      "learning_rate": 8.847781217750258e-05,
      "loss": 19.3227,
      "step": 2833
    },
    {
      "epoch": 2.84,
      "grad_norm": 15466.6591796875,
      "learning_rate": 8.847265221878225e-05,
      "loss": 14.5418,
      "step": 2834
    },
    {
      "epoch": 2.84,
      "grad_norm": 2996.46044921875,
      "learning_rate": 8.846749226006193e-05,
      "loss": 29.4582,
      "step": 2835
    },
    {
      "epoch": 2.84,
      "grad_norm": 59036.59765625,
      "learning_rate": 8.846233230134159e-05,
      "loss": 18.133,
      "step": 2836
    },
    {
      "epoch": 2.84,
      "grad_norm": 36506.421875,
      "learning_rate": 8.845717234262126e-05,
      "loss": 21.8459,
      "step": 2837
    },
    {
      "epoch": 2.84,
      "grad_norm": 6908.43994140625,
      "learning_rate": 8.845201238390092e-05,
      "loss": 13.1821,
      "step": 2838
    },
    {
      "epoch": 2.84,
      "grad_norm": 8464.5859375,
      "learning_rate": 8.84468524251806e-05,
      "loss": 22.4819,
      "step": 2839
    },
    {
      "epoch": 2.84,
      "grad_norm": 21617.46875,
      "learning_rate": 8.844169246646027e-05,
      "loss": 23.4749,
      "step": 2840
    },
    {
      "epoch": 2.84,
      "grad_norm": 11868.0380859375,
      "learning_rate": 8.843653250773995e-05,
      "loss": 26.9124,
      "step": 2841
    },
    {
      "epoch": 2.84,
      "grad_norm": 4955.34765625,
      "learning_rate": 8.843137254901961e-05,
      "loss": 13.3021,
      "step": 2842
    },
    {
      "epoch": 2.85,
      "grad_norm": 13939.310546875,
      "learning_rate": 8.842621259029928e-05,
      "loss": 14.3278,
      "step": 2843
    },
    {
      "epoch": 2.85,
      "grad_norm": 4267.0634765625,
      "learning_rate": 8.842105263157894e-05,
      "loss": 30.6664,
      "step": 2844
    },
    {
      "epoch": 2.85,
      "grad_norm": 16735.587890625,
      "learning_rate": 8.841589267285862e-05,
      "loss": 42.7036,
      "step": 2845
    },
    {
      "epoch": 2.85,
      "grad_norm": 19663.23828125,
      "learning_rate": 8.84107327141383e-05,
      "loss": 20.1461,
      "step": 2846
    },
    {
      "epoch": 2.85,
      "grad_norm": 10117.0986328125,
      "learning_rate": 8.840557275541797e-05,
      "loss": 11.7523,
      "step": 2847
    },
    {
      "epoch": 2.85,
      "grad_norm": 8855.2373046875,
      "learning_rate": 8.840041279669763e-05,
      "loss": 14.177,
      "step": 2848
    },
    {
      "epoch": 2.85,
      "grad_norm": 13496.9521484375,
      "learning_rate": 8.83952528379773e-05,
      "loss": 21.2599,
      "step": 2849
    },
    {
      "epoch": 2.85,
      "grad_norm": 15622.6630859375,
      "learning_rate": 8.839009287925696e-05,
      "loss": 24.7235,
      "step": 2850
    },
    {
      "epoch": 2.85,
      "grad_norm": 3669.73291015625,
      "learning_rate": 8.838493292053664e-05,
      "loss": 15.1926,
      "step": 2851
    },
    {
      "epoch": 2.85,
      "grad_norm": 39401.5625,
      "learning_rate": 8.837977296181631e-05,
      "loss": 22.1322,
      "step": 2852
    },
    {
      "epoch": 2.86,
      "grad_norm": 19113.609375,
      "learning_rate": 8.837461300309597e-05,
      "loss": 22.5956,
      "step": 2853
    },
    {
      "epoch": 2.86,
      "grad_norm": 5483.8447265625,
      "learning_rate": 8.836945304437565e-05,
      "loss": 13.9879,
      "step": 2854
    },
    {
      "epoch": 2.86,
      "grad_norm": 8558.3955078125,
      "learning_rate": 8.836429308565531e-05,
      "loss": 22.8674,
      "step": 2855
    },
    {
      "epoch": 2.86,
      "grad_norm": 3929.728515625,
      "learning_rate": 8.835913312693498e-05,
      "loss": 23.5842,
      "step": 2856
    },
    {
      "epoch": 2.86,
      "grad_norm": 3118.323486328125,
      "learning_rate": 8.835397316821466e-05,
      "loss": 23.4788,
      "step": 2857
    },
    {
      "epoch": 2.86,
      "grad_norm": 1210.2864990234375,
      "learning_rate": 8.834881320949433e-05,
      "loss": 12.3135,
      "step": 2858
    },
    {
      "epoch": 2.86,
      "grad_norm": 45421.8515625,
      "learning_rate": 8.8343653250774e-05,
      "loss": 30.1879,
      "step": 2859
    },
    {
      "epoch": 2.86,
      "grad_norm": 33618.9375,
      "learning_rate": 8.833849329205367e-05,
      "loss": 20.2758,
      "step": 2860
    },
    {
      "epoch": 2.86,
      "grad_norm": 10586.6435546875,
      "learning_rate": 8.833333333333333e-05,
      "loss": 12.9829,
      "step": 2861
    },
    {
      "epoch": 2.86,
      "grad_norm": 10086.8115234375,
      "learning_rate": 8.8328173374613e-05,
      "loss": 17.916,
      "step": 2862
    },
    {
      "epoch": 2.87,
      "grad_norm": 6226.15380859375,
      "learning_rate": 8.832301341589268e-05,
      "loss": 27.1395,
      "step": 2863
    },
    {
      "epoch": 2.87,
      "grad_norm": 7677.9912109375,
      "learning_rate": 8.831785345717235e-05,
      "loss": 19.6535,
      "step": 2864
    },
    {
      "epoch": 2.87,
      "grad_norm": 3920.01220703125,
      "learning_rate": 8.831269349845201e-05,
      "loss": 14.0132,
      "step": 2865
    },
    {
      "epoch": 2.87,
      "grad_norm": 12044.408203125,
      "learning_rate": 8.830753353973169e-05,
      "loss": 19.2163,
      "step": 2866
    },
    {
      "epoch": 2.87,
      "grad_norm": 8751.0595703125,
      "learning_rate": 8.830237358101135e-05,
      "loss": 16.7403,
      "step": 2867
    },
    {
      "epoch": 2.87,
      "grad_norm": 3666.43408203125,
      "learning_rate": 8.829721362229102e-05,
      "loss": 11.6795,
      "step": 2868
    },
    {
      "epoch": 2.87,
      "grad_norm": 6947.837890625,
      "learning_rate": 8.82920536635707e-05,
      "loss": 18.2629,
      "step": 2869
    },
    {
      "epoch": 2.87,
      "grad_norm": 2170.2490234375,
      "learning_rate": 8.828689370485037e-05,
      "loss": 15.1723,
      "step": 2870
    },
    {
      "epoch": 2.87,
      "grad_norm": 14349.8974609375,
      "learning_rate": 8.828173374613003e-05,
      "loss": 18.8334,
      "step": 2871
    },
    {
      "epoch": 2.87,
      "grad_norm": 5204.84716796875,
      "learning_rate": 8.82765737874097e-05,
      "loss": 15.4714,
      "step": 2872
    },
    {
      "epoch": 2.88,
      "grad_norm": 35879.7109375,
      "learning_rate": 8.827141382868937e-05,
      "loss": 17.1773,
      "step": 2873
    },
    {
      "epoch": 2.88,
      "grad_norm": 5135.8876953125,
      "learning_rate": 8.826625386996904e-05,
      "loss": 14.7905,
      "step": 2874
    },
    {
      "epoch": 2.88,
      "grad_norm": 6150.89013671875,
      "learning_rate": 8.826109391124872e-05,
      "loss": 22.9939,
      "step": 2875
    },
    {
      "epoch": 2.88,
      "grad_norm": 372.55950927734375,
      "learning_rate": 8.825593395252838e-05,
      "loss": 12.6032,
      "step": 2876
    },
    {
      "epoch": 2.88,
      "grad_norm": 5046.61328125,
      "learning_rate": 8.825077399380805e-05,
      "loss": 13.811,
      "step": 2877
    },
    {
      "epoch": 2.88,
      "grad_norm": 9707.642578125,
      "learning_rate": 8.824561403508772e-05,
      "loss": 19.009,
      "step": 2878
    },
    {
      "epoch": 2.88,
      "grad_norm": 6591.72314453125,
      "learning_rate": 8.824045407636739e-05,
      "loss": 24.457,
      "step": 2879
    },
    {
      "epoch": 2.88,
      "grad_norm": 2974.808837890625,
      "learning_rate": 8.823529411764706e-05,
      "loss": 14.6159,
      "step": 2880
    },
    {
      "epoch": 2.88,
      "grad_norm": 4509.626953125,
      "learning_rate": 8.823013415892674e-05,
      "loss": 23.8087,
      "step": 2881
    },
    {
      "epoch": 2.88,
      "grad_norm": 2673.61474609375,
      "learning_rate": 8.82249742002064e-05,
      "loss": 11.1112,
      "step": 2882
    },
    {
      "epoch": 2.89,
      "grad_norm": 5451.40087890625,
      "learning_rate": 8.821981424148607e-05,
      "loss": 20.4489,
      "step": 2883
    },
    {
      "epoch": 2.89,
      "grad_norm": 2756.880859375,
      "learning_rate": 8.821465428276574e-05,
      "loss": 10.1236,
      "step": 2884
    },
    {
      "epoch": 2.89,
      "grad_norm": 4677.0302734375,
      "learning_rate": 8.820949432404541e-05,
      "loss": 30.2389,
      "step": 2885
    },
    {
      "epoch": 2.89,
      "grad_norm": 3802.309326171875,
      "learning_rate": 8.820433436532508e-05,
      "loss": 16.7038,
      "step": 2886
    },
    {
      "epoch": 2.89,
      "grad_norm": 2221.1826171875,
      "learning_rate": 8.819917440660476e-05,
      "loss": 16.7952,
      "step": 2887
    },
    {
      "epoch": 2.89,
      "grad_norm": 10602.185546875,
      "learning_rate": 8.819401444788442e-05,
      "loss": 25.0655,
      "step": 2888
    },
    {
      "epoch": 2.89,
      "grad_norm": 1749.1583251953125,
      "learning_rate": 8.818885448916408e-05,
      "loss": 13.6029,
      "step": 2889
    },
    {
      "epoch": 2.89,
      "grad_norm": 1619.803955078125,
      "learning_rate": 8.818369453044376e-05,
      "loss": 16.5014,
      "step": 2890
    },
    {
      "epoch": 2.89,
      "grad_norm": 4004.961181640625,
      "learning_rate": 8.817853457172343e-05,
      "loss": 16.3836,
      "step": 2891
    },
    {
      "epoch": 2.89,
      "grad_norm": 1657.7034912109375,
      "learning_rate": 8.81733746130031e-05,
      "loss": 15.7125,
      "step": 2892
    },
    {
      "epoch": 2.9,
      "grad_norm": 13889.2734375,
      "learning_rate": 8.816821465428277e-05,
      "loss": 24.7201,
      "step": 2893
    },
    {
      "epoch": 2.9,
      "grad_norm": 19842.1328125,
      "learning_rate": 8.816305469556244e-05,
      "loss": 29.2165,
      "step": 2894
    },
    {
      "epoch": 2.9,
      "grad_norm": 13275.341796875,
      "learning_rate": 8.81578947368421e-05,
      "loss": 19.4567,
      "step": 2895
    },
    {
      "epoch": 2.9,
      "grad_norm": 9237.654296875,
      "learning_rate": 8.815273477812178e-05,
      "loss": 13.3416,
      "step": 2896
    },
    {
      "epoch": 2.9,
      "grad_norm": 7889.1708984375,
      "learning_rate": 8.814757481940145e-05,
      "loss": 18.5084,
      "step": 2897
    },
    {
      "epoch": 2.9,
      "grad_norm": 30299.865234375,
      "learning_rate": 8.814241486068112e-05,
      "loss": 15.1708,
      "step": 2898
    },
    {
      "epoch": 2.9,
      "grad_norm": 2768.287841796875,
      "learning_rate": 8.813725490196079e-05,
      "loss": 24.2925,
      "step": 2899
    },
    {
      "epoch": 2.9,
      "grad_norm": 17033.77734375,
      "learning_rate": 8.813209494324046e-05,
      "loss": 29.9898,
      "step": 2900
    },
    {
      "epoch": 2.9,
      "grad_norm": 15109.8310546875,
      "learning_rate": 8.812693498452012e-05,
      "loss": 19.908,
      "step": 2901
    },
    {
      "epoch": 2.9,
      "grad_norm": 1881.944091796875,
      "learning_rate": 8.81217750257998e-05,
      "loss": 14.992,
      "step": 2902
    },
    {
      "epoch": 2.91,
      "grad_norm": 2731.596923828125,
      "learning_rate": 8.811661506707947e-05,
      "loss": 15.2685,
      "step": 2903
    },
    {
      "epoch": 2.91,
      "grad_norm": 15602.744140625,
      "learning_rate": 8.811145510835914e-05,
      "loss": 13.7237,
      "step": 2904
    },
    {
      "epoch": 2.91,
      "grad_norm": 8213.3427734375,
      "learning_rate": 8.81062951496388e-05,
      "loss": 16.343,
      "step": 2905
    },
    {
      "epoch": 2.91,
      "grad_norm": 1068.5845947265625,
      "learning_rate": 8.810113519091848e-05,
      "loss": 12.8131,
      "step": 2906
    },
    {
      "epoch": 2.91,
      "grad_norm": 6845.50146484375,
      "learning_rate": 8.809597523219814e-05,
      "loss": 19.0139,
      "step": 2907
    },
    {
      "epoch": 2.91,
      "grad_norm": 3758.673583984375,
      "learning_rate": 8.809081527347782e-05,
      "loss": 22.4401,
      "step": 2908
    },
    {
      "epoch": 2.91,
      "grad_norm": 15852.0283203125,
      "learning_rate": 8.808565531475749e-05,
      "loss": 18.874,
      "step": 2909
    },
    {
      "epoch": 2.91,
      "grad_norm": 12422.0908203125,
      "learning_rate": 8.808049535603715e-05,
      "loss": 23.0173,
      "step": 2910
    },
    {
      "epoch": 2.91,
      "grad_norm": 5728.77197265625,
      "learning_rate": 8.807533539731683e-05,
      "loss": 18.5683,
      "step": 2911
    },
    {
      "epoch": 2.91,
      "grad_norm": 6881.59912109375,
      "learning_rate": 8.807017543859649e-05,
      "loss": 23.3144,
      "step": 2912
    },
    {
      "epoch": 2.92,
      "grad_norm": 3579.39013671875,
      "learning_rate": 8.806501547987616e-05,
      "loss": 16.8757,
      "step": 2913
    },
    {
      "epoch": 2.92,
      "grad_norm": 10566.2587890625,
      "learning_rate": 8.805985552115584e-05,
      "loss": 14.0758,
      "step": 2914
    },
    {
      "epoch": 2.92,
      "grad_norm": 2230.861572265625,
      "learning_rate": 8.805469556243551e-05,
      "loss": 13.8399,
      "step": 2915
    },
    {
      "epoch": 2.92,
      "grad_norm": 12744.6806640625,
      "learning_rate": 8.804953560371517e-05,
      "loss": 14.2717,
      "step": 2916
    },
    {
      "epoch": 2.92,
      "grad_norm": 15873.396484375,
      "learning_rate": 8.804437564499485e-05,
      "loss": 15.5213,
      "step": 2917
    },
    {
      "epoch": 2.92,
      "grad_norm": 7004.31005859375,
      "learning_rate": 8.80392156862745e-05,
      "loss": 15.8855,
      "step": 2918
    },
    {
      "epoch": 2.92,
      "grad_norm": 12414.0439453125,
      "learning_rate": 8.803405572755418e-05,
      "loss": 23.9345,
      "step": 2919
    },
    {
      "epoch": 2.92,
      "grad_norm": 2682.6708984375,
      "learning_rate": 8.802889576883386e-05,
      "loss": 18.5737,
      "step": 2920
    },
    {
      "epoch": 2.92,
      "grad_norm": 4394.3994140625,
      "learning_rate": 8.802373581011353e-05,
      "loss": 12.748,
      "step": 2921
    },
    {
      "epoch": 2.92,
      "grad_norm": 59429.34765625,
      "learning_rate": 8.801857585139319e-05,
      "loss": 25.8898,
      "step": 2922
    },
    {
      "epoch": 2.93,
      "grad_norm": 4721.20068359375,
      "learning_rate": 8.801341589267287e-05,
      "loss": 13.1829,
      "step": 2923
    },
    {
      "epoch": 2.93,
      "grad_norm": 3736.470947265625,
      "learning_rate": 8.800825593395253e-05,
      "loss": 14.1533,
      "step": 2924
    },
    {
      "epoch": 2.93,
      "grad_norm": 5895.45703125,
      "learning_rate": 8.80030959752322e-05,
      "loss": 13.8861,
      "step": 2925
    },
    {
      "epoch": 2.93,
      "grad_norm": 20225.912109375,
      "learning_rate": 8.799793601651188e-05,
      "loss": 15.959,
      "step": 2926
    },
    {
      "epoch": 2.93,
      "grad_norm": 24633.029296875,
      "learning_rate": 8.799277605779154e-05,
      "loss": 22.2617,
      "step": 2927
    },
    {
      "epoch": 2.93,
      "grad_norm": 2812.6328125,
      "learning_rate": 8.798761609907121e-05,
      "loss": 18.6336,
      "step": 2928
    },
    {
      "epoch": 2.93,
      "grad_norm": 2194.49560546875,
      "learning_rate": 8.798245614035087e-05,
      "loss": 19.9332,
      "step": 2929
    },
    {
      "epoch": 2.93,
      "grad_norm": 2653.42626953125,
      "learning_rate": 8.797729618163055e-05,
      "loss": 15.1562,
      "step": 2930
    },
    {
      "epoch": 2.93,
      "grad_norm": 12865.7421875,
      "learning_rate": 8.797213622291022e-05,
      "loss": 20.0457,
      "step": 2931
    },
    {
      "epoch": 2.93,
      "grad_norm": 35911.640625,
      "learning_rate": 8.79669762641899e-05,
      "loss": 23.5933,
      "step": 2932
    },
    {
      "epoch": 2.94,
      "grad_norm": 3986.367431640625,
      "learning_rate": 8.796181630546956e-05,
      "loss": 22.0667,
      "step": 2933
    },
    {
      "epoch": 2.94,
      "grad_norm": 4773.11962890625,
      "learning_rate": 8.795665634674923e-05,
      "loss": 14.3657,
      "step": 2934
    },
    {
      "epoch": 2.94,
      "grad_norm": 29970.818359375,
      "learning_rate": 8.795149638802889e-05,
      "loss": 18.1355,
      "step": 2935
    },
    {
      "epoch": 2.94,
      "grad_norm": 12380.4658203125,
      "learning_rate": 8.794633642930858e-05,
      "loss": 18.0027,
      "step": 2936
    },
    {
      "epoch": 2.94,
      "grad_norm": 2670.656494140625,
      "learning_rate": 8.794117647058824e-05,
      "loss": 14.7457,
      "step": 2937
    },
    {
      "epoch": 2.94,
      "grad_norm": 10553.5546875,
      "learning_rate": 8.793601651186792e-05,
      "loss": 26.3056,
      "step": 2938
    },
    {
      "epoch": 2.94,
      "grad_norm": 8141.22802734375,
      "learning_rate": 8.793085655314758e-05,
      "loss": 15.0826,
      "step": 2939
    },
    {
      "epoch": 2.94,
      "grad_norm": 9416.84375,
      "learning_rate": 8.792569659442725e-05,
      "loss": 14.9668,
      "step": 2940
    },
    {
      "epoch": 2.94,
      "grad_norm": 3799.746826171875,
      "learning_rate": 8.792053663570691e-05,
      "loss": 14.1824,
      "step": 2941
    },
    {
      "epoch": 2.94,
      "grad_norm": 2901.747314453125,
      "learning_rate": 8.79153766769866e-05,
      "loss": 23.235,
      "step": 2942
    },
    {
      "epoch": 2.95,
      "grad_norm": 56998.7890625,
      "learning_rate": 8.791021671826626e-05,
      "loss": 15.607,
      "step": 2943
    },
    {
      "epoch": 2.95,
      "grad_norm": 63638.4140625,
      "learning_rate": 8.790505675954592e-05,
      "loss": 14.3077,
      "step": 2944
    },
    {
      "epoch": 2.95,
      "grad_norm": 1431.9766845703125,
      "learning_rate": 8.78998968008256e-05,
      "loss": 17.7214,
      "step": 2945
    },
    {
      "epoch": 2.95,
      "grad_norm": 2776.48681640625,
      "learning_rate": 8.789473684210526e-05,
      "loss": 12.5945,
      "step": 2946
    },
    {
      "epoch": 2.95,
      "grad_norm": 4318.455078125,
      "learning_rate": 8.788957688338493e-05,
      "loss": 25.4659,
      "step": 2947
    },
    {
      "epoch": 2.95,
      "grad_norm": 95101.3046875,
      "learning_rate": 8.78844169246646e-05,
      "loss": 17.8738,
      "step": 2948
    },
    {
      "epoch": 2.95,
      "grad_norm": 5853.4716796875,
      "learning_rate": 8.787925696594428e-05,
      "loss": 15.8596,
      "step": 2949
    },
    {
      "epoch": 2.95,
      "grad_norm": 6463.703125,
      "learning_rate": 8.787409700722394e-05,
      "loss": 13.0231,
      "step": 2950
    },
    {
      "epoch": 2.95,
      "grad_norm": 16961.73828125,
      "learning_rate": 8.786893704850362e-05,
      "loss": 21.8049,
      "step": 2951
    },
    {
      "epoch": 2.95,
      "grad_norm": 3250.847900390625,
      "learning_rate": 8.786377708978328e-05,
      "loss": 15.3008,
      "step": 2952
    },
    {
      "epoch": 2.96,
      "grad_norm": 6867.6064453125,
      "learning_rate": 8.785861713106297e-05,
      "loss": 14.9276,
      "step": 2953
    },
    {
      "epoch": 2.96,
      "grad_norm": 5539.3740234375,
      "learning_rate": 8.785345717234263e-05,
      "loss": 11.9111,
      "step": 2954
    },
    {
      "epoch": 2.96,
      "grad_norm": 1478.299072265625,
      "learning_rate": 8.78482972136223e-05,
      "loss": 13.8774,
      "step": 2955
    },
    {
      "epoch": 2.96,
      "grad_norm": 5404.220703125,
      "learning_rate": 8.784313725490196e-05,
      "loss": 18.6313,
      "step": 2956
    },
    {
      "epoch": 2.96,
      "grad_norm": 1740.38671875,
      "learning_rate": 8.783797729618164e-05,
      "loss": 10.5728,
      "step": 2957
    },
    {
      "epoch": 2.96,
      "grad_norm": 8346.869140625,
      "learning_rate": 8.78328173374613e-05,
      "loss": 18.3308,
      "step": 2958
    },
    {
      "epoch": 2.96,
      "grad_norm": 2807.235107421875,
      "learning_rate": 8.782765737874099e-05,
      "loss": 11.4139,
      "step": 2959
    },
    {
      "epoch": 2.96,
      "grad_norm": 15819.0146484375,
      "learning_rate": 8.782249742002065e-05,
      "loss": 22.002,
      "step": 2960
    },
    {
      "epoch": 2.96,
      "grad_norm": 2301.233154296875,
      "learning_rate": 8.781733746130031e-05,
      "loss": 18.9051,
      "step": 2961
    },
    {
      "epoch": 2.96,
      "grad_norm": 6334.8544921875,
      "learning_rate": 8.781217750257998e-05,
      "loss": 20.935,
      "step": 2962
    },
    {
      "epoch": 2.97,
      "grad_norm": 73304.8984375,
      "learning_rate": 8.780701754385964e-05,
      "loss": 25.0934,
      "step": 2963
    },
    {
      "epoch": 2.97,
      "grad_norm": 23590.78125,
      "learning_rate": 8.780185758513932e-05,
      "loss": 16.7522,
      "step": 2964
    },
    {
      "epoch": 2.97,
      "grad_norm": 82536.3203125,
      "learning_rate": 8.779669762641899e-05,
      "loss": 26.0031,
      "step": 2965
    },
    {
      "epoch": 2.97,
      "grad_norm": 2668.705810546875,
      "learning_rate": 8.779153766769867e-05,
      "loss": 20.2043,
      "step": 2966
    },
    {
      "epoch": 2.97,
      "grad_norm": 3991.19921875,
      "learning_rate": 8.778637770897833e-05,
      "loss": 19.9184,
      "step": 2967
    },
    {
      "epoch": 2.97,
      "grad_norm": 3938.790771484375,
      "learning_rate": 8.7781217750258e-05,
      "loss": 11.9528,
      "step": 2968
    },
    {
      "epoch": 2.97,
      "grad_norm": 2401.825439453125,
      "learning_rate": 8.777605779153766e-05,
      "loss": 12.0706,
      "step": 2969
    },
    {
      "epoch": 2.97,
      "grad_norm": 6702.81103515625,
      "learning_rate": 8.777089783281735e-05,
      "loss": 17.234,
      "step": 2970
    },
    {
      "epoch": 2.97,
      "grad_norm": 12941.14453125,
      "learning_rate": 8.776573787409701e-05,
      "loss": 14.8281,
      "step": 2971
    },
    {
      "epoch": 2.97,
      "grad_norm": 5334.3740234375,
      "learning_rate": 8.776057791537669e-05,
      "loss": 19.3112,
      "step": 2972
    },
    {
      "epoch": 2.98,
      "grad_norm": 3723.7900390625,
      "learning_rate": 8.775541795665635e-05,
      "loss": 20.1181,
      "step": 2973
    },
    {
      "epoch": 2.98,
      "grad_norm": 43029.640625,
      "learning_rate": 8.775025799793602e-05,
      "loss": 25.4921,
      "step": 2974
    },
    {
      "epoch": 2.98,
      "grad_norm": 13991.9404296875,
      "learning_rate": 8.774509803921568e-05,
      "loss": 22.5625,
      "step": 2975
    },
    {
      "epoch": 2.98,
      "grad_norm": 9387.849609375,
      "learning_rate": 8.773993808049537e-05,
      "loss": 15.6967,
      "step": 2976
    },
    {
      "epoch": 2.98,
      "grad_norm": 4992.228515625,
      "learning_rate": 8.773477812177503e-05,
      "loss": 16.8664,
      "step": 2977
    },
    {
      "epoch": 2.98,
      "grad_norm": 537.4649047851562,
      "learning_rate": 8.772961816305469e-05,
      "loss": 17.0596,
      "step": 2978
    },
    {
      "epoch": 2.98,
      "grad_norm": 12496.7314453125,
      "learning_rate": 8.772445820433437e-05,
      "loss": 14.3591,
      "step": 2979
    },
    {
      "epoch": 2.98,
      "grad_norm": 12910.94921875,
      "learning_rate": 8.771929824561403e-05,
      "loss": 15.0378,
      "step": 2980
    },
    {
      "epoch": 2.98,
      "grad_norm": 2346.32470703125,
      "learning_rate": 8.771413828689372e-05,
      "loss": 14.7223,
      "step": 2981
    },
    {
      "epoch": 2.98,
      "grad_norm": 10310.5771484375,
      "learning_rate": 8.770897832817338e-05,
      "loss": 19.9048,
      "step": 2982
    },
    {
      "epoch": 2.99,
      "grad_norm": 28203.595703125,
      "learning_rate": 8.770381836945305e-05,
      "loss": 13.9372,
      "step": 2983
    },
    {
      "epoch": 2.99,
      "grad_norm": 8975.587890625,
      "learning_rate": 8.769865841073271e-05,
      "loss": 14.9351,
      "step": 2984
    },
    {
      "epoch": 2.99,
      "grad_norm": 14125.71484375,
      "learning_rate": 8.769349845201239e-05,
      "loss": 24.6669,
      "step": 2985
    },
    {
      "epoch": 2.99,
      "grad_norm": 9937.3193359375,
      "learning_rate": 8.768833849329205e-05,
      "loss": 22.2899,
      "step": 2986
    },
    {
      "epoch": 2.99,
      "grad_norm": 10499.3828125,
      "learning_rate": 8.768317853457174e-05,
      "loss": 26.12,
      "step": 2987
    },
    {
      "epoch": 2.99,
      "grad_norm": 3479.3994140625,
      "learning_rate": 8.76780185758514e-05,
      "loss": 14.351,
      "step": 2988
    },
    {
      "epoch": 2.99,
      "grad_norm": 6387.46826171875,
      "learning_rate": 8.767285861713107e-05,
      "loss": 17.117,
      "step": 2989
    },
    {
      "epoch": 2.99,
      "grad_norm": 1288.3978271484375,
      "learning_rate": 8.766769865841073e-05,
      "loss": 12.0669,
      "step": 2990
    },
    {
      "epoch": 2.99,
      "grad_norm": 1770.5206298828125,
      "learning_rate": 8.766253869969041e-05,
      "loss": 18.0172,
      "step": 2991
    },
    {
      "epoch": 2.99,
      "grad_norm": 1715.065673828125,
      "learning_rate": 8.765737874097007e-05,
      "loss": 16.8262,
      "step": 2992
    },
    {
      "epoch": 3.0,
      "grad_norm": 10292.7958984375,
      "learning_rate": 8.765221878224976e-05,
      "loss": 15.2356,
      "step": 2993
    },
    {
      "epoch": 3.0,
      "grad_norm": 1338.01171875,
      "learning_rate": 8.764705882352942e-05,
      "loss": 11.0542,
      "step": 2994
    },
    {
      "epoch": 3.0,
      "grad_norm": 4489.7841796875,
      "learning_rate": 8.764189886480909e-05,
      "loss": 16.9493,
      "step": 2995
    },
    {
      "epoch": 3.0,
      "grad_norm": 3520.404052734375,
      "learning_rate": 8.763673890608875e-05,
      "loss": 15.0469,
      "step": 2996
    },
    {
      "epoch": 3.0,
      "grad_norm": 289492.1875,
      "learning_rate": 8.763157894736841e-05,
      "loss": 35.2289,
      "step": 2997
    },
    {
      "epoch": 3.0,
      "grad_norm": 19807.166015625,
      "learning_rate": 8.76264189886481e-05,
      "loss": 13.8418,
      "step": 2998
    },
    {
      "epoch": 3.0,
      "grad_norm": 1273.4732666015625,
      "learning_rate": 8.762125902992776e-05,
      "loss": 14.7072,
      "step": 2999
    },
    {
      "epoch": 3.0,
      "grad_norm": 31014.453125,
      "learning_rate": 8.761609907120744e-05,
      "loss": 19.9709,
      "step": 3000
    },
    {
      "epoch": 3.0,
      "grad_norm": 1777.741943359375,
      "learning_rate": 8.76109391124871e-05,
      "loss": 16.7383,
      "step": 3001
    },
    {
      "epoch": 3.01,
      "grad_norm": 4346.2490234375,
      "learning_rate": 8.760577915376677e-05,
      "loss": 14.0748,
      "step": 3002
    },
    {
      "epoch": 3.01,
      "grad_norm": 14240.8408203125,
      "learning_rate": 8.760061919504643e-05,
      "loss": 19.4008,
      "step": 3003
    },
    {
      "epoch": 3.01,
      "grad_norm": 3813.572021484375,
      "learning_rate": 8.759545923632612e-05,
      "loss": 12.1112,
      "step": 3004
    },
    {
      "epoch": 3.01,
      "grad_norm": 26116.31640625,
      "learning_rate": 8.759029927760578e-05,
      "loss": 19.6517,
      "step": 3005
    },
    {
      "epoch": 3.01,
      "grad_norm": 39701.1875,
      "learning_rate": 8.758513931888546e-05,
      "loss": 13.1312,
      "step": 3006
    },
    {
      "epoch": 3.01,
      "grad_norm": 21918.86328125,
      "learning_rate": 8.757997936016512e-05,
      "loss": 14.9527,
      "step": 3007
    },
    {
      "epoch": 3.01,
      "grad_norm": 925.4100341796875,
      "learning_rate": 8.757481940144479e-05,
      "loss": 13.7752,
      "step": 3008
    },
    {
      "epoch": 3.01,
      "grad_norm": 863.4995727539062,
      "learning_rate": 8.756965944272447e-05,
      "loss": 14.7108,
      "step": 3009
    },
    {
      "epoch": 3.01,
      "grad_norm": 6294.314453125,
      "learning_rate": 8.756449948400414e-05,
      "loss": 18.7721,
      "step": 3010
    },
    {
      "epoch": 3.01,
      "grad_norm": 3433.8125,
      "learning_rate": 8.75593395252838e-05,
      "loss": 14.826,
      "step": 3011
    },
    {
      "epoch": 3.02,
      "grad_norm": 8370.4462890625,
      "learning_rate": 8.755417956656348e-05,
      "loss": 17.3212,
      "step": 3012
    },
    {
      "epoch": 3.02,
      "grad_norm": 7192.22265625,
      "learning_rate": 8.754901960784314e-05,
      "loss": 22.501,
      "step": 3013
    },
    {
      "epoch": 3.02,
      "grad_norm": 6812.07421875,
      "learning_rate": 8.75438596491228e-05,
      "loss": 25.5662,
      "step": 3014
    },
    {
      "epoch": 3.02,
      "grad_norm": 8753.9677734375,
      "learning_rate": 8.753869969040249e-05,
      "loss": 28.7806,
      "step": 3015
    },
    {
      "epoch": 3.02,
      "grad_norm": 4182.2705078125,
      "learning_rate": 8.753353973168215e-05,
      "loss": 10.1632,
      "step": 3016
    },
    {
      "epoch": 3.02,
      "grad_norm": 5191.82666015625,
      "learning_rate": 8.752837977296182e-05,
      "loss": 29.1311,
      "step": 3017
    },
    {
      "epoch": 3.02,
      "grad_norm": 2980.347412109375,
      "learning_rate": 8.752321981424148e-05,
      "loss": 10.7677,
      "step": 3018
    },
    {
      "epoch": 3.02,
      "grad_norm": 3993.924560546875,
      "learning_rate": 8.751805985552116e-05,
      "loss": 13.188,
      "step": 3019
    },
    {
      "epoch": 3.02,
      "grad_norm": 27531.994140625,
      "learning_rate": 8.751289989680082e-05,
      "loss": 15.1526,
      "step": 3020
    },
    {
      "epoch": 3.02,
      "grad_norm": 2278.3427734375,
      "learning_rate": 8.750773993808051e-05,
      "loss": 12.2282,
      "step": 3021
    },
    {
      "epoch": 3.03,
      "grad_norm": 6162.490234375,
      "learning_rate": 8.750257997936017e-05,
      "loss": 21.0715,
      "step": 3022
    },
    {
      "epoch": 3.03,
      "grad_norm": 1776.833740234375,
      "learning_rate": 8.749742002063984e-05,
      "loss": 13.4498,
      "step": 3023
    },
    {
      "epoch": 3.03,
      "grad_norm": 13780.140625,
      "learning_rate": 8.74922600619195e-05,
      "loss": 19.7743,
      "step": 3024
    },
    {
      "epoch": 3.03,
      "grad_norm": 108063.765625,
      "learning_rate": 8.748710010319918e-05,
      "loss": 17.2374,
      "step": 3025
    },
    {
      "epoch": 3.03,
      "grad_norm": 18652.228515625,
      "learning_rate": 8.748194014447885e-05,
      "loss": 15.0671,
      "step": 3026
    },
    {
      "epoch": 3.03,
      "grad_norm": 22351.078125,
      "learning_rate": 8.747678018575853e-05,
      "loss": 24.3118,
      "step": 3027
    },
    {
      "epoch": 3.03,
      "grad_norm": 3557.285888671875,
      "learning_rate": 8.747162022703819e-05,
      "loss": 20.4702,
      "step": 3028
    },
    {
      "epoch": 3.03,
      "grad_norm": 4759.7861328125,
      "learning_rate": 8.746646026831786e-05,
      "loss": 16.1865,
      "step": 3029
    },
    {
      "epoch": 3.03,
      "grad_norm": 6530.2939453125,
      "learning_rate": 8.746130030959752e-05,
      "loss": 17.8188,
      "step": 3030
    },
    {
      "epoch": 3.03,
      "grad_norm": 3477.82275390625,
      "learning_rate": 8.74561403508772e-05,
      "loss": 11.9962,
      "step": 3031
    },
    {
      "epoch": 3.04,
      "grad_norm": 30788.814453125,
      "learning_rate": 8.745098039215687e-05,
      "loss": 19.568,
      "step": 3032
    },
    {
      "epoch": 3.04,
      "grad_norm": 2961.143310546875,
      "learning_rate": 8.744582043343653e-05,
      "loss": 14.36,
      "step": 3033
    },
    {
      "epoch": 3.04,
      "grad_norm": 6562.17333984375,
      "learning_rate": 8.744066047471621e-05,
      "loss": 18.2099,
      "step": 3034
    },
    {
      "epoch": 3.04,
      "grad_norm": 17625.19921875,
      "learning_rate": 8.743550051599587e-05,
      "loss": 16.1911,
      "step": 3035
    },
    {
      "epoch": 3.04,
      "grad_norm": 1124.5029296875,
      "learning_rate": 8.743034055727554e-05,
      "loss": 12.5792,
      "step": 3036
    },
    {
      "epoch": 3.04,
      "grad_norm": 2662.283447265625,
      "learning_rate": 8.742518059855522e-05,
      "loss": 14.2521,
      "step": 3037
    },
    {
      "epoch": 3.04,
      "grad_norm": 6025.07373046875,
      "learning_rate": 8.742002063983489e-05,
      "loss": 12.8258,
      "step": 3038
    },
    {
      "epoch": 3.04,
      "grad_norm": 8460.3056640625,
      "learning_rate": 8.741486068111455e-05,
      "loss": 18.4641,
      "step": 3039
    },
    {
      "epoch": 3.04,
      "grad_norm": 2066.90185546875,
      "learning_rate": 8.740970072239423e-05,
      "loss": 14.5366,
      "step": 3040
    },
    {
      "epoch": 3.04,
      "grad_norm": 5140.44921875,
      "learning_rate": 8.740454076367389e-05,
      "loss": 16.7739,
      "step": 3041
    },
    {
      "epoch": 3.05,
      "grad_norm": 4935.33642578125,
      "learning_rate": 8.739938080495356e-05,
      "loss": 13.5968,
      "step": 3042
    },
    {
      "epoch": 3.05,
      "grad_norm": 9486.7509765625,
      "learning_rate": 8.739422084623324e-05,
      "loss": 16.6796,
      "step": 3043
    },
    {
      "epoch": 3.05,
      "grad_norm": 2778.91796875,
      "learning_rate": 8.738906088751291e-05,
      "loss": 16.9013,
      "step": 3044
    },
    {
      "epoch": 3.05,
      "grad_norm": 25069.986328125,
      "learning_rate": 8.738390092879257e-05,
      "loss": 21.3058,
      "step": 3045
    },
    {
      "epoch": 3.05,
      "grad_norm": 7284.9609375,
      "learning_rate": 8.737874097007225e-05,
      "loss": 22.2922,
      "step": 3046
    },
    {
      "epoch": 3.05,
      "grad_norm": 4173.2578125,
      "learning_rate": 8.737358101135191e-05,
      "loss": 13.5251,
      "step": 3047
    },
    {
      "epoch": 3.05,
      "grad_norm": 3736.604736328125,
      "learning_rate": 8.736842105263158e-05,
      "loss": 17.3742,
      "step": 3048
    },
    {
      "epoch": 3.05,
      "grad_norm": 15278.734375,
      "learning_rate": 8.736326109391126e-05,
      "loss": 28.2932,
      "step": 3049
    },
    {
      "epoch": 3.05,
      "grad_norm": 3031.52783203125,
      "learning_rate": 8.735810113519092e-05,
      "loss": 17.5761,
      "step": 3050
    },
    {
      "epoch": 3.05,
      "grad_norm": 14069.2998046875,
      "learning_rate": 8.73529411764706e-05,
      "loss": 17.3801,
      "step": 3051
    },
    {
      "epoch": 3.06,
      "grad_norm": 22071.724609375,
      "learning_rate": 8.734778121775026e-05,
      "loss": 18.1703,
      "step": 3052
    },
    {
      "epoch": 3.06,
      "grad_norm": 23742.24609375,
      "learning_rate": 8.734262125902993e-05,
      "loss": 24.958,
      "step": 3053
    },
    {
      "epoch": 3.06,
      "grad_norm": 6988.21240234375,
      "learning_rate": 8.73374613003096e-05,
      "loss": 13.9751,
      "step": 3054
    },
    {
      "epoch": 3.06,
      "grad_norm": 1319.0927734375,
      "learning_rate": 8.733230134158928e-05,
      "loss": 14.9608,
      "step": 3055
    },
    {
      "epoch": 3.06,
      "grad_norm": 3455.525146484375,
      "learning_rate": 8.732714138286894e-05,
      "loss": 17.1051,
      "step": 3056
    },
    {
      "epoch": 3.06,
      "grad_norm": 3906.390869140625,
      "learning_rate": 8.732198142414861e-05,
      "loss": 15.2811,
      "step": 3057
    },
    {
      "epoch": 3.06,
      "grad_norm": 5508.28173828125,
      "learning_rate": 8.731682146542828e-05,
      "loss": 17.254,
      "step": 3058
    },
    {
      "epoch": 3.06,
      "grad_norm": 15997.4345703125,
      "learning_rate": 8.731166150670795e-05,
      "loss": 17.0197,
      "step": 3059
    },
    {
      "epoch": 3.06,
      "grad_norm": 6896.25439453125,
      "learning_rate": 8.730650154798762e-05,
      "loss": 18.7465,
      "step": 3060
    },
    {
      "epoch": 3.06,
      "grad_norm": 6634.2919921875,
      "learning_rate": 8.73013415892673e-05,
      "loss": 24.1603,
      "step": 3061
    },
    {
      "epoch": 3.07,
      "grad_norm": 12120.1640625,
      "learning_rate": 8.729618163054696e-05,
      "loss": 23.2728,
      "step": 3062
    },
    {
      "epoch": 3.07,
      "grad_norm": 6750.7138671875,
      "learning_rate": 8.729102167182663e-05,
      "loss": 17.2172,
      "step": 3063
    },
    {
      "epoch": 3.07,
      "grad_norm": 32792.47265625,
      "learning_rate": 8.72858617131063e-05,
      "loss": 27.9188,
      "step": 3064
    },
    {
      "epoch": 3.07,
      "grad_norm": 31505.90625,
      "learning_rate": 8.728070175438597e-05,
      "loss": 17.0596,
      "step": 3065
    },
    {
      "epoch": 3.07,
      "grad_norm": 42430.32421875,
      "learning_rate": 8.727554179566564e-05,
      "loss": 16.317,
      "step": 3066
    },
    {
      "epoch": 3.07,
      "grad_norm": 5971.4228515625,
      "learning_rate": 8.727038183694532e-05,
      "loss": 12.8445,
      "step": 3067
    },
    {
      "epoch": 3.07,
      "grad_norm": 8890.0029296875,
      "learning_rate": 8.726522187822498e-05,
      "loss": 12.9383,
      "step": 3068
    },
    {
      "epoch": 3.07,
      "grad_norm": 57933.28515625,
      "learning_rate": 8.726006191950464e-05,
      "loss": 22.6173,
      "step": 3069
    },
    {
      "epoch": 3.07,
      "grad_norm": 14797.2265625,
      "learning_rate": 8.725490196078432e-05,
      "loss": 22.2648,
      "step": 3070
    },
    {
      "epoch": 3.07,
      "grad_norm": 18431.1640625,
      "learning_rate": 8.724974200206399e-05,
      "loss": 21.5364,
      "step": 3071
    },
    {
      "epoch": 3.08,
      "grad_norm": 8317.3330078125,
      "learning_rate": 8.724458204334366e-05,
      "loss": 15.0811,
      "step": 3072
    },
    {
      "epoch": 3.08,
      "grad_norm": 34393.98828125,
      "learning_rate": 8.723942208462333e-05,
      "loss": 25.5779,
      "step": 3073
    },
    {
      "epoch": 3.08,
      "grad_norm": 1199.9110107421875,
      "learning_rate": 8.7234262125903e-05,
      "loss": 12.1091,
      "step": 3074
    },
    {
      "epoch": 3.08,
      "grad_norm": 3689.9853515625,
      "learning_rate": 8.722910216718266e-05,
      "loss": 18.0121,
      "step": 3075
    },
    {
      "epoch": 3.08,
      "grad_norm": 4264.06689453125,
      "learning_rate": 8.722394220846234e-05,
      "loss": 18.5116,
      "step": 3076
    },
    {
      "epoch": 3.08,
      "grad_norm": 8474.2021484375,
      "learning_rate": 8.721878224974201e-05,
      "loss": 17.737,
      "step": 3077
    },
    {
      "epoch": 3.08,
      "grad_norm": 8134.99267578125,
      "learning_rate": 8.721362229102168e-05,
      "loss": 20.8848,
      "step": 3078
    },
    {
      "epoch": 3.08,
      "grad_norm": 6758.8671875,
      "learning_rate": 8.720846233230135e-05,
      "loss": 17.7026,
      "step": 3079
    },
    {
      "epoch": 3.08,
      "grad_norm": 8975.16015625,
      "learning_rate": 8.720330237358102e-05,
      "loss": 16.678,
      "step": 3080
    },
    {
      "epoch": 3.08,
      "grad_norm": 2649.72998046875,
      "learning_rate": 8.719814241486068e-05,
      "loss": 19.9058,
      "step": 3081
    },
    {
      "epoch": 3.09,
      "grad_norm": 13903.60546875,
      "learning_rate": 8.719298245614036e-05,
      "loss": 16.8116,
      "step": 3082
    },
    {
      "epoch": 3.09,
      "grad_norm": 10594.2099609375,
      "learning_rate": 8.718782249742003e-05,
      "loss": 36.1636,
      "step": 3083
    },
    {
      "epoch": 3.09,
      "grad_norm": 5790.7216796875,
      "learning_rate": 8.71826625386997e-05,
      "loss": 17.9776,
      "step": 3084
    },
    {
      "epoch": 3.09,
      "grad_norm": 2974.256103515625,
      "learning_rate": 8.717750257997937e-05,
      "loss": 14.8333,
      "step": 3085
    },
    {
      "epoch": 3.09,
      "grad_norm": 1460.728759765625,
      "learning_rate": 8.717234262125903e-05,
      "loss": 11.6136,
      "step": 3086
    },
    {
      "epoch": 3.09,
      "grad_norm": 2958.100830078125,
      "learning_rate": 8.71671826625387e-05,
      "loss": 14.646,
      "step": 3087
    },
    {
      "epoch": 3.09,
      "grad_norm": 5689.83740234375,
      "learning_rate": 8.716202270381838e-05,
      "loss": 19.3222,
      "step": 3088
    },
    {
      "epoch": 3.09,
      "grad_norm": 5130.61865234375,
      "learning_rate": 8.715686274509805e-05,
      "loss": 13.4526,
      "step": 3089
    },
    {
      "epoch": 3.09,
      "grad_norm": 5293.7431640625,
      "learning_rate": 8.715170278637771e-05,
      "loss": 17.4396,
      "step": 3090
    },
    {
      "epoch": 3.09,
      "grad_norm": 4726.27001953125,
      "learning_rate": 8.714654282765739e-05,
      "loss": 21.5787,
      "step": 3091
    },
    {
      "epoch": 3.1,
      "grad_norm": 3010.3876953125,
      "learning_rate": 8.714138286893705e-05,
      "loss": 10.8908,
      "step": 3092
    },
    {
      "epoch": 3.1,
      "grad_norm": 5620.1318359375,
      "learning_rate": 8.713622291021672e-05,
      "loss": 18.264,
      "step": 3093
    },
    {
      "epoch": 3.1,
      "grad_norm": 2003.6495361328125,
      "learning_rate": 8.71310629514964e-05,
      "loss": 20.1955,
      "step": 3094
    },
    {
      "epoch": 3.1,
      "grad_norm": 7500.08544921875,
      "learning_rate": 8.712590299277607e-05,
      "loss": 21.0932,
      "step": 3095
    },
    {
      "epoch": 3.1,
      "grad_norm": 4199.24853515625,
      "learning_rate": 8.712074303405573e-05,
      "loss": 13.0943,
      "step": 3096
    },
    {
      "epoch": 3.1,
      "grad_norm": 10581.783203125,
      "learning_rate": 8.71155830753354e-05,
      "loss": 25.7369,
      "step": 3097
    },
    {
      "epoch": 3.1,
      "grad_norm": 3012.183349609375,
      "learning_rate": 8.711042311661507e-05,
      "loss": 14.5819,
      "step": 3098
    },
    {
      "epoch": 3.1,
      "grad_norm": 3329.15478515625,
      "learning_rate": 8.710526315789474e-05,
      "loss": 12.625,
      "step": 3099
    },
    {
      "epoch": 3.1,
      "grad_norm": 6296.802734375,
      "learning_rate": 8.710010319917442e-05,
      "loss": 13.3741,
      "step": 3100
    },
    {
      "epoch": 3.1,
      "grad_norm": 7512.548828125,
      "learning_rate": 8.709494324045409e-05,
      "loss": 16.7186,
      "step": 3101
    },
    {
      "epoch": 3.11,
      "grad_norm": 3257.293701171875,
      "learning_rate": 8.708978328173375e-05,
      "loss": 15.5826,
      "step": 3102
    },
    {
      "epoch": 3.11,
      "grad_norm": 2312.038818359375,
      "learning_rate": 8.708462332301341e-05,
      "loss": 14.3681,
      "step": 3103
    },
    {
      "epoch": 3.11,
      "grad_norm": 2018.0604248046875,
      "learning_rate": 8.707946336429309e-05,
      "loss": 14.0568,
      "step": 3104
    },
    {
      "epoch": 3.11,
      "grad_norm": 5351.6015625,
      "learning_rate": 8.707430340557276e-05,
      "loss": 16.9854,
      "step": 3105
    },
    {
      "epoch": 3.11,
      "grad_norm": 1564.35986328125,
      "learning_rate": 8.706914344685244e-05,
      "loss": 11.9939,
      "step": 3106
    },
    {
      "epoch": 3.11,
      "grad_norm": 2256.8193359375,
      "learning_rate": 8.70639834881321e-05,
      "loss": 22.9544,
      "step": 3107
    },
    {
      "epoch": 3.11,
      "grad_norm": 13906.8095703125,
      "learning_rate": 8.705882352941177e-05,
      "loss": 22.7083,
      "step": 3108
    },
    {
      "epoch": 3.11,
      "grad_norm": 5467.259765625,
      "learning_rate": 8.705366357069143e-05,
      "loss": 17.8953,
      "step": 3109
    },
    {
      "epoch": 3.11,
      "grad_norm": 45093.8515625,
      "learning_rate": 8.70485036119711e-05,
      "loss": 20.639,
      "step": 3110
    },
    {
      "epoch": 3.11,
      "grad_norm": 6792.20751953125,
      "learning_rate": 8.704334365325078e-05,
      "loss": 10.6809,
      "step": 3111
    },
    {
      "epoch": 3.12,
      "grad_norm": 3985.841552734375,
      "learning_rate": 8.703818369453046e-05,
      "loss": 17.8695,
      "step": 3112
    },
    {
      "epoch": 3.12,
      "grad_norm": 1516.851318359375,
      "learning_rate": 8.703302373581012e-05,
      "loss": 13.0155,
      "step": 3113
    },
    {
      "epoch": 3.12,
      "grad_norm": 4511.76806640625,
      "learning_rate": 8.702786377708979e-05,
      "loss": 10.5697,
      "step": 3114
    },
    {
      "epoch": 3.12,
      "grad_norm": 4980.142578125,
      "learning_rate": 8.702270381836945e-05,
      "loss": 11.6859,
      "step": 3115
    },
    {
      "epoch": 3.12,
      "grad_norm": 13012.328125,
      "learning_rate": 8.701754385964913e-05,
      "loss": 15.243,
      "step": 3116
    },
    {
      "epoch": 3.12,
      "grad_norm": 48946.046875,
      "learning_rate": 8.70123839009288e-05,
      "loss": 14.8315,
      "step": 3117
    },
    {
      "epoch": 3.12,
      "grad_norm": 7517.5693359375,
      "learning_rate": 8.700722394220848e-05,
      "loss": 21.0739,
      "step": 3118
    },
    {
      "epoch": 3.12,
      "grad_norm": 3906.1923828125,
      "learning_rate": 8.700206398348814e-05,
      "loss": 17.9354,
      "step": 3119
    },
    {
      "epoch": 3.12,
      "grad_norm": 5313.95361328125,
      "learning_rate": 8.699690402476781e-05,
      "loss": 24.2457,
      "step": 3120
    },
    {
      "epoch": 3.12,
      "grad_norm": 3447.51904296875,
      "learning_rate": 8.699174406604747e-05,
      "loss": 12.2604,
      "step": 3121
    },
    {
      "epoch": 3.13,
      "grad_norm": 4021.504150390625,
      "learning_rate": 8.698658410732715e-05,
      "loss": 12.9675,
      "step": 3122
    },
    {
      "epoch": 3.13,
      "grad_norm": 1564.470458984375,
      "learning_rate": 8.698142414860682e-05,
      "loss": 12.4815,
      "step": 3123
    },
    {
      "epoch": 3.13,
      "grad_norm": 8621.1884765625,
      "learning_rate": 8.697626418988648e-05,
      "loss": 25.4229,
      "step": 3124
    },
    {
      "epoch": 3.13,
      "grad_norm": 29952.451171875,
      "learning_rate": 8.697110423116616e-05,
      "loss": 24.4354,
      "step": 3125
    },
    {
      "epoch": 3.13,
      "grad_norm": 2057.21142578125,
      "learning_rate": 8.696594427244582e-05,
      "loss": 14.6488,
      "step": 3126
    },
    {
      "epoch": 3.13,
      "grad_norm": 35862.88671875,
      "learning_rate": 8.696078431372549e-05,
      "loss": 21.0551,
      "step": 3127
    },
    {
      "epoch": 3.13,
      "grad_norm": 12411.2734375,
      "learning_rate": 8.695562435500517e-05,
      "loss": 12.4235,
      "step": 3128
    },
    {
      "epoch": 3.13,
      "grad_norm": 3942.245361328125,
      "learning_rate": 8.695046439628484e-05,
      "loss": 13.7857,
      "step": 3129
    },
    {
      "epoch": 3.13,
      "grad_norm": 6988.15380859375,
      "learning_rate": 8.69453044375645e-05,
      "loss": 16.8748,
      "step": 3130
    },
    {
      "epoch": 3.13,
      "grad_norm": 1894.8699951171875,
      "learning_rate": 8.694014447884418e-05,
      "loss": 13.5075,
      "step": 3131
    },
    {
      "epoch": 3.14,
      "grad_norm": 3199.11376953125,
      "learning_rate": 8.693498452012384e-05,
      "loss": 21.4083,
      "step": 3132
    },
    {
      "epoch": 3.14,
      "grad_norm": 1276.842529296875,
      "learning_rate": 8.692982456140351e-05,
      "loss": 10.1714,
      "step": 3133
    },
    {
      "epoch": 3.14,
      "grad_norm": 4026.577392578125,
      "learning_rate": 8.692466460268319e-05,
      "loss": 16.9069,
      "step": 3134
    },
    {
      "epoch": 3.14,
      "grad_norm": 1506.806884765625,
      "learning_rate": 8.691950464396286e-05,
      "loss": 11.0355,
      "step": 3135
    },
    {
      "epoch": 3.14,
      "grad_norm": 8270.2080078125,
      "learning_rate": 8.691434468524252e-05,
      "loss": 11.7483,
      "step": 3136
    },
    {
      "epoch": 3.14,
      "grad_norm": 9356.52734375,
      "learning_rate": 8.69091847265222e-05,
      "loss": 19.6049,
      "step": 3137
    },
    {
      "epoch": 3.14,
      "grad_norm": 7727.15234375,
      "learning_rate": 8.690402476780186e-05,
      "loss": 14.5137,
      "step": 3138
    },
    {
      "epoch": 3.14,
      "grad_norm": 18544.12890625,
      "learning_rate": 8.689886480908153e-05,
      "loss": 19.9032,
      "step": 3139
    },
    {
      "epoch": 3.14,
      "grad_norm": 2191.85302734375,
      "learning_rate": 8.68937048503612e-05,
      "loss": 13.3039,
      "step": 3140
    },
    {
      "epoch": 3.14,
      "grad_norm": 1800.432861328125,
      "learning_rate": 8.688854489164087e-05,
      "loss": 15.9784,
      "step": 3141
    },
    {
      "epoch": 3.15,
      "grad_norm": 4096.92333984375,
      "learning_rate": 8.688338493292054e-05,
      "loss": 11.6488,
      "step": 3142
    },
    {
      "epoch": 3.15,
      "grad_norm": 2802.80517578125,
      "learning_rate": 8.68782249742002e-05,
      "loss": 12.7839,
      "step": 3143
    },
    {
      "epoch": 3.15,
      "grad_norm": 2858.674072265625,
      "learning_rate": 8.687306501547988e-05,
      "loss": 14.8135,
      "step": 3144
    },
    {
      "epoch": 3.15,
      "grad_norm": 11812.5927734375,
      "learning_rate": 8.686790505675955e-05,
      "loss": 24.8629,
      "step": 3145
    },
    {
      "epoch": 3.15,
      "grad_norm": 16562.912109375,
      "learning_rate": 8.686274509803923e-05,
      "loss": 12.332,
      "step": 3146
    },
    {
      "epoch": 3.15,
      "grad_norm": 13149.8310546875,
      "learning_rate": 8.685758513931889e-05,
      "loss": 12.7573,
      "step": 3147
    },
    {
      "epoch": 3.15,
      "grad_norm": 19023.5859375,
      "learning_rate": 8.685242518059856e-05,
      "loss": 16.0921,
      "step": 3148
    },
    {
      "epoch": 3.15,
      "grad_norm": 2062.9892578125,
      "learning_rate": 8.684726522187822e-05,
      "loss": 15.1123,
      "step": 3149
    },
    {
      "epoch": 3.15,
      "grad_norm": 1879.6881103515625,
      "learning_rate": 8.68421052631579e-05,
      "loss": 12.0026,
      "step": 3150
    },
    {
      "epoch": 3.15,
      "grad_norm": 12132.8818359375,
      "learning_rate": 8.683694530443757e-05,
      "loss": 14.0213,
      "step": 3151
    },
    {
      "epoch": 3.16,
      "grad_norm": 7539.09423828125,
      "learning_rate": 8.683178534571725e-05,
      "loss": 13.9569,
      "step": 3152
    },
    {
      "epoch": 3.16,
      "grad_norm": 6534.59033203125,
      "learning_rate": 8.682662538699691e-05,
      "loss": 19.9535,
      "step": 3153
    },
    {
      "epoch": 3.16,
      "grad_norm": 2794.143310546875,
      "learning_rate": 8.682146542827658e-05,
      "loss": 12.0339,
      "step": 3154
    },
    {
      "epoch": 3.16,
      "grad_norm": 6544.58837890625,
      "learning_rate": 8.681630546955624e-05,
      "loss": 14.3572,
      "step": 3155
    },
    {
      "epoch": 3.16,
      "grad_norm": 4583.525390625,
      "learning_rate": 8.681114551083592e-05,
      "loss": 18.0573,
      "step": 3156
    },
    {
      "epoch": 3.16,
      "grad_norm": 8492.45703125,
      "learning_rate": 8.680598555211559e-05,
      "loss": 15.3926,
      "step": 3157
    },
    {
      "epoch": 3.16,
      "grad_norm": 15340.5048828125,
      "learning_rate": 8.680082559339525e-05,
      "loss": 14.9797,
      "step": 3158
    },
    {
      "epoch": 3.16,
      "grad_norm": 5001.720703125,
      "learning_rate": 8.679566563467493e-05,
      "loss": 21.6592,
      "step": 3159
    },
    {
      "epoch": 3.16,
      "grad_norm": 7505.8125,
      "learning_rate": 8.679050567595459e-05,
      "loss": 13.1989,
      "step": 3160
    },
    {
      "epoch": 3.16,
      "grad_norm": 19579.923828125,
      "learning_rate": 8.678534571723426e-05,
      "loss": 16.9735,
      "step": 3161
    },
    {
      "epoch": 3.17,
      "grad_norm": 6713.4013671875,
      "learning_rate": 8.678018575851394e-05,
      "loss": 14.0077,
      "step": 3162
    },
    {
      "epoch": 3.17,
      "grad_norm": 6296.39794921875,
      "learning_rate": 8.677502579979361e-05,
      "loss": 24.0919,
      "step": 3163
    },
    {
      "epoch": 3.17,
      "grad_norm": 12141.48828125,
      "learning_rate": 8.676986584107327e-05,
      "loss": 22.7395,
      "step": 3164
    },
    {
      "epoch": 3.17,
      "grad_norm": 5770.39111328125,
      "learning_rate": 8.676470588235295e-05,
      "loss": 22.1146,
      "step": 3165
    },
    {
      "epoch": 3.17,
      "grad_norm": 2897.069091796875,
      "learning_rate": 8.675954592363261e-05,
      "loss": 16.216,
      "step": 3166
    },
    {
      "epoch": 3.17,
      "grad_norm": 1904.3907470703125,
      "learning_rate": 8.675438596491228e-05,
      "loss": 16.8617,
      "step": 3167
    },
    {
      "epoch": 3.17,
      "grad_norm": 3873.283203125,
      "learning_rate": 8.674922600619196e-05,
      "loss": 14.3506,
      "step": 3168
    },
    {
      "epoch": 3.17,
      "grad_norm": 2778.9453125,
      "learning_rate": 8.674406604747163e-05,
      "loss": 18.6427,
      "step": 3169
    },
    {
      "epoch": 3.17,
      "grad_norm": 2044.300537109375,
      "learning_rate": 8.673890608875129e-05,
      "loss": 18.3129,
      "step": 3170
    },
    {
      "epoch": 3.17,
      "grad_norm": 3488.271728515625,
      "learning_rate": 8.673374613003097e-05,
      "loss": 15.0856,
      "step": 3171
    },
    {
      "epoch": 3.18,
      "grad_norm": 981.4137573242188,
      "learning_rate": 8.672858617131063e-05,
      "loss": 12.4477,
      "step": 3172
    },
    {
      "epoch": 3.18,
      "grad_norm": 15147.826171875,
      "learning_rate": 8.67234262125903e-05,
      "loss": 12.5707,
      "step": 3173
    },
    {
      "epoch": 3.18,
      "grad_norm": 10482.5126953125,
      "learning_rate": 8.671826625386998e-05,
      "loss": 14.1175,
      "step": 3174
    },
    {
      "epoch": 3.18,
      "grad_norm": 3662.332763671875,
      "learning_rate": 8.671310629514964e-05,
      "loss": 13.7081,
      "step": 3175
    },
    {
      "epoch": 3.18,
      "grad_norm": 2858.498779296875,
      "learning_rate": 8.670794633642931e-05,
      "loss": 23.5666,
      "step": 3176
    },
    {
      "epoch": 3.18,
      "grad_norm": 2640.51416015625,
      "learning_rate": 8.670278637770897e-05,
      "loss": 16.4807,
      "step": 3177
    },
    {
      "epoch": 3.18,
      "grad_norm": 6147.27099609375,
      "learning_rate": 8.669762641898865e-05,
      "loss": 15.609,
      "step": 3178
    },
    {
      "epoch": 3.18,
      "grad_norm": 2518.468017578125,
      "learning_rate": 8.669246646026832e-05,
      "loss": 11.0738,
      "step": 3179
    },
    {
      "epoch": 3.18,
      "grad_norm": 17466.802734375,
      "learning_rate": 8.6687306501548e-05,
      "loss": 17.4258,
      "step": 3180
    },
    {
      "epoch": 3.18,
      "grad_norm": 1622.732666015625,
      "learning_rate": 8.668214654282766e-05,
      "loss": 16.7823,
      "step": 3181
    },
    {
      "epoch": 3.19,
      "grad_norm": 5427.09912109375,
      "learning_rate": 8.667698658410733e-05,
      "loss": 18.9433,
      "step": 3182
    },
    {
      "epoch": 3.19,
      "grad_norm": 4714.779296875,
      "learning_rate": 8.6671826625387e-05,
      "loss": 15.4909,
      "step": 3183
    },
    {
      "epoch": 3.19,
      "grad_norm": 8237.8955078125,
      "learning_rate": 8.666666666666667e-05,
      "loss": 19.2512,
      "step": 3184
    },
    {
      "epoch": 3.19,
      "grad_norm": 2762.238037109375,
      "learning_rate": 8.666150670794634e-05,
      "loss": 21.6875,
      "step": 3185
    },
    {
      "epoch": 3.19,
      "grad_norm": 8000.35791015625,
      "learning_rate": 8.665634674922602e-05,
      "loss": 12.6395,
      "step": 3186
    },
    {
      "epoch": 3.19,
      "grad_norm": 3846.996826171875,
      "learning_rate": 8.665118679050568e-05,
      "loss": 15.0045,
      "step": 3187
    },
    {
      "epoch": 3.19,
      "grad_norm": 3507.16357421875,
      "learning_rate": 8.664602683178535e-05,
      "loss": 11.7976,
      "step": 3188
    },
    {
      "epoch": 3.19,
      "grad_norm": 5875.1298828125,
      "learning_rate": 8.664086687306501e-05,
      "loss": 12.3848,
      "step": 3189
    },
    {
      "epoch": 3.19,
      "grad_norm": 6034.9453125,
      "learning_rate": 8.663570691434469e-05,
      "loss": 15.2114,
      "step": 3190
    },
    {
      "epoch": 3.19,
      "grad_norm": 10621.76953125,
      "learning_rate": 8.663054695562436e-05,
      "loss": 13.6647,
      "step": 3191
    },
    {
      "epoch": 3.2,
      "grad_norm": 7574.99462890625,
      "learning_rate": 8.662538699690404e-05,
      "loss": 15.8933,
      "step": 3192
    },
    {
      "epoch": 3.2,
      "grad_norm": 4737.322265625,
      "learning_rate": 8.66202270381837e-05,
      "loss": 20.0073,
      "step": 3193
    },
    {
      "epoch": 3.2,
      "grad_norm": 43728.26171875,
      "learning_rate": 8.661506707946336e-05,
      "loss": 15.7994,
      "step": 3194
    },
    {
      "epoch": 3.2,
      "grad_norm": 4154.853515625,
      "learning_rate": 8.660990712074303e-05,
      "loss": 11.1035,
      "step": 3195
    },
    {
      "epoch": 3.2,
      "grad_norm": 10042.3544921875,
      "learning_rate": 8.660474716202271e-05,
      "loss": 17.0677,
      "step": 3196
    },
    {
      "epoch": 3.2,
      "grad_norm": 12995.056640625,
      "learning_rate": 8.659958720330238e-05,
      "loss": 16.3904,
      "step": 3197
    },
    {
      "epoch": 3.2,
      "grad_norm": 4276.9267578125,
      "learning_rate": 8.659442724458204e-05,
      "loss": 14.3753,
      "step": 3198
    },
    {
      "epoch": 3.2,
      "grad_norm": 12296.986328125,
      "learning_rate": 8.658926728586172e-05,
      "loss": 25.2896,
      "step": 3199
    },
    {
      "epoch": 3.2,
      "grad_norm": 5658.7392578125,
      "learning_rate": 8.658410732714138e-05,
      "loss": 17.2193,
      "step": 3200
    },
    {
      "epoch": 3.2,
      "grad_norm": 5759.71435546875,
      "learning_rate": 8.657894736842105e-05,
      "loss": 15.9092,
      "step": 3201
    },
    {
      "epoch": 3.21,
      "grad_norm": 6524.8154296875,
      "learning_rate": 8.657378740970073e-05,
      "loss": 16.2294,
      "step": 3202
    },
    {
      "epoch": 3.21,
      "grad_norm": 6260.6201171875,
      "learning_rate": 8.65686274509804e-05,
      "loss": 17.4192,
      "step": 3203
    },
    {
      "epoch": 3.21,
      "grad_norm": 1120.234130859375,
      "learning_rate": 8.656346749226006e-05,
      "loss": 10.8803,
      "step": 3204
    },
    {
      "epoch": 3.21,
      "grad_norm": 29356.009765625,
      "learning_rate": 8.655830753353974e-05,
      "loss": 16.2575,
      "step": 3205
    },
    {
      "epoch": 3.21,
      "grad_norm": 8048.18408203125,
      "learning_rate": 8.65531475748194e-05,
      "loss": 13.6345,
      "step": 3206
    },
    {
      "epoch": 3.21,
      "grad_norm": 10466.0986328125,
      "learning_rate": 8.654798761609907e-05,
      "loss": 15.8951,
      "step": 3207
    },
    {
      "epoch": 3.21,
      "grad_norm": 2533.5498046875,
      "learning_rate": 8.654282765737875e-05,
      "loss": 16.7531,
      "step": 3208
    },
    {
      "epoch": 3.21,
      "grad_norm": 8966.6171875,
      "learning_rate": 8.653766769865842e-05,
      "loss": 13.3889,
      "step": 3209
    },
    {
      "epoch": 3.21,
      "grad_norm": 2601.88623046875,
      "learning_rate": 8.653250773993808e-05,
      "loss": 16.113,
      "step": 3210
    },
    {
      "epoch": 3.21,
      "grad_norm": 3103.648681640625,
      "learning_rate": 8.652734778121774e-05,
      "loss": 19.8722,
      "step": 3211
    },
    {
      "epoch": 3.22,
      "grad_norm": 3612.661376953125,
      "learning_rate": 8.652218782249742e-05,
      "loss": 11.6716,
      "step": 3212
    },
    {
      "epoch": 3.22,
      "grad_norm": 8847.9267578125,
      "learning_rate": 8.65170278637771e-05,
      "loss": 16.1514,
      "step": 3213
    },
    {
      "epoch": 3.22,
      "grad_norm": 3520.0244140625,
      "learning_rate": 8.651186790505677e-05,
      "loss": 11.8505,
      "step": 3214
    },
    {
      "epoch": 3.22,
      "grad_norm": 9503.33203125,
      "learning_rate": 8.650670794633643e-05,
      "loss": 16.177,
      "step": 3215
    },
    {
      "epoch": 3.22,
      "grad_norm": 3792.78955078125,
      "learning_rate": 8.65015479876161e-05,
      "loss": 16.8371,
      "step": 3216
    },
    {
      "epoch": 3.22,
      "grad_norm": 47951.99609375,
      "learning_rate": 8.649638802889576e-05,
      "loss": 18.5335,
      "step": 3217
    },
    {
      "epoch": 3.22,
      "grad_norm": 12653.345703125,
      "learning_rate": 8.649122807017544e-05,
      "loss": 13.8794,
      "step": 3218
    },
    {
      "epoch": 3.22,
      "grad_norm": 2618.7158203125,
      "learning_rate": 8.648606811145511e-05,
      "loss": 20.1389,
      "step": 3219
    },
    {
      "epoch": 3.22,
      "grad_norm": 4537.5927734375,
      "learning_rate": 8.648090815273479e-05,
      "loss": 14.1716,
      "step": 3220
    },
    {
      "epoch": 3.22,
      "grad_norm": 2767.634033203125,
      "learning_rate": 8.647574819401445e-05,
      "loss": 22.5305,
      "step": 3221
    },
    {
      "epoch": 3.23,
      "grad_norm": 2601.296630859375,
      "learning_rate": 8.647058823529412e-05,
      "loss": 16.0859,
      "step": 3222
    },
    {
      "epoch": 3.23,
      "grad_norm": 3768.147705078125,
      "learning_rate": 8.646542827657378e-05,
      "loss": 12.0325,
      "step": 3223
    },
    {
      "epoch": 3.23,
      "grad_norm": 3453.0458984375,
      "learning_rate": 8.646026831785346e-05,
      "loss": 14.181,
      "step": 3224
    },
    {
      "epoch": 3.23,
      "grad_norm": 10284.3115234375,
      "learning_rate": 8.645510835913313e-05,
      "loss": 13.6388,
      "step": 3225
    },
    {
      "epoch": 3.23,
      "grad_norm": 5807.6982421875,
      "learning_rate": 8.644994840041281e-05,
      "loss": 10.0386,
      "step": 3226
    },
    {
      "epoch": 3.23,
      "grad_norm": 5896.94287109375,
      "learning_rate": 8.644478844169247e-05,
      "loss": 15.0647,
      "step": 3227
    },
    {
      "epoch": 3.23,
      "grad_norm": 1785.8048095703125,
      "learning_rate": 8.643962848297214e-05,
      "loss": 21.1481,
      "step": 3228
    },
    {
      "epoch": 3.23,
      "grad_norm": 3810.8564453125,
      "learning_rate": 8.64344685242518e-05,
      "loss": 17.2133,
      "step": 3229
    },
    {
      "epoch": 3.23,
      "grad_norm": 9114.048828125,
      "learning_rate": 8.642930856553148e-05,
      "loss": 17.6689,
      "step": 3230
    },
    {
      "epoch": 3.23,
      "grad_norm": 962.641357421875,
      "learning_rate": 8.642414860681115e-05,
      "loss": 14.6663,
      "step": 3231
    },
    {
      "epoch": 3.24,
      "grad_norm": 9901.12890625,
      "learning_rate": 8.641898864809081e-05,
      "loss": 14.9457,
      "step": 3232
    },
    {
      "epoch": 3.24,
      "grad_norm": 3822.638427734375,
      "learning_rate": 8.641382868937049e-05,
      "loss": 14.9957,
      "step": 3233
    },
    {
      "epoch": 3.24,
      "grad_norm": 4047.4931640625,
      "learning_rate": 8.640866873065015e-05,
      "loss": 13.471,
      "step": 3234
    },
    {
      "epoch": 3.24,
      "grad_norm": 33038.06640625,
      "learning_rate": 8.640350877192982e-05,
      "loss": 16.1136,
      "step": 3235
    },
    {
      "epoch": 3.24,
      "grad_norm": 5113.94384765625,
      "learning_rate": 8.63983488132095e-05,
      "loss": 12.6077,
      "step": 3236
    },
    {
      "epoch": 3.24,
      "grad_norm": 13638.4892578125,
      "learning_rate": 8.639318885448917e-05,
      "loss": 14.4083,
      "step": 3237
    },
    {
      "epoch": 3.24,
      "grad_norm": 1756.9288330078125,
      "learning_rate": 8.638802889576883e-05,
      "loss": 14.2209,
      "step": 3238
    },
    {
      "epoch": 3.24,
      "grad_norm": 7835.96875,
      "learning_rate": 8.638286893704851e-05,
      "loss": 12.0592,
      "step": 3239
    },
    {
      "epoch": 3.24,
      "grad_norm": 4483.587890625,
      "learning_rate": 8.637770897832817e-05,
      "loss": 25.3335,
      "step": 3240
    },
    {
      "epoch": 3.24,
      "grad_norm": 966.285888671875,
      "learning_rate": 8.637254901960784e-05,
      "loss": 11.3932,
      "step": 3241
    },
    {
      "epoch": 3.25,
      "grad_norm": 8198.34375,
      "learning_rate": 8.636738906088752e-05,
      "loss": 21.9092,
      "step": 3242
    },
    {
      "epoch": 3.25,
      "grad_norm": 4367.1953125,
      "learning_rate": 8.63622291021672e-05,
      "loss": 17.2471,
      "step": 3243
    },
    {
      "epoch": 3.25,
      "grad_norm": 26960.71875,
      "learning_rate": 8.635706914344685e-05,
      "loss": 11.8168,
      "step": 3244
    },
    {
      "epoch": 3.25,
      "grad_norm": 9794.615234375,
      "learning_rate": 8.635190918472653e-05,
      "loss": 15.0815,
      "step": 3245
    },
    {
      "epoch": 3.25,
      "grad_norm": 4931.609375,
      "learning_rate": 8.634674922600619e-05,
      "loss": 12.1004,
      "step": 3246
    },
    {
      "epoch": 3.25,
      "grad_norm": 2013.2021484375,
      "learning_rate": 8.634158926728586e-05,
      "loss": 14.4571,
      "step": 3247
    },
    {
      "epoch": 3.25,
      "grad_norm": 6079.21044921875,
      "learning_rate": 8.633642930856554e-05,
      "loss": 28.8309,
      "step": 3248
    },
    {
      "epoch": 3.25,
      "grad_norm": 14149.1748046875,
      "learning_rate": 8.63312693498452e-05,
      "loss": 13.8303,
      "step": 3249
    },
    {
      "epoch": 3.25,
      "grad_norm": 22485.439453125,
      "learning_rate": 8.632610939112487e-05,
      "loss": 11.7186,
      "step": 3250
    },
    {
      "epoch": 3.25,
      "grad_norm": 4529.22119140625,
      "learning_rate": 8.632094943240454e-05,
      "loss": 15.1033,
      "step": 3251
    },
    {
      "epoch": 3.26,
      "grad_norm": 2851.58984375,
      "learning_rate": 8.631578947368421e-05,
      "loss": 13.585,
      "step": 3252
    },
    {
      "epoch": 3.26,
      "grad_norm": 4815.55615234375,
      "learning_rate": 8.631062951496388e-05,
      "loss": 21.5765,
      "step": 3253
    },
    {
      "epoch": 3.26,
      "grad_norm": 6724.9013671875,
      "learning_rate": 8.630546955624356e-05,
      "loss": 11.5028,
      "step": 3254
    },
    {
      "epoch": 3.26,
      "grad_norm": 5361.52490234375,
      "learning_rate": 8.630030959752322e-05,
      "loss": 16.9827,
      "step": 3255
    },
    {
      "epoch": 3.26,
      "grad_norm": 10268.3583984375,
      "learning_rate": 8.62951496388029e-05,
      "loss": 13.752,
      "step": 3256
    },
    {
      "epoch": 3.26,
      "grad_norm": 24663.521484375,
      "learning_rate": 8.628998968008256e-05,
      "loss": 15.8211,
      "step": 3257
    },
    {
      "epoch": 3.26,
      "grad_norm": 3102.165283203125,
      "learning_rate": 8.628482972136224e-05,
      "loss": 15.3794,
      "step": 3258
    },
    {
      "epoch": 3.26,
      "grad_norm": 991.4002075195312,
      "learning_rate": 8.62796697626419e-05,
      "loss": 12.7739,
      "step": 3259
    },
    {
      "epoch": 3.26,
      "grad_norm": 6295.4365234375,
      "learning_rate": 8.627450980392158e-05,
      "loss": 24.1074,
      "step": 3260
    },
    {
      "epoch": 3.26,
      "grad_norm": 8160.28857421875,
      "learning_rate": 8.626934984520124e-05,
      "loss": 15.0697,
      "step": 3261
    },
    {
      "epoch": 3.27,
      "grad_norm": 4119.79052734375,
      "learning_rate": 8.626418988648091e-05,
      "loss": 15.3566,
      "step": 3262
    },
    {
      "epoch": 3.27,
      "grad_norm": 9346.7998046875,
      "learning_rate": 8.625902992776058e-05,
      "loss": 17.52,
      "step": 3263
    },
    {
      "epoch": 3.27,
      "grad_norm": 3506.96337890625,
      "learning_rate": 8.625386996904025e-05,
      "loss": 15.7644,
      "step": 3264
    },
    {
      "epoch": 3.27,
      "grad_norm": 13894.318359375,
      "learning_rate": 8.624871001031992e-05,
      "loss": 13.815,
      "step": 3265
    },
    {
      "epoch": 3.27,
      "grad_norm": 7183.3271484375,
      "learning_rate": 8.624355005159959e-05,
      "loss": 16.3391,
      "step": 3266
    },
    {
      "epoch": 3.27,
      "grad_norm": 13001.5615234375,
      "learning_rate": 8.623839009287926e-05,
      "loss": 18.2362,
      "step": 3267
    },
    {
      "epoch": 3.27,
      "grad_norm": 6500.01025390625,
      "learning_rate": 8.623323013415892e-05,
      "loss": 12.5208,
      "step": 3268
    },
    {
      "epoch": 3.27,
      "grad_norm": 1845.37646484375,
      "learning_rate": 8.62280701754386e-05,
      "loss": 15.1296,
      "step": 3269
    },
    {
      "epoch": 3.27,
      "grad_norm": 11182.154296875,
      "learning_rate": 8.622291021671827e-05,
      "loss": 15.4495,
      "step": 3270
    },
    {
      "epoch": 3.27,
      "grad_norm": 5275.20654296875,
      "learning_rate": 8.621775025799794e-05,
      "loss": 17.1266,
      "step": 3271
    },
    {
      "epoch": 3.28,
      "grad_norm": 8133.2265625,
      "learning_rate": 8.62125902992776e-05,
      "loss": 15.6236,
      "step": 3272
    },
    {
      "epoch": 3.28,
      "grad_norm": 5024.1494140625,
      "learning_rate": 8.620743034055728e-05,
      "loss": 12.8742,
      "step": 3273
    },
    {
      "epoch": 3.28,
      "grad_norm": 6286.05126953125,
      "learning_rate": 8.620227038183694e-05,
      "loss": 12.8648,
      "step": 3274
    },
    {
      "epoch": 3.28,
      "grad_norm": 2805.49365234375,
      "learning_rate": 8.619711042311663e-05,
      "loss": 19.3082,
      "step": 3275
    },
    {
      "epoch": 3.28,
      "grad_norm": 9431.9541015625,
      "learning_rate": 8.619195046439629e-05,
      "loss": 15.9796,
      "step": 3276
    },
    {
      "epoch": 3.28,
      "grad_norm": 1453.6424560546875,
      "learning_rate": 8.618679050567596e-05,
      "loss": 13.275,
      "step": 3277
    },
    {
      "epoch": 3.28,
      "grad_norm": 5399.1162109375,
      "learning_rate": 8.618163054695563e-05,
      "loss": 23.0049,
      "step": 3278
    },
    {
      "epoch": 3.28,
      "grad_norm": 4799.83984375,
      "learning_rate": 8.61764705882353e-05,
      "loss": 15.7632,
      "step": 3279
    },
    {
      "epoch": 3.28,
      "grad_norm": 6976.35009765625,
      "learning_rate": 8.617131062951496e-05,
      "loss": 17.0634,
      "step": 3280
    },
    {
      "epoch": 3.28,
      "grad_norm": 23745.658203125,
      "learning_rate": 8.616615067079465e-05,
      "loss": 13.9557,
      "step": 3281
    },
    {
      "epoch": 3.29,
      "grad_norm": 7166.33740234375,
      "learning_rate": 8.616099071207431e-05,
      "loss": 15.8585,
      "step": 3282
    },
    {
      "epoch": 3.29,
      "grad_norm": 2822.91064453125,
      "learning_rate": 8.615583075335397e-05,
      "loss": 16.0192,
      "step": 3283
    },
    {
      "epoch": 3.29,
      "grad_norm": 7189.45263671875,
      "learning_rate": 8.615067079463365e-05,
      "loss": 17.0187,
      "step": 3284
    },
    {
      "epoch": 3.29,
      "grad_norm": 919.6010131835938,
      "learning_rate": 8.61455108359133e-05,
      "loss": 12.218,
      "step": 3285
    },
    {
      "epoch": 3.29,
      "grad_norm": 5102.15673828125,
      "learning_rate": 8.6140350877193e-05,
      "loss": 12.4328,
      "step": 3286
    },
    {
      "epoch": 3.29,
      "grad_norm": 11508.30859375,
      "learning_rate": 8.613519091847266e-05,
      "loss": 13.4076,
      "step": 3287
    },
    {
      "epoch": 3.29,
      "grad_norm": 6339.462890625,
      "learning_rate": 8.613003095975233e-05,
      "loss": 16.7676,
      "step": 3288
    },
    {
      "epoch": 3.29,
      "grad_norm": 3334.227294921875,
      "learning_rate": 8.612487100103199e-05,
      "loss": 20.2831,
      "step": 3289
    },
    {
      "epoch": 3.29,
      "grad_norm": 2579.576416015625,
      "learning_rate": 8.611971104231167e-05,
      "loss": 15.1514,
      "step": 3290
    },
    {
      "epoch": 3.29,
      "grad_norm": 2443.21630859375,
      "learning_rate": 8.611455108359133e-05,
      "loss": 13.6171,
      "step": 3291
    },
    {
      "epoch": 3.3,
      "grad_norm": 7835.45263671875,
      "learning_rate": 8.610939112487101e-05,
      "loss": 24.6877,
      "step": 3292
    },
    {
      "epoch": 3.3,
      "grad_norm": 19052.75390625,
      "learning_rate": 8.610423116615068e-05,
      "loss": 13.0522,
      "step": 3293
    },
    {
      "epoch": 3.3,
      "grad_norm": 41269.3125,
      "learning_rate": 8.609907120743035e-05,
      "loss": 12.5733,
      "step": 3294
    },
    {
      "epoch": 3.3,
      "grad_norm": 28444.169921875,
      "learning_rate": 8.609391124871001e-05,
      "loss": 16.3544,
      "step": 3295
    },
    {
      "epoch": 3.3,
      "grad_norm": 4241.2421875,
      "learning_rate": 8.608875128998969e-05,
      "loss": 14.6237,
      "step": 3296
    },
    {
      "epoch": 3.3,
      "grad_norm": 35979.65625,
      "learning_rate": 8.608359133126935e-05,
      "loss": 18.334,
      "step": 3297
    },
    {
      "epoch": 3.3,
      "grad_norm": 10977.998046875,
      "learning_rate": 8.607843137254903e-05,
      "loss": 20.8297,
      "step": 3298
    },
    {
      "epoch": 3.3,
      "grad_norm": 9898.60546875,
      "learning_rate": 8.60732714138287e-05,
      "loss": 22.5304,
      "step": 3299
    },
    {
      "epoch": 3.3,
      "grad_norm": 4629.65185546875,
      "learning_rate": 8.606811145510836e-05,
      "loss": 12.6521,
      "step": 3300
    },
    {
      "epoch": 3.3,
      "grad_norm": 5121.166015625,
      "learning_rate": 8.606295149638803e-05,
      "loss": 15.0293,
      "step": 3301
    },
    {
      "epoch": 3.31,
      "grad_norm": 8998.8544921875,
      "learning_rate": 8.605779153766769e-05,
      "loss": 26.7205,
      "step": 3302
    },
    {
      "epoch": 3.31,
      "grad_norm": 871.5718994140625,
      "learning_rate": 8.605263157894738e-05,
      "loss": 14.7994,
      "step": 3303
    },
    {
      "epoch": 3.31,
      "grad_norm": 36387.90625,
      "learning_rate": 8.604747162022704e-05,
      "loss": 14.9122,
      "step": 3304
    },
    {
      "epoch": 3.31,
      "grad_norm": 5484.177734375,
      "learning_rate": 8.604231166150672e-05,
      "loss": 11.6293,
      "step": 3305
    },
    {
      "epoch": 3.31,
      "grad_norm": 4325.564453125,
      "learning_rate": 8.603715170278638e-05,
      "loss": 24.6973,
      "step": 3306
    },
    {
      "epoch": 3.31,
      "grad_norm": 11350.529296875,
      "learning_rate": 8.603199174406605e-05,
      "loss": 15.651,
      "step": 3307
    },
    {
      "epoch": 3.31,
      "grad_norm": 11177.599609375,
      "learning_rate": 8.602683178534571e-05,
      "loss": 20.6248,
      "step": 3308
    },
    {
      "epoch": 3.31,
      "grad_norm": 19443.6640625,
      "learning_rate": 8.60216718266254e-05,
      "loss": 13.2799,
      "step": 3309
    },
    {
      "epoch": 3.31,
      "grad_norm": 6365.8515625,
      "learning_rate": 8.601651186790506e-05,
      "loss": 14.9795,
      "step": 3310
    },
    {
      "epoch": 3.31,
      "grad_norm": 8575.064453125,
      "learning_rate": 8.601135190918474e-05,
      "loss": 12.8767,
      "step": 3311
    },
    {
      "epoch": 3.32,
      "grad_norm": 7738.34521484375,
      "learning_rate": 8.60061919504644e-05,
      "loss": 14.9322,
      "step": 3312
    },
    {
      "epoch": 3.32,
      "grad_norm": 9939.53125,
      "learning_rate": 8.600103199174407e-05,
      "loss": 20.7092,
      "step": 3313
    },
    {
      "epoch": 3.32,
      "grad_norm": 4079.89794921875,
      "learning_rate": 8.599587203302373e-05,
      "loss": 13.0757,
      "step": 3314
    },
    {
      "epoch": 3.32,
      "grad_norm": 5095.287109375,
      "learning_rate": 8.599071207430342e-05,
      "loss": 11.9286,
      "step": 3315
    },
    {
      "epoch": 3.32,
      "grad_norm": 1432.68896484375,
      "learning_rate": 8.598555211558308e-05,
      "loss": 12.3675,
      "step": 3316
    },
    {
      "epoch": 3.32,
      "grad_norm": 1302.780517578125,
      "learning_rate": 8.598039215686276e-05,
      "loss": 10.79,
      "step": 3317
    },
    {
      "epoch": 3.32,
      "grad_norm": 73299.7109375,
      "learning_rate": 8.597523219814242e-05,
      "loss": 12.1768,
      "step": 3318
    },
    {
      "epoch": 3.32,
      "grad_norm": 54985.13671875,
      "learning_rate": 8.597007223942208e-05,
      "loss": 15.4685,
      "step": 3319
    },
    {
      "epoch": 3.32,
      "grad_norm": 1264.8836669921875,
      "learning_rate": 8.596491228070177e-05,
      "loss": 12.5268,
      "step": 3320
    },
    {
      "epoch": 3.32,
      "grad_norm": 6867.51123046875,
      "learning_rate": 8.595975232198143e-05,
      "loss": 13.4896,
      "step": 3321
    },
    {
      "epoch": 3.33,
      "grad_norm": 1040.66796875,
      "learning_rate": 8.59545923632611e-05,
      "loss": 15.06,
      "step": 3322
    },
    {
      "epoch": 3.33,
      "grad_norm": 25657.33984375,
      "learning_rate": 8.594943240454076e-05,
      "loss": 13.4041,
      "step": 3323
    },
    {
      "epoch": 3.33,
      "grad_norm": 7800.626953125,
      "learning_rate": 8.594427244582044e-05,
      "loss": 11.463,
      "step": 3324
    },
    {
      "epoch": 3.33,
      "grad_norm": 490.2149353027344,
      "learning_rate": 8.59391124871001e-05,
      "loss": 11.0111,
      "step": 3325
    },
    {
      "epoch": 3.33,
      "grad_norm": 22915.349609375,
      "learning_rate": 8.593395252837979e-05,
      "loss": 19.4957,
      "step": 3326
    },
    {
      "epoch": 3.33,
      "grad_norm": 5581.0810546875,
      "learning_rate": 8.592879256965945e-05,
      "loss": 14.7169,
      "step": 3327
    },
    {
      "epoch": 3.33,
      "grad_norm": 9964.8056640625,
      "learning_rate": 8.592363261093912e-05,
      "loss": 13.1806,
      "step": 3328
    },
    {
      "epoch": 3.33,
      "grad_norm": 5840.595703125,
      "learning_rate": 8.591847265221878e-05,
      "loss": 15.075,
      "step": 3329
    },
    {
      "epoch": 3.33,
      "grad_norm": 4678.46826171875,
      "learning_rate": 8.591331269349846e-05,
      "loss": 16.2819,
      "step": 3330
    },
    {
      "epoch": 3.33,
      "grad_norm": 4537.11181640625,
      "learning_rate": 8.590815273477813e-05,
      "loss": 12.2353,
      "step": 3331
    },
    {
      "epoch": 3.34,
      "grad_norm": 5354.73486328125,
      "learning_rate": 8.59029927760578e-05,
      "loss": 13.352,
      "step": 3332
    },
    {
      "epoch": 3.34,
      "grad_norm": 1561.23095703125,
      "learning_rate": 8.589783281733747e-05,
      "loss": 10.3988,
      "step": 3333
    },
    {
      "epoch": 3.34,
      "grad_norm": 1453.36572265625,
      "learning_rate": 8.589267285861714e-05,
      "loss": 11.37,
      "step": 3334
    },
    {
      "epoch": 3.34,
      "grad_norm": 1440.427978515625,
      "learning_rate": 8.58875128998968e-05,
      "loss": 11.763,
      "step": 3335
    },
    {
      "epoch": 3.34,
      "grad_norm": 24752.833984375,
      "learning_rate": 8.588235294117646e-05,
      "loss": 25.2834,
      "step": 3336
    },
    {
      "epoch": 3.34,
      "grad_norm": 2809.962890625,
      "learning_rate": 8.587719298245615e-05,
      "loss": 14.3737,
      "step": 3337
    },
    {
      "epoch": 3.34,
      "grad_norm": 10294.9658203125,
      "learning_rate": 8.587203302373581e-05,
      "loss": 16.7523,
      "step": 3338
    },
    {
      "epoch": 3.34,
      "grad_norm": 36432.78125,
      "learning_rate": 8.586687306501549e-05,
      "loss": 20.962,
      "step": 3339
    },
    {
      "epoch": 3.34,
      "grad_norm": 3128.90771484375,
      "learning_rate": 8.586171310629515e-05,
      "loss": 25.2718,
      "step": 3340
    },
    {
      "epoch": 3.34,
      "grad_norm": 2716.578125,
      "learning_rate": 8.585655314757482e-05,
      "loss": 25.3954,
      "step": 3341
    },
    {
      "epoch": 3.35,
      "grad_norm": 16881.814453125,
      "learning_rate": 8.585139318885448e-05,
      "loss": 16.0953,
      "step": 3342
    },
    {
      "epoch": 3.35,
      "grad_norm": 2646.42333984375,
      "learning_rate": 8.584623323013417e-05,
      "loss": 15.2264,
      "step": 3343
    },
    {
      "epoch": 3.35,
      "grad_norm": 9680.525390625,
      "learning_rate": 8.584107327141383e-05,
      "loss": 15.7164,
      "step": 3344
    },
    {
      "epoch": 3.35,
      "grad_norm": 2940.440673828125,
      "learning_rate": 8.58359133126935e-05,
      "loss": 22.6864,
      "step": 3345
    },
    {
      "epoch": 3.35,
      "grad_norm": 8974.95703125,
      "learning_rate": 8.583075335397317e-05,
      "loss": 18.2076,
      "step": 3346
    },
    {
      "epoch": 3.35,
      "grad_norm": 48507.94921875,
      "learning_rate": 8.582559339525284e-05,
      "loss": 13.5133,
      "step": 3347
    },
    {
      "epoch": 3.35,
      "grad_norm": 6629.92919921875,
      "learning_rate": 8.582043343653252e-05,
      "loss": 20.6778,
      "step": 3348
    },
    {
      "epoch": 3.35,
      "grad_norm": 42245.51171875,
      "learning_rate": 8.581527347781219e-05,
      "loss": 39.6828,
      "step": 3349
    },
    {
      "epoch": 3.35,
      "grad_norm": 1426.0679931640625,
      "learning_rate": 8.581011351909185e-05,
      "loss": 15.1079,
      "step": 3350
    },
    {
      "epoch": 3.35,
      "grad_norm": 5445.515625,
      "learning_rate": 8.580495356037153e-05,
      "loss": 18.7229,
      "step": 3351
    },
    {
      "epoch": 3.36,
      "grad_norm": 11256.736328125,
      "learning_rate": 8.579979360165119e-05,
      "loss": 31.9703,
      "step": 3352
    },
    {
      "epoch": 3.36,
      "grad_norm": 4265.4384765625,
      "learning_rate": 8.579463364293086e-05,
      "loss": 12.3777,
      "step": 3353
    },
    {
      "epoch": 3.36,
      "grad_norm": 2006.634033203125,
      "learning_rate": 8.578947368421054e-05,
      "loss": 14.1786,
      "step": 3354
    },
    {
      "epoch": 3.36,
      "grad_norm": 13080.6689453125,
      "learning_rate": 8.57843137254902e-05,
      "loss": 14.5879,
      "step": 3355
    },
    {
      "epoch": 3.36,
      "grad_norm": 24332.822265625,
      "learning_rate": 8.577915376676987e-05,
      "loss": 20.8284,
      "step": 3356
    },
    {
      "epoch": 3.36,
      "grad_norm": 7040.265625,
      "learning_rate": 8.577399380804953e-05,
      "loss": 13.7746,
      "step": 3357
    },
    {
      "epoch": 3.36,
      "grad_norm": 4016.083740234375,
      "learning_rate": 8.576883384932921e-05,
      "loss": 17.4429,
      "step": 3358
    },
    {
      "epoch": 3.36,
      "grad_norm": 8485.9970703125,
      "learning_rate": 8.576367389060888e-05,
      "loss": 22.8047,
      "step": 3359
    },
    {
      "epoch": 3.36,
      "grad_norm": 6793.3955078125,
      "learning_rate": 8.575851393188856e-05,
      "loss": 10.1408,
      "step": 3360
    },
    {
      "epoch": 3.36,
      "grad_norm": 8259.138671875,
      "learning_rate": 8.575335397316822e-05,
      "loss": 19.0911,
      "step": 3361
    },
    {
      "epoch": 3.37,
      "grad_norm": 71720.015625,
      "learning_rate": 8.574819401444789e-05,
      "loss": 15.2962,
      "step": 3362
    },
    {
      "epoch": 3.37,
      "grad_norm": 995.7633056640625,
      "learning_rate": 8.574303405572755e-05,
      "loss": 12.1043,
      "step": 3363
    },
    {
      "epoch": 3.37,
      "grad_norm": 8725.876953125,
      "learning_rate": 8.573787409700723e-05,
      "loss": 22.0104,
      "step": 3364
    },
    {
      "epoch": 3.37,
      "grad_norm": 4493.08935546875,
      "learning_rate": 8.57327141382869e-05,
      "loss": 17.1437,
      "step": 3365
    },
    {
      "epoch": 3.37,
      "grad_norm": 2379.397216796875,
      "learning_rate": 8.572755417956658e-05,
      "loss": 22.3753,
      "step": 3366
    },
    {
      "epoch": 3.37,
      "grad_norm": 17490.51953125,
      "learning_rate": 8.572239422084624e-05,
      "loss": 19.6063,
      "step": 3367
    },
    {
      "epoch": 3.37,
      "grad_norm": 3689.856689453125,
      "learning_rate": 8.571723426212591e-05,
      "loss": 13.261,
      "step": 3368
    },
    {
      "epoch": 3.37,
      "grad_norm": 3624.608642578125,
      "learning_rate": 8.571207430340557e-05,
      "loss": 13.6217,
      "step": 3369
    },
    {
      "epoch": 3.37,
      "grad_norm": 2552.0537109375,
      "learning_rate": 8.570691434468525e-05,
      "loss": 18.9834,
      "step": 3370
    },
    {
      "epoch": 3.37,
      "grad_norm": 3697.720703125,
      "learning_rate": 8.570175438596492e-05,
      "loss": 17.9241,
      "step": 3371
    },
    {
      "epoch": 3.38,
      "grad_norm": 6710.84423828125,
      "learning_rate": 8.569659442724458e-05,
      "loss": 12.0532,
      "step": 3372
    },
    {
      "epoch": 3.38,
      "grad_norm": 5616.921875,
      "learning_rate": 8.569143446852426e-05,
      "loss": 12.7759,
      "step": 3373
    },
    {
      "epoch": 3.38,
      "grad_norm": 6031.24951171875,
      "learning_rate": 8.568627450980392e-05,
      "loss": 18.3964,
      "step": 3374
    },
    {
      "epoch": 3.38,
      "grad_norm": 48343.05859375,
      "learning_rate": 8.568111455108359e-05,
      "loss": 11.518,
      "step": 3375
    },
    {
      "epoch": 3.38,
      "grad_norm": 3214.502685546875,
      "learning_rate": 8.567595459236327e-05,
      "loss": 17.8427,
      "step": 3376
    },
    {
      "epoch": 3.38,
      "grad_norm": 4664.71826171875,
      "learning_rate": 8.567079463364294e-05,
      "loss": 12.5117,
      "step": 3377
    },
    {
      "epoch": 3.38,
      "grad_norm": 5585.697265625,
      "learning_rate": 8.56656346749226e-05,
      "loss": 21.9302,
      "step": 3378
    },
    {
      "epoch": 3.38,
      "grad_norm": 3889.222900390625,
      "learning_rate": 8.566047471620228e-05,
      "loss": 17.782,
      "step": 3379
    },
    {
      "epoch": 3.38,
      "grad_norm": 5134.7001953125,
      "learning_rate": 8.565531475748194e-05,
      "loss": 26.5422,
      "step": 3380
    },
    {
      "epoch": 3.38,
      "grad_norm": 1680.5283203125,
      "learning_rate": 8.565015479876161e-05,
      "loss": 15.4546,
      "step": 3381
    },
    {
      "epoch": 3.39,
      "grad_norm": 5604.8173828125,
      "learning_rate": 8.564499484004129e-05,
      "loss": 12.7373,
      "step": 3382
    },
    {
      "epoch": 3.39,
      "grad_norm": 3902.27490234375,
      "learning_rate": 8.563983488132096e-05,
      "loss": 17.0599,
      "step": 3383
    },
    {
      "epoch": 3.39,
      "grad_norm": 22539.892578125,
      "learning_rate": 8.563467492260062e-05,
      "loss": 15.4904,
      "step": 3384
    },
    {
      "epoch": 3.39,
      "grad_norm": 2379.505615234375,
      "learning_rate": 8.56295149638803e-05,
      "loss": 13.4406,
      "step": 3385
    },
    {
      "epoch": 3.39,
      "grad_norm": 96950.5,
      "learning_rate": 8.562435500515996e-05,
      "loss": 21.7741,
      "step": 3386
    },
    {
      "epoch": 3.39,
      "grad_norm": 17255.080078125,
      "learning_rate": 8.561919504643963e-05,
      "loss": 20.576,
      "step": 3387
    },
    {
      "epoch": 3.39,
      "grad_norm": 9981.3818359375,
      "learning_rate": 8.561403508771931e-05,
      "loss": 10.4497,
      "step": 3388
    },
    {
      "epoch": 3.39,
      "grad_norm": 4599.34521484375,
      "learning_rate": 8.560887512899898e-05,
      "loss": 13.2077,
      "step": 3389
    },
    {
      "epoch": 3.39,
      "grad_norm": 9293.013671875,
      "learning_rate": 8.560371517027864e-05,
      "loss": 14.3884,
      "step": 3390
    },
    {
      "epoch": 3.39,
      "grad_norm": 8503.1142578125,
      "learning_rate": 8.55985552115583e-05,
      "loss": 13.556,
      "step": 3391
    },
    {
      "epoch": 3.4,
      "grad_norm": 8671.9189453125,
      "learning_rate": 8.559339525283798e-05,
      "loss": 13.6678,
      "step": 3392
    },
    {
      "epoch": 3.4,
      "grad_norm": 3248.33203125,
      "learning_rate": 8.558823529411765e-05,
      "loss": 15.4922,
      "step": 3393
    },
    {
      "epoch": 3.4,
      "grad_norm": 1833.465087890625,
      "learning_rate": 8.558307533539733e-05,
      "loss": 15.0848,
      "step": 3394
    },
    {
      "epoch": 3.4,
      "grad_norm": 4200.25634765625,
      "learning_rate": 8.557791537667699e-05,
      "loss": 13.605,
      "step": 3395
    },
    {
      "epoch": 3.4,
      "grad_norm": 1564.19384765625,
      "learning_rate": 8.557275541795666e-05,
      "loss": 11.1578,
      "step": 3396
    },
    {
      "epoch": 3.4,
      "grad_norm": 10695.58203125,
      "learning_rate": 8.556759545923632e-05,
      "loss": 11.3132,
      "step": 3397
    },
    {
      "epoch": 3.4,
      "grad_norm": 3520.024658203125,
      "learning_rate": 8.5562435500516e-05,
      "loss": 13.6877,
      "step": 3398
    },
    {
      "epoch": 3.4,
      "grad_norm": 7243.845703125,
      "learning_rate": 8.555727554179567e-05,
      "loss": 13.1549,
      "step": 3399
    },
    {
      "epoch": 3.4,
      "grad_norm": 2971.791259765625,
      "learning_rate": 8.555211558307535e-05,
      "loss": 13.3601,
      "step": 3400
    },
    {
      "epoch": 3.4,
      "grad_norm": 3739.97900390625,
      "learning_rate": 8.554695562435501e-05,
      "loss": 23.2636,
      "step": 3401
    },
    {
      "epoch": 3.41,
      "grad_norm": 2958.13037109375,
      "learning_rate": 8.554179566563468e-05,
      "loss": 13.4615,
      "step": 3402
    },
    {
      "epoch": 3.41,
      "grad_norm": 5625.37841796875,
      "learning_rate": 8.553663570691434e-05,
      "loss": 12.8104,
      "step": 3403
    },
    {
      "epoch": 3.41,
      "grad_norm": 15611.8671875,
      "learning_rate": 8.553147574819402e-05,
      "loss": 14.1016,
      "step": 3404
    },
    {
      "epoch": 3.41,
      "grad_norm": 7891.72216796875,
      "learning_rate": 8.552631578947369e-05,
      "loss": 15.82,
      "step": 3405
    },
    {
      "epoch": 3.41,
      "grad_norm": 2321.624755859375,
      "learning_rate": 8.552115583075337e-05,
      "loss": 11.9115,
      "step": 3406
    },
    {
      "epoch": 3.41,
      "grad_norm": 11762.4345703125,
      "learning_rate": 8.551599587203303e-05,
      "loss": 17.1561,
      "step": 3407
    },
    {
      "epoch": 3.41,
      "grad_norm": 58040.07421875,
      "learning_rate": 8.551083591331269e-05,
      "loss": 19.1563,
      "step": 3408
    },
    {
      "epoch": 3.41,
      "grad_norm": 9851.7353515625,
      "learning_rate": 8.550567595459236e-05,
      "loss": 18.6684,
      "step": 3409
    },
    {
      "epoch": 3.41,
      "grad_norm": 4637.71875,
      "learning_rate": 8.550051599587204e-05,
      "loss": 21.1622,
      "step": 3410
    },
    {
      "epoch": 3.41,
      "grad_norm": 8252.3896484375,
      "learning_rate": 8.549535603715171e-05,
      "loss": 16.5455,
      "step": 3411
    },
    {
      "epoch": 3.42,
      "grad_norm": 6352.5859375,
      "learning_rate": 8.549019607843137e-05,
      "loss": 11.9345,
      "step": 3412
    },
    {
      "epoch": 3.42,
      "grad_norm": 7062.5126953125,
      "learning_rate": 8.548503611971105e-05,
      "loss": 27.4525,
      "step": 3413
    },
    {
      "epoch": 3.42,
      "grad_norm": 16279.119140625,
      "learning_rate": 8.547987616099071e-05,
      "loss": 16.3145,
      "step": 3414
    },
    {
      "epoch": 3.42,
      "grad_norm": 13255.044921875,
      "learning_rate": 8.547471620227038e-05,
      "loss": 13.7695,
      "step": 3415
    },
    {
      "epoch": 3.42,
      "grad_norm": 3587.87451171875,
      "learning_rate": 8.546955624355006e-05,
      "loss": 17.2406,
      "step": 3416
    },
    {
      "epoch": 3.42,
      "grad_norm": 22446.828125,
      "learning_rate": 8.546439628482973e-05,
      "loss": 15.4542,
      "step": 3417
    },
    {
      "epoch": 3.42,
      "grad_norm": 29094.134765625,
      "learning_rate": 8.54592363261094e-05,
      "loss": 20.2527,
      "step": 3418
    },
    {
      "epoch": 3.42,
      "grad_norm": 1254.5728759765625,
      "learning_rate": 8.545407636738907e-05,
      "loss": 11.9048,
      "step": 3419
    },
    {
      "epoch": 3.42,
      "grad_norm": 4146.65478515625,
      "learning_rate": 8.544891640866873e-05,
      "loss": 19.2293,
      "step": 3420
    },
    {
      "epoch": 3.42,
      "grad_norm": 3912.21435546875,
      "learning_rate": 8.54437564499484e-05,
      "loss": 15.7927,
      "step": 3421
    },
    {
      "epoch": 3.43,
      "grad_norm": 888.713134765625,
      "learning_rate": 8.543859649122808e-05,
      "loss": 10.5104,
      "step": 3422
    },
    {
      "epoch": 3.43,
      "grad_norm": 5164.29638671875,
      "learning_rate": 8.543343653250775e-05,
      "loss": 11.5953,
      "step": 3423
    },
    {
      "epoch": 3.43,
      "grad_norm": 12744.626953125,
      "learning_rate": 8.542827657378741e-05,
      "loss": 13.3609,
      "step": 3424
    },
    {
      "epoch": 3.43,
      "grad_norm": 7327.9365234375,
      "learning_rate": 8.542311661506708e-05,
      "loss": 21.1925,
      "step": 3425
    },
    {
      "epoch": 3.43,
      "grad_norm": 2871.268798828125,
      "learning_rate": 8.541795665634675e-05,
      "loss": 30.784,
      "step": 3426
    },
    {
      "epoch": 3.43,
      "grad_norm": 18679.88671875,
      "learning_rate": 8.541279669762642e-05,
      "loss": 28.3926,
      "step": 3427
    },
    {
      "epoch": 3.43,
      "grad_norm": 2194.667236328125,
      "learning_rate": 8.54076367389061e-05,
      "loss": 13.9106,
      "step": 3428
    },
    {
      "epoch": 3.43,
      "grad_norm": 14913.6396484375,
      "learning_rate": 8.540247678018576e-05,
      "loss": 21.0469,
      "step": 3429
    },
    {
      "epoch": 3.43,
      "grad_norm": 26463.779296875,
      "learning_rate": 8.539731682146543e-05,
      "loss": 29.7693,
      "step": 3430
    },
    {
      "epoch": 3.43,
      "grad_norm": 4142.2255859375,
      "learning_rate": 8.53921568627451e-05,
      "loss": 14.9505,
      "step": 3431
    },
    {
      "epoch": 3.44,
      "grad_norm": 7802.68701171875,
      "learning_rate": 8.538699690402477e-05,
      "loss": 14.591,
      "step": 3432
    },
    {
      "epoch": 3.44,
      "grad_norm": 3248.237060546875,
      "learning_rate": 8.538183694530444e-05,
      "loss": 15.4398,
      "step": 3433
    },
    {
      "epoch": 3.44,
      "grad_norm": 2545.839599609375,
      "learning_rate": 8.537667698658412e-05,
      "loss": 21.5646,
      "step": 3434
    },
    {
      "epoch": 3.44,
      "grad_norm": 12270.5009765625,
      "learning_rate": 8.537151702786378e-05,
      "loss": 14.7283,
      "step": 3435
    },
    {
      "epoch": 3.44,
      "grad_norm": 3347.974365234375,
      "learning_rate": 8.536635706914345e-05,
      "loss": 13.9092,
      "step": 3436
    },
    {
      "epoch": 3.44,
      "grad_norm": 7321.31689453125,
      "learning_rate": 8.536119711042312e-05,
      "loss": 13.6481,
      "step": 3437
    },
    {
      "epoch": 3.44,
      "grad_norm": 1051.6485595703125,
      "learning_rate": 8.535603715170279e-05,
      "loss": 12.8903,
      "step": 3438
    },
    {
      "epoch": 3.44,
      "grad_norm": 3452.2509765625,
      "learning_rate": 8.535087719298246e-05,
      "loss": 15.229,
      "step": 3439
    },
    {
      "epoch": 3.44,
      "grad_norm": 3730.03759765625,
      "learning_rate": 8.534571723426214e-05,
      "loss": 9.881,
      "step": 3440
    },
    {
      "epoch": 3.44,
      "grad_norm": 7119.68359375,
      "learning_rate": 8.53405572755418e-05,
      "loss": 21.2054,
      "step": 3441
    },
    {
      "epoch": 3.45,
      "grad_norm": 3384.8681640625,
      "learning_rate": 8.533539731682147e-05,
      "loss": 11.7726,
      "step": 3442
    },
    {
      "epoch": 3.45,
      "grad_norm": 69572.0390625,
      "learning_rate": 8.533023735810114e-05,
      "loss": 16.6394,
      "step": 3443
    },
    {
      "epoch": 3.45,
      "grad_norm": 3908.94873046875,
      "learning_rate": 8.532507739938081e-05,
      "loss": 15.3047,
      "step": 3444
    },
    {
      "epoch": 3.45,
      "grad_norm": 16300.9404296875,
      "learning_rate": 8.531991744066048e-05,
      "loss": 16.1399,
      "step": 3445
    },
    {
      "epoch": 3.45,
      "grad_norm": 3365.30029296875,
      "learning_rate": 8.531475748194015e-05,
      "loss": 9.7607,
      "step": 3446
    },
    {
      "epoch": 3.45,
      "grad_norm": 11080.87109375,
      "learning_rate": 8.530959752321982e-05,
      "loss": 23.4247,
      "step": 3447
    },
    {
      "epoch": 3.45,
      "grad_norm": 2192.453857421875,
      "learning_rate": 8.530443756449948e-05,
      "loss": 12.5538,
      "step": 3448
    },
    {
      "epoch": 3.45,
      "grad_norm": 2465.422607421875,
      "learning_rate": 8.529927760577916e-05,
      "loss": 10.7878,
      "step": 3449
    },
    {
      "epoch": 3.45,
      "grad_norm": 13551.466796875,
      "learning_rate": 8.529411764705883e-05,
      "loss": 13.827,
      "step": 3450
    },
    {
      "epoch": 3.45,
      "grad_norm": 3543.300537109375,
      "learning_rate": 8.52889576883385e-05,
      "loss": 19.1186,
      "step": 3451
    },
    {
      "epoch": 3.46,
      "grad_norm": 3430.768310546875,
      "learning_rate": 8.528379772961817e-05,
      "loss": 18.5759,
      "step": 3452
    },
    {
      "epoch": 3.46,
      "grad_norm": 5628.43994140625,
      "learning_rate": 8.527863777089784e-05,
      "loss": 20.8179,
      "step": 3453
    },
    {
      "epoch": 3.46,
      "grad_norm": 17956.384765625,
      "learning_rate": 8.52734778121775e-05,
      "loss": 11.6823,
      "step": 3454
    },
    {
      "epoch": 3.46,
      "grad_norm": 782.8577270507812,
      "learning_rate": 8.526831785345718e-05,
      "loss": 14.4179,
      "step": 3455
    },
    {
      "epoch": 3.46,
      "grad_norm": 2317.59423828125,
      "learning_rate": 8.526315789473685e-05,
      "loss": 10.7494,
      "step": 3456
    },
    {
      "epoch": 3.46,
      "grad_norm": 2826.498291015625,
      "learning_rate": 8.525799793601652e-05,
      "loss": 14.1281,
      "step": 3457
    },
    {
      "epoch": 3.46,
      "grad_norm": 6451.93017578125,
      "learning_rate": 8.525283797729619e-05,
      "loss": 18.7329,
      "step": 3458
    },
    {
      "epoch": 3.46,
      "grad_norm": 2066.33935546875,
      "learning_rate": 8.524767801857586e-05,
      "loss": 16.3773,
      "step": 3459
    },
    {
      "epoch": 3.46,
      "grad_norm": 4243.453125,
      "learning_rate": 8.524251805985552e-05,
      "loss": 19.7222,
      "step": 3460
    },
    {
      "epoch": 3.46,
      "grad_norm": 6529.21044921875,
      "learning_rate": 8.52373581011352e-05,
      "loss": 14.0042,
      "step": 3461
    },
    {
      "epoch": 3.47,
      "grad_norm": 4524.56982421875,
      "learning_rate": 8.523219814241487e-05,
      "loss": 12.3719,
      "step": 3462
    },
    {
      "epoch": 3.47,
      "grad_norm": 7062.0986328125,
      "learning_rate": 8.522703818369453e-05,
      "loss": 12.0011,
      "step": 3463
    },
    {
      "epoch": 3.47,
      "grad_norm": 5649.39892578125,
      "learning_rate": 8.52218782249742e-05,
      "loss": 14.3298,
      "step": 3464
    },
    {
      "epoch": 3.47,
      "grad_norm": 1466.059814453125,
      "learning_rate": 8.521671826625387e-05,
      "loss": 20.5644,
      "step": 3465
    },
    {
      "epoch": 3.47,
      "grad_norm": 2300.9091796875,
      "learning_rate": 8.521155830753354e-05,
      "loss": 12.2954,
      "step": 3466
    },
    {
      "epoch": 3.47,
      "grad_norm": 3689.160400390625,
      "learning_rate": 8.520639834881322e-05,
      "loss": 11.8473,
      "step": 3467
    },
    {
      "epoch": 3.47,
      "grad_norm": 4949.55126953125,
      "learning_rate": 8.520123839009289e-05,
      "loss": 15.9895,
      "step": 3468
    },
    {
      "epoch": 3.47,
      "grad_norm": 30987.76953125,
      "learning_rate": 8.519607843137255e-05,
      "loss": 13.5472,
      "step": 3469
    },
    {
      "epoch": 3.47,
      "grad_norm": 15158.404296875,
      "learning_rate": 8.519091847265223e-05,
      "loss": 13.0434,
      "step": 3470
    },
    {
      "epoch": 3.47,
      "grad_norm": 11674.8095703125,
      "learning_rate": 8.518575851393189e-05,
      "loss": 21.1254,
      "step": 3471
    },
    {
      "epoch": 3.48,
      "grad_norm": 5851.22021484375,
      "learning_rate": 8.518059855521156e-05,
      "loss": 13.9616,
      "step": 3472
    },
    {
      "epoch": 3.48,
      "grad_norm": 4269.2900390625,
      "learning_rate": 8.517543859649124e-05,
      "loss": 12.3736,
      "step": 3473
    },
    {
      "epoch": 3.48,
      "grad_norm": 20457.193359375,
      "learning_rate": 8.517027863777091e-05,
      "loss": 26.6283,
      "step": 3474
    },
    {
      "epoch": 3.48,
      "grad_norm": 4121.09619140625,
      "learning_rate": 8.516511867905057e-05,
      "loss": 11.1277,
      "step": 3475
    },
    {
      "epoch": 3.48,
      "grad_norm": 4364.55859375,
      "learning_rate": 8.515995872033025e-05,
      "loss": 10.3341,
      "step": 3476
    },
    {
      "epoch": 3.48,
      "grad_norm": 12093.921875,
      "learning_rate": 8.51547987616099e-05,
      "loss": 14.351,
      "step": 3477
    },
    {
      "epoch": 3.48,
      "grad_norm": 13434.1220703125,
      "learning_rate": 8.514963880288958e-05,
      "loss": 17.2605,
      "step": 3478
    },
    {
      "epoch": 3.48,
      "grad_norm": 76385.2578125,
      "learning_rate": 8.514447884416926e-05,
      "loss": 13.0285,
      "step": 3479
    },
    {
      "epoch": 3.48,
      "grad_norm": 6171.9814453125,
      "learning_rate": 8.513931888544892e-05,
      "loss": 19.107,
      "step": 3480
    },
    {
      "epoch": 3.48,
      "grad_norm": 5971.36181640625,
      "learning_rate": 8.513415892672859e-05,
      "loss": 18.4598,
      "step": 3481
    },
    {
      "epoch": 3.49,
      "grad_norm": 2623.624755859375,
      "learning_rate": 8.512899896800825e-05,
      "loss": 27.0875,
      "step": 3482
    },
    {
      "epoch": 3.49,
      "grad_norm": 7636.1533203125,
      "learning_rate": 8.512383900928793e-05,
      "loss": 17.0691,
      "step": 3483
    },
    {
      "epoch": 3.49,
      "grad_norm": 4655.513671875,
      "learning_rate": 8.51186790505676e-05,
      "loss": 15.71,
      "step": 3484
    },
    {
      "epoch": 3.49,
      "grad_norm": 7219.81396484375,
      "learning_rate": 8.511351909184728e-05,
      "loss": 13.7011,
      "step": 3485
    },
    {
      "epoch": 3.49,
      "grad_norm": 6509.13525390625,
      "learning_rate": 8.510835913312694e-05,
      "loss": 12.7635,
      "step": 3486
    },
    {
      "epoch": 3.49,
      "grad_norm": 5283.61474609375,
      "learning_rate": 8.510319917440661e-05,
      "loss": 14.6076,
      "step": 3487
    },
    {
      "epoch": 3.49,
      "grad_norm": 6669.2333984375,
      "learning_rate": 8.509803921568627e-05,
      "loss": 16.8905,
      "step": 3488
    },
    {
      "epoch": 3.49,
      "grad_norm": 5677.26611328125,
      "learning_rate": 8.509287925696595e-05,
      "loss": 17.0709,
      "step": 3489
    },
    {
      "epoch": 3.49,
      "grad_norm": 3009.909912109375,
      "learning_rate": 8.508771929824562e-05,
      "loss": 13.969,
      "step": 3490
    },
    {
      "epoch": 3.49,
      "grad_norm": 9013.4384765625,
      "learning_rate": 8.50825593395253e-05,
      "loss": 15.4365,
      "step": 3491
    },
    {
      "epoch": 3.5,
      "grad_norm": 10874.5068359375,
      "learning_rate": 8.507739938080496e-05,
      "loss": 16.393,
      "step": 3492
    },
    {
      "epoch": 3.5,
      "grad_norm": 1421.1844482421875,
      "learning_rate": 8.507223942208463e-05,
      "loss": 11.4403,
      "step": 3493
    },
    {
      "epoch": 3.5,
      "grad_norm": 2565.360107421875,
      "learning_rate": 8.506707946336429e-05,
      "loss": 11.4132,
      "step": 3494
    },
    {
      "epoch": 3.5,
      "grad_norm": 2672.9970703125,
      "learning_rate": 8.506191950464397e-05,
      "loss": 17.8895,
      "step": 3495
    },
    {
      "epoch": 3.5,
      "grad_norm": 1484.8416748046875,
      "learning_rate": 8.505675954592364e-05,
      "loss": 11.3136,
      "step": 3496
    },
    {
      "epoch": 3.5,
      "grad_norm": 11972.2421875,
      "learning_rate": 8.50515995872033e-05,
      "loss": 17.9977,
      "step": 3497
    },
    {
      "epoch": 3.5,
      "grad_norm": 78695.4765625,
      "learning_rate": 8.504643962848298e-05,
      "loss": 16.3633,
      "step": 3498
    },
    {
      "epoch": 3.5,
      "grad_norm": 6851.4765625,
      "learning_rate": 8.504127966976264e-05,
      "loss": 19.1896,
      "step": 3499
    },
    {
      "epoch": 3.5,
      "grad_norm": 15258.6875,
      "learning_rate": 8.503611971104231e-05,
      "loss": 13.9581,
      "step": 3500
    },
    {
      "epoch": 3.5,
      "grad_norm": 15430.955078125,
      "learning_rate": 8.503095975232199e-05,
      "loss": 11.2856,
      "step": 3501
    },
    {
      "epoch": 3.51,
      "grad_norm": 19251.138671875,
      "learning_rate": 8.502579979360166e-05,
      "loss": 15.62,
      "step": 3502
    },
    {
      "epoch": 3.51,
      "grad_norm": 8821.33203125,
      "learning_rate": 8.502063983488132e-05,
      "loss": 14.638,
      "step": 3503
    },
    {
      "epoch": 3.51,
      "grad_norm": 3733.703369140625,
      "learning_rate": 8.5015479876161e-05,
      "loss": 14.9053,
      "step": 3504
    },
    {
      "epoch": 3.51,
      "grad_norm": 10092.3720703125,
      "learning_rate": 8.501031991744066e-05,
      "loss": 15.2398,
      "step": 3505
    },
    {
      "epoch": 3.51,
      "grad_norm": 2359.27587890625,
      "learning_rate": 8.500515995872033e-05,
      "loss": 12.3482,
      "step": 3506
    },
    {
      "epoch": 3.51,
      "grad_norm": 9161.30078125,
      "learning_rate": 8.5e-05,
      "loss": 18.0081,
      "step": 3507
    },
    {
      "epoch": 3.51,
      "grad_norm": 52648.12890625,
      "learning_rate": 8.499484004127968e-05,
      "loss": 25.4199,
      "step": 3508
    },
    {
      "epoch": 3.51,
      "grad_norm": 1586.967529296875,
      "learning_rate": 8.498968008255934e-05,
      "loss": 14.8166,
      "step": 3509
    },
    {
      "epoch": 3.51,
      "grad_norm": 16248.595703125,
      "learning_rate": 8.498452012383902e-05,
      "loss": 20.6427,
      "step": 3510
    },
    {
      "epoch": 3.51,
      "grad_norm": 836.1430053710938,
      "learning_rate": 8.497936016511868e-05,
      "loss": 13.2729,
      "step": 3511
    },
    {
      "epoch": 3.52,
      "grad_norm": 1809.4112548828125,
      "learning_rate": 8.497420020639835e-05,
      "loss": 12.636,
      "step": 3512
    },
    {
      "epoch": 3.52,
      "grad_norm": 5981.1181640625,
      "learning_rate": 8.496904024767803e-05,
      "loss": 17.1168,
      "step": 3513
    },
    {
      "epoch": 3.52,
      "grad_norm": 2873.138427734375,
      "learning_rate": 8.49638802889577e-05,
      "loss": 11.4081,
      "step": 3514
    },
    {
      "epoch": 3.52,
      "grad_norm": 20574.15234375,
      "learning_rate": 8.495872033023736e-05,
      "loss": 22.1021,
      "step": 3515
    },
    {
      "epoch": 3.52,
      "grad_norm": 2754.683349609375,
      "learning_rate": 8.495356037151702e-05,
      "loss": 13.5346,
      "step": 3516
    },
    {
      "epoch": 3.52,
      "grad_norm": 819.7753295898438,
      "learning_rate": 8.49484004127967e-05,
      "loss": 11.3726,
      "step": 3517
    },
    {
      "epoch": 3.52,
      "grad_norm": 5512.24560546875,
      "learning_rate": 8.494324045407637e-05,
      "loss": 16.8534,
      "step": 3518
    },
    {
      "epoch": 3.52,
      "grad_norm": 3386.529296875,
      "learning_rate": 8.493808049535605e-05,
      "loss": 18.3271,
      "step": 3519
    },
    {
      "epoch": 3.52,
      "grad_norm": 5472.19970703125,
      "learning_rate": 8.493292053663571e-05,
      "loss": 12.9371,
      "step": 3520
    },
    {
      "epoch": 3.52,
      "grad_norm": 1436.662841796875,
      "learning_rate": 8.492776057791538e-05,
      "loss": 13.5511,
      "step": 3521
    },
    {
      "epoch": 3.53,
      "grad_norm": 3532.17041015625,
      "learning_rate": 8.492260061919504e-05,
      "loss": 17.6441,
      "step": 3522
    },
    {
      "epoch": 3.53,
      "grad_norm": 16625.326171875,
      "learning_rate": 8.491744066047472e-05,
      "loss": 15.2174,
      "step": 3523
    },
    {
      "epoch": 3.53,
      "grad_norm": 1349.6927490234375,
      "learning_rate": 8.491228070175439e-05,
      "loss": 10.9765,
      "step": 3524
    },
    {
      "epoch": 3.53,
      "grad_norm": 12638.666015625,
      "learning_rate": 8.490712074303407e-05,
      "loss": 20.4946,
      "step": 3525
    },
    {
      "epoch": 3.53,
      "grad_norm": 6177.33447265625,
      "learning_rate": 8.490196078431373e-05,
      "loss": 16.4248,
      "step": 3526
    },
    {
      "epoch": 3.53,
      "grad_norm": 2167.5830078125,
      "learning_rate": 8.48968008255934e-05,
      "loss": 17.698,
      "step": 3527
    },
    {
      "epoch": 3.53,
      "grad_norm": 633.7003173828125,
      "learning_rate": 8.489164086687306e-05,
      "loss": 10.4487,
      "step": 3528
    },
    {
      "epoch": 3.53,
      "grad_norm": 3822.299072265625,
      "learning_rate": 8.488648090815274e-05,
      "loss": 16.0991,
      "step": 3529
    },
    {
      "epoch": 3.53,
      "grad_norm": 2127.3837890625,
      "learning_rate": 8.488132094943241e-05,
      "loss": 14.3852,
      "step": 3530
    },
    {
      "epoch": 3.53,
      "grad_norm": 30744.4921875,
      "learning_rate": 8.487616099071209e-05,
      "loss": 27.2026,
      "step": 3531
    },
    {
      "epoch": 3.54,
      "grad_norm": 6360.48583984375,
      "learning_rate": 8.487100103199175e-05,
      "loss": 19.1754,
      "step": 3532
    },
    {
      "epoch": 3.54,
      "grad_norm": 1285.91015625,
      "learning_rate": 8.486584107327141e-05,
      "loss": 12.5125,
      "step": 3533
    },
    {
      "epoch": 3.54,
      "grad_norm": 6530.236328125,
      "learning_rate": 8.486068111455108e-05,
      "loss": 18.4561,
      "step": 3534
    },
    {
      "epoch": 3.54,
      "grad_norm": 3628.2421875,
      "learning_rate": 8.485552115583076e-05,
      "loss": 15.4839,
      "step": 3535
    },
    {
      "epoch": 3.54,
      "grad_norm": 4607.451171875,
      "learning_rate": 8.485036119711043e-05,
      "loss": 10.9954,
      "step": 3536
    },
    {
      "epoch": 3.54,
      "grad_norm": 2614.752197265625,
      "learning_rate": 8.484520123839009e-05,
      "loss": 10.074,
      "step": 3537
    },
    {
      "epoch": 3.54,
      "grad_norm": 8247.744140625,
      "learning_rate": 8.484004127966977e-05,
      "loss": 26.7668,
      "step": 3538
    },
    {
      "epoch": 3.54,
      "grad_norm": 9845.25,
      "learning_rate": 8.483488132094943e-05,
      "loss": 15.9748,
      "step": 3539
    },
    {
      "epoch": 3.54,
      "grad_norm": 11805.8525390625,
      "learning_rate": 8.48297213622291e-05,
      "loss": 24.1023,
      "step": 3540
    },
    {
      "epoch": 3.54,
      "grad_norm": 8571.5859375,
      "learning_rate": 8.482456140350878e-05,
      "loss": 13.7973,
      "step": 3541
    },
    {
      "epoch": 3.55,
      "grad_norm": 1562.808349609375,
      "learning_rate": 8.481940144478845e-05,
      "loss": 12.3402,
      "step": 3542
    },
    {
      "epoch": 3.55,
      "grad_norm": 1562.47705078125,
      "learning_rate": 8.481424148606811e-05,
      "loss": 12.9576,
      "step": 3543
    },
    {
      "epoch": 3.55,
      "grad_norm": 2124.7099609375,
      "learning_rate": 8.480908152734779e-05,
      "loss": 12.187,
      "step": 3544
    },
    {
      "epoch": 3.55,
      "grad_norm": 3437.847900390625,
      "learning_rate": 8.480392156862745e-05,
      "loss": 18.6519,
      "step": 3545
    },
    {
      "epoch": 3.55,
      "grad_norm": 3257.53369140625,
      "learning_rate": 8.479876160990712e-05,
      "loss": 11.715,
      "step": 3546
    },
    {
      "epoch": 3.55,
      "grad_norm": 9749.974609375,
      "learning_rate": 8.47936016511868e-05,
      "loss": 20.2978,
      "step": 3547
    },
    {
      "epoch": 3.55,
      "grad_norm": 3319.0009765625,
      "learning_rate": 8.478844169246647e-05,
      "loss": 11.5992,
      "step": 3548
    },
    {
      "epoch": 3.55,
      "grad_norm": 10685.44921875,
      "learning_rate": 8.478328173374613e-05,
      "loss": 23.8328,
      "step": 3549
    },
    {
      "epoch": 3.55,
      "grad_norm": 175794.96875,
      "learning_rate": 8.47781217750258e-05,
      "loss": 15.6291,
      "step": 3550
    },
    {
      "epoch": 3.55,
      "grad_norm": 9252.3310546875,
      "learning_rate": 8.477296181630547e-05,
      "loss": 13.801,
      "step": 3551
    },
    {
      "epoch": 3.56,
      "grad_norm": 3541.9482421875,
      "learning_rate": 8.476780185758514e-05,
      "loss": 13.4774,
      "step": 3552
    },
    {
      "epoch": 3.56,
      "grad_norm": 4035.236328125,
      "learning_rate": 8.476264189886482e-05,
      "loss": 15.3681,
      "step": 3553
    },
    {
      "epoch": 3.56,
      "grad_norm": 7283.22509765625,
      "learning_rate": 8.475748194014448e-05,
      "loss": 17.1882,
      "step": 3554
    },
    {
      "epoch": 3.56,
      "grad_norm": 12421.7568359375,
      "learning_rate": 8.475232198142415e-05,
      "loss": 18.0967,
      "step": 3555
    },
    {
      "epoch": 3.56,
      "grad_norm": 6821.02001953125,
      "learning_rate": 8.474716202270381e-05,
      "loss": 14.055,
      "step": 3556
    },
    {
      "epoch": 3.56,
      "grad_norm": 1738.8336181640625,
      "learning_rate": 8.474200206398349e-05,
      "loss": 14.3782,
      "step": 3557
    },
    {
      "epoch": 3.56,
      "grad_norm": 21785.896484375,
      "learning_rate": 8.473684210526316e-05,
      "loss": 16.232,
      "step": 3558
    },
    {
      "epoch": 3.56,
      "grad_norm": 2302.65185546875,
      "learning_rate": 8.473168214654284e-05,
      "loss": 12.1313,
      "step": 3559
    },
    {
      "epoch": 3.56,
      "grad_norm": 9828.2607421875,
      "learning_rate": 8.47265221878225e-05,
      "loss": 14.3488,
      "step": 3560
    },
    {
      "epoch": 3.56,
      "grad_norm": 7758.55615234375,
      "learning_rate": 8.472136222910217e-05,
      "loss": 28.3309,
      "step": 3561
    },
    {
      "epoch": 3.57,
      "grad_norm": 1491.9222412109375,
      "learning_rate": 8.471620227038183e-05,
      "loss": 17.3989,
      "step": 3562
    },
    {
      "epoch": 3.57,
      "grad_norm": 8391.0966796875,
      "learning_rate": 8.471104231166151e-05,
      "loss": 12.1114,
      "step": 3563
    },
    {
      "epoch": 3.57,
      "grad_norm": 11609.4697265625,
      "learning_rate": 8.470588235294118e-05,
      "loss": 19.4233,
      "step": 3564
    },
    {
      "epoch": 3.57,
      "grad_norm": 3074.538330078125,
      "learning_rate": 8.470072239422086e-05,
      "loss": 14.247,
      "step": 3565
    },
    {
      "epoch": 3.57,
      "grad_norm": 33631.61328125,
      "learning_rate": 8.469556243550052e-05,
      "loss": 18.1569,
      "step": 3566
    },
    {
      "epoch": 3.57,
      "grad_norm": 6514.392578125,
      "learning_rate": 8.469040247678019e-05,
      "loss": 16.2378,
      "step": 3567
    },
    {
      "epoch": 3.57,
      "grad_norm": 1716.0172119140625,
      "learning_rate": 8.468524251805985e-05,
      "loss": 11.8193,
      "step": 3568
    },
    {
      "epoch": 3.57,
      "grad_norm": 10757.3251953125,
      "learning_rate": 8.468008255933953e-05,
      "loss": 25.2859,
      "step": 3569
    },
    {
      "epoch": 3.57,
      "grad_norm": 7632.98779296875,
      "learning_rate": 8.46749226006192e-05,
      "loss": 11.2536,
      "step": 3570
    },
    {
      "epoch": 3.57,
      "grad_norm": 556.7214965820312,
      "learning_rate": 8.466976264189886e-05,
      "loss": 22.2065,
      "step": 3571
    },
    {
      "epoch": 3.58,
      "grad_norm": 2140.66259765625,
      "learning_rate": 8.466460268317854e-05,
      "loss": 12.9162,
      "step": 3572
    },
    {
      "epoch": 3.58,
      "grad_norm": 2205.469482421875,
      "learning_rate": 8.46594427244582e-05,
      "loss": 12.8633,
      "step": 3573
    },
    {
      "epoch": 3.58,
      "grad_norm": 2035.4117431640625,
      "learning_rate": 8.465428276573787e-05,
      "loss": 13.9054,
      "step": 3574
    },
    {
      "epoch": 3.58,
      "grad_norm": 2058.44970703125,
      "learning_rate": 8.464912280701755e-05,
      "loss": 13.8195,
      "step": 3575
    },
    {
      "epoch": 3.58,
      "grad_norm": 8146.7216796875,
      "learning_rate": 8.464396284829722e-05,
      "loss": 15.8881,
      "step": 3576
    },
    {
      "epoch": 3.58,
      "grad_norm": 3979.16552734375,
      "learning_rate": 8.463880288957688e-05,
      "loss": 14.9799,
      "step": 3577
    },
    {
      "epoch": 3.58,
      "grad_norm": 11072.009765625,
      "learning_rate": 8.463364293085656e-05,
      "loss": 16.271,
      "step": 3578
    },
    {
      "epoch": 3.58,
      "grad_norm": 51901.67578125,
      "learning_rate": 8.462848297213622e-05,
      "loss": 19.7096,
      "step": 3579
    },
    {
      "epoch": 3.58,
      "grad_norm": 4493.01318359375,
      "learning_rate": 8.462332301341591e-05,
      "loss": 16.1216,
      "step": 3580
    },
    {
      "epoch": 3.58,
      "grad_norm": 3726.2021484375,
      "learning_rate": 8.461816305469557e-05,
      "loss": 12.8586,
      "step": 3581
    },
    {
      "epoch": 3.59,
      "grad_norm": 1888.77685546875,
      "learning_rate": 8.461300309597524e-05,
      "loss": 12.3776,
      "step": 3582
    },
    {
      "epoch": 3.59,
      "grad_norm": 4118.69921875,
      "learning_rate": 8.46078431372549e-05,
      "loss": 13.9272,
      "step": 3583
    },
    {
      "epoch": 3.59,
      "grad_norm": 3388.593505859375,
      "learning_rate": 8.460268317853458e-05,
      "loss": 10.8434,
      "step": 3584
    },
    {
      "epoch": 3.59,
      "grad_norm": 9242.14453125,
      "learning_rate": 8.459752321981424e-05,
      "loss": 13.8118,
      "step": 3585
    },
    {
      "epoch": 3.59,
      "grad_norm": 6118.9296875,
      "learning_rate": 8.459236326109391e-05,
      "loss": 20.601,
      "step": 3586
    },
    {
      "epoch": 3.59,
      "grad_norm": 1964.8642578125,
      "learning_rate": 8.458720330237359e-05,
      "loss": 12.6355,
      "step": 3587
    },
    {
      "epoch": 3.59,
      "grad_norm": 2703.0302734375,
      "learning_rate": 8.458204334365325e-05,
      "loss": 13.9216,
      "step": 3588
    },
    {
      "epoch": 3.59,
      "grad_norm": 581.6823120117188,
      "learning_rate": 8.457688338493292e-05,
      "loss": 11.8798,
      "step": 3589
    },
    {
      "epoch": 3.59,
      "grad_norm": 4617.46923828125,
      "learning_rate": 8.457172342621258e-05,
      "loss": 29.0412,
      "step": 3590
    },
    {
      "epoch": 3.59,
      "grad_norm": 8630.7265625,
      "learning_rate": 8.456656346749226e-05,
      "loss": 15.6043,
      "step": 3591
    },
    {
      "epoch": 3.6,
      "grad_norm": 15885.13671875,
      "learning_rate": 8.456140350877193e-05,
      "loss": 18.746,
      "step": 3592
    },
    {
      "epoch": 3.6,
      "grad_norm": 4843.36181640625,
      "learning_rate": 8.455624355005161e-05,
      "loss": 13.8614,
      "step": 3593
    },
    {
      "epoch": 3.6,
      "grad_norm": 5324.49658203125,
      "learning_rate": 8.455108359133127e-05,
      "loss": 11.2936,
      "step": 3594
    },
    {
      "epoch": 3.6,
      "grad_norm": 3562.4931640625,
      "learning_rate": 8.454592363261094e-05,
      "loss": 18.7865,
      "step": 3595
    },
    {
      "epoch": 3.6,
      "grad_norm": 6207.294921875,
      "learning_rate": 8.45407636738906e-05,
      "loss": 13.0858,
      "step": 3596
    },
    {
      "epoch": 3.6,
      "grad_norm": 69000.75,
      "learning_rate": 8.453560371517029e-05,
      "loss": 20.1642,
      "step": 3597
    },
    {
      "epoch": 3.6,
      "grad_norm": 4186.6337890625,
      "learning_rate": 8.453044375644995e-05,
      "loss": 13.343,
      "step": 3598
    },
    {
      "epoch": 3.6,
      "grad_norm": 1432.4683837890625,
      "learning_rate": 8.452528379772963e-05,
      "loss": 13.9378,
      "step": 3599
    },
    {
      "epoch": 3.6,
      "grad_norm": 1693.927734375,
      "learning_rate": 8.452012383900929e-05,
      "loss": 14.7607,
      "step": 3600
    },
    {
      "epoch": 3.6,
      "grad_norm": 1624.9178466796875,
      "learning_rate": 8.451496388028896e-05,
      "loss": 14.2176,
      "step": 3601
    },
    {
      "epoch": 3.61,
      "grad_norm": 6605.58984375,
      "learning_rate": 8.450980392156862e-05,
      "loss": 10.7097,
      "step": 3602
    },
    {
      "epoch": 3.61,
      "grad_norm": 5662.3447265625,
      "learning_rate": 8.450464396284831e-05,
      "loss": 13.5453,
      "step": 3603
    },
    {
      "epoch": 3.61,
      "grad_norm": 9095.3408203125,
      "learning_rate": 8.449948400412797e-05,
      "loss": 12.9166,
      "step": 3604
    },
    {
      "epoch": 3.61,
      "grad_norm": 2120.49169921875,
      "learning_rate": 8.449432404540763e-05,
      "loss": 13.066,
      "step": 3605
    },
    {
      "epoch": 3.61,
      "grad_norm": 18760.1953125,
      "learning_rate": 8.448916408668731e-05,
      "loss": 11.6942,
      "step": 3606
    },
    {
      "epoch": 3.61,
      "grad_norm": 5434.6123046875,
      "learning_rate": 8.448400412796697e-05,
      "loss": 18.9017,
      "step": 3607
    },
    {
      "epoch": 3.61,
      "grad_norm": 1731.819580078125,
      "learning_rate": 8.447884416924666e-05,
      "loss": 18.2356,
      "step": 3608
    },
    {
      "epoch": 3.61,
      "grad_norm": 2519.14892578125,
      "learning_rate": 8.447368421052632e-05,
      "loss": 13.5165,
      "step": 3609
    },
    {
      "epoch": 3.61,
      "grad_norm": 12603.5908203125,
      "learning_rate": 8.4468524251806e-05,
      "loss": 13.8887,
      "step": 3610
    },
    {
      "epoch": 3.61,
      "grad_norm": 2378.42333984375,
      "learning_rate": 8.446336429308565e-05,
      "loss": 14.7576,
      "step": 3611
    },
    {
      "epoch": 3.62,
      "grad_norm": 2495.952392578125,
      "learning_rate": 8.445820433436533e-05,
      "loss": 13.9171,
      "step": 3612
    },
    {
      "epoch": 3.62,
      "grad_norm": 7123.78369140625,
      "learning_rate": 8.445304437564499e-05,
      "loss": 14.2756,
      "step": 3613
    },
    {
      "epoch": 3.62,
      "grad_norm": 9309.453125,
      "learning_rate": 8.444788441692468e-05,
      "loss": 13.6242,
      "step": 3614
    },
    {
      "epoch": 3.62,
      "grad_norm": 826.9694213867188,
      "learning_rate": 8.444272445820434e-05,
      "loss": 12.1271,
      "step": 3615
    },
    {
      "epoch": 3.62,
      "grad_norm": 5915.6455078125,
      "learning_rate": 8.443756449948401e-05,
      "loss": 13.6273,
      "step": 3616
    },
    {
      "epoch": 3.62,
      "grad_norm": 5990.38037109375,
      "learning_rate": 8.443240454076367e-05,
      "loss": 11.3502,
      "step": 3617
    },
    {
      "epoch": 3.62,
      "grad_norm": 1101.8211669921875,
      "learning_rate": 8.442724458204335e-05,
      "loss": 12.1485,
      "step": 3618
    },
    {
      "epoch": 3.62,
      "grad_norm": 653.6031494140625,
      "learning_rate": 8.442208462332301e-05,
      "loss": 9.8721,
      "step": 3619
    },
    {
      "epoch": 3.62,
      "grad_norm": 2727.5078125,
      "learning_rate": 8.44169246646027e-05,
      "loss": 10.7588,
      "step": 3620
    },
    {
      "epoch": 3.62,
      "grad_norm": 609.3563232421875,
      "learning_rate": 8.441176470588236e-05,
      "loss": 12.8487,
      "step": 3621
    },
    {
      "epoch": 3.63,
      "grad_norm": 2118.222900390625,
      "learning_rate": 8.440660474716202e-05,
      "loss": 15.4253,
      "step": 3622
    },
    {
      "epoch": 3.63,
      "grad_norm": 15000.70703125,
      "learning_rate": 8.44014447884417e-05,
      "loss": 19.8679,
      "step": 3623
    },
    {
      "epoch": 3.63,
      "grad_norm": 4781.947265625,
      "learning_rate": 8.439628482972136e-05,
      "loss": 11.4445,
      "step": 3624
    },
    {
      "epoch": 3.63,
      "grad_norm": 22174.07421875,
      "learning_rate": 8.439112487100104e-05,
      "loss": 18.3449,
      "step": 3625
    },
    {
      "epoch": 3.63,
      "grad_norm": 8297.7431640625,
      "learning_rate": 8.43859649122807e-05,
      "loss": 20.4956,
      "step": 3626
    },
    {
      "epoch": 3.63,
      "grad_norm": 2873.85595703125,
      "learning_rate": 8.438080495356038e-05,
      "loss": 23.7639,
      "step": 3627
    },
    {
      "epoch": 3.63,
      "grad_norm": 6944.31103515625,
      "learning_rate": 8.437564499484004e-05,
      "loss": 18.4529,
      "step": 3628
    },
    {
      "epoch": 3.63,
      "grad_norm": 3950.648681640625,
      "learning_rate": 8.437048503611971e-05,
      "loss": 19.4687,
      "step": 3629
    },
    {
      "epoch": 3.63,
      "grad_norm": 63249.421875,
      "learning_rate": 8.436532507739938e-05,
      "loss": 27.4536,
      "step": 3630
    },
    {
      "epoch": 3.63,
      "grad_norm": 8972.328125,
      "learning_rate": 8.436016511867906e-05,
      "loss": 14.4101,
      "step": 3631
    },
    {
      "epoch": 3.64,
      "grad_norm": 7319.12841796875,
      "learning_rate": 8.435500515995872e-05,
      "loss": 20.0754,
      "step": 3632
    },
    {
      "epoch": 3.64,
      "grad_norm": 1746.13232421875,
      "learning_rate": 8.43498452012384e-05,
      "loss": 41.2207,
      "step": 3633
    },
    {
      "epoch": 3.64,
      "grad_norm": 9123.34375,
      "learning_rate": 8.434468524251806e-05,
      "loss": 22.0437,
      "step": 3634
    },
    {
      "epoch": 3.64,
      "grad_norm": 4956.42236328125,
      "learning_rate": 8.433952528379773e-05,
      "loss": 17.8893,
      "step": 3635
    },
    {
      "epoch": 3.64,
      "grad_norm": 10982.333984375,
      "learning_rate": 8.433436532507741e-05,
      "loss": 13.6281,
      "step": 3636
    },
    {
      "epoch": 3.64,
      "grad_norm": 4971.73046875,
      "learning_rate": 8.432920536635708e-05,
      "loss": 11.7112,
      "step": 3637
    },
    {
      "epoch": 3.64,
      "grad_norm": 6784.07275390625,
      "learning_rate": 8.432404540763674e-05,
      "loss": 10.0454,
      "step": 3638
    },
    {
      "epoch": 3.64,
      "grad_norm": 10371.3046875,
      "learning_rate": 8.431888544891642e-05,
      "loss": 16.0221,
      "step": 3639
    },
    {
      "epoch": 3.64,
      "grad_norm": 15301.6357421875,
      "learning_rate": 8.431372549019608e-05,
      "loss": 16.6303,
      "step": 3640
    },
    {
      "epoch": 3.64,
      "grad_norm": 7079.97216796875,
      "learning_rate": 8.430856553147574e-05,
      "loss": 17.3813,
      "step": 3641
    },
    {
      "epoch": 3.65,
      "grad_norm": 20843.66796875,
      "learning_rate": 8.430340557275543e-05,
      "loss": 16.4614,
      "step": 3642
    },
    {
      "epoch": 3.65,
      "grad_norm": 7223.869140625,
      "learning_rate": 8.429824561403509e-05,
      "loss": 13.5433,
      "step": 3643
    },
    {
      "epoch": 3.65,
      "grad_norm": 5093.25732421875,
      "learning_rate": 8.429308565531476e-05,
      "loss": 13.8709,
      "step": 3644
    },
    {
      "epoch": 3.65,
      "grad_norm": 9424.1826171875,
      "learning_rate": 8.428792569659443e-05,
      "loss": 18.6278,
      "step": 3645
    },
    {
      "epoch": 3.65,
      "grad_norm": 2386.57421875,
      "learning_rate": 8.42827657378741e-05,
      "loss": 17.083,
      "step": 3646
    },
    {
      "epoch": 3.65,
      "grad_norm": 2267.62890625,
      "learning_rate": 8.427760577915376e-05,
      "loss": 12.8243,
      "step": 3647
    },
    {
      "epoch": 3.65,
      "grad_norm": 9667.0751953125,
      "learning_rate": 8.427244582043345e-05,
      "loss": 11.9263,
      "step": 3648
    },
    {
      "epoch": 3.65,
      "grad_norm": 10497.029296875,
      "learning_rate": 8.426728586171311e-05,
      "loss": 19.6479,
      "step": 3649
    },
    {
      "epoch": 3.65,
      "grad_norm": 5992.51953125,
      "learning_rate": 8.426212590299278e-05,
      "loss": 13.9531,
      "step": 3650
    },
    {
      "epoch": 3.65,
      "grad_norm": 11437.619140625,
      "learning_rate": 8.425696594427245e-05,
      "loss": 19.9614,
      "step": 3651
    },
    {
      "epoch": 3.66,
      "grad_norm": 8546.533203125,
      "learning_rate": 8.425180598555212e-05,
      "loss": 17.8553,
      "step": 3652
    },
    {
      "epoch": 3.66,
      "grad_norm": 1543.498779296875,
      "learning_rate": 8.42466460268318e-05,
      "loss": 17.6049,
      "step": 3653
    },
    {
      "epoch": 3.66,
      "grad_norm": 1848.0794677734375,
      "learning_rate": 8.424148606811147e-05,
      "loss": 12.6968,
      "step": 3654
    },
    {
      "epoch": 3.66,
      "grad_norm": 1262.1397705078125,
      "learning_rate": 8.423632610939113e-05,
      "loss": 14.844,
      "step": 3655
    },
    {
      "epoch": 3.66,
      "grad_norm": 7562.2373046875,
      "learning_rate": 8.42311661506708e-05,
      "loss": 27.7247,
      "step": 3656
    },
    {
      "epoch": 3.66,
      "grad_norm": 3249.18603515625,
      "learning_rate": 8.422600619195047e-05,
      "loss": 10.2545,
      "step": 3657
    },
    {
      "epoch": 3.66,
      "grad_norm": 1638.4854736328125,
      "learning_rate": 8.422084623323013e-05,
      "loss": 12.1413,
      "step": 3658
    },
    {
      "epoch": 3.66,
      "grad_norm": 8704.6962890625,
      "learning_rate": 8.421568627450981e-05,
      "loss": 12.5447,
      "step": 3659
    },
    {
      "epoch": 3.66,
      "grad_norm": 8741.5869140625,
      "learning_rate": 8.421052631578948e-05,
      "loss": 28.3428,
      "step": 3660
    },
    {
      "epoch": 3.66,
      "grad_norm": 7682.52783203125,
      "learning_rate": 8.420536635706915e-05,
      "loss": 16.2511,
      "step": 3661
    },
    {
      "epoch": 3.67,
      "grad_norm": 4491.87841796875,
      "learning_rate": 8.420020639834881e-05,
      "loss": 18.4427,
      "step": 3662
    },
    {
      "epoch": 3.67,
      "grad_norm": 11810.9248046875,
      "learning_rate": 8.419504643962849e-05,
      "loss": 13.5148,
      "step": 3663
    },
    {
      "epoch": 3.67,
      "grad_norm": 8151.20751953125,
      "learning_rate": 8.418988648090816e-05,
      "loss": 16.9194,
      "step": 3664
    },
    {
      "epoch": 3.67,
      "grad_norm": 3067.562744140625,
      "learning_rate": 8.418472652218783e-05,
      "loss": 11.2363,
      "step": 3665
    },
    {
      "epoch": 3.67,
      "grad_norm": 3112.187744140625,
      "learning_rate": 8.41795665634675e-05,
      "loss": 18.2915,
      "step": 3666
    },
    {
      "epoch": 3.67,
      "grad_norm": 6135.03515625,
      "learning_rate": 8.417440660474717e-05,
      "loss": 11.35,
      "step": 3667
    },
    {
      "epoch": 3.67,
      "grad_norm": 11472.1728515625,
      "learning_rate": 8.416924664602683e-05,
      "loss": 17.702,
      "step": 3668
    },
    {
      "epoch": 3.67,
      "grad_norm": 6770.033203125,
      "learning_rate": 8.41640866873065e-05,
      "loss": 27.2076,
      "step": 3669
    },
    {
      "epoch": 3.67,
      "grad_norm": 10134.2734375,
      "learning_rate": 8.415892672858618e-05,
      "loss": 24.1892,
      "step": 3670
    },
    {
      "epoch": 3.67,
      "grad_norm": 20471.130859375,
      "learning_rate": 8.415376676986585e-05,
      "loss": 17.665,
      "step": 3671
    },
    {
      "epoch": 3.68,
      "grad_norm": 17109.134765625,
      "learning_rate": 8.414860681114552e-05,
      "loss": 31.3177,
      "step": 3672
    },
    {
      "epoch": 3.68,
      "grad_norm": 1835.1976318359375,
      "learning_rate": 8.414344685242519e-05,
      "loss": 27.0223,
      "step": 3673
    },
    {
      "epoch": 3.68,
      "grad_norm": 3717.86328125,
      "learning_rate": 8.413828689370485e-05,
      "loss": 13.1467,
      "step": 3674
    },
    {
      "epoch": 3.68,
      "grad_norm": 18010.31640625,
      "learning_rate": 8.413312693498453e-05,
      "loss": 26.2961,
      "step": 3675
    },
    {
      "epoch": 3.68,
      "grad_norm": 4723.23583984375,
      "learning_rate": 8.41279669762642e-05,
      "loss": 23.0621,
      "step": 3676
    },
    {
      "epoch": 3.68,
      "grad_norm": 1222.264404296875,
      "learning_rate": 8.412280701754386e-05,
      "loss": 15.6427,
      "step": 3677
    },
    {
      "epoch": 3.68,
      "grad_norm": 12269.822265625,
      "learning_rate": 8.411764705882354e-05,
      "loss": 17.5175,
      "step": 3678
    },
    {
      "epoch": 3.68,
      "grad_norm": 24199.98046875,
      "learning_rate": 8.41124871001032e-05,
      "loss": 18.009,
      "step": 3679
    },
    {
      "epoch": 3.68,
      "grad_norm": 1899.16357421875,
      "learning_rate": 8.410732714138287e-05,
      "loss": 14.4154,
      "step": 3680
    },
    {
      "epoch": 3.68,
      "grad_norm": 9270.06640625,
      "learning_rate": 8.410216718266255e-05,
      "loss": 20.682,
      "step": 3681
    },
    {
      "epoch": 3.69,
      "grad_norm": 9508.896484375,
      "learning_rate": 8.409700722394222e-05,
      "loss": 16.4182,
      "step": 3682
    },
    {
      "epoch": 3.69,
      "grad_norm": 4878.8759765625,
      "learning_rate": 8.409184726522188e-05,
      "loss": 13.5485,
      "step": 3683
    },
    {
      "epoch": 3.69,
      "grad_norm": 4110.7109375,
      "learning_rate": 8.408668730650156e-05,
      "loss": 15.0182,
      "step": 3684
    },
    {
      "epoch": 3.69,
      "grad_norm": 4248.1611328125,
      "learning_rate": 8.408152734778122e-05,
      "loss": 12.0502,
      "step": 3685
    },
    {
      "epoch": 3.69,
      "grad_norm": 2552.651123046875,
      "learning_rate": 8.407636738906089e-05,
      "loss": 12.5974,
      "step": 3686
    },
    {
      "epoch": 3.69,
      "grad_norm": 7546.47314453125,
      "learning_rate": 8.407120743034057e-05,
      "loss": 14.7552,
      "step": 3687
    },
    {
      "epoch": 3.69,
      "grad_norm": 24104.322265625,
      "learning_rate": 8.406604747162024e-05,
      "loss": 13.4773,
      "step": 3688
    },
    {
      "epoch": 3.69,
      "grad_norm": 2444.532958984375,
      "learning_rate": 8.40608875128999e-05,
      "loss": 21.1517,
      "step": 3689
    },
    {
      "epoch": 3.69,
      "grad_norm": 3141.515869140625,
      "learning_rate": 8.405572755417958e-05,
      "loss": 17.3515,
      "step": 3690
    },
    {
      "epoch": 3.69,
      "grad_norm": 2294.7822265625,
      "learning_rate": 8.405056759545924e-05,
      "loss": 11.1671,
      "step": 3691
    },
    {
      "epoch": 3.7,
      "grad_norm": 5461.44140625,
      "learning_rate": 8.404540763673891e-05,
      "loss": 16.8679,
      "step": 3692
    },
    {
      "epoch": 3.7,
      "grad_norm": 3044.04443359375,
      "learning_rate": 8.404024767801859e-05,
      "loss": 25.6783,
      "step": 3693
    },
    {
      "epoch": 3.7,
      "grad_norm": 14226.220703125,
      "learning_rate": 8.403508771929825e-05,
      "loss": 26.1272,
      "step": 3694
    },
    {
      "epoch": 3.7,
      "grad_norm": 2789.31982421875,
      "learning_rate": 8.402992776057792e-05,
      "loss": 19.2576,
      "step": 3695
    },
    {
      "epoch": 3.7,
      "grad_norm": 3157.417236328125,
      "learning_rate": 8.402476780185758e-05,
      "loss": 10.8626,
      "step": 3696
    },
    {
      "epoch": 3.7,
      "grad_norm": 10984.162109375,
      "learning_rate": 8.401960784313726e-05,
      "loss": 13.9223,
      "step": 3697
    },
    {
      "epoch": 3.7,
      "grad_norm": 18873.7890625,
      "learning_rate": 8.401444788441693e-05,
      "loss": 16.665,
      "step": 3698
    },
    {
      "epoch": 3.7,
      "grad_norm": 8334.8232421875,
      "learning_rate": 8.40092879256966e-05,
      "loss": 17.2206,
      "step": 3699
    },
    {
      "epoch": 3.7,
      "grad_norm": 5681.68701171875,
      "learning_rate": 8.400412796697627e-05,
      "loss": 28.2353,
      "step": 3700
    },
    {
      "epoch": 3.7,
      "grad_norm": 5718.482421875,
      "learning_rate": 8.399896800825594e-05,
      "loss": 15.989,
      "step": 3701
    },
    {
      "epoch": 3.71,
      "grad_norm": 4899.79833984375,
      "learning_rate": 8.39938080495356e-05,
      "loss": 15.5402,
      "step": 3702
    },
    {
      "epoch": 3.71,
      "grad_norm": 53867.9765625,
      "learning_rate": 8.398864809081528e-05,
      "loss": 19.1639,
      "step": 3703
    },
    {
      "epoch": 3.71,
      "grad_norm": 11864.7451171875,
      "learning_rate": 8.398348813209495e-05,
      "loss": 16.5872,
      "step": 3704
    },
    {
      "epoch": 3.71,
      "grad_norm": 879.9765014648438,
      "learning_rate": 8.397832817337463e-05,
      "loss": 17.7726,
      "step": 3705
    },
    {
      "epoch": 3.71,
      "grad_norm": 3548.71142578125,
      "learning_rate": 8.397316821465429e-05,
      "loss": 13.3105,
      "step": 3706
    },
    {
      "epoch": 3.71,
      "grad_norm": 22373.470703125,
      "learning_rate": 8.396800825593396e-05,
      "loss": 13.2931,
      "step": 3707
    },
    {
      "epoch": 3.71,
      "grad_norm": 2223.6416015625,
      "learning_rate": 8.396284829721362e-05,
      "loss": 13.7118,
      "step": 3708
    },
    {
      "epoch": 3.71,
      "grad_norm": 9095.3916015625,
      "learning_rate": 8.39576883384933e-05,
      "loss": 17.3268,
      "step": 3709
    },
    {
      "epoch": 3.71,
      "grad_norm": 9965.94140625,
      "learning_rate": 8.395252837977297e-05,
      "loss": 17.4923,
      "step": 3710
    },
    {
      "epoch": 3.71,
      "grad_norm": 6807.09814453125,
      "learning_rate": 8.394736842105263e-05,
      "loss": 13.6831,
      "step": 3711
    },
    {
      "epoch": 3.72,
      "grad_norm": 8253.4736328125,
      "learning_rate": 8.39422084623323e-05,
      "loss": 16.6059,
      "step": 3712
    },
    {
      "epoch": 3.72,
      "grad_norm": 2399.042236328125,
      "learning_rate": 8.393704850361197e-05,
      "loss": 11.53,
      "step": 3713
    },
    {
      "epoch": 3.72,
      "grad_norm": 6974.2509765625,
      "learning_rate": 8.393188854489164e-05,
      "loss": 19.8492,
      "step": 3714
    },
    {
      "epoch": 3.72,
      "grad_norm": 11478.0859375,
      "learning_rate": 8.392672858617132e-05,
      "loss": 16.1346,
      "step": 3715
    },
    {
      "epoch": 3.72,
      "grad_norm": 6336.9716796875,
      "learning_rate": 8.392156862745099e-05,
      "loss": 12.9213,
      "step": 3716
    },
    {
      "epoch": 3.72,
      "grad_norm": 4208.98291015625,
      "learning_rate": 8.391640866873065e-05,
      "loss": 17.6556,
      "step": 3717
    },
    {
      "epoch": 3.72,
      "grad_norm": 4614.1669921875,
      "learning_rate": 8.391124871001033e-05,
      "loss": 33.4538,
      "step": 3718
    },
    {
      "epoch": 3.72,
      "grad_norm": 3818.50244140625,
      "learning_rate": 8.390608875128999e-05,
      "loss": 17.6257,
      "step": 3719
    },
    {
      "epoch": 3.72,
      "grad_norm": 3280.443359375,
      "learning_rate": 8.390092879256966e-05,
      "loss": 26.2511,
      "step": 3720
    },
    {
      "epoch": 3.72,
      "grad_norm": 27293.185546875,
      "learning_rate": 8.389576883384934e-05,
      "loss": 16.691,
      "step": 3721
    },
    {
      "epoch": 3.73,
      "grad_norm": 968.7113037109375,
      "learning_rate": 8.389060887512901e-05,
      "loss": 11.5669,
      "step": 3722
    },
    {
      "epoch": 3.73,
      "grad_norm": 1240.9847412109375,
      "learning_rate": 8.388544891640867e-05,
      "loss": 10.8795,
      "step": 3723
    },
    {
      "epoch": 3.73,
      "grad_norm": 6086.64404296875,
      "learning_rate": 8.388028895768835e-05,
      "loss": 16.7319,
      "step": 3724
    },
    {
      "epoch": 3.73,
      "grad_norm": 12778.4072265625,
      "learning_rate": 8.387512899896801e-05,
      "loss": 16.8749,
      "step": 3725
    },
    {
      "epoch": 3.73,
      "grad_norm": 1096.3280029296875,
      "learning_rate": 8.386996904024768e-05,
      "loss": 20.3757,
      "step": 3726
    },
    {
      "epoch": 3.73,
      "grad_norm": 5189.7021484375,
      "learning_rate": 8.386480908152736e-05,
      "loss": 18.7009,
      "step": 3727
    },
    {
      "epoch": 3.73,
      "grad_norm": 7978.49755859375,
      "learning_rate": 8.385964912280703e-05,
      "loss": 19.568,
      "step": 3728
    },
    {
      "epoch": 3.73,
      "grad_norm": 11283.3603515625,
      "learning_rate": 8.385448916408669e-05,
      "loss": 15.6494,
      "step": 3729
    },
    {
      "epoch": 3.73,
      "grad_norm": 19565.744140625,
      "learning_rate": 8.384932920536635e-05,
      "loss": 19.9319,
      "step": 3730
    },
    {
      "epoch": 3.73,
      "grad_norm": 3800.32666015625,
      "learning_rate": 8.384416924664603e-05,
      "loss": 13.3172,
      "step": 3731
    },
    {
      "epoch": 3.74,
      "grad_norm": 3846.56982421875,
      "learning_rate": 8.38390092879257e-05,
      "loss": 12.1335,
      "step": 3732
    },
    {
      "epoch": 3.74,
      "grad_norm": 8224.7294921875,
      "learning_rate": 8.383384932920538e-05,
      "loss": 16.2893,
      "step": 3733
    },
    {
      "epoch": 3.74,
      "grad_norm": 6517.26220703125,
      "learning_rate": 8.382868937048504e-05,
      "loss": 10.9568,
      "step": 3734
    },
    {
      "epoch": 3.74,
      "grad_norm": 2413.009765625,
      "learning_rate": 8.382352941176471e-05,
      "loss": 21.1484,
      "step": 3735
    },
    {
      "epoch": 3.74,
      "grad_norm": 5659.98583984375,
      "learning_rate": 8.381836945304437e-05,
      "loss": 11.3463,
      "step": 3736
    },
    {
      "epoch": 3.74,
      "grad_norm": 16468.552734375,
      "learning_rate": 8.381320949432405e-05,
      "loss": 10.8092,
      "step": 3737
    },
    {
      "epoch": 3.74,
      "grad_norm": 5777.376953125,
      "learning_rate": 8.380804953560372e-05,
      "loss": 12.6648,
      "step": 3738
    },
    {
      "epoch": 3.74,
      "grad_norm": 6592.2451171875,
      "learning_rate": 8.38028895768834e-05,
      "loss": 16.0484,
      "step": 3739
    },
    {
      "epoch": 3.74,
      "grad_norm": 1779.611083984375,
      "learning_rate": 8.379772961816306e-05,
      "loss": 13.5045,
      "step": 3740
    },
    {
      "epoch": 3.74,
      "grad_norm": 8720.9912109375,
      "learning_rate": 8.379256965944273e-05,
      "loss": 11.5475,
      "step": 3741
    },
    {
      "epoch": 3.75,
      "grad_norm": 7195.34619140625,
      "learning_rate": 8.378740970072239e-05,
      "loss": 23.572,
      "step": 3742
    },
    {
      "epoch": 3.75,
      "grad_norm": 1137.6710205078125,
      "learning_rate": 8.378224974200207e-05,
      "loss": 16.9943,
      "step": 3743
    },
    {
      "epoch": 3.75,
      "grad_norm": 13589.927734375,
      "learning_rate": 8.377708978328174e-05,
      "loss": 23.9967,
      "step": 3744
    },
    {
      "epoch": 3.75,
      "grad_norm": 3078.767333984375,
      "learning_rate": 8.377192982456142e-05,
      "loss": 14.419,
      "step": 3745
    },
    {
      "epoch": 3.75,
      "grad_norm": 4085.02685546875,
      "learning_rate": 8.376676986584108e-05,
      "loss": 25.7082,
      "step": 3746
    },
    {
      "epoch": 3.75,
      "grad_norm": 3074.88623046875,
      "learning_rate": 8.376160990712074e-05,
      "loss": 12.0978,
      "step": 3747
    },
    {
      "epoch": 3.75,
      "grad_norm": 1197.891845703125,
      "learning_rate": 8.375644994840041e-05,
      "loss": 17.0017,
      "step": 3748
    },
    {
      "epoch": 3.75,
      "grad_norm": 3863.027587890625,
      "learning_rate": 8.375128998968009e-05,
      "loss": 11.1588,
      "step": 3749
    },
    {
      "epoch": 3.75,
      "grad_norm": 3035.41162109375,
      "learning_rate": 8.374613003095976e-05,
      "loss": 14.1274,
      "step": 3750
    },
    {
      "epoch": 3.75,
      "grad_norm": 1514.9197998046875,
      "learning_rate": 8.374097007223942e-05,
      "loss": 13.3599,
      "step": 3751
    },
    {
      "epoch": 3.76,
      "grad_norm": 9092.466796875,
      "learning_rate": 8.37358101135191e-05,
      "loss": 12.879,
      "step": 3752
    },
    {
      "epoch": 3.76,
      "grad_norm": 5237.64111328125,
      "learning_rate": 8.373065015479876e-05,
      "loss": 12.4399,
      "step": 3753
    },
    {
      "epoch": 3.76,
      "grad_norm": 6519.9111328125,
      "learning_rate": 8.372549019607843e-05,
      "loss": 13.9001,
      "step": 3754
    },
    {
      "epoch": 3.76,
      "grad_norm": 45873.62890625,
      "learning_rate": 8.372033023735811e-05,
      "loss": 12.2243,
      "step": 3755
    },
    {
      "epoch": 3.76,
      "grad_norm": 3131.07275390625,
      "learning_rate": 8.371517027863778e-05,
      "loss": 13.6334,
      "step": 3756
    },
    {
      "epoch": 3.76,
      "grad_norm": 7898.3330078125,
      "learning_rate": 8.371001031991744e-05,
      "loss": 13.6516,
      "step": 3757
    },
    {
      "epoch": 3.76,
      "grad_norm": 5566.14208984375,
      "learning_rate": 8.370485036119712e-05,
      "loss": 17.2316,
      "step": 3758
    },
    {
      "epoch": 3.76,
      "grad_norm": 22239.158203125,
      "learning_rate": 8.369969040247678e-05,
      "loss": 16.612,
      "step": 3759
    },
    {
      "epoch": 3.76,
      "grad_norm": 1057.4776611328125,
      "learning_rate": 8.369453044375645e-05,
      "loss": 11.7104,
      "step": 3760
    },
    {
      "epoch": 3.76,
      "grad_norm": 5024.6298828125,
      "learning_rate": 8.368937048503613e-05,
      "loss": 15.7549,
      "step": 3761
    },
    {
      "epoch": 3.77,
      "grad_norm": 8576.6875,
      "learning_rate": 8.36842105263158e-05,
      "loss": 15.6874,
      "step": 3762
    },
    {
      "epoch": 3.77,
      "grad_norm": 1537.543701171875,
      "learning_rate": 8.367905056759546e-05,
      "loss": 11.8249,
      "step": 3763
    },
    {
      "epoch": 3.77,
      "grad_norm": 2802.1328125,
      "learning_rate": 8.367389060887514e-05,
      "loss": 11.5599,
      "step": 3764
    },
    {
      "epoch": 3.77,
      "grad_norm": 14719.580078125,
      "learning_rate": 8.36687306501548e-05,
      "loss": 15.1371,
      "step": 3765
    },
    {
      "epoch": 3.77,
      "grad_norm": 25853.44140625,
      "learning_rate": 8.366357069143447e-05,
      "loss": 18.3979,
      "step": 3766
    },
    {
      "epoch": 3.77,
      "grad_norm": 9439.849609375,
      "learning_rate": 8.365841073271415e-05,
      "loss": 28.0725,
      "step": 3767
    },
    {
      "epoch": 3.77,
      "grad_norm": 2213.446044921875,
      "learning_rate": 8.365325077399381e-05,
      "loss": 11.916,
      "step": 3768
    },
    {
      "epoch": 3.77,
      "grad_norm": 7522.50927734375,
      "learning_rate": 8.364809081527348e-05,
      "loss": 14.1428,
      "step": 3769
    },
    {
      "epoch": 3.77,
      "grad_norm": 3529.49951171875,
      "learning_rate": 8.364293085655314e-05,
      "loss": 15.6981,
      "step": 3770
    },
    {
      "epoch": 3.77,
      "grad_norm": 10287.072265625,
      "learning_rate": 8.363777089783282e-05,
      "loss": 23.4208,
      "step": 3771
    },
    {
      "epoch": 3.78,
      "grad_norm": 2955.225830078125,
      "learning_rate": 8.363261093911249e-05,
      "loss": 18.6626,
      "step": 3772
    },
    {
      "epoch": 3.78,
      "grad_norm": 3687.33349609375,
      "learning_rate": 8.362745098039217e-05,
      "loss": 12.4947,
      "step": 3773
    },
    {
      "epoch": 3.78,
      "grad_norm": 1084.8388671875,
      "learning_rate": 8.362229102167183e-05,
      "loss": 9.928,
      "step": 3774
    },
    {
      "epoch": 3.78,
      "grad_norm": 4043.632568359375,
      "learning_rate": 8.36171310629515e-05,
      "loss": 15.9401,
      "step": 3775
    },
    {
      "epoch": 3.78,
      "grad_norm": 3908.11083984375,
      "learning_rate": 8.361197110423116e-05,
      "loss": 18.5809,
      "step": 3776
    },
    {
      "epoch": 3.78,
      "grad_norm": 9091.87890625,
      "learning_rate": 8.360681114551084e-05,
      "loss": 15.6147,
      "step": 3777
    },
    {
      "epoch": 3.78,
      "grad_norm": 1594.5985107421875,
      "learning_rate": 8.360165118679051e-05,
      "loss": 11.8534,
      "step": 3778
    },
    {
      "epoch": 3.78,
      "grad_norm": 2122.411376953125,
      "learning_rate": 8.359649122807019e-05,
      "loss": 13.6757,
      "step": 3779
    },
    {
      "epoch": 3.78,
      "grad_norm": 14682.552734375,
      "learning_rate": 8.359133126934985e-05,
      "loss": 21.857,
      "step": 3780
    },
    {
      "epoch": 3.78,
      "grad_norm": 18296.916015625,
      "learning_rate": 8.358617131062952e-05,
      "loss": 16.4531,
      "step": 3781
    },
    {
      "epoch": 3.79,
      "grad_norm": 26273.16015625,
      "learning_rate": 8.358101135190918e-05,
      "loss": 19.358,
      "step": 3782
    },
    {
      "epoch": 3.79,
      "grad_norm": 3566.276123046875,
      "learning_rate": 8.357585139318886e-05,
      "loss": 24.3152,
      "step": 3783
    },
    {
      "epoch": 3.79,
      "grad_norm": 20659.986328125,
      "learning_rate": 8.357069143446853e-05,
      "loss": 14.5314,
      "step": 3784
    },
    {
      "epoch": 3.79,
      "grad_norm": 6772.7998046875,
      "learning_rate": 8.35655314757482e-05,
      "loss": 16.0509,
      "step": 3785
    },
    {
      "epoch": 3.79,
      "grad_norm": 1554.495849609375,
      "learning_rate": 8.356037151702787e-05,
      "loss": 10.168,
      "step": 3786
    },
    {
      "epoch": 3.79,
      "grad_norm": 36445.7890625,
      "learning_rate": 8.355521155830753e-05,
      "loss": 16.0841,
      "step": 3787
    },
    {
      "epoch": 3.79,
      "grad_norm": 3841.3974609375,
      "learning_rate": 8.35500515995872e-05,
      "loss": 23.394,
      "step": 3788
    },
    {
      "epoch": 3.79,
      "grad_norm": 2573.952880859375,
      "learning_rate": 8.354489164086688e-05,
      "loss": 12.918,
      "step": 3789
    },
    {
      "epoch": 3.79,
      "grad_norm": 4786.1708984375,
      "learning_rate": 8.353973168214655e-05,
      "loss": 13.4449,
      "step": 3790
    },
    {
      "epoch": 3.79,
      "grad_norm": 31564.078125,
      "learning_rate": 8.353457172342621e-05,
      "loss": 22.8019,
      "step": 3791
    },
    {
      "epoch": 3.8,
      "grad_norm": 2002.05078125,
      "learning_rate": 8.352941176470589e-05,
      "loss": 27.4276,
      "step": 3792
    },
    {
      "epoch": 3.8,
      "grad_norm": 14748.58984375,
      "learning_rate": 8.352425180598555e-05,
      "loss": 12.536,
      "step": 3793
    },
    {
      "epoch": 3.8,
      "grad_norm": 8984.916015625,
      "learning_rate": 8.351909184726522e-05,
      "loss": 11.047,
      "step": 3794
    },
    {
      "epoch": 3.8,
      "grad_norm": 5687.673828125,
      "learning_rate": 8.35139318885449e-05,
      "loss": 16.6135,
      "step": 3795
    },
    {
      "epoch": 3.8,
      "grad_norm": 9935.1806640625,
      "learning_rate": 8.350877192982457e-05,
      "loss": 15.2935,
      "step": 3796
    },
    {
      "epoch": 3.8,
      "grad_norm": 6777.11181640625,
      "learning_rate": 8.350361197110423e-05,
      "loss": 34.1912,
      "step": 3797
    },
    {
      "epoch": 3.8,
      "grad_norm": 13825.0263671875,
      "learning_rate": 8.349845201238391e-05,
      "loss": 18.4014,
      "step": 3798
    },
    {
      "epoch": 3.8,
      "grad_norm": 4111.84130859375,
      "learning_rate": 8.349329205366357e-05,
      "loss": 13.2953,
      "step": 3799
    },
    {
      "epoch": 3.8,
      "grad_norm": 2943.949951171875,
      "learning_rate": 8.348813209494324e-05,
      "loss": 13.5639,
      "step": 3800
    },
    {
      "epoch": 3.8,
      "grad_norm": 13831.09375,
      "learning_rate": 8.348297213622292e-05,
      "loss": 12.4723,
      "step": 3801
    },
    {
      "epoch": 3.81,
      "grad_norm": 14965.2109375,
      "learning_rate": 8.347781217750258e-05,
      "loss": 19.7754,
      "step": 3802
    },
    {
      "epoch": 3.81,
      "grad_norm": 3285.12109375,
      "learning_rate": 8.347265221878225e-05,
      "loss": 20.9933,
      "step": 3803
    },
    {
      "epoch": 3.81,
      "grad_norm": 2400.72412109375,
      "learning_rate": 8.346749226006192e-05,
      "loss": 11.1867,
      "step": 3804
    },
    {
      "epoch": 3.81,
      "grad_norm": 35119.75390625,
      "learning_rate": 8.346233230134159e-05,
      "loss": 26.2041,
      "step": 3805
    },
    {
      "epoch": 3.81,
      "grad_norm": 3053.475341796875,
      "learning_rate": 8.345717234262126e-05,
      "loss": 12.0782,
      "step": 3806
    },
    {
      "epoch": 3.81,
      "grad_norm": 3315.20849609375,
      "learning_rate": 8.345201238390094e-05,
      "loss": 15.7133,
      "step": 3807
    },
    {
      "epoch": 3.81,
      "grad_norm": 1355.568603515625,
      "learning_rate": 8.34468524251806e-05,
      "loss": 17.3766,
      "step": 3808
    },
    {
      "epoch": 3.81,
      "grad_norm": 1993.9107666015625,
      "learning_rate": 8.344169246646027e-05,
      "loss": 18.2536,
      "step": 3809
    },
    {
      "epoch": 3.81,
      "grad_norm": 3698.5498046875,
      "learning_rate": 8.343653250773994e-05,
      "loss": 12.0448,
      "step": 3810
    },
    {
      "epoch": 3.81,
      "grad_norm": 11206.0908203125,
      "learning_rate": 8.343137254901961e-05,
      "loss": 17.9973,
      "step": 3811
    },
    {
      "epoch": 3.82,
      "grad_norm": 4038.91015625,
      "learning_rate": 8.342621259029928e-05,
      "loss": 16.2518,
      "step": 3812
    },
    {
      "epoch": 3.82,
      "grad_norm": 2552.537841796875,
      "learning_rate": 8.342105263157896e-05,
      "loss": 16.9627,
      "step": 3813
    },
    {
      "epoch": 3.82,
      "grad_norm": 4802.6923828125,
      "learning_rate": 8.341589267285862e-05,
      "loss": 13.432,
      "step": 3814
    },
    {
      "epoch": 3.82,
      "grad_norm": 5478.7822265625,
      "learning_rate": 8.34107327141383e-05,
      "loss": 12.3753,
      "step": 3815
    },
    {
      "epoch": 3.82,
      "grad_norm": 5989.013671875,
      "learning_rate": 8.340557275541796e-05,
      "loss": 11.1706,
      "step": 3816
    },
    {
      "epoch": 3.82,
      "grad_norm": 5477.8193359375,
      "learning_rate": 8.340041279669763e-05,
      "loss": 14.7044,
      "step": 3817
    },
    {
      "epoch": 3.82,
      "grad_norm": 5008.6494140625,
      "learning_rate": 8.33952528379773e-05,
      "loss": 12.2806,
      "step": 3818
    },
    {
      "epoch": 3.82,
      "grad_norm": 3290.8427734375,
      "learning_rate": 8.339009287925697e-05,
      "loss": 22.2998,
      "step": 3819
    },
    {
      "epoch": 3.82,
      "grad_norm": 169249.796875,
      "learning_rate": 8.338493292053664e-05,
      "loss": 14.0284,
      "step": 3820
    },
    {
      "epoch": 3.82,
      "grad_norm": 4739.62158203125,
      "learning_rate": 8.33797729618163e-05,
      "loss": 13.9547,
      "step": 3821
    },
    {
      "epoch": 3.83,
      "grad_norm": 1143.8070068359375,
      "learning_rate": 8.337461300309598e-05,
      "loss": 10.6537,
      "step": 3822
    },
    {
      "epoch": 3.83,
      "grad_norm": 5349.3623046875,
      "learning_rate": 8.336945304437565e-05,
      "loss": 12.5456,
      "step": 3823
    },
    {
      "epoch": 3.83,
      "grad_norm": 9814.76953125,
      "learning_rate": 8.336429308565532e-05,
      "loss": 11.9075,
      "step": 3824
    },
    {
      "epoch": 3.83,
      "grad_norm": 65512.23046875,
      "learning_rate": 8.335913312693499e-05,
      "loss": 26.7594,
      "step": 3825
    },
    {
      "epoch": 3.83,
      "grad_norm": 3288.06201171875,
      "learning_rate": 8.335397316821466e-05,
      "loss": 15.9294,
      "step": 3826
    },
    {
      "epoch": 3.83,
      "grad_norm": 341.2011413574219,
      "learning_rate": 8.334881320949432e-05,
      "loss": 10.5838,
      "step": 3827
    },
    {
      "epoch": 3.83,
      "grad_norm": 12739.44140625,
      "learning_rate": 8.3343653250774e-05,
      "loss": 33.7279,
      "step": 3828
    },
    {
      "epoch": 3.83,
      "grad_norm": 6047.05078125,
      "learning_rate": 8.333849329205367e-05,
      "loss": 15.5894,
      "step": 3829
    },
    {
      "epoch": 3.83,
      "grad_norm": 2755.68896484375,
      "learning_rate": 8.333333333333334e-05,
      "loss": 17.7086,
      "step": 3830
    },
    {
      "epoch": 3.83,
      "grad_norm": 16317.791015625,
      "learning_rate": 8.3328173374613e-05,
      "loss": 11.8745,
      "step": 3831
    },
    {
      "epoch": 3.84,
      "grad_norm": 5268.673828125,
      "learning_rate": 8.332301341589268e-05,
      "loss": 16.263,
      "step": 3832
    },
    {
      "epoch": 3.84,
      "grad_norm": 1115.1943359375,
      "learning_rate": 8.331785345717234e-05,
      "loss": 11.0554,
      "step": 3833
    },
    {
      "epoch": 3.84,
      "grad_norm": 4359.58203125,
      "learning_rate": 8.331269349845202e-05,
      "loss": 13.8557,
      "step": 3834
    },
    {
      "epoch": 3.84,
      "grad_norm": 5199.01171875,
      "learning_rate": 8.330753353973169e-05,
      "loss": 13.1303,
      "step": 3835
    },
    {
      "epoch": 3.84,
      "grad_norm": 2590.1181640625,
      "learning_rate": 8.330237358101135e-05,
      "loss": 11.3494,
      "step": 3836
    },
    {
      "epoch": 3.84,
      "grad_norm": 14624.28125,
      "learning_rate": 8.329721362229103e-05,
      "loss": 19.9499,
      "step": 3837
    },
    {
      "epoch": 3.84,
      "grad_norm": 11978.7216796875,
      "learning_rate": 8.329205366357069e-05,
      "loss": 14.838,
      "step": 3838
    },
    {
      "epoch": 3.84,
      "grad_norm": 7829.15185546875,
      "learning_rate": 8.328689370485036e-05,
      "loss": 14.0724,
      "step": 3839
    },
    {
      "epoch": 3.84,
      "grad_norm": 31780.779296875,
      "learning_rate": 8.328173374613004e-05,
      "loss": 18.188,
      "step": 3840
    },
    {
      "epoch": 3.84,
      "grad_norm": 2101.155517578125,
      "learning_rate": 8.327657378740971e-05,
      "loss": 15.2685,
      "step": 3841
    },
    {
      "epoch": 3.85,
      "grad_norm": 7771.5546875,
      "learning_rate": 8.327141382868937e-05,
      "loss": 13.3202,
      "step": 3842
    },
    {
      "epoch": 3.85,
      "grad_norm": 1188.574951171875,
      "learning_rate": 8.326625386996905e-05,
      "loss": 11.1126,
      "step": 3843
    },
    {
      "epoch": 3.85,
      "grad_norm": 372.71295166015625,
      "learning_rate": 8.32610939112487e-05,
      "loss": 14.9082,
      "step": 3844
    },
    {
      "epoch": 3.85,
      "grad_norm": 8091.8193359375,
      "learning_rate": 8.325593395252838e-05,
      "loss": 18.3694,
      "step": 3845
    },
    {
      "epoch": 3.85,
      "grad_norm": 1732.640869140625,
      "learning_rate": 8.325077399380806e-05,
      "loss": 14.9411,
      "step": 3846
    },
    {
      "epoch": 3.85,
      "grad_norm": 17574.73046875,
      "learning_rate": 8.324561403508773e-05,
      "loss": 13.3032,
      "step": 3847
    },
    {
      "epoch": 3.85,
      "grad_norm": 3443.20849609375,
      "learning_rate": 8.324045407636739e-05,
      "loss": 12.5962,
      "step": 3848
    },
    {
      "epoch": 3.85,
      "grad_norm": 10828.4990234375,
      "learning_rate": 8.323529411764707e-05,
      "loss": 12.2704,
      "step": 3849
    },
    {
      "epoch": 3.85,
      "grad_norm": 1993.870849609375,
      "learning_rate": 8.323013415892673e-05,
      "loss": 24.6777,
      "step": 3850
    },
    {
      "epoch": 3.85,
      "grad_norm": 8643.2568359375,
      "learning_rate": 8.32249742002064e-05,
      "loss": 18.417,
      "step": 3851
    },
    {
      "epoch": 3.86,
      "grad_norm": 3296.56640625,
      "learning_rate": 8.321981424148608e-05,
      "loss": 16.462,
      "step": 3852
    },
    {
      "epoch": 3.86,
      "grad_norm": 2232.9560546875,
      "learning_rate": 8.321465428276575e-05,
      "loss": 16.5519,
      "step": 3853
    },
    {
      "epoch": 3.86,
      "grad_norm": 5838.6455078125,
      "learning_rate": 8.320949432404541e-05,
      "loss": 15.1261,
      "step": 3854
    },
    {
      "epoch": 3.86,
      "grad_norm": 4471.0048828125,
      "learning_rate": 8.320433436532507e-05,
      "loss": 11.9788,
      "step": 3855
    },
    {
      "epoch": 3.86,
      "grad_norm": 9210.18359375,
      "learning_rate": 8.319917440660475e-05,
      "loss": 13.9599,
      "step": 3856
    },
    {
      "epoch": 3.86,
      "grad_norm": 2396.218994140625,
      "learning_rate": 8.319401444788442e-05,
      "loss": 15.6137,
      "step": 3857
    },
    {
      "epoch": 3.86,
      "grad_norm": 6326.634765625,
      "learning_rate": 8.31888544891641e-05,
      "loss": 15.9575,
      "step": 3858
    },
    {
      "epoch": 3.86,
      "grad_norm": 4463.6982421875,
      "learning_rate": 8.318369453044376e-05,
      "loss": 16.0238,
      "step": 3859
    },
    {
      "epoch": 3.86,
      "grad_norm": 4519.36962890625,
      "learning_rate": 8.317853457172343e-05,
      "loss": 9.7569,
      "step": 3860
    },
    {
      "epoch": 3.86,
      "grad_norm": 3734.5166015625,
      "learning_rate": 8.317337461300309e-05,
      "loss": 13.255,
      "step": 3861
    },
    {
      "epoch": 3.87,
      "grad_norm": 5225.37109375,
      "learning_rate": 8.316821465428277e-05,
      "loss": 13.1539,
      "step": 3862
    },
    {
      "epoch": 3.87,
      "grad_norm": 5486.818359375,
      "learning_rate": 8.316305469556244e-05,
      "loss": 15.1924,
      "step": 3863
    },
    {
      "epoch": 3.87,
      "grad_norm": 5070.708984375,
      "learning_rate": 8.315789473684212e-05,
      "loss": 14.6439,
      "step": 3864
    },
    {
      "epoch": 3.87,
      "grad_norm": 6046.384765625,
      "learning_rate": 8.315273477812178e-05,
      "loss": 14.5139,
      "step": 3865
    },
    {
      "epoch": 3.87,
      "grad_norm": 1038.83154296875,
      "learning_rate": 8.314757481940145e-05,
      "loss": 10.3679,
      "step": 3866
    },
    {
      "epoch": 3.87,
      "grad_norm": 6573.75830078125,
      "learning_rate": 8.314241486068111e-05,
      "loss": 19.5419,
      "step": 3867
    },
    {
      "epoch": 3.87,
      "grad_norm": 2854.39208984375,
      "learning_rate": 8.313725490196079e-05,
      "loss": 23.4493,
      "step": 3868
    },
    {
      "epoch": 3.87,
      "grad_norm": 6159.52001953125,
      "learning_rate": 8.313209494324046e-05,
      "loss": 15.7664,
      "step": 3869
    },
    {
      "epoch": 3.87,
      "grad_norm": 6312.34619140625,
      "learning_rate": 8.312693498452014e-05,
      "loss": 20.7064,
      "step": 3870
    },
    {
      "epoch": 3.87,
      "grad_norm": 19242.888671875,
      "learning_rate": 8.31217750257998e-05,
      "loss": 16.7289,
      "step": 3871
    },
    {
      "epoch": 3.88,
      "grad_norm": 6610.95166015625,
      "learning_rate": 8.311661506707946e-05,
      "loss": 12.8013,
      "step": 3872
    },
    {
      "epoch": 3.88,
      "grad_norm": 8132.4921875,
      "learning_rate": 8.311145510835913e-05,
      "loss": 15.1234,
      "step": 3873
    },
    {
      "epoch": 3.88,
      "grad_norm": 4980.87890625,
      "learning_rate": 8.31062951496388e-05,
      "loss": 15.4643,
      "step": 3874
    },
    {
      "epoch": 3.88,
      "grad_norm": 4928.578125,
      "learning_rate": 8.310113519091848e-05,
      "loss": 18.8261,
      "step": 3875
    },
    {
      "epoch": 3.88,
      "grad_norm": 6893.90283203125,
      "learning_rate": 8.309597523219814e-05,
      "loss": 12.8248,
      "step": 3876
    },
    {
      "epoch": 3.88,
      "grad_norm": 6884.99169921875,
      "learning_rate": 8.309081527347782e-05,
      "loss": 11.653,
      "step": 3877
    },
    {
      "epoch": 3.88,
      "grad_norm": 3877.619140625,
      "learning_rate": 8.308565531475748e-05,
      "loss": 17.7696,
      "step": 3878
    },
    {
      "epoch": 3.88,
      "grad_norm": 2036.1676025390625,
      "learning_rate": 8.308049535603715e-05,
      "loss": 17.9828,
      "step": 3879
    },
    {
      "epoch": 3.88,
      "grad_norm": 2536.006591796875,
      "learning_rate": 8.307533539731683e-05,
      "loss": 13.6563,
      "step": 3880
    },
    {
      "epoch": 3.88,
      "grad_norm": 5203.38134765625,
      "learning_rate": 8.30701754385965e-05,
      "loss": 12.3844,
      "step": 3881
    },
    {
      "epoch": 3.89,
      "grad_norm": 4853.15966796875,
      "learning_rate": 8.306501547987616e-05,
      "loss": 18.6719,
      "step": 3882
    },
    {
      "epoch": 3.89,
      "grad_norm": 6029.40869140625,
      "learning_rate": 8.305985552115584e-05,
      "loss": 14.6679,
      "step": 3883
    },
    {
      "epoch": 3.89,
      "grad_norm": 2218.394775390625,
      "learning_rate": 8.30546955624355e-05,
      "loss": 13.6895,
      "step": 3884
    },
    {
      "epoch": 3.89,
      "grad_norm": 6701.96435546875,
      "learning_rate": 8.304953560371519e-05,
      "loss": 14.1948,
      "step": 3885
    },
    {
      "epoch": 3.89,
      "grad_norm": 2491.890869140625,
      "learning_rate": 8.304437564499485e-05,
      "loss": 11.5243,
      "step": 3886
    },
    {
      "epoch": 3.89,
      "grad_norm": 2044.2822265625,
      "learning_rate": 8.303921568627452e-05,
      "loss": 13.1041,
      "step": 3887
    },
    {
      "epoch": 3.89,
      "grad_norm": 1370.52294921875,
      "learning_rate": 8.303405572755418e-05,
      "loss": 10.6397,
      "step": 3888
    },
    {
      "epoch": 3.89,
      "grad_norm": 5139.35498046875,
      "learning_rate": 8.302889576883386e-05,
      "loss": 11.1631,
      "step": 3889
    },
    {
      "epoch": 3.89,
      "grad_norm": 1403.2310791015625,
      "learning_rate": 8.302373581011352e-05,
      "loss": 12.5081,
      "step": 3890
    },
    {
      "epoch": 3.89,
      "grad_norm": 2737.92529296875,
      "learning_rate": 8.301857585139319e-05,
      "loss": 13.1847,
      "step": 3891
    },
    {
      "epoch": 3.9,
      "grad_norm": 1375.4776611328125,
      "learning_rate": 8.301341589267287e-05,
      "loss": 15.1122,
      "step": 3892
    },
    {
      "epoch": 3.9,
      "grad_norm": 4859.37255859375,
      "learning_rate": 8.300825593395253e-05,
      "loss": 13.4681,
      "step": 3893
    },
    {
      "epoch": 3.9,
      "grad_norm": 766.1369018554688,
      "learning_rate": 8.30030959752322e-05,
      "loss": 14.4555,
      "step": 3894
    },
    {
      "epoch": 3.9,
      "grad_norm": 2861.973876953125,
      "learning_rate": 8.299793601651186e-05,
      "loss": 14.3478,
      "step": 3895
    },
    {
      "epoch": 3.9,
      "grad_norm": 4037.378173828125,
      "learning_rate": 8.299277605779154e-05,
      "loss": 18.9873,
      "step": 3896
    },
    {
      "epoch": 3.9,
      "grad_norm": 5321.35986328125,
      "learning_rate": 8.298761609907121e-05,
      "loss": 15.623,
      "step": 3897
    },
    {
      "epoch": 3.9,
      "grad_norm": 11040.353515625,
      "learning_rate": 8.298245614035089e-05,
      "loss": 13.1841,
      "step": 3898
    },
    {
      "epoch": 3.9,
      "grad_norm": 4162.39453125,
      "learning_rate": 8.297729618163055e-05,
      "loss": 13.6049,
      "step": 3899
    },
    {
      "epoch": 3.9,
      "grad_norm": 1851.3905029296875,
      "learning_rate": 8.297213622291022e-05,
      "loss": 10.5838,
      "step": 3900
    },
    {
      "epoch": 3.9,
      "grad_norm": 11777.3427734375,
      "learning_rate": 8.296697626418988e-05,
      "loss": 16.7961,
      "step": 3901
    },
    {
      "epoch": 3.91,
      "grad_norm": 4830.80078125,
      "learning_rate": 8.296181630546957e-05,
      "loss": 14.2826,
      "step": 3902
    },
    {
      "epoch": 3.91,
      "grad_norm": 4576.42431640625,
      "learning_rate": 8.295665634674923e-05,
      "loss": 15.9082,
      "step": 3903
    },
    {
      "epoch": 3.91,
      "grad_norm": 2160.641357421875,
      "learning_rate": 8.29514963880289e-05,
      "loss": 12.1808,
      "step": 3904
    },
    {
      "epoch": 3.91,
      "grad_norm": 1956.0911865234375,
      "learning_rate": 8.294633642930857e-05,
      "loss": 14.7918,
      "step": 3905
    },
    {
      "epoch": 3.91,
      "grad_norm": 10572.8759765625,
      "learning_rate": 8.294117647058824e-05,
      "loss": 18.482,
      "step": 3906
    },
    {
      "epoch": 3.91,
      "grad_norm": 14704.8779296875,
      "learning_rate": 8.29360165118679e-05,
      "loss": 16.0158,
      "step": 3907
    },
    {
      "epoch": 3.91,
      "grad_norm": 318.69329833984375,
      "learning_rate": 8.293085655314758e-05,
      "loss": 10.0435,
      "step": 3908
    },
    {
      "epoch": 3.91,
      "grad_norm": 3546.454345703125,
      "learning_rate": 8.292569659442725e-05,
      "loss": 17.0814,
      "step": 3909
    },
    {
      "epoch": 3.91,
      "grad_norm": 4741.84130859375,
      "learning_rate": 8.292053663570691e-05,
      "loss": 14.0219,
      "step": 3910
    },
    {
      "epoch": 3.91,
      "grad_norm": 3906.260498046875,
      "learning_rate": 8.291537667698659e-05,
      "loss": 13.6814,
      "step": 3911
    },
    {
      "epoch": 3.92,
      "grad_norm": 1013.4552612304688,
      "learning_rate": 8.291021671826625e-05,
      "loss": 17.9049,
      "step": 3912
    },
    {
      "epoch": 3.92,
      "grad_norm": 2565.543701171875,
      "learning_rate": 8.290505675954592e-05,
      "loss": 15.732,
      "step": 3913
    },
    {
      "epoch": 3.92,
      "grad_norm": 3904.03369140625,
      "learning_rate": 8.28998968008256e-05,
      "loss": 12.1876,
      "step": 3914
    },
    {
      "epoch": 3.92,
      "grad_norm": 2234.0849609375,
      "learning_rate": 8.289473684210527e-05,
      "loss": 13.4365,
      "step": 3915
    },
    {
      "epoch": 3.92,
      "grad_norm": 10228.783203125,
      "learning_rate": 8.288957688338493e-05,
      "loss": 15.8788,
      "step": 3916
    },
    {
      "epoch": 3.92,
      "grad_norm": 4186.9658203125,
      "learning_rate": 8.288441692466461e-05,
      "loss": 12.3679,
      "step": 3917
    },
    {
      "epoch": 3.92,
      "grad_norm": 2796.64892578125,
      "learning_rate": 8.287925696594427e-05,
      "loss": 10.5771,
      "step": 3918
    },
    {
      "epoch": 3.92,
      "grad_norm": 1288.81640625,
      "learning_rate": 8.287409700722396e-05,
      "loss": 12.2475,
      "step": 3919
    },
    {
      "epoch": 3.92,
      "grad_norm": 1811.1488037109375,
      "learning_rate": 8.286893704850362e-05,
      "loss": 15.4486,
      "step": 3920
    },
    {
      "epoch": 3.92,
      "grad_norm": 3328.3369140625,
      "learning_rate": 8.286377708978329e-05,
      "loss": 16.6774,
      "step": 3921
    },
    {
      "epoch": 3.93,
      "grad_norm": 9254.7763671875,
      "learning_rate": 8.285861713106295e-05,
      "loss": 13.8527,
      "step": 3922
    },
    {
      "epoch": 3.93,
      "grad_norm": 2208.43505859375,
      "learning_rate": 8.285345717234263e-05,
      "loss": 10.7129,
      "step": 3923
    },
    {
      "epoch": 3.93,
      "grad_norm": 2844.458740234375,
      "learning_rate": 8.284829721362229e-05,
      "loss": 11.6209,
      "step": 3924
    },
    {
      "epoch": 3.93,
      "grad_norm": 4324.00048828125,
      "learning_rate": 8.284313725490198e-05,
      "loss": 13.3074,
      "step": 3925
    },
    {
      "epoch": 3.93,
      "grad_norm": 10141.5009765625,
      "learning_rate": 8.283797729618164e-05,
      "loss": 14.1827,
      "step": 3926
    },
    {
      "epoch": 3.93,
      "grad_norm": 15664.9052734375,
      "learning_rate": 8.28328173374613e-05,
      "loss": 15.4757,
      "step": 3927
    },
    {
      "epoch": 3.93,
      "grad_norm": 2867.227294921875,
      "learning_rate": 8.282765737874097e-05,
      "loss": 13.8351,
      "step": 3928
    },
    {
      "epoch": 3.93,
      "grad_norm": 4479.029296875,
      "learning_rate": 8.282249742002063e-05,
      "loss": 14.7505,
      "step": 3929
    },
    {
      "epoch": 3.93,
      "grad_norm": 35640.9453125,
      "learning_rate": 8.281733746130032e-05,
      "loss": 13.3172,
      "step": 3930
    },
    {
      "epoch": 3.93,
      "grad_norm": 33532.55859375,
      "learning_rate": 8.281217750257998e-05,
      "loss": 13.5442,
      "step": 3931
    },
    {
      "epoch": 3.94,
      "grad_norm": 5545.93505859375,
      "learning_rate": 8.280701754385966e-05,
      "loss": 14.1302,
      "step": 3932
    },
    {
      "epoch": 3.94,
      "grad_norm": 11085.4931640625,
      "learning_rate": 8.280185758513932e-05,
      "loss": 19.5798,
      "step": 3933
    },
    {
      "epoch": 3.94,
      "grad_norm": 6838.91357421875,
      "learning_rate": 8.279669762641899e-05,
      "loss": 11.974,
      "step": 3934
    },
    {
      "epoch": 3.94,
      "grad_norm": 3446.139892578125,
      "learning_rate": 8.279153766769865e-05,
      "loss": 10.7711,
      "step": 3935
    },
    {
      "epoch": 3.94,
      "grad_norm": 12814.2978515625,
      "learning_rate": 8.278637770897834e-05,
      "loss": 17.1474,
      "step": 3936
    },
    {
      "epoch": 3.94,
      "grad_norm": 5899.07666015625,
      "learning_rate": 8.2781217750258e-05,
      "loss": 18.7539,
      "step": 3937
    },
    {
      "epoch": 3.94,
      "grad_norm": 2744.248291015625,
      "learning_rate": 8.277605779153768e-05,
      "loss": 17.2646,
      "step": 3938
    },
    {
      "epoch": 3.94,
      "grad_norm": 2813.0185546875,
      "learning_rate": 8.277089783281734e-05,
      "loss": 12.5744,
      "step": 3939
    },
    {
      "epoch": 3.94,
      "grad_norm": 1954.5797119140625,
      "learning_rate": 8.276573787409701e-05,
      "loss": 12.7436,
      "step": 3940
    },
    {
      "epoch": 3.94,
      "grad_norm": 11106.7001953125,
      "learning_rate": 8.276057791537667e-05,
      "loss": 15.3553,
      "step": 3941
    },
    {
      "epoch": 3.95,
      "grad_norm": 4848.73779296875,
      "learning_rate": 8.275541795665636e-05,
      "loss": 12.6591,
      "step": 3942
    },
    {
      "epoch": 3.95,
      "grad_norm": 72864.0078125,
      "learning_rate": 8.275025799793602e-05,
      "loss": 19.2341,
      "step": 3943
    },
    {
      "epoch": 3.95,
      "grad_norm": 5592.99951171875,
      "learning_rate": 8.274509803921568e-05,
      "loss": 14.1754,
      "step": 3944
    },
    {
      "epoch": 3.95,
      "grad_norm": 17914.61328125,
      "learning_rate": 8.273993808049536e-05,
      "loss": 17.8718,
      "step": 3945
    },
    {
      "epoch": 3.95,
      "grad_norm": 8891.4677734375,
      "learning_rate": 8.273477812177502e-05,
      "loss": 15.107,
      "step": 3946
    },
    {
      "epoch": 3.95,
      "grad_norm": 1978.314208984375,
      "learning_rate": 8.272961816305471e-05,
      "loss": 12.1745,
      "step": 3947
    },
    {
      "epoch": 3.95,
      "grad_norm": 19652.9921875,
      "learning_rate": 8.272445820433437e-05,
      "loss": 14.6092,
      "step": 3948
    },
    {
      "epoch": 3.95,
      "grad_norm": 8020.82958984375,
      "learning_rate": 8.271929824561404e-05,
      "loss": 13.3691,
      "step": 3949
    },
    {
      "epoch": 3.95,
      "grad_norm": 6594.453125,
      "learning_rate": 8.27141382868937e-05,
      "loss": 13.4744,
      "step": 3950
    },
    {
      "epoch": 3.95,
      "grad_norm": 5169.08642578125,
      "learning_rate": 8.270897832817338e-05,
      "loss": 14.6547,
      "step": 3951
    },
    {
      "epoch": 3.96,
      "grad_norm": 3588.496826171875,
      "learning_rate": 8.270381836945304e-05,
      "loss": 11.8426,
      "step": 3952
    },
    {
      "epoch": 3.96,
      "grad_norm": 1797.6595458984375,
      "learning_rate": 8.269865841073273e-05,
      "loss": 11.2842,
      "step": 3953
    },
    {
      "epoch": 3.96,
      "grad_norm": 11481.517578125,
      "learning_rate": 8.269349845201239e-05,
      "loss": 21.5609,
      "step": 3954
    },
    {
      "epoch": 3.96,
      "grad_norm": 2005.9654541015625,
      "learning_rate": 8.268833849329206e-05,
      "loss": 12.0433,
      "step": 3955
    },
    {
      "epoch": 3.96,
      "grad_norm": 14537.8251953125,
      "learning_rate": 8.268317853457172e-05,
      "loss": 23.1124,
      "step": 3956
    },
    {
      "epoch": 3.96,
      "grad_norm": 4318.31396484375,
      "learning_rate": 8.26780185758514e-05,
      "loss": 12.6968,
      "step": 3957
    },
    {
      "epoch": 3.96,
      "grad_norm": 1576.9935302734375,
      "learning_rate": 8.267285861713107e-05,
      "loss": 11.5459,
      "step": 3958
    },
    {
      "epoch": 3.96,
      "grad_norm": 10049.59375,
      "learning_rate": 8.266769865841075e-05,
      "loss": 13.3483,
      "step": 3959
    },
    {
      "epoch": 3.96,
      "grad_norm": 1852.8128662109375,
      "learning_rate": 8.266253869969041e-05,
      "loss": 11.5449,
      "step": 3960
    },
    {
      "epoch": 3.96,
      "grad_norm": 3176.474853515625,
      "learning_rate": 8.265737874097008e-05,
      "loss": 13.4273,
      "step": 3961
    },
    {
      "epoch": 3.97,
      "grad_norm": 3021.189208984375,
      "learning_rate": 8.265221878224974e-05,
      "loss": 28.5588,
      "step": 3962
    },
    {
      "epoch": 3.97,
      "grad_norm": 3897.348388671875,
      "learning_rate": 8.26470588235294e-05,
      "loss": 16.8345,
      "step": 3963
    },
    {
      "epoch": 3.97,
      "grad_norm": 2496.417724609375,
      "learning_rate": 8.264189886480909e-05,
      "loss": 21.0256,
      "step": 3964
    },
    {
      "epoch": 3.97,
      "grad_norm": 1163.4327392578125,
      "learning_rate": 8.263673890608875e-05,
      "loss": 11.5036,
      "step": 3965
    },
    {
      "epoch": 3.97,
      "grad_norm": 3205.344970703125,
      "learning_rate": 8.263157894736843e-05,
      "loss": 22.317,
      "step": 3966
    },
    {
      "epoch": 3.97,
      "grad_norm": 5816.61669921875,
      "learning_rate": 8.262641898864809e-05,
      "loss": 18.3188,
      "step": 3967
    },
    {
      "epoch": 3.97,
      "grad_norm": 14634.8486328125,
      "learning_rate": 8.262125902992776e-05,
      "loss": 14.9788,
      "step": 3968
    },
    {
      "epoch": 3.97,
      "grad_norm": 2484.33984375,
      "learning_rate": 8.261609907120742e-05,
      "loss": 14.5467,
      "step": 3969
    },
    {
      "epoch": 3.97,
      "grad_norm": 11684.626953125,
      "learning_rate": 8.261093911248711e-05,
      "loss": 17.7342,
      "step": 3970
    },
    {
      "epoch": 3.97,
      "grad_norm": 8608.81640625,
      "learning_rate": 8.260577915376677e-05,
      "loss": 21.1052,
      "step": 3971
    },
    {
      "epoch": 3.98,
      "grad_norm": 3655.191162109375,
      "learning_rate": 8.260061919504645e-05,
      "loss": 14.2417,
      "step": 3972
    },
    {
      "epoch": 3.98,
      "grad_norm": 6969.50390625,
      "learning_rate": 8.259545923632611e-05,
      "loss": 13.5014,
      "step": 3973
    },
    {
      "epoch": 3.98,
      "grad_norm": 4830.51025390625,
      "learning_rate": 8.259029927760578e-05,
      "loss": 21.2632,
      "step": 3974
    },
    {
      "epoch": 3.98,
      "grad_norm": 4636.92431640625,
      "learning_rate": 8.258513931888546e-05,
      "loss": 15.0004,
      "step": 3975
    },
    {
      "epoch": 3.98,
      "grad_norm": 15262.390625,
      "learning_rate": 8.257997936016513e-05,
      "loss": 19.3515,
      "step": 3976
    },
    {
      "epoch": 3.98,
      "grad_norm": 3360.063720703125,
      "learning_rate": 8.25748194014448e-05,
      "loss": 12.2256,
      "step": 3977
    },
    {
      "epoch": 3.98,
      "grad_norm": 15996.548828125,
      "learning_rate": 8.256965944272447e-05,
      "loss": 11.8546,
      "step": 3978
    },
    {
      "epoch": 3.98,
      "grad_norm": 8327.955078125,
      "learning_rate": 8.256449948400413e-05,
      "loss": 18.871,
      "step": 3979
    },
    {
      "epoch": 3.98,
      "grad_norm": 6724.36962890625,
      "learning_rate": 8.255933952528379e-05,
      "loss": 13.0185,
      "step": 3980
    },
    {
      "epoch": 3.98,
      "grad_norm": 8284.1064453125,
      "learning_rate": 8.255417956656348e-05,
      "loss": 20.56,
      "step": 3981
    },
    {
      "epoch": 3.99,
      "grad_norm": 2392.915283203125,
      "learning_rate": 8.254901960784314e-05,
      "loss": 13.5177,
      "step": 3982
    },
    {
      "epoch": 3.99,
      "grad_norm": 20864.58203125,
      "learning_rate": 8.254385964912281e-05,
      "loss": 15.4832,
      "step": 3983
    },
    {
      "epoch": 3.99,
      "grad_norm": 6851.6005859375,
      "learning_rate": 8.253869969040247e-05,
      "loss": 18.5518,
      "step": 3984
    },
    {
      "epoch": 3.99,
      "grad_norm": 2406.7939453125,
      "learning_rate": 8.253353973168215e-05,
      "loss": 29.3435,
      "step": 3985
    },
    {
      "epoch": 3.99,
      "grad_norm": 4993.7607421875,
      "learning_rate": 8.252837977296182e-05,
      "loss": 14.4273,
      "step": 3986
    },
    {
      "epoch": 3.99,
      "grad_norm": 5591.0361328125,
      "learning_rate": 8.25232198142415e-05,
      "loss": 23.953,
      "step": 3987
    },
    {
      "epoch": 3.99,
      "grad_norm": 3768.872802734375,
      "learning_rate": 8.251805985552116e-05,
      "loss": 14.2588,
      "step": 3988
    },
    {
      "epoch": 3.99,
      "grad_norm": 1665.290771484375,
      "learning_rate": 8.251289989680083e-05,
      "loss": 10.5449,
      "step": 3989
    },
    {
      "epoch": 3.99,
      "grad_norm": 7728.6171875,
      "learning_rate": 8.25077399380805e-05,
      "loss": 11.7631,
      "step": 3990
    },
    {
      "epoch": 3.99,
      "grad_norm": 4323.62353515625,
      "learning_rate": 8.250257997936017e-05,
      "loss": 19.4004,
      "step": 3991
    },
    {
      "epoch": 4.0,
      "grad_norm": 7797.3955078125,
      "learning_rate": 8.249742002063984e-05,
      "loss": 16.4884,
      "step": 3992
    },
    {
      "epoch": 4.0,
      "grad_norm": 18710.404296875,
      "learning_rate": 8.249226006191952e-05,
      "loss": 15.7895,
      "step": 3993
    },
    {
      "epoch": 4.0,
      "grad_norm": 4675.50634765625,
      "learning_rate": 8.248710010319918e-05,
      "loss": 15.9853,
      "step": 3994
    },
    {
      "epoch": 4.0,
      "grad_norm": 11745.8486328125,
      "learning_rate": 8.248194014447885e-05,
      "loss": 18.7034,
      "step": 3995
    },
    {
      "epoch": 4.0,
      "grad_norm": 23866.99609375,
      "learning_rate": 8.247678018575851e-05,
      "loss": 16.0038,
      "step": 3996
    },
    {
      "epoch": 4.0,
      "grad_norm": 868.7318115234375,
      "learning_rate": 8.247162022703818e-05,
      "loss": 11.3023,
      "step": 3997
    },
    {
      "epoch": 4.0,
      "grad_norm": 4290.43798828125,
      "learning_rate": 8.246646026831786e-05,
      "loss": 22.4748,
      "step": 3998
    },
    {
      "epoch": 4.0,
      "grad_norm": 2175.842041015625,
      "learning_rate": 8.246130030959752e-05,
      "loss": 11.4283,
      "step": 3999
    },
    {
      "epoch": 4.0,
      "grad_norm": 20812.6171875,
      "learning_rate": 8.24561403508772e-05,
      "loss": 16.1856,
      "step": 4000
    },
    {
      "epoch": 4.01,
      "grad_norm": 6474.87255859375,
      "learning_rate": 8.245098039215686e-05,
      "loss": 14.8211,
      "step": 4001
    },
    {
      "epoch": 4.01,
      "grad_norm": 2320.587890625,
      "learning_rate": 8.244582043343653e-05,
      "loss": 12.9167,
      "step": 4002
    },
    {
      "epoch": 4.01,
      "grad_norm": 3482.4892578125,
      "learning_rate": 8.244066047471621e-05,
      "loss": 15.0262,
      "step": 4003
    },
    {
      "epoch": 4.01,
      "grad_norm": 3271.002197265625,
      "learning_rate": 8.243550051599588e-05,
      "loss": 10.5997,
      "step": 4004
    },
    {
      "epoch": 4.01,
      "grad_norm": 6035.4775390625,
      "learning_rate": 8.243034055727554e-05,
      "loss": 14.0456,
      "step": 4005
    },
    {
      "epoch": 4.01,
      "grad_norm": 2105.968505859375,
      "learning_rate": 8.242518059855522e-05,
      "loss": 13.6107,
      "step": 4006
    },
    {
      "epoch": 4.01,
      "grad_norm": 25234.435546875,
      "learning_rate": 8.242002063983488e-05,
      "loss": 18.3862,
      "step": 4007
    },
    {
      "epoch": 4.01,
      "grad_norm": 2232.913330078125,
      "learning_rate": 8.241486068111455e-05,
      "loss": 11.4055,
      "step": 4008
    },
    {
      "epoch": 4.01,
      "grad_norm": 13694.556640625,
      "learning_rate": 8.240970072239423e-05,
      "loss": 17.7153,
      "step": 4009
    },
    {
      "epoch": 4.01,
      "grad_norm": 6102.9453125,
      "learning_rate": 8.24045407636739e-05,
      "loss": 15.0566,
      "step": 4010
    },
    {
      "epoch": 4.02,
      "grad_norm": 2016.8690185546875,
      "learning_rate": 8.239938080495356e-05,
      "loss": 11.4089,
      "step": 4011
    },
    {
      "epoch": 4.02,
      "grad_norm": 1219.16455078125,
      "learning_rate": 8.239422084623324e-05,
      "loss": 12.4167,
      "step": 4012
    },
    {
      "epoch": 4.02,
      "grad_norm": 10195.8642578125,
      "learning_rate": 8.23890608875129e-05,
      "loss": 21.2923,
      "step": 4013
    },
    {
      "epoch": 4.02,
      "grad_norm": 3944.061279296875,
      "learning_rate": 8.238390092879257e-05,
      "loss": 17.863,
      "step": 4014
    },
    {
      "epoch": 4.02,
      "grad_norm": 6876.5673828125,
      "learning_rate": 8.237874097007225e-05,
      "loss": 12.1327,
      "step": 4015
    },
    {
      "epoch": 4.02,
      "grad_norm": 4750.978515625,
      "learning_rate": 8.237358101135191e-05,
      "loss": 11.1933,
      "step": 4016
    },
    {
      "epoch": 4.02,
      "grad_norm": 956.6885986328125,
      "learning_rate": 8.236842105263158e-05,
      "loss": 12.2132,
      "step": 4017
    },
    {
      "epoch": 4.02,
      "grad_norm": 1271.910400390625,
      "learning_rate": 8.236326109391125e-05,
      "loss": 14.5911,
      "step": 4018
    },
    {
      "epoch": 4.02,
      "grad_norm": 766.4451293945312,
      "learning_rate": 8.235810113519092e-05,
      "loss": 10.6521,
      "step": 4019
    },
    {
      "epoch": 4.02,
      "grad_norm": 1460.4554443359375,
      "learning_rate": 8.23529411764706e-05,
      "loss": 13.0281,
      "step": 4020
    },
    {
      "epoch": 4.03,
      "grad_norm": 3610.7705078125,
      "learning_rate": 8.234778121775027e-05,
      "loss": 19.5255,
      "step": 4021
    },
    {
      "epoch": 4.03,
      "grad_norm": 4098.5185546875,
      "learning_rate": 8.234262125902993e-05,
      "loss": 14.391,
      "step": 4022
    },
    {
      "epoch": 4.03,
      "grad_norm": 2817.89697265625,
      "learning_rate": 8.23374613003096e-05,
      "loss": 15.6879,
      "step": 4023
    },
    {
      "epoch": 4.03,
      "grad_norm": 3730.916015625,
      "learning_rate": 8.233230134158927e-05,
      "loss": 16.5816,
      "step": 4024
    },
    {
      "epoch": 4.03,
      "grad_norm": 2681.785888671875,
      "learning_rate": 8.232714138286894e-05,
      "loss": 14.5305,
      "step": 4025
    },
    {
      "epoch": 4.03,
      "grad_norm": 4013.856689453125,
      "learning_rate": 8.232198142414861e-05,
      "loss": 14.5178,
      "step": 4026
    },
    {
      "epoch": 4.03,
      "grad_norm": 2717.53564453125,
      "learning_rate": 8.231682146542829e-05,
      "loss": 12.8052,
      "step": 4027
    },
    {
      "epoch": 4.03,
      "grad_norm": 8885.5654296875,
      "learning_rate": 8.231166150670795e-05,
      "loss": 13.4642,
      "step": 4028
    },
    {
      "epoch": 4.03,
      "grad_norm": 1577.1453857421875,
      "learning_rate": 8.230650154798762e-05,
      "loss": 12.7463,
      "step": 4029
    },
    {
      "epoch": 4.03,
      "grad_norm": 3678.536865234375,
      "learning_rate": 8.230134158926729e-05,
      "loss": 11.4531,
      "step": 4030
    },
    {
      "epoch": 4.04,
      "grad_norm": 6563.19384765625,
      "learning_rate": 8.229618163054696e-05,
      "loss": 11.1711,
      "step": 4031
    },
    {
      "epoch": 4.04,
      "grad_norm": 823.4652099609375,
      "learning_rate": 8.229102167182663e-05,
      "loss": 19.4838,
      "step": 4032
    },
    {
      "epoch": 4.04,
      "grad_norm": 3139.96826171875,
      "learning_rate": 8.22858617131063e-05,
      "loss": 23.465,
      "step": 4033
    },
    {
      "epoch": 4.04,
      "grad_norm": 4346.388671875,
      "learning_rate": 8.228070175438597e-05,
      "loss": 15.5583,
      "step": 4034
    },
    {
      "epoch": 4.04,
      "grad_norm": 23203.490234375,
      "learning_rate": 8.227554179566563e-05,
      "loss": 12.3314,
      "step": 4035
    },
    {
      "epoch": 4.04,
      "grad_norm": 36125.62890625,
      "learning_rate": 8.22703818369453e-05,
      "loss": 12.1515,
      "step": 4036
    },
    {
      "epoch": 4.04,
      "grad_norm": 4191.580078125,
      "learning_rate": 8.226522187822498e-05,
      "loss": 16.2238,
      "step": 4037
    },
    {
      "epoch": 4.04,
      "grad_norm": 16800.55078125,
      "learning_rate": 8.226006191950465e-05,
      "loss": 15.2993,
      "step": 4038
    },
    {
      "epoch": 4.04,
      "grad_norm": 3840.685791015625,
      "learning_rate": 8.225490196078432e-05,
      "loss": 14.7987,
      "step": 4039
    },
    {
      "epoch": 4.04,
      "grad_norm": 2936.6767578125,
      "learning_rate": 8.224974200206399e-05,
      "loss": 10.747,
      "step": 4040
    },
    {
      "epoch": 4.05,
      "grad_norm": 4454.31787109375,
      "learning_rate": 8.224458204334365e-05,
      "loss": 12.1059,
      "step": 4041
    },
    {
      "epoch": 4.05,
      "grad_norm": 4872.62109375,
      "learning_rate": 8.223942208462333e-05,
      "loss": 13.7349,
      "step": 4042
    },
    {
      "epoch": 4.05,
      "grad_norm": 8354.5791015625,
      "learning_rate": 8.2234262125903e-05,
      "loss": 14.3186,
      "step": 4043
    },
    {
      "epoch": 4.05,
      "grad_norm": 3632.600830078125,
      "learning_rate": 8.222910216718267e-05,
      "loss": 11.2122,
      "step": 4044
    },
    {
      "epoch": 4.05,
      "grad_norm": 22046.794921875,
      "learning_rate": 8.222394220846234e-05,
      "loss": 17.417,
      "step": 4045
    },
    {
      "epoch": 4.05,
      "grad_norm": 18101.244140625,
      "learning_rate": 8.221878224974201e-05,
      "loss": 17.1072,
      "step": 4046
    },
    {
      "epoch": 4.05,
      "grad_norm": 5847.591796875,
      "learning_rate": 8.221362229102167e-05,
      "loss": 14.2448,
      "step": 4047
    },
    {
      "epoch": 4.05,
      "grad_norm": 13273.7138671875,
      "learning_rate": 8.220846233230135e-05,
      "loss": 15.1783,
      "step": 4048
    },
    {
      "epoch": 4.05,
      "grad_norm": 13522.7841796875,
      "learning_rate": 8.220330237358102e-05,
      "loss": 11.2764,
      "step": 4049
    },
    {
      "epoch": 4.05,
      "grad_norm": 2608.29052734375,
      "learning_rate": 8.21981424148607e-05,
      "loss": 14.3,
      "step": 4050
    },
    {
      "epoch": 4.06,
      "grad_norm": 1586.1422119140625,
      "learning_rate": 8.219298245614036e-05,
      "loss": 14.1366,
      "step": 4051
    },
    {
      "epoch": 4.06,
      "grad_norm": 1199.492919921875,
      "learning_rate": 8.218782249742002e-05,
      "loss": 10.9333,
      "step": 4052
    },
    {
      "epoch": 4.06,
      "grad_norm": 7937.03955078125,
      "learning_rate": 8.218266253869969e-05,
      "loss": 12.8256,
      "step": 4053
    },
    {
      "epoch": 4.06,
      "grad_norm": 3862.953857421875,
      "learning_rate": 8.217750257997937e-05,
      "loss": 13.4245,
      "step": 4054
    },
    {
      "epoch": 4.06,
      "grad_norm": 4992.947265625,
      "learning_rate": 8.217234262125904e-05,
      "loss": 12.4224,
      "step": 4055
    },
    {
      "epoch": 4.06,
      "grad_norm": 23304.8984375,
      "learning_rate": 8.21671826625387e-05,
      "loss": 17.1898,
      "step": 4056
    },
    {
      "epoch": 4.06,
      "grad_norm": 6997.41943359375,
      "learning_rate": 8.216202270381838e-05,
      "loss": 10.4594,
      "step": 4057
    },
    {
      "epoch": 4.06,
      "grad_norm": 5709.86669921875,
      "learning_rate": 8.215686274509804e-05,
      "loss": 13.5551,
      "step": 4058
    },
    {
      "epoch": 4.06,
      "grad_norm": 7470.216796875,
      "learning_rate": 8.215170278637771e-05,
      "loss": 24.854,
      "step": 4059
    },
    {
      "epoch": 4.06,
      "grad_norm": 3136.513916015625,
      "learning_rate": 8.214654282765739e-05,
      "loss": 12.0938,
      "step": 4060
    },
    {
      "epoch": 4.07,
      "grad_norm": 2308.7802734375,
      "learning_rate": 8.214138286893706e-05,
      "loss": 26.4258,
      "step": 4061
    },
    {
      "epoch": 4.07,
      "grad_norm": 4526.89208984375,
      "learning_rate": 8.213622291021672e-05,
      "loss": 15.6823,
      "step": 4062
    },
    {
      "epoch": 4.07,
      "grad_norm": 8384.1806640625,
      "learning_rate": 8.21310629514964e-05,
      "loss": 17.5123,
      "step": 4063
    },
    {
      "epoch": 4.07,
      "grad_norm": 3717.686767578125,
      "learning_rate": 8.212590299277606e-05,
      "loss": 17.7976,
      "step": 4064
    },
    {
      "epoch": 4.07,
      "grad_norm": 1408.07763671875,
      "learning_rate": 8.212074303405573e-05,
      "loss": 10.2341,
      "step": 4065
    },
    {
      "epoch": 4.07,
      "grad_norm": 66019.0859375,
      "learning_rate": 8.21155830753354e-05,
      "loss": 15.8975,
      "step": 4066
    },
    {
      "epoch": 4.07,
      "grad_norm": 3902.326904296875,
      "learning_rate": 8.211042311661508e-05,
      "loss": 11.7755,
      "step": 4067
    },
    {
      "epoch": 4.07,
      "grad_norm": 10992.6142578125,
      "learning_rate": 8.210526315789474e-05,
      "loss": 13.9,
      "step": 4068
    },
    {
      "epoch": 4.07,
      "grad_norm": 5727.5,
      "learning_rate": 8.21001031991744e-05,
      "loss": 18.0729,
      "step": 4069
    },
    {
      "epoch": 4.07,
      "grad_norm": 1686.587158203125,
      "learning_rate": 8.209494324045408e-05,
      "loss": 11.2462,
      "step": 4070
    },
    {
      "epoch": 4.08,
      "grad_norm": 2597.94091796875,
      "learning_rate": 8.208978328173375e-05,
      "loss": 13.3378,
      "step": 4071
    },
    {
      "epoch": 4.08,
      "grad_norm": 4291.1513671875,
      "learning_rate": 8.208462332301343e-05,
      "loss": 15.395,
      "step": 4072
    },
    {
      "epoch": 4.08,
      "grad_norm": 4712.125,
      "learning_rate": 8.207946336429309e-05,
      "loss": 17.4189,
      "step": 4073
    },
    {
      "epoch": 4.08,
      "grad_norm": 20194.267578125,
      "learning_rate": 8.207430340557276e-05,
      "loss": 15.4418,
      "step": 4074
    },
    {
      "epoch": 4.08,
      "grad_norm": 10057.734375,
      "learning_rate": 8.206914344685242e-05,
      "loss": 19.8018,
      "step": 4075
    },
    {
      "epoch": 4.08,
      "grad_norm": 2627.177490234375,
      "learning_rate": 8.20639834881321e-05,
      "loss": 11.5906,
      "step": 4076
    },
    {
      "epoch": 4.08,
      "grad_norm": 3006.36181640625,
      "learning_rate": 8.205882352941177e-05,
      "loss": 13.8777,
      "step": 4077
    },
    {
      "epoch": 4.08,
      "grad_norm": 38908.01171875,
      "learning_rate": 8.205366357069145e-05,
      "loss": 18.7866,
      "step": 4078
    },
    {
      "epoch": 4.08,
      "grad_norm": 1429.3245849609375,
      "learning_rate": 8.20485036119711e-05,
      "loss": 11.6175,
      "step": 4079
    },
    {
      "epoch": 4.08,
      "grad_norm": 2711.791259765625,
      "learning_rate": 8.204334365325078e-05,
      "loss": 22.8439,
      "step": 4080
    },
    {
      "epoch": 4.09,
      "grad_norm": 15353.4638671875,
      "learning_rate": 8.203818369453044e-05,
      "loss": 14.6598,
      "step": 4081
    },
    {
      "epoch": 4.09,
      "grad_norm": 3923.91455078125,
      "learning_rate": 8.203302373581012e-05,
      "loss": 12.3851,
      "step": 4082
    },
    {
      "epoch": 4.09,
      "grad_norm": 6340.1806640625,
      "learning_rate": 8.202786377708979e-05,
      "loss": 14.1904,
      "step": 4083
    },
    {
      "epoch": 4.09,
      "grad_norm": 2770.263916015625,
      "learning_rate": 8.202270381836947e-05,
      "loss": 12.6448,
      "step": 4084
    },
    {
      "epoch": 4.09,
      "grad_norm": 6994.92138671875,
      "learning_rate": 8.201754385964913e-05,
      "loss": 12.5059,
      "step": 4085
    },
    {
      "epoch": 4.09,
      "grad_norm": 16149.880859375,
      "learning_rate": 8.20123839009288e-05,
      "loss": 16.5321,
      "step": 4086
    },
    {
      "epoch": 4.09,
      "grad_norm": 6855.62646484375,
      "learning_rate": 8.200722394220846e-05,
      "loss": 13.6663,
      "step": 4087
    },
    {
      "epoch": 4.09,
      "grad_norm": 2342.51025390625,
      "learning_rate": 8.200206398348814e-05,
      "loss": 13.0081,
      "step": 4088
    },
    {
      "epoch": 4.09,
      "grad_norm": 8203.287109375,
      "learning_rate": 8.199690402476781e-05,
      "loss": 13.929,
      "step": 4089
    },
    {
      "epoch": 4.09,
      "grad_norm": 5110.87255859375,
      "learning_rate": 8.199174406604747e-05,
      "loss": 21.3641,
      "step": 4090
    },
    {
      "epoch": 4.1,
      "grad_norm": 4584.78564453125,
      "learning_rate": 8.198658410732715e-05,
      "loss": 18.0704,
      "step": 4091
    },
    {
      "epoch": 4.1,
      "grad_norm": 8146.419921875,
      "learning_rate": 8.198142414860681e-05,
      "loss": 16.0047,
      "step": 4092
    },
    {
      "epoch": 4.1,
      "grad_norm": 3539.517822265625,
      "learning_rate": 8.197626418988648e-05,
      "loss": 14.0694,
      "step": 4093
    },
    {
      "epoch": 4.1,
      "grad_norm": 18267.099609375,
      "learning_rate": 8.197110423116616e-05,
      "loss": 15.9039,
      "step": 4094
    },
    {
      "epoch": 4.1,
      "grad_norm": 3686.239013671875,
      "learning_rate": 8.196594427244583e-05,
      "loss": 18.2113,
      "step": 4095
    },
    {
      "epoch": 4.1,
      "grad_norm": 6186.38720703125,
      "learning_rate": 8.196078431372549e-05,
      "loss": 12.6581,
      "step": 4096
    },
    {
      "epoch": 4.1,
      "grad_norm": 3469.032958984375,
      "learning_rate": 8.195562435500517e-05,
      "loss": 13.1718,
      "step": 4097
    },
    {
      "epoch": 4.1,
      "grad_norm": 20197.0703125,
      "learning_rate": 8.195046439628483e-05,
      "loss": 24.8878,
      "step": 4098
    },
    {
      "epoch": 4.1,
      "grad_norm": 10619.2685546875,
      "learning_rate": 8.19453044375645e-05,
      "loss": 16.0584,
      "step": 4099
    },
    {
      "epoch": 4.1,
      "grad_norm": 4624.31396484375,
      "learning_rate": 8.194014447884418e-05,
      "loss": 13.1406,
      "step": 4100
    },
    {
      "epoch": 4.11,
      "grad_norm": 9818.8056640625,
      "learning_rate": 8.193498452012385e-05,
      "loss": 12.1009,
      "step": 4101
    },
    {
      "epoch": 4.11,
      "grad_norm": 2246.37890625,
      "learning_rate": 8.192982456140351e-05,
      "loss": 12.3441,
      "step": 4102
    },
    {
      "epoch": 4.11,
      "grad_norm": 5510.82177734375,
      "learning_rate": 8.192466460268319e-05,
      "loss": 14.6228,
      "step": 4103
    },
    {
      "epoch": 4.11,
      "grad_norm": 32605.244140625,
      "learning_rate": 8.191950464396285e-05,
      "loss": 13.3854,
      "step": 4104
    },
    {
      "epoch": 4.11,
      "grad_norm": 13614.1748046875,
      "learning_rate": 8.191434468524252e-05,
      "loss": 16.1023,
      "step": 4105
    },
    {
      "epoch": 4.11,
      "grad_norm": 14112.2744140625,
      "learning_rate": 8.19091847265222e-05,
      "loss": 18.1176,
      "step": 4106
    },
    {
      "epoch": 4.11,
      "grad_norm": 5056.23974609375,
      "learning_rate": 8.190402476780186e-05,
      "loss": 13.5165,
      "step": 4107
    },
    {
      "epoch": 4.11,
      "grad_norm": 9048.51171875,
      "learning_rate": 8.189886480908153e-05,
      "loss": 13.9542,
      "step": 4108
    },
    {
      "epoch": 4.11,
      "grad_norm": 1312.6903076171875,
      "learning_rate": 8.189370485036119e-05,
      "loss": 15.4191,
      "step": 4109
    },
    {
      "epoch": 4.11,
      "grad_norm": 4362.7685546875,
      "learning_rate": 8.188854489164087e-05,
      "loss": 16.5396,
      "step": 4110
    },
    {
      "epoch": 4.12,
      "grad_norm": 2666.28271484375,
      "learning_rate": 8.188338493292054e-05,
      "loss": 9.6913,
      "step": 4111
    },
    {
      "epoch": 4.12,
      "grad_norm": 7341.2841796875,
      "learning_rate": 8.187822497420022e-05,
      "loss": 21.7216,
      "step": 4112
    },
    {
      "epoch": 4.12,
      "grad_norm": 8339.40625,
      "learning_rate": 8.187306501547988e-05,
      "loss": 15.1084,
      "step": 4113
    },
    {
      "epoch": 4.12,
      "grad_norm": 2275.728759765625,
      "learning_rate": 8.186790505675955e-05,
      "loss": 13.3898,
      "step": 4114
    },
    {
      "epoch": 4.12,
      "grad_norm": 2841.76953125,
      "learning_rate": 8.186274509803921e-05,
      "loss": 11.9553,
      "step": 4115
    },
    {
      "epoch": 4.12,
      "grad_norm": 21082.892578125,
      "learning_rate": 8.185758513931889e-05,
      "loss": 15.9315,
      "step": 4116
    },
    {
      "epoch": 4.12,
      "grad_norm": 1681.5325927734375,
      "learning_rate": 8.185242518059856e-05,
      "loss": 12.0053,
      "step": 4117
    },
    {
      "epoch": 4.12,
      "grad_norm": 6787.16162109375,
      "learning_rate": 8.184726522187824e-05,
      "loss": 13.0425,
      "step": 4118
    },
    {
      "epoch": 4.12,
      "grad_norm": 1961.4761962890625,
      "learning_rate": 8.18421052631579e-05,
      "loss": 12.5391,
      "step": 4119
    },
    {
      "epoch": 4.12,
      "grad_norm": 3013.2275390625,
      "learning_rate": 8.183694530443757e-05,
      "loss": 11.4578,
      "step": 4120
    },
    {
      "epoch": 4.13,
      "grad_norm": 2541.659912109375,
      "learning_rate": 8.183178534571723e-05,
      "loss": 9.3308,
      "step": 4121
    },
    {
      "epoch": 4.13,
      "grad_norm": 1361.0538330078125,
      "learning_rate": 8.182662538699691e-05,
      "loss": 13.1235,
      "step": 4122
    },
    {
      "epoch": 4.13,
      "grad_norm": 17500.673828125,
      "learning_rate": 8.182146542827658e-05,
      "loss": 11.9877,
      "step": 4123
    },
    {
      "epoch": 4.13,
      "grad_norm": 2622.21484375,
      "learning_rate": 8.181630546955624e-05,
      "loss": 11.2105,
      "step": 4124
    },
    {
      "epoch": 4.13,
      "grad_norm": 1464.2611083984375,
      "learning_rate": 8.181114551083592e-05,
      "loss": 12.6475,
      "step": 4125
    },
    {
      "epoch": 4.13,
      "grad_norm": 1088.678466796875,
      "learning_rate": 8.180598555211558e-05,
      "loss": 9.8481,
      "step": 4126
    },
    {
      "epoch": 4.13,
      "grad_norm": 1702.1097412109375,
      "learning_rate": 8.180082559339525e-05,
      "loss": 12.7915,
      "step": 4127
    },
    {
      "epoch": 4.13,
      "grad_norm": 30021.185546875,
      "learning_rate": 8.179566563467493e-05,
      "loss": 13.9383,
      "step": 4128
    },
    {
      "epoch": 4.13,
      "grad_norm": 1437.447265625,
      "learning_rate": 8.17905056759546e-05,
      "loss": 11.73,
      "step": 4129
    },
    {
      "epoch": 4.13,
      "grad_norm": 7855.5224609375,
      "learning_rate": 8.178534571723426e-05,
      "loss": 16.3962,
      "step": 4130
    },
    {
      "epoch": 4.14,
      "grad_norm": 1443.76806640625,
      "learning_rate": 8.178018575851394e-05,
      "loss": 16.9777,
      "step": 4131
    },
    {
      "epoch": 4.14,
      "grad_norm": 2061.981201171875,
      "learning_rate": 8.17750257997936e-05,
      "loss": 11.0656,
      "step": 4132
    },
    {
      "epoch": 4.14,
      "grad_norm": 1374.52294921875,
      "learning_rate": 8.176986584107327e-05,
      "loss": 11.1121,
      "step": 4133
    },
    {
      "epoch": 4.14,
      "grad_norm": 2233.21337890625,
      "learning_rate": 8.176470588235295e-05,
      "loss": 11.4013,
      "step": 4134
    },
    {
      "epoch": 4.14,
      "grad_norm": 33515.26171875,
      "learning_rate": 8.175954592363262e-05,
      "loss": 15.7576,
      "step": 4135
    },
    {
      "epoch": 4.14,
      "grad_norm": 1666.74853515625,
      "learning_rate": 8.175438596491228e-05,
      "loss": 12.0716,
      "step": 4136
    },
    {
      "epoch": 4.14,
      "grad_norm": 2214.926513671875,
      "learning_rate": 8.174922600619196e-05,
      "loss": 11.1708,
      "step": 4137
    },
    {
      "epoch": 4.14,
      "grad_norm": 3469.6533203125,
      "learning_rate": 8.174406604747162e-05,
      "loss": 17.9264,
      "step": 4138
    },
    {
      "epoch": 4.14,
      "grad_norm": 1058.388916015625,
      "learning_rate": 8.173890608875129e-05,
      "loss": 12.6605,
      "step": 4139
    },
    {
      "epoch": 4.14,
      "grad_norm": 2788.154541015625,
      "learning_rate": 8.173374613003097e-05,
      "loss": 22.9373,
      "step": 4140
    },
    {
      "epoch": 4.15,
      "grad_norm": 2850.240966796875,
      "learning_rate": 8.172858617131063e-05,
      "loss": 14.806,
      "step": 4141
    },
    {
      "epoch": 4.15,
      "grad_norm": 6833.33935546875,
      "learning_rate": 8.17234262125903e-05,
      "loss": 14.3886,
      "step": 4142
    },
    {
      "epoch": 4.15,
      "grad_norm": 2948.281982421875,
      "learning_rate": 8.171826625386996e-05,
      "loss": 14.9167,
      "step": 4143
    },
    {
      "epoch": 4.15,
      "grad_norm": 1125.85205078125,
      "learning_rate": 8.171310629514964e-05,
      "loss": 11.8957,
      "step": 4144
    },
    {
      "epoch": 4.15,
      "grad_norm": 6582.408203125,
      "learning_rate": 8.170794633642931e-05,
      "loss": 18.9115,
      "step": 4145
    },
    {
      "epoch": 4.15,
      "grad_norm": 10401.4912109375,
      "learning_rate": 8.170278637770899e-05,
      "loss": 13.1717,
      "step": 4146
    },
    {
      "epoch": 4.15,
      "grad_norm": 913.615234375,
      "learning_rate": 8.169762641898865e-05,
      "loss": 11.2501,
      "step": 4147
    },
    {
      "epoch": 4.15,
      "grad_norm": 7383.36865234375,
      "learning_rate": 8.169246646026832e-05,
      "loss": 13.8865,
      "step": 4148
    },
    {
      "epoch": 4.15,
      "grad_norm": 1242.3875732421875,
      "learning_rate": 8.168730650154798e-05,
      "loss": 14.2452,
      "step": 4149
    },
    {
      "epoch": 4.15,
      "grad_norm": 13444.41015625,
      "learning_rate": 8.168214654282766e-05,
      "loss": 11.3111,
      "step": 4150
    },
    {
      "epoch": 4.16,
      "grad_norm": 3208.84326171875,
      "learning_rate": 8.167698658410733e-05,
      "loss": 16.7134,
      "step": 4151
    },
    {
      "epoch": 4.16,
      "grad_norm": 2765.014892578125,
      "learning_rate": 8.167182662538701e-05,
      "loss": 11.2293,
      "step": 4152
    },
    {
      "epoch": 4.16,
      "grad_norm": 5001.21044921875,
      "learning_rate": 8.166666666666667e-05,
      "loss": 11.9106,
      "step": 4153
    },
    {
      "epoch": 4.16,
      "grad_norm": 10966.24609375,
      "learning_rate": 8.166150670794634e-05,
      "loss": 14.2782,
      "step": 4154
    },
    {
      "epoch": 4.16,
      "grad_norm": 1563.667724609375,
      "learning_rate": 8.1656346749226e-05,
      "loss": 15.9308,
      "step": 4155
    },
    {
      "epoch": 4.16,
      "grad_norm": 24006.8359375,
      "learning_rate": 8.165118679050568e-05,
      "loss": 16.0048,
      "step": 4156
    },
    {
      "epoch": 4.16,
      "grad_norm": 969.9866943359375,
      "learning_rate": 8.164602683178535e-05,
      "loss": 10.725,
      "step": 4157
    },
    {
      "epoch": 4.16,
      "grad_norm": 3426.63134765625,
      "learning_rate": 8.164086687306501e-05,
      "loss": 13.6132,
      "step": 4158
    },
    {
      "epoch": 4.16,
      "grad_norm": 4042.3984375,
      "learning_rate": 8.163570691434469e-05,
      "loss": 16.4313,
      "step": 4159
    },
    {
      "epoch": 4.16,
      "grad_norm": 3492.341796875,
      "learning_rate": 8.163054695562435e-05,
      "loss": 11.3122,
      "step": 4160
    },
    {
      "epoch": 4.17,
      "grad_norm": 9951.408203125,
      "learning_rate": 8.162538699690402e-05,
      "loss": 16.4836,
      "step": 4161
    },
    {
      "epoch": 4.17,
      "grad_norm": 18946.98828125,
      "learning_rate": 8.16202270381837e-05,
      "loss": 16.2471,
      "step": 4162
    },
    {
      "epoch": 4.17,
      "grad_norm": 5198.09716796875,
      "learning_rate": 8.161506707946337e-05,
      "loss": 15.7644,
      "step": 4163
    },
    {
      "epoch": 4.17,
      "grad_norm": 1473.0675048828125,
      "learning_rate": 8.160990712074303e-05,
      "loss": 13.1663,
      "step": 4164
    },
    {
      "epoch": 4.17,
      "grad_norm": 1274.5091552734375,
      "learning_rate": 8.160474716202271e-05,
      "loss": 14.4469,
      "step": 4165
    },
    {
      "epoch": 4.17,
      "grad_norm": 4628.271484375,
      "learning_rate": 8.159958720330237e-05,
      "loss": 13.74,
      "step": 4166
    },
    {
      "epoch": 4.17,
      "grad_norm": 3118.68701171875,
      "learning_rate": 8.159442724458204e-05,
      "loss": 13.7864,
      "step": 4167
    },
    {
      "epoch": 4.17,
      "grad_norm": 10610.37109375,
      "learning_rate": 8.158926728586172e-05,
      "loss": 13.4868,
      "step": 4168
    },
    {
      "epoch": 4.17,
      "grad_norm": 1231.2509765625,
      "learning_rate": 8.158410732714139e-05,
      "loss": 12.0537,
      "step": 4169
    },
    {
      "epoch": 4.17,
      "grad_norm": 3597.664794921875,
      "learning_rate": 8.157894736842105e-05,
      "loss": 11.466,
      "step": 4170
    },
    {
      "epoch": 4.18,
      "grad_norm": 2037.6968994140625,
      "learning_rate": 8.157378740970073e-05,
      "loss": 12.8352,
      "step": 4171
    },
    {
      "epoch": 4.18,
      "grad_norm": 8681.2255859375,
      "learning_rate": 8.156862745098039e-05,
      "loss": 14.1468,
      "step": 4172
    },
    {
      "epoch": 4.18,
      "grad_norm": 2425.98583984375,
      "learning_rate": 8.156346749226006e-05,
      "loss": 15.4387,
      "step": 4173
    },
    {
      "epoch": 4.18,
      "grad_norm": 1533.9482421875,
      "learning_rate": 8.155830753353974e-05,
      "loss": 10.8921,
      "step": 4174
    },
    {
      "epoch": 4.18,
      "grad_norm": 3511.30078125,
      "learning_rate": 8.155314757481941e-05,
      "loss": 14.4582,
      "step": 4175
    },
    {
      "epoch": 4.18,
      "grad_norm": 5402.37060546875,
      "learning_rate": 8.154798761609907e-05,
      "loss": 18.0622,
      "step": 4176
    },
    {
      "epoch": 4.18,
      "grad_norm": 6004.15185546875,
      "learning_rate": 8.154282765737874e-05,
      "loss": 14.2377,
      "step": 4177
    },
    {
      "epoch": 4.18,
      "grad_norm": 1942.208251953125,
      "learning_rate": 8.153766769865841e-05,
      "loss": 13.8159,
      "step": 4178
    },
    {
      "epoch": 4.18,
      "grad_norm": 2199.05908203125,
      "learning_rate": 8.153250773993808e-05,
      "loss": 12.4024,
      "step": 4179
    },
    {
      "epoch": 4.18,
      "grad_norm": 5576.4140625,
      "learning_rate": 8.152734778121776e-05,
      "loss": 11.2475,
      "step": 4180
    },
    {
      "epoch": 4.19,
      "grad_norm": 2608.356201171875,
      "learning_rate": 8.152218782249742e-05,
      "loss": 30.1927,
      "step": 4181
    },
    {
      "epoch": 4.19,
      "grad_norm": 8357.6708984375,
      "learning_rate": 8.15170278637771e-05,
      "loss": 14.011,
      "step": 4182
    },
    {
      "epoch": 4.19,
      "grad_norm": 102068.484375,
      "learning_rate": 8.151186790505676e-05,
      "loss": 16.5742,
      "step": 4183
    },
    {
      "epoch": 4.19,
      "grad_norm": 310.806640625,
      "learning_rate": 8.150670794633643e-05,
      "loss": 10.4258,
      "step": 4184
    },
    {
      "epoch": 4.19,
      "grad_norm": 16625.138671875,
      "learning_rate": 8.15015479876161e-05,
      "loss": 16.9033,
      "step": 4185
    },
    {
      "epoch": 4.19,
      "grad_norm": 2170.633544921875,
      "learning_rate": 8.149638802889578e-05,
      "loss": 16.1638,
      "step": 4186
    },
    {
      "epoch": 4.19,
      "grad_norm": 2562.180419921875,
      "learning_rate": 8.149122807017544e-05,
      "loss": 11.7196,
      "step": 4187
    },
    {
      "epoch": 4.19,
      "grad_norm": 3428.077392578125,
      "learning_rate": 8.148606811145511e-05,
      "loss": 14.4642,
      "step": 4188
    },
    {
      "epoch": 4.19,
      "grad_norm": 2200.83740234375,
      "learning_rate": 8.148090815273478e-05,
      "loss": 20.5903,
      "step": 4189
    },
    {
      "epoch": 4.19,
      "grad_norm": 12058.576171875,
      "learning_rate": 8.147574819401445e-05,
      "loss": 17.721,
      "step": 4190
    },
    {
      "epoch": 4.2,
      "grad_norm": 2264.149658203125,
      "learning_rate": 8.147058823529412e-05,
      "loss": 18.6246,
      "step": 4191
    },
    {
      "epoch": 4.2,
      "grad_norm": 6863.75048828125,
      "learning_rate": 8.14654282765738e-05,
      "loss": 14.6021,
      "step": 4192
    },
    {
      "epoch": 4.2,
      "grad_norm": 4712.83740234375,
      "learning_rate": 8.146026831785346e-05,
      "loss": 16.5942,
      "step": 4193
    },
    {
      "epoch": 4.2,
      "grad_norm": 3303.92724609375,
      "learning_rate": 8.145510835913312e-05,
      "loss": 15.4919,
      "step": 4194
    },
    {
      "epoch": 4.2,
      "grad_norm": 3322.88818359375,
      "learning_rate": 8.14499484004128e-05,
      "loss": 13.6888,
      "step": 4195
    },
    {
      "epoch": 4.2,
      "grad_norm": 2001.7835693359375,
      "learning_rate": 8.144478844169247e-05,
      "loss": 11.7872,
      "step": 4196
    },
    {
      "epoch": 4.2,
      "grad_norm": 1268.286376953125,
      "learning_rate": 8.143962848297214e-05,
      "loss": 10.5795,
      "step": 4197
    },
    {
      "epoch": 4.2,
      "grad_norm": 16255.69140625,
      "learning_rate": 8.14344685242518e-05,
      "loss": 25.1816,
      "step": 4198
    },
    {
      "epoch": 4.2,
      "grad_norm": 5988.6005859375,
      "learning_rate": 8.142930856553148e-05,
      "loss": 10.2372,
      "step": 4199
    },
    {
      "epoch": 4.2,
      "grad_norm": 2311.84375,
      "learning_rate": 8.142414860681114e-05,
      "loss": 12.6246,
      "step": 4200
    },
    {
      "epoch": 4.21,
      "grad_norm": 3293.86865234375,
      "learning_rate": 8.141898864809082e-05,
      "loss": 13.1161,
      "step": 4201
    },
    {
      "epoch": 4.21,
      "grad_norm": 14108.8095703125,
      "learning_rate": 8.141382868937049e-05,
      "loss": 20.6189,
      "step": 4202
    },
    {
      "epoch": 4.21,
      "grad_norm": 4384.28857421875,
      "learning_rate": 8.140866873065016e-05,
      "loss": 19.5031,
      "step": 4203
    },
    {
      "epoch": 4.21,
      "grad_norm": 5474.53759765625,
      "learning_rate": 8.140350877192983e-05,
      "loss": 21.0127,
      "step": 4204
    },
    {
      "epoch": 4.21,
      "grad_norm": 6287.16455078125,
      "learning_rate": 8.13983488132095e-05,
      "loss": 14.7399,
      "step": 4205
    },
    {
      "epoch": 4.21,
      "grad_norm": 4123.60107421875,
      "learning_rate": 8.139318885448916e-05,
      "loss": 13.7612,
      "step": 4206
    },
    {
      "epoch": 4.21,
      "grad_norm": 5160.38232421875,
      "learning_rate": 8.138802889576885e-05,
      "loss": 16.5109,
      "step": 4207
    },
    {
      "epoch": 4.21,
      "grad_norm": 1862.4630126953125,
      "learning_rate": 8.138286893704851e-05,
      "loss": 12.1027,
      "step": 4208
    },
    {
      "epoch": 4.21,
      "grad_norm": 1128.81103515625,
      "learning_rate": 8.137770897832818e-05,
      "loss": 10.5496,
      "step": 4209
    },
    {
      "epoch": 4.21,
      "grad_norm": 27580.244140625,
      "learning_rate": 8.137254901960785e-05,
      "loss": 12.9071,
      "step": 4210
    },
    {
      "epoch": 4.22,
      "grad_norm": 2376.0263671875,
      "learning_rate": 8.136738906088752e-05,
      "loss": 12.7657,
      "step": 4211
    },
    {
      "epoch": 4.22,
      "grad_norm": 16773.505859375,
      "learning_rate": 8.136222910216718e-05,
      "loss": 21.3957,
      "step": 4212
    },
    {
      "epoch": 4.22,
      "grad_norm": 1620.5262451171875,
      "learning_rate": 8.135706914344686e-05,
      "loss": 11.6651,
      "step": 4213
    },
    {
      "epoch": 4.22,
      "grad_norm": 4906.3505859375,
      "learning_rate": 8.135190918472653e-05,
      "loss": 13.3166,
      "step": 4214
    },
    {
      "epoch": 4.22,
      "grad_norm": 1301.6607666015625,
      "learning_rate": 8.134674922600619e-05,
      "loss": 14.2129,
      "step": 4215
    },
    {
      "epoch": 4.22,
      "grad_norm": 2072.5048828125,
      "learning_rate": 8.134158926728587e-05,
      "loss": 13.8097,
      "step": 4216
    },
    {
      "epoch": 4.22,
      "grad_norm": 11187.3466796875,
      "learning_rate": 8.133642930856553e-05,
      "loss": 16.3503,
      "step": 4217
    },
    {
      "epoch": 4.22,
      "grad_norm": 4542.6044921875,
      "learning_rate": 8.13312693498452e-05,
      "loss": 14.3756,
      "step": 4218
    },
    {
      "epoch": 4.22,
      "grad_norm": 6131.90478515625,
      "learning_rate": 8.132610939112488e-05,
      "loss": 10.9652,
      "step": 4219
    },
    {
      "epoch": 4.22,
      "grad_norm": 20392.978515625,
      "learning_rate": 8.132094943240455e-05,
      "loss": 16.4459,
      "step": 4220
    },
    {
      "epoch": 4.23,
      "grad_norm": 1495.0972900390625,
      "learning_rate": 8.131578947368421e-05,
      "loss": 12.9035,
      "step": 4221
    },
    {
      "epoch": 4.23,
      "grad_norm": 2158.35107421875,
      "learning_rate": 8.131062951496389e-05,
      "loss": 11.5456,
      "step": 4222
    },
    {
      "epoch": 4.23,
      "grad_norm": 10486.1689453125,
      "learning_rate": 8.130546955624355e-05,
      "loss": 16.7414,
      "step": 4223
    },
    {
      "epoch": 4.23,
      "grad_norm": 4199.1875,
      "learning_rate": 8.130030959752323e-05,
      "loss": 12.4651,
      "step": 4224
    },
    {
      "epoch": 4.23,
      "grad_norm": 4895.6572265625,
      "learning_rate": 8.12951496388029e-05,
      "loss": 15.4107,
      "step": 4225
    },
    {
      "epoch": 4.23,
      "grad_norm": 10191.6015625,
      "learning_rate": 8.128998968008257e-05,
      "loss": 15.3343,
      "step": 4226
    },
    {
      "epoch": 4.23,
      "grad_norm": 12555.162109375,
      "learning_rate": 8.128482972136223e-05,
      "loss": 17.5789,
      "step": 4227
    },
    {
      "epoch": 4.23,
      "grad_norm": 6470.43408203125,
      "learning_rate": 8.12796697626419e-05,
      "loss": 14.1096,
      "step": 4228
    },
    {
      "epoch": 4.23,
      "grad_norm": 7205.70556640625,
      "learning_rate": 8.127450980392157e-05,
      "loss": 14.9912,
      "step": 4229
    },
    {
      "epoch": 4.23,
      "grad_norm": 1468.695556640625,
      "learning_rate": 8.126934984520124e-05,
      "loss": 12.868,
      "step": 4230
    },
    {
      "epoch": 4.24,
      "grad_norm": 5861.0771484375,
      "learning_rate": 8.126418988648092e-05,
      "loss": 11.8375,
      "step": 4231
    },
    {
      "epoch": 4.24,
      "grad_norm": 16532.54296875,
      "learning_rate": 8.125902992776058e-05,
      "loss": 13.9207,
      "step": 4232
    },
    {
      "epoch": 4.24,
      "grad_norm": 4636.54833984375,
      "learning_rate": 8.125386996904025e-05,
      "loss": 13.8423,
      "step": 4233
    },
    {
      "epoch": 4.24,
      "grad_norm": 2058.2109375,
      "learning_rate": 8.124871001031991e-05,
      "loss": 12.7793,
      "step": 4234
    },
    {
      "epoch": 4.24,
      "grad_norm": 13433.8115234375,
      "learning_rate": 8.12435500515996e-05,
      "loss": 16.0747,
      "step": 4235
    },
    {
      "epoch": 4.24,
      "grad_norm": 8882.1416015625,
      "learning_rate": 8.123839009287926e-05,
      "loss": 16.7177,
      "step": 4236
    },
    {
      "epoch": 4.24,
      "grad_norm": 3069.605224609375,
      "learning_rate": 8.123323013415894e-05,
      "loss": 11.0062,
      "step": 4237
    },
    {
      "epoch": 4.24,
      "grad_norm": 21705.830078125,
      "learning_rate": 8.12280701754386e-05,
      "loss": 32.9239,
      "step": 4238
    },
    {
      "epoch": 4.24,
      "grad_norm": 11727.4560546875,
      "learning_rate": 8.122291021671827e-05,
      "loss": 15.1531,
      "step": 4239
    },
    {
      "epoch": 4.24,
      "grad_norm": 4997.9248046875,
      "learning_rate": 8.121775025799793e-05,
      "loss": 17.9251,
      "step": 4240
    },
    {
      "epoch": 4.25,
      "grad_norm": 4331.763671875,
      "learning_rate": 8.121259029927762e-05,
      "loss": 11.617,
      "step": 4241
    },
    {
      "epoch": 4.25,
      "grad_norm": 2738.9130859375,
      "learning_rate": 8.120743034055728e-05,
      "loss": 14.859,
      "step": 4242
    },
    {
      "epoch": 4.25,
      "grad_norm": 7512.0439453125,
      "learning_rate": 8.120227038183696e-05,
      "loss": 15.8248,
      "step": 4243
    },
    {
      "epoch": 4.25,
      "grad_norm": 2290.62646484375,
      "learning_rate": 8.119711042311662e-05,
      "loss": 14.3427,
      "step": 4244
    },
    {
      "epoch": 4.25,
      "grad_norm": 1051.447998046875,
      "learning_rate": 8.119195046439629e-05,
      "loss": 10.693,
      "step": 4245
    },
    {
      "epoch": 4.25,
      "grad_norm": 6519.22509765625,
      "learning_rate": 8.118679050567595e-05,
      "loss": 14.3892,
      "step": 4246
    },
    {
      "epoch": 4.25,
      "grad_norm": 5507.4091796875,
      "learning_rate": 8.118163054695564e-05,
      "loss": 18.5309,
      "step": 4247
    },
    {
      "epoch": 4.25,
      "grad_norm": 28309.083984375,
      "learning_rate": 8.11764705882353e-05,
      "loss": 22.691,
      "step": 4248
    },
    {
      "epoch": 4.25,
      "grad_norm": 3108.576171875,
      "learning_rate": 8.117131062951496e-05,
      "loss": 13.5031,
      "step": 4249
    },
    {
      "epoch": 4.25,
      "grad_norm": 2899.3974609375,
      "learning_rate": 8.116615067079464e-05,
      "loss": 13.525,
      "step": 4250
    },
    {
      "epoch": 4.26,
      "grad_norm": 5308.2646484375,
      "learning_rate": 8.11609907120743e-05,
      "loss": 14.2562,
      "step": 4251
    },
    {
      "epoch": 4.26,
      "grad_norm": 2281.050537109375,
      "learning_rate": 8.115583075335399e-05,
      "loss": 23.8057,
      "step": 4252
    },
    {
      "epoch": 4.26,
      "grad_norm": 2687.737548828125,
      "learning_rate": 8.115067079463365e-05,
      "loss": 13.5006,
      "step": 4253
    },
    {
      "epoch": 4.26,
      "grad_norm": 1882.127197265625,
      "learning_rate": 8.114551083591332e-05,
      "loss": 14.0044,
      "step": 4254
    },
    {
      "epoch": 4.26,
      "grad_norm": 2159.11279296875,
      "learning_rate": 8.114035087719298e-05,
      "loss": 10.6527,
      "step": 4255
    },
    {
      "epoch": 4.26,
      "grad_norm": 10183.3271484375,
      "learning_rate": 8.113519091847266e-05,
      "loss": 11.8875,
      "step": 4256
    },
    {
      "epoch": 4.26,
      "grad_norm": 17245.046875,
      "learning_rate": 8.113003095975232e-05,
      "loss": 14.9009,
      "step": 4257
    },
    {
      "epoch": 4.26,
      "grad_norm": 7578.19482421875,
      "learning_rate": 8.1124871001032e-05,
      "loss": 11.5526,
      "step": 4258
    },
    {
      "epoch": 4.26,
      "grad_norm": 7681.41357421875,
      "learning_rate": 8.111971104231167e-05,
      "loss": 20.4241,
      "step": 4259
    },
    {
      "epoch": 4.26,
      "grad_norm": 6656.68701171875,
      "learning_rate": 8.111455108359134e-05,
      "loss": 15.7342,
      "step": 4260
    },
    {
      "epoch": 4.27,
      "grad_norm": 14230.19140625,
      "learning_rate": 8.1109391124871e-05,
      "loss": 14.9395,
      "step": 4261
    },
    {
      "epoch": 4.27,
      "grad_norm": 1858.2401123046875,
      "learning_rate": 8.110423116615068e-05,
      "loss": 11.7515,
      "step": 4262
    },
    {
      "epoch": 4.27,
      "grad_norm": 18218.6171875,
      "learning_rate": 8.109907120743035e-05,
      "loss": 14.4201,
      "step": 4263
    },
    {
      "epoch": 4.27,
      "grad_norm": 26897.560546875,
      "learning_rate": 8.109391124871003e-05,
      "loss": 13.1338,
      "step": 4264
    },
    {
      "epoch": 4.27,
      "grad_norm": 3718.552734375,
      "learning_rate": 8.108875128998969e-05,
      "loss": 14.6063,
      "step": 4265
    },
    {
      "epoch": 4.27,
      "grad_norm": 2266.2666015625,
      "learning_rate": 8.108359133126935e-05,
      "loss": 10.805,
      "step": 4266
    },
    {
      "epoch": 4.27,
      "grad_norm": 4576.90478515625,
      "learning_rate": 8.107843137254902e-05,
      "loss": 17.4358,
      "step": 4267
    },
    {
      "epoch": 4.27,
      "grad_norm": 26417.74609375,
      "learning_rate": 8.107327141382868e-05,
      "loss": 13.9149,
      "step": 4268
    },
    {
      "epoch": 4.27,
      "grad_norm": 7904.92626953125,
      "learning_rate": 8.106811145510837e-05,
      "loss": 13.032,
      "step": 4269
    },
    {
      "epoch": 4.27,
      "grad_norm": 2378.83935546875,
      "learning_rate": 8.106295149638803e-05,
      "loss": 16.493,
      "step": 4270
    },
    {
      "epoch": 4.28,
      "grad_norm": 8318.1142578125,
      "learning_rate": 8.10577915376677e-05,
      "loss": 18.906,
      "step": 4271
    },
    {
      "epoch": 4.28,
      "grad_norm": 4331.3916015625,
      "learning_rate": 8.105263157894737e-05,
      "loss": 19.3074,
      "step": 4272
    },
    {
      "epoch": 4.28,
      "grad_norm": 6955.97802734375,
      "learning_rate": 8.104747162022704e-05,
      "loss": 15.6166,
      "step": 4273
    },
    {
      "epoch": 4.28,
      "grad_norm": 8856.9912109375,
      "learning_rate": 8.10423116615067e-05,
      "loss": 10.9048,
      "step": 4274
    },
    {
      "epoch": 4.28,
      "grad_norm": 3951.2626953125,
      "learning_rate": 8.103715170278639e-05,
      "loss": 11.2937,
      "step": 4275
    },
    {
      "epoch": 4.28,
      "grad_norm": 3085.461181640625,
      "learning_rate": 8.103199174406605e-05,
      "loss": 21.5731,
      "step": 4276
    },
    {
      "epoch": 4.28,
      "grad_norm": 5687.04638671875,
      "learning_rate": 8.102683178534573e-05,
      "loss": 13.5588,
      "step": 4277
    },
    {
      "epoch": 4.28,
      "grad_norm": 20762.81640625,
      "learning_rate": 8.102167182662539e-05,
      "loss": 13.5523,
      "step": 4278
    },
    {
      "epoch": 4.28,
      "grad_norm": 2404.090087890625,
      "learning_rate": 8.101651186790506e-05,
      "loss": 15.3884,
      "step": 4279
    },
    {
      "epoch": 4.28,
      "grad_norm": 3350.784423828125,
      "learning_rate": 8.101135190918474e-05,
      "loss": 12.5353,
      "step": 4280
    },
    {
      "epoch": 4.29,
      "grad_norm": 1524.441650390625,
      "learning_rate": 8.100619195046441e-05,
      "loss": 12.3331,
      "step": 4281
    },
    {
      "epoch": 4.29,
      "grad_norm": 10156.365234375,
      "learning_rate": 8.100103199174407e-05,
      "loss": 15.6047,
      "step": 4282
    },
    {
      "epoch": 4.29,
      "grad_norm": 2644.207763671875,
      "learning_rate": 8.099587203302373e-05,
      "loss": 11.5482,
      "step": 4283
    },
    {
      "epoch": 4.29,
      "grad_norm": 6094.22705078125,
      "learning_rate": 8.099071207430341e-05,
      "loss": 17.6684,
      "step": 4284
    },
    {
      "epoch": 4.29,
      "grad_norm": 10336.3212890625,
      "learning_rate": 8.098555211558307e-05,
      "loss": 12.8116,
      "step": 4285
    },
    {
      "epoch": 4.29,
      "grad_norm": 40556.1953125,
      "learning_rate": 8.098039215686276e-05,
      "loss": 17.4091,
      "step": 4286
    },
    {
      "epoch": 4.29,
      "grad_norm": 2445.144287109375,
      "learning_rate": 8.097523219814242e-05,
      "loss": 9.6637,
      "step": 4287
    },
    {
      "epoch": 4.29,
      "grad_norm": 4067.995849609375,
      "learning_rate": 8.097007223942209e-05,
      "loss": 17.3645,
      "step": 4288
    },
    {
      "epoch": 4.29,
      "grad_norm": 9018.26953125,
      "learning_rate": 8.096491228070175e-05,
      "loss": 17.9383,
      "step": 4289
    },
    {
      "epoch": 4.29,
      "grad_norm": 9570.9248046875,
      "learning_rate": 8.095975232198143e-05,
      "loss": 13.5073,
      "step": 4290
    },
    {
      "epoch": 4.3,
      "grad_norm": 2077.534912109375,
      "learning_rate": 8.095459236326109e-05,
      "loss": 15.6247,
      "step": 4291
    },
    {
      "epoch": 4.3,
      "grad_norm": 5735.4228515625,
      "learning_rate": 8.094943240454078e-05,
      "loss": 19.9363,
      "step": 4292
    },
    {
      "epoch": 4.3,
      "grad_norm": 7211.42724609375,
      "learning_rate": 8.094427244582044e-05,
      "loss": 14.7533,
      "step": 4293
    },
    {
      "epoch": 4.3,
      "grad_norm": 5007.89697265625,
      "learning_rate": 8.093911248710011e-05,
      "loss": 19.5166,
      "step": 4294
    },
    {
      "epoch": 4.3,
      "grad_norm": 8325.51171875,
      "learning_rate": 8.093395252837977e-05,
      "loss": 18.912,
      "step": 4295
    },
    {
      "epoch": 4.3,
      "grad_norm": 3385.130615234375,
      "learning_rate": 8.092879256965945e-05,
      "loss": 11.2896,
      "step": 4296
    },
    {
      "epoch": 4.3,
      "grad_norm": 2047.1192626953125,
      "learning_rate": 8.092363261093912e-05,
      "loss": 22.196,
      "step": 4297
    },
    {
      "epoch": 4.3,
      "grad_norm": 6088.25830078125,
      "learning_rate": 8.09184726522188e-05,
      "loss": 15.7892,
      "step": 4298
    },
    {
      "epoch": 4.3,
      "grad_norm": 2717.3330078125,
      "learning_rate": 8.091331269349846e-05,
      "loss": 11.9621,
      "step": 4299
    },
    {
      "epoch": 4.3,
      "grad_norm": 1414.9090576171875,
      "learning_rate": 8.090815273477813e-05,
      "loss": 13.2634,
      "step": 4300
    },
    {
      "epoch": 4.31,
      "grad_norm": 29185.76171875,
      "learning_rate": 8.090299277605779e-05,
      "loss": 17.3556,
      "step": 4301
    },
    {
      "epoch": 4.31,
      "grad_norm": 6748.345703125,
      "learning_rate": 8.089783281733745e-05,
      "loss": 18.8718,
      "step": 4302
    },
    {
      "epoch": 4.31,
      "grad_norm": 7873.16064453125,
      "learning_rate": 8.089267285861714e-05,
      "loss": 14.5364,
      "step": 4303
    },
    {
      "epoch": 4.31,
      "grad_norm": 1413.156494140625,
      "learning_rate": 8.08875128998968e-05,
      "loss": 12.0272,
      "step": 4304
    },
    {
      "epoch": 4.31,
      "grad_norm": 1812.3797607421875,
      "learning_rate": 8.088235294117648e-05,
      "loss": 10.917,
      "step": 4305
    },
    {
      "epoch": 4.31,
      "grad_norm": 3692.53271484375,
      "learning_rate": 8.087719298245614e-05,
      "loss": 13.7503,
      "step": 4306
    },
    {
      "epoch": 4.31,
      "grad_norm": 2722.0400390625,
      "learning_rate": 8.087203302373581e-05,
      "loss": 14.92,
      "step": 4307
    },
    {
      "epoch": 4.31,
      "grad_norm": 6676.15771484375,
      "learning_rate": 8.086687306501549e-05,
      "loss": 12.5481,
      "step": 4308
    },
    {
      "epoch": 4.31,
      "grad_norm": 2044.257080078125,
      "learning_rate": 8.086171310629516e-05,
      "loss": 17.8799,
      "step": 4309
    },
    {
      "epoch": 4.31,
      "grad_norm": 14956.55078125,
      "learning_rate": 8.085655314757482e-05,
      "loss": 12.2114,
      "step": 4310
    },
    {
      "epoch": 4.32,
      "grad_norm": 3743.4384765625,
      "learning_rate": 8.08513931888545e-05,
      "loss": 15.8317,
      "step": 4311
    },
    {
      "epoch": 4.32,
      "grad_norm": 4286.56591796875,
      "learning_rate": 8.084623323013416e-05,
      "loss": 12.3661,
      "step": 4312
    },
    {
      "epoch": 4.32,
      "grad_norm": 3562.37109375,
      "learning_rate": 8.084107327141383e-05,
      "loss": 13.8505,
      "step": 4313
    },
    {
      "epoch": 4.32,
      "grad_norm": 21783.87109375,
      "learning_rate": 8.083591331269351e-05,
      "loss": 12.7095,
      "step": 4314
    },
    {
      "epoch": 4.32,
      "grad_norm": 937.338134765625,
      "learning_rate": 8.083075335397318e-05,
      "loss": 14.386,
      "step": 4315
    },
    {
      "epoch": 4.32,
      "grad_norm": 3365.00732421875,
      "learning_rate": 8.082559339525284e-05,
      "loss": 15.0514,
      "step": 4316
    },
    {
      "epoch": 4.32,
      "grad_norm": 1361.2784423828125,
      "learning_rate": 8.082043343653252e-05,
      "loss": 12.1124,
      "step": 4317
    },
    {
      "epoch": 4.32,
      "grad_norm": 9521.6982421875,
      "learning_rate": 8.081527347781218e-05,
      "loss": 23.2156,
      "step": 4318
    },
    {
      "epoch": 4.32,
      "grad_norm": 5961.8984375,
      "learning_rate": 8.081011351909184e-05,
      "loss": 23.8233,
      "step": 4319
    },
    {
      "epoch": 4.32,
      "grad_norm": 2477.798828125,
      "learning_rate": 8.080495356037153e-05,
      "loss": 12.6288,
      "step": 4320
    },
    {
      "epoch": 4.33,
      "grad_norm": 4698.20166015625,
      "learning_rate": 8.079979360165119e-05,
      "loss": 14.4813,
      "step": 4321
    },
    {
      "epoch": 4.33,
      "grad_norm": 18900.17578125,
      "learning_rate": 8.079463364293086e-05,
      "loss": 22.5042,
      "step": 4322
    },
    {
      "epoch": 4.33,
      "grad_norm": 11193.0048828125,
      "learning_rate": 8.078947368421052e-05,
      "loss": 10.5351,
      "step": 4323
    },
    {
      "epoch": 4.33,
      "grad_norm": 13413.7080078125,
      "learning_rate": 8.07843137254902e-05,
      "loss": 13.3163,
      "step": 4324
    },
    {
      "epoch": 4.33,
      "grad_norm": 6222.48583984375,
      "learning_rate": 8.077915376676987e-05,
      "loss": 12.2849,
      "step": 4325
    },
    {
      "epoch": 4.33,
      "grad_norm": 3035.731201171875,
      "learning_rate": 8.077399380804955e-05,
      "loss": 10.601,
      "step": 4326
    },
    {
      "epoch": 4.33,
      "grad_norm": 3431.41796875,
      "learning_rate": 8.076883384932921e-05,
      "loss": 19.8922,
      "step": 4327
    },
    {
      "epoch": 4.33,
      "grad_norm": 3039.904052734375,
      "learning_rate": 8.076367389060888e-05,
      "loss": 12.0108,
      "step": 4328
    },
    {
      "epoch": 4.33,
      "grad_norm": 6769.6103515625,
      "learning_rate": 8.075851393188854e-05,
      "loss": 11.8929,
      "step": 4329
    },
    {
      "epoch": 4.33,
      "grad_norm": 5885.40380859375,
      "learning_rate": 8.075335397316822e-05,
      "loss": 17.5595,
      "step": 4330
    },
    {
      "epoch": 4.34,
      "grad_norm": 17352.73828125,
      "learning_rate": 8.074819401444789e-05,
      "loss": 14.4724,
      "step": 4331
    },
    {
      "epoch": 4.34,
      "grad_norm": 636.5303955078125,
      "learning_rate": 8.074303405572757e-05,
      "loss": 17.9091,
      "step": 4332
    },
    {
      "epoch": 4.34,
      "grad_norm": 3699.251220703125,
      "learning_rate": 8.073787409700723e-05,
      "loss": 12.5931,
      "step": 4333
    },
    {
      "epoch": 4.34,
      "grad_norm": 999.6830444335938,
      "learning_rate": 8.07327141382869e-05,
      "loss": 12.0396,
      "step": 4334
    },
    {
      "epoch": 4.34,
      "grad_norm": 5655.337890625,
      "learning_rate": 8.072755417956656e-05,
      "loss": 14.5905,
      "step": 4335
    },
    {
      "epoch": 4.34,
      "grad_norm": 2116.16943359375,
      "learning_rate": 8.072239422084624e-05,
      "loss": 20.5376,
      "step": 4336
    },
    {
      "epoch": 4.34,
      "grad_norm": 2085.513427734375,
      "learning_rate": 8.071723426212591e-05,
      "loss": 15.5952,
      "step": 4337
    },
    {
      "epoch": 4.34,
      "grad_norm": 5466.083984375,
      "learning_rate": 8.071207430340557e-05,
      "loss": 13.932,
      "step": 4338
    },
    {
      "epoch": 4.34,
      "grad_norm": 9832.990234375,
      "learning_rate": 8.070691434468525e-05,
      "loss": 15.5139,
      "step": 4339
    },
    {
      "epoch": 4.34,
      "grad_norm": 3363.040771484375,
      "learning_rate": 8.070175438596491e-05,
      "loss": 13.1797,
      "step": 4340
    },
    {
      "epoch": 4.35,
      "grad_norm": 4921.04541015625,
      "learning_rate": 8.069659442724458e-05,
      "loss": 12.8515,
      "step": 4341
    },
    {
      "epoch": 4.35,
      "grad_norm": 2416.4931640625,
      "learning_rate": 8.069143446852426e-05,
      "loss": 11.6928,
      "step": 4342
    },
    {
      "epoch": 4.35,
      "grad_norm": 3006.77880859375,
      "learning_rate": 8.068627450980393e-05,
      "loss": 14.6245,
      "step": 4343
    },
    {
      "epoch": 4.35,
      "grad_norm": 1626.7364501953125,
      "learning_rate": 8.06811145510836e-05,
      "loss": 14.2292,
      "step": 4344
    },
    {
      "epoch": 4.35,
      "grad_norm": 2494.20849609375,
      "learning_rate": 8.067595459236327e-05,
      "loss": 15.0272,
      "step": 4345
    },
    {
      "epoch": 4.35,
      "grad_norm": 14490.0400390625,
      "learning_rate": 8.067079463364293e-05,
      "loss": 11.7943,
      "step": 4346
    },
    {
      "epoch": 4.35,
      "grad_norm": 16242.8359375,
      "learning_rate": 8.06656346749226e-05,
      "loss": 17.6002,
      "step": 4347
    },
    {
      "epoch": 4.35,
      "grad_norm": 3255.78076171875,
      "learning_rate": 8.066047471620228e-05,
      "loss": 21.3437,
      "step": 4348
    },
    {
      "epoch": 4.35,
      "grad_norm": 3172.28076171875,
      "learning_rate": 8.065531475748195e-05,
      "loss": 12.5566,
      "step": 4349
    },
    {
      "epoch": 4.35,
      "grad_norm": 4369.318359375,
      "learning_rate": 8.065015479876161e-05,
      "loss": 13.1112,
      "step": 4350
    },
    {
      "epoch": 4.36,
      "grad_norm": 10118.5,
      "learning_rate": 8.064499484004129e-05,
      "loss": 14.7587,
      "step": 4351
    },
    {
      "epoch": 4.36,
      "grad_norm": 68620.375,
      "learning_rate": 8.063983488132095e-05,
      "loss": 28.2347,
      "step": 4352
    },
    {
      "epoch": 4.36,
      "grad_norm": 4370.99560546875,
      "learning_rate": 8.063467492260062e-05,
      "loss": 18.1927,
      "step": 4353
    },
    {
      "epoch": 4.36,
      "grad_norm": 13753.537109375,
      "learning_rate": 8.06295149638803e-05,
      "loss": 15.9691,
      "step": 4354
    },
    {
      "epoch": 4.36,
      "grad_norm": 11935.296875,
      "learning_rate": 8.062435500515996e-05,
      "loss": 11.0343,
      "step": 4355
    },
    {
      "epoch": 4.36,
      "grad_norm": 4548.33447265625,
      "learning_rate": 8.061919504643963e-05,
      "loss": 20.0685,
      "step": 4356
    },
    {
      "epoch": 4.36,
      "grad_norm": 2339.224609375,
      "learning_rate": 8.06140350877193e-05,
      "loss": 17.8381,
      "step": 4357
    },
    {
      "epoch": 4.36,
      "grad_norm": 3755.93701171875,
      "learning_rate": 8.060887512899897e-05,
      "loss": 13.5605,
      "step": 4358
    },
    {
      "epoch": 4.36,
      "grad_norm": 1973.962158203125,
      "learning_rate": 8.060371517027864e-05,
      "loss": 11.7153,
      "step": 4359
    },
    {
      "epoch": 4.36,
      "grad_norm": 602.2708740234375,
      "learning_rate": 8.059855521155832e-05,
      "loss": 14.082,
      "step": 4360
    },
    {
      "epoch": 4.37,
      "grad_norm": 24260.40625,
      "learning_rate": 8.059339525283798e-05,
      "loss": 19.3473,
      "step": 4361
    },
    {
      "epoch": 4.37,
      "grad_norm": 15531.169921875,
      "learning_rate": 8.058823529411765e-05,
      "loss": 16.2391,
      "step": 4362
    },
    {
      "epoch": 4.37,
      "grad_norm": 34930.70703125,
      "learning_rate": 8.058307533539731e-05,
      "loss": 17.5865,
      "step": 4363
    },
    {
      "epoch": 4.37,
      "grad_norm": 6608.916015625,
      "learning_rate": 8.057791537667699e-05,
      "loss": 16.4386,
      "step": 4364
    },
    {
      "epoch": 4.37,
      "grad_norm": 1248.06884765625,
      "learning_rate": 8.057275541795666e-05,
      "loss": 13.4536,
      "step": 4365
    },
    {
      "epoch": 4.37,
      "grad_norm": 4485.064453125,
      "learning_rate": 8.056759545923634e-05,
      "loss": 13.9615,
      "step": 4366
    },
    {
      "epoch": 4.37,
      "grad_norm": 5501.90966796875,
      "learning_rate": 8.0562435500516e-05,
      "loss": 18.6712,
      "step": 4367
    },
    {
      "epoch": 4.37,
      "grad_norm": 9744.6005859375,
      "learning_rate": 8.055727554179567e-05,
      "loss": 12.0881,
      "step": 4368
    },
    {
      "epoch": 4.37,
      "grad_norm": 4102.8173828125,
      "learning_rate": 8.055211558307533e-05,
      "loss": 14.5434,
      "step": 4369
    },
    {
      "epoch": 4.37,
      "grad_norm": 11005.8984375,
      "learning_rate": 8.054695562435501e-05,
      "loss": 21.3114,
      "step": 4370
    },
    {
      "epoch": 4.38,
      "grad_norm": 7795.04833984375,
      "learning_rate": 8.054179566563468e-05,
      "loss": 12.8374,
      "step": 4371
    },
    {
      "epoch": 4.38,
      "grad_norm": 7702.35888671875,
      "learning_rate": 8.053663570691436e-05,
      "loss": 12.4182,
      "step": 4372
    },
    {
      "epoch": 4.38,
      "grad_norm": 5440.87646484375,
      "learning_rate": 8.053147574819402e-05,
      "loss": 14.9461,
      "step": 4373
    },
    {
      "epoch": 4.38,
      "grad_norm": 8552.541015625,
      "learning_rate": 8.052631578947368e-05,
      "loss": 13.1946,
      "step": 4374
    },
    {
      "epoch": 4.38,
      "grad_norm": 5685.69140625,
      "learning_rate": 8.052115583075335e-05,
      "loss": 19.056,
      "step": 4375
    },
    {
      "epoch": 4.38,
      "grad_norm": 904.686767578125,
      "learning_rate": 8.051599587203303e-05,
      "loss": 14.8315,
      "step": 4376
    },
    {
      "epoch": 4.38,
      "grad_norm": 4525.11962890625,
      "learning_rate": 8.05108359133127e-05,
      "loss": 14.751,
      "step": 4377
    },
    {
      "epoch": 4.38,
      "grad_norm": 7304.517578125,
      "learning_rate": 8.050567595459236e-05,
      "loss": 19.1689,
      "step": 4378
    },
    {
      "epoch": 4.38,
      "grad_norm": 4686.70361328125,
      "learning_rate": 8.050051599587204e-05,
      "loss": 15.142,
      "step": 4379
    },
    {
      "epoch": 4.38,
      "grad_norm": 5225.5771484375,
      "learning_rate": 8.04953560371517e-05,
      "loss": 13.5182,
      "step": 4380
    },
    {
      "epoch": 4.39,
      "grad_norm": 2621.439208984375,
      "learning_rate": 8.049019607843137e-05,
      "loss": 17.6156,
      "step": 4381
    },
    {
      "epoch": 4.39,
      "grad_norm": 12765.501953125,
      "learning_rate": 8.048503611971105e-05,
      "loss": 17.3886,
      "step": 4382
    },
    {
      "epoch": 4.39,
      "grad_norm": 1488.2908935546875,
      "learning_rate": 8.047987616099072e-05,
      "loss": 11.6364,
      "step": 4383
    },
    {
      "epoch": 4.39,
      "grad_norm": 6413.5947265625,
      "learning_rate": 8.047471620227038e-05,
      "loss": 12.3787,
      "step": 4384
    },
    {
      "epoch": 4.39,
      "grad_norm": 7331.919921875,
      "learning_rate": 8.046955624355006e-05,
      "loss": 13.0316,
      "step": 4385
    },
    {
      "epoch": 4.39,
      "grad_norm": 5020.29150390625,
      "learning_rate": 8.046439628482972e-05,
      "loss": 14.6929,
      "step": 4386
    },
    {
      "epoch": 4.39,
      "grad_norm": 1521.3785400390625,
      "learning_rate": 8.04592363261094e-05,
      "loss": 15.6037,
      "step": 4387
    },
    {
      "epoch": 4.39,
      "grad_norm": 1791.326904296875,
      "learning_rate": 8.045407636738907e-05,
      "loss": 14.9429,
      "step": 4388
    },
    {
      "epoch": 4.39,
      "grad_norm": 15512.125,
      "learning_rate": 8.044891640866874e-05,
      "loss": 16.2397,
      "step": 4389
    },
    {
      "epoch": 4.39,
      "grad_norm": 1208.7535400390625,
      "learning_rate": 8.04437564499484e-05,
      "loss": 15.5187,
      "step": 4390
    },
    {
      "epoch": 4.4,
      "grad_norm": 23596.50390625,
      "learning_rate": 8.043859649122807e-05,
      "loss": 29.6937,
      "step": 4391
    },
    {
      "epoch": 4.4,
      "grad_norm": 7819.84375,
      "learning_rate": 8.043343653250774e-05,
      "loss": 25.0174,
      "step": 4392
    },
    {
      "epoch": 4.4,
      "grad_norm": 4149.3720703125,
      "learning_rate": 8.042827657378741e-05,
      "loss": 12.4712,
      "step": 4393
    },
    {
      "epoch": 4.4,
      "grad_norm": 9810.3896484375,
      "learning_rate": 8.042311661506709e-05,
      "loss": 13.1645,
      "step": 4394
    },
    {
      "epoch": 4.4,
      "grad_norm": 1163.6051025390625,
      "learning_rate": 8.041795665634675e-05,
      "loss": 12.6283,
      "step": 4395
    },
    {
      "epoch": 4.4,
      "grad_norm": 1933.439453125,
      "learning_rate": 8.041279669762642e-05,
      "loss": 13.8568,
      "step": 4396
    },
    {
      "epoch": 4.4,
      "grad_norm": 3055.48974609375,
      "learning_rate": 8.040763673890609e-05,
      "loss": 12.5109,
      "step": 4397
    },
    {
      "epoch": 4.4,
      "grad_norm": 46543.8125,
      "learning_rate": 8.040247678018576e-05,
      "loss": 17.595,
      "step": 4398
    },
    {
      "epoch": 4.4,
      "grad_norm": 1219.5106201171875,
      "learning_rate": 8.039731682146543e-05,
      "loss": 16.6688,
      "step": 4399
    },
    {
      "epoch": 4.4,
      "grad_norm": 27758.4296875,
      "learning_rate": 8.039215686274511e-05,
      "loss": 22.6043,
      "step": 4400
    },
    {
      "epoch": 4.41,
      "grad_norm": 5665.6103515625,
      "learning_rate": 8.038699690402477e-05,
      "loss": 21.2964,
      "step": 4401
    },
    {
      "epoch": 4.41,
      "grad_norm": 5797.4619140625,
      "learning_rate": 8.038183694530444e-05,
      "loss": 16.9834,
      "step": 4402
    },
    {
      "epoch": 4.41,
      "grad_norm": 2611.32373046875,
      "learning_rate": 8.03766769865841e-05,
      "loss": 10.5723,
      "step": 4403
    },
    {
      "epoch": 4.41,
      "grad_norm": 9939.6572265625,
      "learning_rate": 8.037151702786378e-05,
      "loss": 11.2468,
      "step": 4404
    },
    {
      "epoch": 4.41,
      "grad_norm": 9941.83203125,
      "learning_rate": 8.036635706914345e-05,
      "loss": 19.1524,
      "step": 4405
    },
    {
      "epoch": 4.41,
      "grad_norm": 8585.5859375,
      "learning_rate": 8.036119711042313e-05,
      "loss": 16.3512,
      "step": 4406
    },
    {
      "epoch": 4.41,
      "grad_norm": 3805.75146484375,
      "learning_rate": 8.035603715170279e-05,
      "loss": 10.5732,
      "step": 4407
    },
    {
      "epoch": 4.41,
      "grad_norm": 2284.203369140625,
      "learning_rate": 8.035087719298246e-05,
      "loss": 11.2124,
      "step": 4408
    },
    {
      "epoch": 4.41,
      "grad_norm": 17447.87109375,
      "learning_rate": 8.034571723426213e-05,
      "loss": 13.2755,
      "step": 4409
    },
    {
      "epoch": 4.41,
      "grad_norm": 5552.8232421875,
      "learning_rate": 8.03405572755418e-05,
      "loss": 17.3237,
      "step": 4410
    },
    {
      "epoch": 4.42,
      "grad_norm": 6266.875,
      "learning_rate": 8.033539731682147e-05,
      "loss": 10.3674,
      "step": 4411
    },
    {
      "epoch": 4.42,
      "grad_norm": 4040.384765625,
      "learning_rate": 8.033023735810114e-05,
      "loss": 15.4045,
      "step": 4412
    },
    {
      "epoch": 4.42,
      "grad_norm": 24095.474609375,
      "learning_rate": 8.032507739938081e-05,
      "loss": 16.994,
      "step": 4413
    },
    {
      "epoch": 4.42,
      "grad_norm": 10079.0126953125,
      "learning_rate": 8.031991744066047e-05,
      "loss": 15.3872,
      "step": 4414
    },
    {
      "epoch": 4.42,
      "grad_norm": 10026.169921875,
      "learning_rate": 8.031475748194015e-05,
      "loss": 13.0359,
      "step": 4415
    },
    {
      "epoch": 4.42,
      "grad_norm": 3927.869384765625,
      "learning_rate": 8.030959752321982e-05,
      "loss": 13.2388,
      "step": 4416
    },
    {
      "epoch": 4.42,
      "grad_norm": 18089.26953125,
      "learning_rate": 8.03044375644995e-05,
      "loss": 20.3042,
      "step": 4417
    },
    {
      "epoch": 4.42,
      "grad_norm": 2449.944580078125,
      "learning_rate": 8.029927760577916e-05,
      "loss": 15.7434,
      "step": 4418
    },
    {
      "epoch": 4.42,
      "grad_norm": 2509.048095703125,
      "learning_rate": 8.029411764705883e-05,
      "loss": 16.8756,
      "step": 4419
    },
    {
      "epoch": 4.42,
      "grad_norm": 3767.074462890625,
      "learning_rate": 8.028895768833849e-05,
      "loss": 14.7941,
      "step": 4420
    },
    {
      "epoch": 4.43,
      "grad_norm": 2956.3193359375,
      "learning_rate": 8.028379772961817e-05,
      "loss": 16.6574,
      "step": 4421
    },
    {
      "epoch": 4.43,
      "grad_norm": 6613.68310546875,
      "learning_rate": 8.027863777089784e-05,
      "loss": 22.2637,
      "step": 4422
    },
    {
      "epoch": 4.43,
      "grad_norm": 4144.56689453125,
      "learning_rate": 8.027347781217751e-05,
      "loss": 19.0813,
      "step": 4423
    },
    {
      "epoch": 4.43,
      "grad_norm": 21879.587890625,
      "learning_rate": 8.026831785345718e-05,
      "loss": 15.5813,
      "step": 4424
    },
    {
      "epoch": 4.43,
      "grad_norm": 4794.68017578125,
      "learning_rate": 8.026315789473685e-05,
      "loss": 21.7345,
      "step": 4425
    },
    {
      "epoch": 4.43,
      "grad_norm": 9684.466796875,
      "learning_rate": 8.025799793601651e-05,
      "loss": 13.5856,
      "step": 4426
    },
    {
      "epoch": 4.43,
      "grad_norm": 4323.82080078125,
      "learning_rate": 8.025283797729619e-05,
      "loss": 18.0285,
      "step": 4427
    },
    {
      "epoch": 4.43,
      "grad_norm": 30049.38671875,
      "learning_rate": 8.024767801857586e-05,
      "loss": 13.7814,
      "step": 4428
    },
    {
      "epoch": 4.43,
      "grad_norm": 2764.9169921875,
      "learning_rate": 8.024251805985552e-05,
      "loss": 12.0181,
      "step": 4429
    },
    {
      "epoch": 4.43,
      "grad_norm": 3222.808349609375,
      "learning_rate": 8.02373581011352e-05,
      "loss": 16.6256,
      "step": 4430
    },
    {
      "epoch": 4.44,
      "grad_norm": 3066.6181640625,
      "learning_rate": 8.023219814241486e-05,
      "loss": 13.313,
      "step": 4431
    },
    {
      "epoch": 4.44,
      "grad_norm": 6315.4560546875,
      "learning_rate": 8.022703818369453e-05,
      "loss": 14.2968,
      "step": 4432
    },
    {
      "epoch": 4.44,
      "grad_norm": 60178.6171875,
      "learning_rate": 8.02218782249742e-05,
      "loss": 26.4187,
      "step": 4433
    },
    {
      "epoch": 4.44,
      "grad_norm": 9944.5693359375,
      "learning_rate": 8.021671826625388e-05,
      "loss": 13.7584,
      "step": 4434
    },
    {
      "epoch": 4.44,
      "grad_norm": 10391.1494140625,
      "learning_rate": 8.021155830753354e-05,
      "loss": 20.5296,
      "step": 4435
    },
    {
      "epoch": 4.44,
      "grad_norm": 1682.135009765625,
      "learning_rate": 8.020639834881322e-05,
      "loss": 18.7319,
      "step": 4436
    },
    {
      "epoch": 4.44,
      "grad_norm": 3825.48046875,
      "learning_rate": 8.020123839009288e-05,
      "loss": 14.7714,
      "step": 4437
    },
    {
      "epoch": 4.44,
      "grad_norm": 3921.974365234375,
      "learning_rate": 8.019607843137255e-05,
      "loss": 14.2074,
      "step": 4438
    },
    {
      "epoch": 4.44,
      "grad_norm": 35240.5,
      "learning_rate": 8.019091847265223e-05,
      "loss": 15.6684,
      "step": 4439
    },
    {
      "epoch": 4.44,
      "grad_norm": 1425.653564453125,
      "learning_rate": 8.01857585139319e-05,
      "loss": 9.6885,
      "step": 4440
    },
    {
      "epoch": 4.45,
      "grad_norm": 1499.52978515625,
      "learning_rate": 8.018059855521156e-05,
      "loss": 12.208,
      "step": 4441
    },
    {
      "epoch": 4.45,
      "grad_norm": 1475.2933349609375,
      "learning_rate": 8.017543859649124e-05,
      "loss": 14.0633,
      "step": 4442
    },
    {
      "epoch": 4.45,
      "grad_norm": 5524.33984375,
      "learning_rate": 8.01702786377709e-05,
      "loss": 12.9978,
      "step": 4443
    },
    {
      "epoch": 4.45,
      "grad_norm": 4607.95849609375,
      "learning_rate": 8.016511867905057e-05,
      "loss": 16.0834,
      "step": 4444
    },
    {
      "epoch": 4.45,
      "grad_norm": 18189.1796875,
      "learning_rate": 8.015995872033025e-05,
      "loss": 15.6373,
      "step": 4445
    },
    {
      "epoch": 4.45,
      "grad_norm": 10625.7373046875,
      "learning_rate": 8.01547987616099e-05,
      "loss": 17.6245,
      "step": 4446
    },
    {
      "epoch": 4.45,
      "grad_norm": 7910.32080078125,
      "learning_rate": 8.014963880288958e-05,
      "loss": 17.7813,
      "step": 4447
    },
    {
      "epoch": 4.45,
      "grad_norm": 22504.859375,
      "learning_rate": 8.014447884416924e-05,
      "loss": 12.4962,
      "step": 4448
    },
    {
      "epoch": 4.45,
      "grad_norm": 9050.1181640625,
      "learning_rate": 8.013931888544892e-05,
      "loss": 15.4142,
      "step": 4449
    },
    {
      "epoch": 4.45,
      "grad_norm": 10135.734375,
      "learning_rate": 8.013415892672859e-05,
      "loss": 12.2154,
      "step": 4450
    },
    {
      "epoch": 4.46,
      "grad_norm": 4293.55712890625,
      "learning_rate": 8.012899896800827e-05,
      "loss": 16.7277,
      "step": 4451
    },
    {
      "epoch": 4.46,
      "grad_norm": 5603.4599609375,
      "learning_rate": 8.012383900928793e-05,
      "loss": 12.6575,
      "step": 4452
    },
    {
      "epoch": 4.46,
      "grad_norm": 1443.119140625,
      "learning_rate": 8.01186790505676e-05,
      "loss": 11.4357,
      "step": 4453
    },
    {
      "epoch": 4.46,
      "grad_norm": 19804.9609375,
      "learning_rate": 8.011351909184726e-05,
      "loss": 13.6413,
      "step": 4454
    },
    {
      "epoch": 4.46,
      "grad_norm": 3378.900634765625,
      "learning_rate": 8.010835913312694e-05,
      "loss": 13.1981,
      "step": 4455
    },
    {
      "epoch": 4.46,
      "grad_norm": 5754.57421875,
      "learning_rate": 8.010319917440661e-05,
      "loss": 14.8766,
      "step": 4456
    },
    {
      "epoch": 4.46,
      "grad_norm": 5543.3056640625,
      "learning_rate": 8.009803921568629e-05,
      "loss": 15.292,
      "step": 4457
    },
    {
      "epoch": 4.46,
      "grad_norm": 4253.39794921875,
      "learning_rate": 8.009287925696595e-05,
      "loss": 12.8219,
      "step": 4458
    },
    {
      "epoch": 4.46,
      "grad_norm": 2424.700439453125,
      "learning_rate": 8.008771929824562e-05,
      "loss": 13.5176,
      "step": 4459
    },
    {
      "epoch": 4.46,
      "grad_norm": 8292.7666015625,
      "learning_rate": 8.008255933952528e-05,
      "loss": 12.2854,
      "step": 4460
    },
    {
      "epoch": 4.47,
      "grad_norm": 5220.3544921875,
      "learning_rate": 8.007739938080496e-05,
      "loss": 16.0876,
      "step": 4461
    },
    {
      "epoch": 4.47,
      "grad_norm": 4462.52978515625,
      "learning_rate": 8.007223942208463e-05,
      "loss": 12.2744,
      "step": 4462
    },
    {
      "epoch": 4.47,
      "grad_norm": 4585.85693359375,
      "learning_rate": 8.006707946336429e-05,
      "loss": 19.4752,
      "step": 4463
    },
    {
      "epoch": 4.47,
      "grad_norm": 2348.671630859375,
      "learning_rate": 8.006191950464397e-05,
      "loss": 15.6677,
      "step": 4464
    },
    {
      "epoch": 4.47,
      "grad_norm": 5169.8095703125,
      "learning_rate": 8.005675954592363e-05,
      "loss": 21.1966,
      "step": 4465
    },
    {
      "epoch": 4.47,
      "grad_norm": 3700.427734375,
      "learning_rate": 8.00515995872033e-05,
      "loss": 14.443,
      "step": 4466
    },
    {
      "epoch": 4.47,
      "grad_norm": 4522.609375,
      "learning_rate": 8.004643962848298e-05,
      "loss": 18.0288,
      "step": 4467
    },
    {
      "epoch": 4.47,
      "grad_norm": 10647.869140625,
      "learning_rate": 8.004127966976265e-05,
      "loss": 14.8452,
      "step": 4468
    },
    {
      "epoch": 4.47,
      "grad_norm": 2719.387451171875,
      "learning_rate": 8.003611971104231e-05,
      "loss": 12.8043,
      "step": 4469
    },
    {
      "epoch": 4.47,
      "grad_norm": 16264.212890625,
      "learning_rate": 8.003095975232199e-05,
      "loss": 15.5164,
      "step": 4470
    },
    {
      "epoch": 4.48,
      "grad_norm": 2747.81640625,
      "learning_rate": 8.002579979360165e-05,
      "loss": 14.1047,
      "step": 4471
    },
    {
      "epoch": 4.48,
      "grad_norm": 4437.35107421875,
      "learning_rate": 8.002063983488132e-05,
      "loss": 14.2624,
      "step": 4472
    },
    {
      "epoch": 4.48,
      "grad_norm": 3692.1923828125,
      "learning_rate": 8.0015479876161e-05,
      "loss": 11.6114,
      "step": 4473
    },
    {
      "epoch": 4.48,
      "grad_norm": 8580.708984375,
      "learning_rate": 8.001031991744067e-05,
      "loss": 13.4157,
      "step": 4474
    },
    {
      "epoch": 4.48,
      "grad_norm": 15121.5322265625,
      "learning_rate": 8.000515995872033e-05,
      "loss": 14.9327,
      "step": 4475
    },
    {
      "epoch": 4.48,
      "grad_norm": 4335.15771484375,
      "learning_rate": 8e-05,
      "loss": 16.6042,
      "step": 4476
    },
    {
      "epoch": 4.48,
      "grad_norm": 2514.0634765625,
      "learning_rate": 7.999484004127967e-05,
      "loss": 16.0165,
      "step": 4477
    },
    {
      "epoch": 4.48,
      "grad_norm": 1763.503173828125,
      "learning_rate": 7.998968008255934e-05,
      "loss": 13.7979,
      "step": 4478
    },
    {
      "epoch": 4.48,
      "grad_norm": 12739.724609375,
      "learning_rate": 7.998452012383902e-05,
      "loss": 16.2871,
      "step": 4479
    },
    {
      "epoch": 4.48,
      "grad_norm": 7612.85546875,
      "learning_rate": 7.997936016511868e-05,
      "loss": 13.5528,
      "step": 4480
    },
    {
      "epoch": 4.49,
      "grad_norm": 10495.798828125,
      "learning_rate": 7.997420020639835e-05,
      "loss": 16.001,
      "step": 4481
    },
    {
      "epoch": 4.49,
      "grad_norm": 10124.1865234375,
      "learning_rate": 7.996904024767801e-05,
      "loss": 18.9964,
      "step": 4482
    },
    {
      "epoch": 4.49,
      "grad_norm": 4644.5703125,
      "learning_rate": 7.996388028895769e-05,
      "loss": 12.4945,
      "step": 4483
    },
    {
      "epoch": 4.49,
      "grad_norm": 17415.630859375,
      "learning_rate": 7.995872033023736e-05,
      "loss": 14.8491,
      "step": 4484
    },
    {
      "epoch": 4.49,
      "grad_norm": 62562.35546875,
      "learning_rate": 7.995356037151704e-05,
      "loss": 14.945,
      "step": 4485
    },
    {
      "epoch": 4.49,
      "grad_norm": 15285.96875,
      "learning_rate": 7.99484004127967e-05,
      "loss": 17.5002,
      "step": 4486
    },
    {
      "epoch": 4.49,
      "grad_norm": 1406.6070556640625,
      "learning_rate": 7.994324045407637e-05,
      "loss": 18.0256,
      "step": 4487
    },
    {
      "epoch": 4.49,
      "grad_norm": 1085.8455810546875,
      "learning_rate": 7.993808049535603e-05,
      "loss": 11.9543,
      "step": 4488
    },
    {
      "epoch": 4.49,
      "grad_norm": 7191.779296875,
      "learning_rate": 7.993292053663571e-05,
      "loss": 16.5704,
      "step": 4489
    },
    {
      "epoch": 4.49,
      "grad_norm": 9558.05859375,
      "learning_rate": 7.992776057791538e-05,
      "loss": 14.3317,
      "step": 4490
    },
    {
      "epoch": 4.5,
      "grad_norm": 3061.94580078125,
      "learning_rate": 7.992260061919506e-05,
      "loss": 13.2855,
      "step": 4491
    },
    {
      "epoch": 4.5,
      "grad_norm": 15835.8662109375,
      "learning_rate": 7.991744066047472e-05,
      "loss": 20.1831,
      "step": 4492
    },
    {
      "epoch": 4.5,
      "grad_norm": 23098.40625,
      "learning_rate": 7.991228070175439e-05,
      "loss": 14.8292,
      "step": 4493
    },
    {
      "epoch": 4.5,
      "grad_norm": 4496.189453125,
      "learning_rate": 7.990712074303405e-05,
      "loss": 15.9486,
      "step": 4494
    },
    {
      "epoch": 4.5,
      "grad_norm": 3173.748046875,
      "learning_rate": 7.990196078431373e-05,
      "loss": 14.4149,
      "step": 4495
    },
    {
      "epoch": 4.5,
      "grad_norm": 688.627197265625,
      "learning_rate": 7.98968008255934e-05,
      "loss": 13.1703,
      "step": 4496
    },
    {
      "epoch": 4.5,
      "grad_norm": 4122.1259765625,
      "learning_rate": 7.989164086687308e-05,
      "loss": 13.8894,
      "step": 4497
    },
    {
      "epoch": 4.5,
      "grad_norm": 11490.8525390625,
      "learning_rate": 7.988648090815274e-05,
      "loss": 14.3595,
      "step": 4498
    },
    {
      "epoch": 4.5,
      "grad_norm": 3995.536865234375,
      "learning_rate": 7.98813209494324e-05,
      "loss": 12.9471,
      "step": 4499
    },
    {
      "epoch": 4.5,
      "grad_norm": 16172.759765625,
      "learning_rate": 7.987616099071207e-05,
      "loss": 22.851,
      "step": 4500
    },
    {
      "epoch": 4.51,
      "grad_norm": 1466.3092041015625,
      "learning_rate": 7.987100103199175e-05,
      "loss": 10.7789,
      "step": 4501
    },
    {
      "epoch": 4.51,
      "grad_norm": 6640.60546875,
      "learning_rate": 7.986584107327142e-05,
      "loss": 16.5184,
      "step": 4502
    },
    {
      "epoch": 4.51,
      "grad_norm": 8098.86474609375,
      "learning_rate": 7.986068111455108e-05,
      "loss": 14.1616,
      "step": 4503
    },
    {
      "epoch": 4.51,
      "grad_norm": 2239.5302734375,
      "learning_rate": 7.985552115583076e-05,
      "loss": 20.1544,
      "step": 4504
    },
    {
      "epoch": 4.51,
      "grad_norm": 22553.025390625,
      "learning_rate": 7.985036119711042e-05,
      "loss": 14.7299,
      "step": 4505
    },
    {
      "epoch": 4.51,
      "grad_norm": 2233.03125,
      "learning_rate": 7.984520123839009e-05,
      "loss": 11.538,
      "step": 4506
    },
    {
      "epoch": 4.51,
      "grad_norm": 8946.712890625,
      "learning_rate": 7.984004127966977e-05,
      "loss": 13.1056,
      "step": 4507
    },
    {
      "epoch": 4.51,
      "grad_norm": 2595.79052734375,
      "learning_rate": 7.983488132094944e-05,
      "loss": 11.9873,
      "step": 4508
    },
    {
      "epoch": 4.51,
      "grad_norm": 13558.5947265625,
      "learning_rate": 7.98297213622291e-05,
      "loss": 21.6144,
      "step": 4509
    },
    {
      "epoch": 4.51,
      "grad_norm": 9864.7353515625,
      "learning_rate": 7.982456140350878e-05,
      "loss": 14.6011,
      "step": 4510
    },
    {
      "epoch": 4.52,
      "grad_norm": 6573.64794921875,
      "learning_rate": 7.981940144478844e-05,
      "loss": 15.0471,
      "step": 4511
    },
    {
      "epoch": 4.52,
      "grad_norm": 15309.4384765625,
      "learning_rate": 7.981424148606811e-05,
      "loss": 12.7194,
      "step": 4512
    },
    {
      "epoch": 4.52,
      "grad_norm": 2309.341796875,
      "learning_rate": 7.980908152734779e-05,
      "loss": 17.8969,
      "step": 4513
    },
    {
      "epoch": 4.52,
      "grad_norm": 3425.766845703125,
      "learning_rate": 7.980392156862746e-05,
      "loss": 11.7177,
      "step": 4514
    },
    {
      "epoch": 4.52,
      "grad_norm": 5988.06298828125,
      "learning_rate": 7.979876160990712e-05,
      "loss": 12.1359,
      "step": 4515
    },
    {
      "epoch": 4.52,
      "grad_norm": 2551.4501953125,
      "learning_rate": 7.979360165118678e-05,
      "loss": 12.377,
      "step": 4516
    },
    {
      "epoch": 4.52,
      "grad_norm": 5101.5,
      "learning_rate": 7.978844169246646e-05,
      "loss": 17.4689,
      "step": 4517
    },
    {
      "epoch": 4.52,
      "grad_norm": 5412.880859375,
      "learning_rate": 7.978328173374613e-05,
      "loss": 11.2599,
      "step": 4518
    },
    {
      "epoch": 4.52,
      "grad_norm": 10578.72265625,
      "learning_rate": 7.977812177502581e-05,
      "loss": 12.7402,
      "step": 4519
    },
    {
      "epoch": 4.52,
      "grad_norm": 13874.626953125,
      "learning_rate": 7.977296181630547e-05,
      "loss": 15.259,
      "step": 4520
    },
    {
      "epoch": 4.53,
      "grad_norm": 14443.6884765625,
      "learning_rate": 7.976780185758514e-05,
      "loss": 18.8891,
      "step": 4521
    },
    {
      "epoch": 4.53,
      "grad_norm": 3951.9599609375,
      "learning_rate": 7.97626418988648e-05,
      "loss": 12.2074,
      "step": 4522
    },
    {
      "epoch": 4.53,
      "grad_norm": 20096.693359375,
      "learning_rate": 7.975748194014448e-05,
      "loss": 22.0123,
      "step": 4523
    },
    {
      "epoch": 4.53,
      "grad_norm": 10268.3740234375,
      "learning_rate": 7.975232198142415e-05,
      "loss": 21.8166,
      "step": 4524
    },
    {
      "epoch": 4.53,
      "grad_norm": 14284.859375,
      "learning_rate": 7.974716202270383e-05,
      "loss": 16.8524,
      "step": 4525
    },
    {
      "epoch": 4.53,
      "grad_norm": 71548.6484375,
      "learning_rate": 7.974200206398349e-05,
      "loss": 14.6193,
      "step": 4526
    },
    {
      "epoch": 4.53,
      "grad_norm": 16679.36328125,
      "learning_rate": 7.973684210526316e-05,
      "loss": 12.8002,
      "step": 4527
    },
    {
      "epoch": 4.53,
      "grad_norm": 3290.840576171875,
      "learning_rate": 7.973168214654282e-05,
      "loss": 19.6288,
      "step": 4528
    },
    {
      "epoch": 4.53,
      "grad_norm": 3040.145751953125,
      "learning_rate": 7.972652218782251e-05,
      "loss": 17.7951,
      "step": 4529
    },
    {
      "epoch": 4.53,
      "grad_norm": 6741.27783203125,
      "learning_rate": 7.972136222910217e-05,
      "loss": 11.5812,
      "step": 4530
    },
    {
      "epoch": 4.54,
      "grad_norm": 12479.1044921875,
      "learning_rate": 7.971620227038185e-05,
      "loss": 12.6931,
      "step": 4531
    },
    {
      "epoch": 4.54,
      "grad_norm": 818.7242431640625,
      "learning_rate": 7.971104231166151e-05,
      "loss": 15.0955,
      "step": 4532
    },
    {
      "epoch": 4.54,
      "grad_norm": 4841.34326171875,
      "learning_rate": 7.970588235294118e-05,
      "loss": 14.2977,
      "step": 4533
    },
    {
      "epoch": 4.54,
      "grad_norm": 14880.865234375,
      "learning_rate": 7.970072239422084e-05,
      "loss": 12.1513,
      "step": 4534
    },
    {
      "epoch": 4.54,
      "grad_norm": 4638.89794921875,
      "learning_rate": 7.969556243550052e-05,
      "loss": 16.1543,
      "step": 4535
    },
    {
      "epoch": 4.54,
      "grad_norm": 7102.27734375,
      "learning_rate": 7.969040247678019e-05,
      "loss": 12.4729,
      "step": 4536
    },
    {
      "epoch": 4.54,
      "grad_norm": 2848.51953125,
      "learning_rate": 7.968524251805985e-05,
      "loss": 15.0877,
      "step": 4537
    },
    {
      "epoch": 4.54,
      "grad_norm": 5735.2578125,
      "learning_rate": 7.968008255933953e-05,
      "loss": 19.8116,
      "step": 4538
    },
    {
      "epoch": 4.54,
      "grad_norm": 30022.080078125,
      "learning_rate": 7.967492260061919e-05,
      "loss": 17.6578,
      "step": 4539
    },
    {
      "epoch": 4.54,
      "grad_norm": 1311.676513671875,
      "learning_rate": 7.966976264189886e-05,
      "loss": 11.0453,
      "step": 4540
    },
    {
      "epoch": 4.55,
      "grad_norm": 5447.5791015625,
      "learning_rate": 7.966460268317854e-05,
      "loss": 14.6984,
      "step": 4541
    },
    {
      "epoch": 4.55,
      "grad_norm": 5931.61865234375,
      "learning_rate": 7.965944272445821e-05,
      "loss": 13.802,
      "step": 4542
    },
    {
      "epoch": 4.55,
      "grad_norm": 16986.28515625,
      "learning_rate": 7.965428276573787e-05,
      "loss": 20.282,
      "step": 4543
    },
    {
      "epoch": 4.55,
      "grad_norm": 26551.91796875,
      "learning_rate": 7.964912280701755e-05,
      "loss": 14.4513,
      "step": 4544
    },
    {
      "epoch": 4.55,
      "grad_norm": 6839.30224609375,
      "learning_rate": 7.964396284829721e-05,
      "loss": 16.7381,
      "step": 4545
    },
    {
      "epoch": 4.55,
      "grad_norm": 2014.9415283203125,
      "learning_rate": 7.96388028895769e-05,
      "loss": 12.4956,
      "step": 4546
    },
    {
      "epoch": 4.55,
      "grad_norm": 5087.80126953125,
      "learning_rate": 7.963364293085656e-05,
      "loss": 16.5542,
      "step": 4547
    },
    {
      "epoch": 4.55,
      "grad_norm": 4700.73681640625,
      "learning_rate": 7.962848297213623e-05,
      "loss": 15.7839,
      "step": 4548
    },
    {
      "epoch": 4.55,
      "grad_norm": 2531.407470703125,
      "learning_rate": 7.96233230134159e-05,
      "loss": 11.0023,
      "step": 4549
    },
    {
      "epoch": 4.55,
      "grad_norm": 12019.80078125,
      "learning_rate": 7.961816305469557e-05,
      "loss": 15.8608,
      "step": 4550
    },
    {
      "epoch": 4.56,
      "grad_norm": 1983.766357421875,
      "learning_rate": 7.961300309597523e-05,
      "loss": 12.5955,
      "step": 4551
    },
    {
      "epoch": 4.56,
      "grad_norm": 8339.5703125,
      "learning_rate": 7.96078431372549e-05,
      "loss": 13.228,
      "step": 4552
    },
    {
      "epoch": 4.56,
      "grad_norm": 7721.86572265625,
      "learning_rate": 7.960268317853458e-05,
      "loss": 19.1919,
      "step": 4553
    },
    {
      "epoch": 4.56,
      "grad_norm": 2071.4462890625,
      "learning_rate": 7.959752321981424e-05,
      "loss": 18.5493,
      "step": 4554
    },
    {
      "epoch": 4.56,
      "grad_norm": 5364.34130859375,
      "learning_rate": 7.959236326109391e-05,
      "loss": 18.1123,
      "step": 4555
    },
    {
      "epoch": 4.56,
      "grad_norm": 40770.51171875,
      "learning_rate": 7.958720330237357e-05,
      "loss": 14.3537,
      "step": 4556
    },
    {
      "epoch": 4.56,
      "grad_norm": 15872.0517578125,
      "learning_rate": 7.958204334365326e-05,
      "loss": 24.2507,
      "step": 4557
    },
    {
      "epoch": 4.56,
      "grad_norm": 9256.341796875,
      "learning_rate": 7.957688338493292e-05,
      "loss": 16.835,
      "step": 4558
    },
    {
      "epoch": 4.56,
      "grad_norm": 113073.90625,
      "learning_rate": 7.95717234262126e-05,
      "loss": 17.2616,
      "step": 4559
    },
    {
      "epoch": 4.56,
      "grad_norm": 3991.786865234375,
      "learning_rate": 7.956656346749226e-05,
      "loss": 15.6628,
      "step": 4560
    },
    {
      "epoch": 4.57,
      "grad_norm": 3506.68017578125,
      "learning_rate": 7.956140350877193e-05,
      "loss": 12.8575,
      "step": 4561
    },
    {
      "epoch": 4.57,
      "grad_norm": 1265.0950927734375,
      "learning_rate": 7.95562435500516e-05,
      "loss": 13.0826,
      "step": 4562
    },
    {
      "epoch": 4.57,
      "grad_norm": 10188.0595703125,
      "learning_rate": 7.955108359133128e-05,
      "loss": 15.3576,
      "step": 4563
    },
    {
      "epoch": 4.57,
      "grad_norm": 3043.4619140625,
      "learning_rate": 7.954592363261094e-05,
      "loss": 12.2629,
      "step": 4564
    },
    {
      "epoch": 4.57,
      "grad_norm": 11857.7001953125,
      "learning_rate": 7.954076367389062e-05,
      "loss": 13.5131,
      "step": 4565
    },
    {
      "epoch": 4.57,
      "grad_norm": 1969.184814453125,
      "learning_rate": 7.953560371517028e-05,
      "loss": 13.5518,
      "step": 4566
    },
    {
      "epoch": 4.57,
      "grad_norm": 23021.021484375,
      "learning_rate": 7.953044375644995e-05,
      "loss": 18.1598,
      "step": 4567
    },
    {
      "epoch": 4.57,
      "grad_norm": 6731.98583984375,
      "learning_rate": 7.952528379772962e-05,
      "loss": 13.7152,
      "step": 4568
    },
    {
      "epoch": 4.57,
      "grad_norm": 12130.39453125,
      "learning_rate": 7.952012383900929e-05,
      "loss": 13.7036,
      "step": 4569
    },
    {
      "epoch": 4.57,
      "grad_norm": 17464.28125,
      "learning_rate": 7.951496388028896e-05,
      "loss": 17.149,
      "step": 4570
    },
    {
      "epoch": 4.58,
      "grad_norm": 2502.53466796875,
      "learning_rate": 7.950980392156863e-05,
      "loss": 17.0024,
      "step": 4571
    },
    {
      "epoch": 4.58,
      "grad_norm": 7349.9931640625,
      "learning_rate": 7.95046439628483e-05,
      "loss": 12.3944,
      "step": 4572
    },
    {
      "epoch": 4.58,
      "grad_norm": 5098.41162109375,
      "learning_rate": 7.949948400412796e-05,
      "loss": 12.8087,
      "step": 4573
    },
    {
      "epoch": 4.58,
      "grad_norm": 6244.42822265625,
      "learning_rate": 7.949432404540765e-05,
      "loss": 19.5562,
      "step": 4574
    },
    {
      "epoch": 4.58,
      "grad_norm": 6475.33203125,
      "learning_rate": 7.948916408668731e-05,
      "loss": 13.0744,
      "step": 4575
    },
    {
      "epoch": 4.58,
      "grad_norm": 17859.955078125,
      "learning_rate": 7.948400412796698e-05,
      "loss": 22.8398,
      "step": 4576
    },
    {
      "epoch": 4.58,
      "grad_norm": 5252.45849609375,
      "learning_rate": 7.947884416924665e-05,
      "loss": 11.1018,
      "step": 4577
    },
    {
      "epoch": 4.58,
      "grad_norm": 6511.61962890625,
      "learning_rate": 7.947368421052632e-05,
      "loss": 17.5947,
      "step": 4578
    },
    {
      "epoch": 4.58,
      "grad_norm": 11595.232421875,
      "learning_rate": 7.946852425180598e-05,
      "loss": 15.702,
      "step": 4579
    },
    {
      "epoch": 4.58,
      "grad_norm": 16842.099609375,
      "learning_rate": 7.946336429308567e-05,
      "loss": 13.58,
      "step": 4580
    },
    {
      "epoch": 4.59,
      "grad_norm": 5737.07666015625,
      "learning_rate": 7.945820433436533e-05,
      "loss": 11.833,
      "step": 4581
    },
    {
      "epoch": 4.59,
      "grad_norm": 6325.1796875,
      "learning_rate": 7.9453044375645e-05,
      "loss": 12.7229,
      "step": 4582
    },
    {
      "epoch": 4.59,
      "grad_norm": 15190.67578125,
      "learning_rate": 7.944788441692467e-05,
      "loss": 14.7713,
      "step": 4583
    },
    {
      "epoch": 4.59,
      "grad_norm": 748.4601440429688,
      "learning_rate": 7.944272445820434e-05,
      "loss": 12.8274,
      "step": 4584
    },
    {
      "epoch": 4.59,
      "grad_norm": 1287.4622802734375,
      "learning_rate": 7.943756449948401e-05,
      "loss": 10.4334,
      "step": 4585
    },
    {
      "epoch": 4.59,
      "grad_norm": 11691.818359375,
      "learning_rate": 7.943240454076369e-05,
      "loss": 17.977,
      "step": 4586
    },
    {
      "epoch": 4.59,
      "grad_norm": 6878.63427734375,
      "learning_rate": 7.942724458204335e-05,
      "loss": 17.7091,
      "step": 4587
    },
    {
      "epoch": 4.59,
      "grad_norm": 10024.935546875,
      "learning_rate": 7.942208462332301e-05,
      "loss": 17.9169,
      "step": 4588
    },
    {
      "epoch": 4.59,
      "grad_norm": 2595.199951171875,
      "learning_rate": 7.941692466460269e-05,
      "loss": 12.1779,
      "step": 4589
    },
    {
      "epoch": 4.59,
      "grad_norm": 2034.27587890625,
      "learning_rate": 7.941176470588235e-05,
      "loss": 11.6634,
      "step": 4590
    },
    {
      "epoch": 4.6,
      "grad_norm": 13677.353515625,
      "learning_rate": 7.940660474716203e-05,
      "loss": 15.5168,
      "step": 4591
    },
    {
      "epoch": 4.6,
      "grad_norm": 4630.74755859375,
      "learning_rate": 7.94014447884417e-05,
      "loss": 12.2841,
      "step": 4592
    },
    {
      "epoch": 4.6,
      "grad_norm": 71832.953125,
      "learning_rate": 7.939628482972137e-05,
      "loss": 19.2387,
      "step": 4593
    },
    {
      "epoch": 4.6,
      "grad_norm": 7944.36572265625,
      "learning_rate": 7.939112487100103e-05,
      "loss": 14.1639,
      "step": 4594
    },
    {
      "epoch": 4.6,
      "grad_norm": 7489.1162109375,
      "learning_rate": 7.93859649122807e-05,
      "loss": 17.3007,
      "step": 4595
    },
    {
      "epoch": 4.6,
      "grad_norm": 29092.130859375,
      "learning_rate": 7.938080495356037e-05,
      "loss": 14.8651,
      "step": 4596
    },
    {
      "epoch": 4.6,
      "grad_norm": 2875.155517578125,
      "learning_rate": 7.937564499484005e-05,
      "loss": 12.9354,
      "step": 4597
    },
    {
      "epoch": 4.6,
      "grad_norm": 10362.5576171875,
      "learning_rate": 7.937048503611972e-05,
      "loss": 22.8818,
      "step": 4598
    },
    {
      "epoch": 4.6,
      "grad_norm": 8218.0244140625,
      "learning_rate": 7.936532507739939e-05,
      "loss": 13.7328,
      "step": 4599
    },
    {
      "epoch": 4.6,
      "grad_norm": 3103.984619140625,
      "learning_rate": 7.936016511867905e-05,
      "loss": 11.8792,
      "step": 4600
    },
    {
      "epoch": 4.61,
      "grad_norm": 2995.15869140625,
      "learning_rate": 7.935500515995873e-05,
      "loss": 14.036,
      "step": 4601
    },
    {
      "epoch": 4.61,
      "grad_norm": 1484.0814208984375,
      "learning_rate": 7.93498452012384e-05,
      "loss": 13.2275,
      "step": 4602
    },
    {
      "epoch": 4.61,
      "grad_norm": 1671.606201171875,
      "learning_rate": 7.934468524251807e-05,
      "loss": 18.8428,
      "step": 4603
    },
    {
      "epoch": 4.61,
      "grad_norm": 1195.8489990234375,
      "learning_rate": 7.933952528379774e-05,
      "loss": 13.6225,
      "step": 4604
    },
    {
      "epoch": 4.61,
      "grad_norm": 7161.78564453125,
      "learning_rate": 7.93343653250774e-05,
      "loss": 16.9809,
      "step": 4605
    },
    {
      "epoch": 4.61,
      "grad_norm": 3411.63623046875,
      "learning_rate": 7.932920536635707e-05,
      "loss": 12.839,
      "step": 4606
    },
    {
      "epoch": 4.61,
      "grad_norm": 19100.810546875,
      "learning_rate": 7.932404540763673e-05,
      "loss": 13.7522,
      "step": 4607
    },
    {
      "epoch": 4.61,
      "grad_norm": 3294.85498046875,
      "learning_rate": 7.931888544891642e-05,
      "loss": 16.427,
      "step": 4608
    },
    {
      "epoch": 4.61,
      "grad_norm": 1451.38720703125,
      "learning_rate": 7.931372549019608e-05,
      "loss": 12.4872,
      "step": 4609
    },
    {
      "epoch": 4.61,
      "grad_norm": 4054.42041015625,
      "learning_rate": 7.930856553147576e-05,
      "loss": 17.789,
      "step": 4610
    },
    {
      "epoch": 4.62,
      "grad_norm": 9460.2607421875,
      "learning_rate": 7.930340557275542e-05,
      "loss": 22.1568,
      "step": 4611
    },
    {
      "epoch": 4.62,
      "grad_norm": 13236.2685546875,
      "learning_rate": 7.929824561403509e-05,
      "loss": 14.1794,
      "step": 4612
    },
    {
      "epoch": 4.62,
      "grad_norm": 136799.46875,
      "learning_rate": 7.929308565531477e-05,
      "loss": 19.0371,
      "step": 4613
    },
    {
      "epoch": 4.62,
      "grad_norm": 9716.3564453125,
      "learning_rate": 7.928792569659444e-05,
      "loss": 11.1018,
      "step": 4614
    },
    {
      "epoch": 4.62,
      "grad_norm": 2540.916748046875,
      "learning_rate": 7.92827657378741e-05,
      "loss": 12.2882,
      "step": 4615
    },
    {
      "epoch": 4.62,
      "grad_norm": 12879.0224609375,
      "learning_rate": 7.927760577915378e-05,
      "loss": 18.5059,
      "step": 4616
    },
    {
      "epoch": 4.62,
      "grad_norm": 4484.6953125,
      "learning_rate": 7.927244582043344e-05,
      "loss": 21.3352,
      "step": 4617
    },
    {
      "epoch": 4.62,
      "grad_norm": 2785.43701171875,
      "learning_rate": 7.926728586171311e-05,
      "loss": 12.7181,
      "step": 4618
    },
    {
      "epoch": 4.62,
      "grad_norm": 30613.013671875,
      "learning_rate": 7.926212590299279e-05,
      "loss": 20.2725,
      "step": 4619
    },
    {
      "epoch": 4.62,
      "grad_norm": 3281.849365234375,
      "learning_rate": 7.925696594427246e-05,
      "loss": 16.6533,
      "step": 4620
    },
    {
      "epoch": 4.63,
      "grad_norm": 38812.796875,
      "learning_rate": 7.925180598555212e-05,
      "loss": 17.1457,
      "step": 4621
    },
    {
      "epoch": 4.63,
      "grad_norm": 5761.95068359375,
      "learning_rate": 7.92466460268318e-05,
      "loss": 13.6825,
      "step": 4622
    },
    {
      "epoch": 4.63,
      "grad_norm": 1885.6630859375,
      "learning_rate": 7.924148606811146e-05,
      "loss": 12.8413,
      "step": 4623
    },
    {
      "epoch": 4.63,
      "grad_norm": 13564.8955078125,
      "learning_rate": 7.923632610939112e-05,
      "loss": 13.4143,
      "step": 4624
    },
    {
      "epoch": 4.63,
      "grad_norm": 4340.94482421875,
      "learning_rate": 7.92311661506708e-05,
      "loss": 13.0681,
      "step": 4625
    },
    {
      "epoch": 4.63,
      "grad_norm": 3829.146484375,
      "learning_rate": 7.922600619195047e-05,
      "loss": 14.0941,
      "step": 4626
    },
    {
      "epoch": 4.63,
      "grad_norm": 1803.716796875,
      "learning_rate": 7.922084623323014e-05,
      "loss": 12.6951,
      "step": 4627
    },
    {
      "epoch": 4.63,
      "grad_norm": 4997.90966796875,
      "learning_rate": 7.92156862745098e-05,
      "loss": 11.7575,
      "step": 4628
    },
    {
      "epoch": 4.63,
      "grad_norm": 11210.2861328125,
      "learning_rate": 7.921052631578948e-05,
      "loss": 14.0831,
      "step": 4629
    },
    {
      "epoch": 4.63,
      "grad_norm": 3196.65673828125,
      "learning_rate": 7.920536635706915e-05,
      "loss": 11.8161,
      "step": 4630
    },
    {
      "epoch": 4.64,
      "grad_norm": 5093.02734375,
      "learning_rate": 7.920020639834883e-05,
      "loss": 16.4866,
      "step": 4631
    },
    {
      "epoch": 4.64,
      "grad_norm": 2185.05322265625,
      "learning_rate": 7.919504643962849e-05,
      "loss": 18.8624,
      "step": 4632
    },
    {
      "epoch": 4.64,
      "grad_norm": 7201.787109375,
      "learning_rate": 7.918988648090816e-05,
      "loss": 13.0942,
      "step": 4633
    },
    {
      "epoch": 4.64,
      "grad_norm": 1017.5606689453125,
      "learning_rate": 7.918472652218782e-05,
      "loss": 10.8003,
      "step": 4634
    },
    {
      "epoch": 4.64,
      "grad_norm": 1220.2650146484375,
      "learning_rate": 7.91795665634675e-05,
      "loss": 11.4584,
      "step": 4635
    },
    {
      "epoch": 4.64,
      "grad_norm": 2406.8212890625,
      "learning_rate": 7.917440660474717e-05,
      "loss": 8.9887,
      "step": 4636
    },
    {
      "epoch": 4.64,
      "grad_norm": 3736.616943359375,
      "learning_rate": 7.916924664602685e-05,
      "loss": 12.7019,
      "step": 4637
    },
    {
      "epoch": 4.64,
      "grad_norm": 1614.7559814453125,
      "learning_rate": 7.91640866873065e-05,
      "loss": 12.69,
      "step": 4638
    },
    {
      "epoch": 4.64,
      "grad_norm": 946.0433959960938,
      "learning_rate": 7.915892672858618e-05,
      "loss": 10.6988,
      "step": 4639
    },
    {
      "epoch": 4.64,
      "grad_norm": 30188.87890625,
      "learning_rate": 7.915376676986584e-05,
      "loss": 20.9654,
      "step": 4640
    },
    {
      "epoch": 4.65,
      "grad_norm": 9186.18359375,
      "learning_rate": 7.91486068111455e-05,
      "loss": 15.253,
      "step": 4641
    },
    {
      "epoch": 4.65,
      "grad_norm": 2742.5791015625,
      "learning_rate": 7.914344685242519e-05,
      "loss": 13.3212,
      "step": 4642
    },
    {
      "epoch": 4.65,
      "grad_norm": 2999.4150390625,
      "learning_rate": 7.913828689370485e-05,
      "loss": 13.7011,
      "step": 4643
    },
    {
      "epoch": 4.65,
      "grad_norm": 6590.58642578125,
      "learning_rate": 7.913312693498453e-05,
      "loss": 16.4977,
      "step": 4644
    },
    {
      "epoch": 4.65,
      "grad_norm": 1561.5445556640625,
      "learning_rate": 7.912796697626419e-05,
      "loss": 12.1053,
      "step": 4645
    },
    {
      "epoch": 4.65,
      "grad_norm": 6192.19140625,
      "learning_rate": 7.912280701754386e-05,
      "loss": 15.0046,
      "step": 4646
    },
    {
      "epoch": 4.65,
      "grad_norm": 4812.69580078125,
      "learning_rate": 7.911764705882354e-05,
      "loss": 16.1314,
      "step": 4647
    },
    {
      "epoch": 4.65,
      "grad_norm": 22681.123046875,
      "learning_rate": 7.911248710010321e-05,
      "loss": 27.8181,
      "step": 4648
    },
    {
      "epoch": 4.65,
      "grad_norm": 5230.90625,
      "learning_rate": 7.910732714138287e-05,
      "loss": 16.665,
      "step": 4649
    },
    {
      "epoch": 4.65,
      "grad_norm": 14789.5048828125,
      "learning_rate": 7.910216718266255e-05,
      "loss": 16.2877,
      "step": 4650
    },
    {
      "epoch": 4.66,
      "grad_norm": 3489.862548828125,
      "learning_rate": 7.909700722394221e-05,
      "loss": 14.8686,
      "step": 4651
    },
    {
      "epoch": 4.66,
      "grad_norm": 3978.420654296875,
      "learning_rate": 7.909184726522188e-05,
      "loss": 14.9054,
      "step": 4652
    },
    {
      "epoch": 4.66,
      "grad_norm": 9318.04296875,
      "learning_rate": 7.908668730650156e-05,
      "loss": 12.4309,
      "step": 4653
    },
    {
      "epoch": 4.66,
      "grad_norm": 879.38818359375,
      "learning_rate": 7.908152734778123e-05,
      "loss": 13.6475,
      "step": 4654
    },
    {
      "epoch": 4.66,
      "grad_norm": 7416.34033203125,
      "learning_rate": 7.907636738906089e-05,
      "loss": 15.8482,
      "step": 4655
    },
    {
      "epoch": 4.66,
      "grad_norm": 1185.8067626953125,
      "learning_rate": 7.907120743034057e-05,
      "loss": 11.8929,
      "step": 4656
    },
    {
      "epoch": 4.66,
      "grad_norm": 1717.8173828125,
      "learning_rate": 7.906604747162023e-05,
      "loss": 13.4264,
      "step": 4657
    },
    {
      "epoch": 4.66,
      "grad_norm": 3004.773193359375,
      "learning_rate": 7.90608875128999e-05,
      "loss": 16.835,
      "step": 4658
    },
    {
      "epoch": 4.66,
      "grad_norm": 5043.43017578125,
      "learning_rate": 7.905572755417958e-05,
      "loss": 14.6184,
      "step": 4659
    },
    {
      "epoch": 4.66,
      "grad_norm": 7591.501953125,
      "learning_rate": 7.905056759545924e-05,
      "loss": 14.0684,
      "step": 4660
    },
    {
      "epoch": 4.67,
      "grad_norm": 1746.7994384765625,
      "learning_rate": 7.904540763673891e-05,
      "loss": 13.2757,
      "step": 4661
    },
    {
      "epoch": 4.67,
      "grad_norm": 11766.3564453125,
      "learning_rate": 7.904024767801857e-05,
      "loss": 16.3339,
      "step": 4662
    },
    {
      "epoch": 4.67,
      "grad_norm": 6153.61669921875,
      "learning_rate": 7.903508771929825e-05,
      "loss": 13.9851,
      "step": 4663
    },
    {
      "epoch": 4.67,
      "grad_norm": 3446.23779296875,
      "learning_rate": 7.902992776057792e-05,
      "loss": 17.3553,
      "step": 4664
    },
    {
      "epoch": 4.67,
      "grad_norm": 1961.01708984375,
      "learning_rate": 7.90247678018576e-05,
      "loss": 11.8244,
      "step": 4665
    },
    {
      "epoch": 4.67,
      "grad_norm": 5195.49658203125,
      "learning_rate": 7.901960784313726e-05,
      "loss": 12.591,
      "step": 4666
    },
    {
      "epoch": 4.67,
      "grad_norm": 5774.33349609375,
      "learning_rate": 7.901444788441693e-05,
      "loss": 19.0965,
      "step": 4667
    },
    {
      "epoch": 4.67,
      "grad_norm": 105266.0,
      "learning_rate": 7.900928792569659e-05,
      "loss": 15.316,
      "step": 4668
    },
    {
      "epoch": 4.67,
      "grad_norm": 8221.98046875,
      "learning_rate": 7.900412796697627e-05,
      "loss": 16.9973,
      "step": 4669
    },
    {
      "epoch": 4.67,
      "grad_norm": 3874.9033203125,
      "learning_rate": 7.899896800825594e-05,
      "loss": 16.8852,
      "step": 4670
    },
    {
      "epoch": 4.68,
      "grad_norm": 3007.367431640625,
      "learning_rate": 7.899380804953562e-05,
      "loss": 13.2072,
      "step": 4671
    },
    {
      "epoch": 4.68,
      "grad_norm": 1566.8006591796875,
      "learning_rate": 7.898864809081528e-05,
      "loss": 14.1206,
      "step": 4672
    },
    {
      "epoch": 4.68,
      "grad_norm": 4135.6904296875,
      "learning_rate": 7.898348813209495e-05,
      "loss": 15.7314,
      "step": 4673
    },
    {
      "epoch": 4.68,
      "grad_norm": 1912.7894287109375,
      "learning_rate": 7.897832817337461e-05,
      "loss": 15.5408,
      "step": 4674
    },
    {
      "epoch": 4.68,
      "grad_norm": 16538.146484375,
      "learning_rate": 7.897316821465429e-05,
      "loss": 13.16,
      "step": 4675
    },
    {
      "epoch": 4.68,
      "grad_norm": 2443.7939453125,
      "learning_rate": 7.896800825593396e-05,
      "loss": 18.5458,
      "step": 4676
    },
    {
      "epoch": 4.68,
      "grad_norm": 5318.13232421875,
      "learning_rate": 7.896284829721362e-05,
      "loss": 20.7632,
      "step": 4677
    },
    {
      "epoch": 4.68,
      "grad_norm": 11810.97265625,
      "learning_rate": 7.89576883384933e-05,
      "loss": 18.2613,
      "step": 4678
    },
    {
      "epoch": 4.68,
      "grad_norm": 9672.0087890625,
      "learning_rate": 7.895252837977296e-05,
      "loss": 12.1658,
      "step": 4679
    },
    {
      "epoch": 4.68,
      "grad_norm": 2859.328125,
      "learning_rate": 7.894736842105263e-05,
      "loss": 16.6772,
      "step": 4680
    },
    {
      "epoch": 4.69,
      "grad_norm": 3137.741455078125,
      "learning_rate": 7.894220846233231e-05,
      "loss": 12.6472,
      "step": 4681
    },
    {
      "epoch": 4.69,
      "grad_norm": 3187.48486328125,
      "learning_rate": 7.893704850361198e-05,
      "loss": 13.818,
      "step": 4682
    },
    {
      "epoch": 4.69,
      "grad_norm": 2343.545654296875,
      "learning_rate": 7.893188854489164e-05,
      "loss": 16.8215,
      "step": 4683
    },
    {
      "epoch": 4.69,
      "grad_norm": 111055.8359375,
      "learning_rate": 7.892672858617132e-05,
      "loss": 17.9336,
      "step": 4684
    },
    {
      "epoch": 4.69,
      "grad_norm": 6777.060546875,
      "learning_rate": 7.892156862745098e-05,
      "loss": 18.5498,
      "step": 4685
    },
    {
      "epoch": 4.69,
      "grad_norm": 8172.77734375,
      "learning_rate": 7.891640866873065e-05,
      "loss": 17.9524,
      "step": 4686
    },
    {
      "epoch": 4.69,
      "grad_norm": 6686.15380859375,
      "learning_rate": 7.891124871001033e-05,
      "loss": 14.189,
      "step": 4687
    },
    {
      "epoch": 4.69,
      "grad_norm": 4543.796875,
      "learning_rate": 7.890608875129e-05,
      "loss": 13.869,
      "step": 4688
    },
    {
      "epoch": 4.69,
      "grad_norm": 15965.150390625,
      "learning_rate": 7.890092879256966e-05,
      "loss": 15.3353,
      "step": 4689
    },
    {
      "epoch": 4.69,
      "grad_norm": 5896.0205078125,
      "learning_rate": 7.889576883384934e-05,
      "loss": 16.3469,
      "step": 4690
    },
    {
      "epoch": 4.7,
      "grad_norm": 3261.218994140625,
      "learning_rate": 7.8890608875129e-05,
      "loss": 14.9773,
      "step": 4691
    },
    {
      "epoch": 4.7,
      "grad_norm": 5217.4501953125,
      "learning_rate": 7.888544891640867e-05,
      "loss": 11.1857,
      "step": 4692
    },
    {
      "epoch": 4.7,
      "grad_norm": 6248.1416015625,
      "learning_rate": 7.888028895768835e-05,
      "loss": 14.7034,
      "step": 4693
    },
    {
      "epoch": 4.7,
      "grad_norm": 11371.4794921875,
      "learning_rate": 7.887512899896802e-05,
      "loss": 14.8116,
      "step": 4694
    },
    {
      "epoch": 4.7,
      "grad_norm": 28925.59765625,
      "learning_rate": 7.886996904024768e-05,
      "loss": 15.2043,
      "step": 4695
    },
    {
      "epoch": 4.7,
      "grad_norm": 8939.0146484375,
      "learning_rate": 7.886480908152734e-05,
      "loss": 21.5986,
      "step": 4696
    },
    {
      "epoch": 4.7,
      "grad_norm": 452430.3125,
      "learning_rate": 7.885964912280702e-05,
      "loss": 20.8209,
      "step": 4697
    },
    {
      "epoch": 4.7,
      "grad_norm": 3438.58203125,
      "learning_rate": 7.885448916408669e-05,
      "loss": 15.2955,
      "step": 4698
    },
    {
      "epoch": 4.7,
      "grad_norm": 16988.658203125,
      "learning_rate": 7.884932920536637e-05,
      "loss": 15.1807,
      "step": 4699
    },
    {
      "epoch": 4.7,
      "grad_norm": 11819.771484375,
      "learning_rate": 7.884416924664603e-05,
      "loss": 10.4969,
      "step": 4700
    },
    {
      "epoch": 4.71,
      "grad_norm": 6420.22265625,
      "learning_rate": 7.88390092879257e-05,
      "loss": 13.65,
      "step": 4701
    },
    {
      "epoch": 4.71,
      "grad_norm": 9992.8955078125,
      "learning_rate": 7.883384932920536e-05,
      "loss": 12.2115,
      "step": 4702
    },
    {
      "epoch": 4.71,
      "grad_norm": 7781.388671875,
      "learning_rate": 7.882868937048504e-05,
      "loss": 12.6303,
      "step": 4703
    },
    {
      "epoch": 4.71,
      "grad_norm": 8861.0205078125,
      "learning_rate": 7.882352941176471e-05,
      "loss": 15.7287,
      "step": 4704
    },
    {
      "epoch": 4.71,
      "grad_norm": 12804.3447265625,
      "learning_rate": 7.881836945304439e-05,
      "loss": 16.2283,
      "step": 4705
    },
    {
      "epoch": 4.71,
      "grad_norm": 2622.504638671875,
      "learning_rate": 7.881320949432405e-05,
      "loss": 17.9831,
      "step": 4706
    },
    {
      "epoch": 4.71,
      "grad_norm": 5290.333984375,
      "learning_rate": 7.880804953560372e-05,
      "loss": 13.8157,
      "step": 4707
    },
    {
      "epoch": 4.71,
      "grad_norm": 3151.764404296875,
      "learning_rate": 7.880288957688338e-05,
      "loss": 16.4007,
      "step": 4708
    },
    {
      "epoch": 4.71,
      "grad_norm": 2229.1318359375,
      "learning_rate": 7.879772961816306e-05,
      "loss": 13.2344,
      "step": 4709
    },
    {
      "epoch": 4.71,
      "grad_norm": 7668.21533203125,
      "learning_rate": 7.879256965944273e-05,
      "loss": 18.1773,
      "step": 4710
    },
    {
      "epoch": 4.72,
      "grad_norm": 14772.4189453125,
      "learning_rate": 7.878740970072241e-05,
      "loss": 10.8857,
      "step": 4711
    },
    {
      "epoch": 4.72,
      "grad_norm": 2245.572265625,
      "learning_rate": 7.878224974200207e-05,
      "loss": 20.919,
      "step": 4712
    },
    {
      "epoch": 4.72,
      "grad_norm": 1917.8878173828125,
      "learning_rate": 7.877708978328173e-05,
      "loss": 11.4685,
      "step": 4713
    },
    {
      "epoch": 4.72,
      "grad_norm": 4252.375,
      "learning_rate": 7.87719298245614e-05,
      "loss": 11.6576,
      "step": 4714
    },
    {
      "epoch": 4.72,
      "grad_norm": 3346.751220703125,
      "learning_rate": 7.876676986584108e-05,
      "loss": 15.4123,
      "step": 4715
    },
    {
      "epoch": 4.72,
      "grad_norm": 83213.2421875,
      "learning_rate": 7.876160990712075e-05,
      "loss": 16.0645,
      "step": 4716
    },
    {
      "epoch": 4.72,
      "grad_norm": 5564.2978515625,
      "learning_rate": 7.875644994840041e-05,
      "loss": 17.3416,
      "step": 4717
    },
    {
      "epoch": 4.72,
      "grad_norm": 30718.318359375,
      "learning_rate": 7.875128998968009e-05,
      "loss": 26.5274,
      "step": 4718
    },
    {
      "epoch": 4.72,
      "grad_norm": 13349.3291015625,
      "learning_rate": 7.874613003095975e-05,
      "loss": 16.0411,
      "step": 4719
    },
    {
      "epoch": 4.72,
      "grad_norm": 14587.9443359375,
      "learning_rate": 7.874097007223942e-05,
      "loss": 11.1434,
      "step": 4720
    },
    {
      "epoch": 4.73,
      "grad_norm": 9073.099609375,
      "learning_rate": 7.87358101135191e-05,
      "loss": 18.5906,
      "step": 4721
    },
    {
      "epoch": 4.73,
      "grad_norm": 502.9525146484375,
      "learning_rate": 7.873065015479877e-05,
      "loss": 9.4248,
      "step": 4722
    },
    {
      "epoch": 4.73,
      "grad_norm": 40680.93359375,
      "learning_rate": 7.872549019607843e-05,
      "loss": 17.5495,
      "step": 4723
    },
    {
      "epoch": 4.73,
      "grad_norm": 10223.259765625,
      "learning_rate": 7.872033023735811e-05,
      "loss": 15.342,
      "step": 4724
    },
    {
      "epoch": 4.73,
      "grad_norm": 4743.052734375,
      "learning_rate": 7.871517027863777e-05,
      "loss": 12.6504,
      "step": 4725
    },
    {
      "epoch": 4.73,
      "grad_norm": 9735.4033203125,
      "learning_rate": 7.871001031991744e-05,
      "loss": 12.592,
      "step": 4726
    },
    {
      "epoch": 4.73,
      "grad_norm": 2811.66357421875,
      "learning_rate": 7.870485036119712e-05,
      "loss": 17.4887,
      "step": 4727
    },
    {
      "epoch": 4.73,
      "grad_norm": 5712.63671875,
      "learning_rate": 7.869969040247679e-05,
      "loss": 14.3576,
      "step": 4728
    },
    {
      "epoch": 4.73,
      "grad_norm": 11602.4765625,
      "learning_rate": 7.869453044375645e-05,
      "loss": 17.9732,
      "step": 4729
    },
    {
      "epoch": 4.73,
      "grad_norm": 3862.853759765625,
      "learning_rate": 7.868937048503611e-05,
      "loss": 13.0294,
      "step": 4730
    },
    {
      "epoch": 4.74,
      "grad_norm": 11305.607421875,
      "learning_rate": 7.868421052631579e-05,
      "loss": 19.9161,
      "step": 4731
    },
    {
      "epoch": 4.74,
      "grad_norm": 3323.332763671875,
      "learning_rate": 7.867905056759546e-05,
      "loss": 13.5104,
      "step": 4732
    },
    {
      "epoch": 4.74,
      "grad_norm": 5160.14794921875,
      "learning_rate": 7.867389060887514e-05,
      "loss": 18.1719,
      "step": 4733
    },
    {
      "epoch": 4.74,
      "grad_norm": 1633.6566162109375,
      "learning_rate": 7.86687306501548e-05,
      "loss": 15.8052,
      "step": 4734
    },
    {
      "epoch": 4.74,
      "grad_norm": 117887.46875,
      "learning_rate": 7.866357069143447e-05,
      "loss": 24.2162,
      "step": 4735
    },
    {
      "epoch": 4.74,
      "grad_norm": 7188.923828125,
      "learning_rate": 7.865841073271413e-05,
      "loss": 17.1925,
      "step": 4736
    },
    {
      "epoch": 4.74,
      "grad_norm": 7435.01123046875,
      "learning_rate": 7.865325077399381e-05,
      "loss": 20.0796,
      "step": 4737
    },
    {
      "epoch": 4.74,
      "grad_norm": 5332.94091796875,
      "learning_rate": 7.864809081527348e-05,
      "loss": 12.9998,
      "step": 4738
    },
    {
      "epoch": 4.74,
      "grad_norm": 35406.30078125,
      "learning_rate": 7.864293085655316e-05,
      "loss": 16.2364,
      "step": 4739
    },
    {
      "epoch": 4.74,
      "grad_norm": 19946.69921875,
      "learning_rate": 7.863777089783282e-05,
      "loss": 15.2157,
      "step": 4740
    },
    {
      "epoch": 4.75,
      "grad_norm": 5583.01416015625,
      "learning_rate": 7.86326109391125e-05,
      "loss": 14.1178,
      "step": 4741
    },
    {
      "epoch": 4.75,
      "grad_norm": 2742.237548828125,
      "learning_rate": 7.862745098039215e-05,
      "loss": 11.5225,
      "step": 4742
    },
    {
      "epoch": 4.75,
      "grad_norm": 8220.3271484375,
      "learning_rate": 7.862229102167183e-05,
      "loss": 21.5528,
      "step": 4743
    },
    {
      "epoch": 4.75,
      "grad_norm": 8875.8994140625,
      "learning_rate": 7.86171310629515e-05,
      "loss": 12.8534,
      "step": 4744
    },
    {
      "epoch": 4.75,
      "grad_norm": 21826.673828125,
      "learning_rate": 7.861197110423118e-05,
      "loss": 12.2743,
      "step": 4745
    },
    {
      "epoch": 4.75,
      "grad_norm": 10543.4951171875,
      "learning_rate": 7.860681114551084e-05,
      "loss": 18.4259,
      "step": 4746
    },
    {
      "epoch": 4.75,
      "grad_norm": 11857.810546875,
      "learning_rate": 7.860165118679051e-05,
      "loss": 27.0916,
      "step": 4747
    },
    {
      "epoch": 4.75,
      "grad_norm": 4904.951171875,
      "learning_rate": 7.859649122807017e-05,
      "loss": 17.3272,
      "step": 4748
    },
    {
      "epoch": 4.75,
      "grad_norm": 16377.2275390625,
      "learning_rate": 7.859133126934985e-05,
      "loss": 18.7887,
      "step": 4749
    },
    {
      "epoch": 4.75,
      "grad_norm": 85331.6015625,
      "learning_rate": 7.858617131062952e-05,
      "loss": 15.7537,
      "step": 4750
    },
    {
      "epoch": 4.76,
      "grad_norm": 14207.732421875,
      "learning_rate": 7.858101135190918e-05,
      "loss": 15.0974,
      "step": 4751
    },
    {
      "epoch": 4.76,
      "grad_norm": 36361.30078125,
      "learning_rate": 7.857585139318886e-05,
      "loss": 14.1999,
      "step": 4752
    },
    {
      "epoch": 4.76,
      "grad_norm": 3834.340087890625,
      "learning_rate": 7.857069143446852e-05,
      "loss": 12.4276,
      "step": 4753
    },
    {
      "epoch": 4.76,
      "grad_norm": 109285.5390625,
      "learning_rate": 7.85655314757482e-05,
      "loss": 18.5602,
      "step": 4754
    },
    {
      "epoch": 4.76,
      "grad_norm": 3011.46923828125,
      "learning_rate": 7.856037151702787e-05,
      "loss": 14.8401,
      "step": 4755
    },
    {
      "epoch": 4.76,
      "grad_norm": 1285.31982421875,
      "learning_rate": 7.855521155830754e-05,
      "loss": 11.87,
      "step": 4756
    },
    {
      "epoch": 4.76,
      "grad_norm": 13227.486328125,
      "learning_rate": 7.85500515995872e-05,
      "loss": 16.0905,
      "step": 4757
    },
    {
      "epoch": 4.76,
      "grad_norm": 1990.4014892578125,
      "learning_rate": 7.854489164086688e-05,
      "loss": 13.9278,
      "step": 4758
    },
    {
      "epoch": 4.76,
      "grad_norm": 18814.470703125,
      "learning_rate": 7.853973168214654e-05,
      "loss": 13.5153,
      "step": 4759
    },
    {
      "epoch": 4.76,
      "grad_norm": 7679.33349609375,
      "learning_rate": 7.853457172342621e-05,
      "loss": 16.3372,
      "step": 4760
    },
    {
      "epoch": 4.77,
      "grad_norm": 12411.7470703125,
      "learning_rate": 7.852941176470589e-05,
      "loss": 16.3926,
      "step": 4761
    },
    {
      "epoch": 4.77,
      "grad_norm": 6956.8779296875,
      "learning_rate": 7.852425180598556e-05,
      "loss": 16.9989,
      "step": 4762
    },
    {
      "epoch": 4.77,
      "grad_norm": 1520.126220703125,
      "learning_rate": 7.851909184726522e-05,
      "loss": 13.2417,
      "step": 4763
    },
    {
      "epoch": 4.77,
      "grad_norm": 9731.52734375,
      "learning_rate": 7.85139318885449e-05,
      "loss": 11.2172,
      "step": 4764
    },
    {
      "epoch": 4.77,
      "grad_norm": 7619.25732421875,
      "learning_rate": 7.850877192982456e-05,
      "loss": 17.5968,
      "step": 4765
    },
    {
      "epoch": 4.77,
      "grad_norm": 2713.119140625,
      "learning_rate": 7.850361197110423e-05,
      "loss": 16.2289,
      "step": 4766
    },
    {
      "epoch": 4.77,
      "grad_norm": 2508.583251953125,
      "learning_rate": 7.849845201238391e-05,
      "loss": 11.4002,
      "step": 4767
    },
    {
      "epoch": 4.77,
      "grad_norm": 3841.8095703125,
      "learning_rate": 7.849329205366357e-05,
      "loss": 20.5197,
      "step": 4768
    },
    {
      "epoch": 4.77,
      "grad_norm": 723.4483032226562,
      "learning_rate": 7.848813209494324e-05,
      "loss": 10.379,
      "step": 4769
    },
    {
      "epoch": 4.77,
      "grad_norm": 5309.3388671875,
      "learning_rate": 7.84829721362229e-05,
      "loss": 12.1524,
      "step": 4770
    },
    {
      "epoch": 4.78,
      "grad_norm": 20260.970703125,
      "learning_rate": 7.847781217750258e-05,
      "loss": 24.752,
      "step": 4771
    },
    {
      "epoch": 4.78,
      "grad_norm": 11699.298828125,
      "learning_rate": 7.847265221878225e-05,
      "loss": 13.8865,
      "step": 4772
    },
    {
      "epoch": 4.78,
      "grad_norm": 2498.517333984375,
      "learning_rate": 7.846749226006193e-05,
      "loss": 12.7387,
      "step": 4773
    },
    {
      "epoch": 4.78,
      "grad_norm": 2084.669189453125,
      "learning_rate": 7.846233230134159e-05,
      "loss": 13.1886,
      "step": 4774
    },
    {
      "epoch": 4.78,
      "grad_norm": 1243.6611328125,
      "learning_rate": 7.845717234262126e-05,
      "loss": 15.4718,
      "step": 4775
    },
    {
      "epoch": 4.78,
      "grad_norm": 12981.189453125,
      "learning_rate": 7.845201238390093e-05,
      "loss": 13.6133,
      "step": 4776
    },
    {
      "epoch": 4.78,
      "grad_norm": 13526.962890625,
      "learning_rate": 7.84468524251806e-05,
      "loss": 10.3055,
      "step": 4777
    },
    {
      "epoch": 4.78,
      "grad_norm": 4769.431640625,
      "learning_rate": 7.844169246646027e-05,
      "loss": 17.6223,
      "step": 4778
    },
    {
      "epoch": 4.78,
      "grad_norm": 3095.106201171875,
      "learning_rate": 7.843653250773995e-05,
      "loss": 16.7342,
      "step": 4779
    },
    {
      "epoch": 4.78,
      "grad_norm": 3247.25244140625,
      "learning_rate": 7.843137254901961e-05,
      "loss": 17.9056,
      "step": 4780
    },
    {
      "epoch": 4.79,
      "grad_norm": 9862.02734375,
      "learning_rate": 7.842621259029928e-05,
      "loss": 14.63,
      "step": 4781
    },
    {
      "epoch": 4.79,
      "grad_norm": 874.3508911132812,
      "learning_rate": 7.842105263157895e-05,
      "loss": 20.1472,
      "step": 4782
    },
    {
      "epoch": 4.79,
      "grad_norm": 2533.766357421875,
      "learning_rate": 7.841589267285862e-05,
      "loss": 13.2874,
      "step": 4783
    },
    {
      "epoch": 4.79,
      "grad_norm": 6638.556640625,
      "learning_rate": 7.84107327141383e-05,
      "loss": 15.9223,
      "step": 4784
    },
    {
      "epoch": 4.79,
      "grad_norm": 2430.220947265625,
      "learning_rate": 7.840557275541796e-05,
      "loss": 11.8368,
      "step": 4785
    },
    {
      "epoch": 4.79,
      "grad_norm": 5441.61865234375,
      "learning_rate": 7.840041279669763e-05,
      "loss": 17.7378,
      "step": 4786
    },
    {
      "epoch": 4.79,
      "grad_norm": 2372.085693359375,
      "learning_rate": 7.839525283797729e-05,
      "loss": 10.4989,
      "step": 4787
    },
    {
      "epoch": 4.79,
      "grad_norm": 4187.49169921875,
      "learning_rate": 7.839009287925697e-05,
      "loss": 11.9561,
      "step": 4788
    },
    {
      "epoch": 4.79,
      "grad_norm": 2273.6923828125,
      "learning_rate": 7.838493292053664e-05,
      "loss": 14.4823,
      "step": 4789
    },
    {
      "epoch": 4.79,
      "grad_norm": 4372.48876953125,
      "learning_rate": 7.837977296181631e-05,
      "loss": 12.6579,
      "step": 4790
    },
    {
      "epoch": 4.8,
      "grad_norm": 3058.671142578125,
      "learning_rate": 7.837461300309598e-05,
      "loss": 13.0167,
      "step": 4791
    },
    {
      "epoch": 4.8,
      "grad_norm": 44262.58984375,
      "learning_rate": 7.836945304437565e-05,
      "loss": 14.6389,
      "step": 4792
    },
    {
      "epoch": 4.8,
      "grad_norm": 4206.88671875,
      "learning_rate": 7.836429308565531e-05,
      "loss": 14.3133,
      "step": 4793
    },
    {
      "epoch": 4.8,
      "grad_norm": 11043.421875,
      "learning_rate": 7.835913312693499e-05,
      "loss": 23.2361,
      "step": 4794
    },
    {
      "epoch": 4.8,
      "grad_norm": 2925.38818359375,
      "learning_rate": 7.835397316821466e-05,
      "loss": 19.4595,
      "step": 4795
    },
    {
      "epoch": 4.8,
      "grad_norm": 9535.4853515625,
      "learning_rate": 7.834881320949433e-05,
      "loss": 22.1825,
      "step": 4796
    },
    {
      "epoch": 4.8,
      "grad_norm": 13530.3583984375,
      "learning_rate": 7.8343653250774e-05,
      "loss": 15.1543,
      "step": 4797
    },
    {
      "epoch": 4.8,
      "grad_norm": 4663.4150390625,
      "learning_rate": 7.833849329205367e-05,
      "loss": 11.7692,
      "step": 4798
    },
    {
      "epoch": 4.8,
      "grad_norm": 4302.48291015625,
      "learning_rate": 7.833333333333333e-05,
      "loss": 12.0022,
      "step": 4799
    },
    {
      "epoch": 4.8,
      "grad_norm": 4956.771484375,
      "learning_rate": 7.8328173374613e-05,
      "loss": 19.8366,
      "step": 4800
    },
    {
      "epoch": 4.81,
      "grad_norm": 4265.0869140625,
      "learning_rate": 7.832301341589268e-05,
      "loss": 14.3533,
      "step": 4801
    },
    {
      "epoch": 4.81,
      "grad_norm": 2538.69970703125,
      "learning_rate": 7.831785345717234e-05,
      "loss": 12.7733,
      "step": 4802
    },
    {
      "epoch": 4.81,
      "grad_norm": 8225.14453125,
      "learning_rate": 7.831269349845202e-05,
      "loss": 13.7939,
      "step": 4803
    },
    {
      "epoch": 4.81,
      "grad_norm": 4660.166015625,
      "learning_rate": 7.830753353973168e-05,
      "loss": 23.4353,
      "step": 4804
    },
    {
      "epoch": 4.81,
      "grad_norm": 15469.6240234375,
      "learning_rate": 7.830237358101135e-05,
      "loss": 15.7469,
      "step": 4805
    },
    {
      "epoch": 4.81,
      "grad_norm": 5154.91650390625,
      "learning_rate": 7.829721362229103e-05,
      "loss": 13.3644,
      "step": 4806
    },
    {
      "epoch": 4.81,
      "grad_norm": 8412.2333984375,
      "learning_rate": 7.82920536635707e-05,
      "loss": 14.9602,
      "step": 4807
    },
    {
      "epoch": 4.81,
      "grad_norm": 4714.4853515625,
      "learning_rate": 7.828689370485036e-05,
      "loss": 13.4194,
      "step": 4808
    },
    {
      "epoch": 4.81,
      "grad_norm": 4550.0830078125,
      "learning_rate": 7.828173374613004e-05,
      "loss": 18.0361,
      "step": 4809
    },
    {
      "epoch": 4.81,
      "grad_norm": 4975.9326171875,
      "learning_rate": 7.82765737874097e-05,
      "loss": 13.4592,
      "step": 4810
    },
    {
      "epoch": 4.82,
      "grad_norm": 1933.1038818359375,
      "learning_rate": 7.827141382868937e-05,
      "loss": 14.7566,
      "step": 4811
    },
    {
      "epoch": 4.82,
      "grad_norm": 36876.69921875,
      "learning_rate": 7.826625386996905e-05,
      "loss": 15.8835,
      "step": 4812
    },
    {
      "epoch": 4.82,
      "grad_norm": 2522.8203125,
      "learning_rate": 7.826109391124872e-05,
      "loss": 18.3944,
      "step": 4813
    },
    {
      "epoch": 4.82,
      "grad_norm": 17974.572265625,
      "learning_rate": 7.825593395252838e-05,
      "loss": 18.0372,
      "step": 4814
    },
    {
      "epoch": 4.82,
      "grad_norm": 42912.28515625,
      "learning_rate": 7.825077399380806e-05,
      "loss": 14.3629,
      "step": 4815
    },
    {
      "epoch": 4.82,
      "grad_norm": 6943.5283203125,
      "learning_rate": 7.824561403508772e-05,
      "loss": 12.5978,
      "step": 4816
    },
    {
      "epoch": 4.82,
      "grad_norm": 6410.419921875,
      "learning_rate": 7.824045407636739e-05,
      "loss": 16.7443,
      "step": 4817
    },
    {
      "epoch": 4.82,
      "grad_norm": 20565.455078125,
      "learning_rate": 7.823529411764707e-05,
      "loss": 13.8079,
      "step": 4818
    },
    {
      "epoch": 4.82,
      "grad_norm": 1558.1566162109375,
      "learning_rate": 7.823013415892674e-05,
      "loss": 13.2144,
      "step": 4819
    },
    {
      "epoch": 4.82,
      "grad_norm": 14438.9638671875,
      "learning_rate": 7.82249742002064e-05,
      "loss": 25.1949,
      "step": 4820
    },
    {
      "epoch": 4.83,
      "grad_norm": 3495.80224609375,
      "learning_rate": 7.821981424148606e-05,
      "loss": 17.3003,
      "step": 4821
    },
    {
      "epoch": 4.83,
      "grad_norm": 11746.6591796875,
      "learning_rate": 7.821465428276574e-05,
      "loss": 14.7678,
      "step": 4822
    },
    {
      "epoch": 4.83,
      "grad_norm": 118851.2734375,
      "learning_rate": 7.820949432404541e-05,
      "loss": 28.663,
      "step": 4823
    },
    {
      "epoch": 4.83,
      "grad_norm": 28885.447265625,
      "learning_rate": 7.820433436532509e-05,
      "loss": 11.9116,
      "step": 4824
    },
    {
      "epoch": 4.83,
      "grad_norm": 13251.7138671875,
      "learning_rate": 7.819917440660475e-05,
      "loss": 17.2248,
      "step": 4825
    },
    {
      "epoch": 4.83,
      "grad_norm": 8532.9482421875,
      "learning_rate": 7.819401444788442e-05,
      "loss": 13.4756,
      "step": 4826
    },
    {
      "epoch": 4.83,
      "grad_norm": 8267.359375,
      "learning_rate": 7.818885448916408e-05,
      "loss": 13.195,
      "step": 4827
    },
    {
      "epoch": 4.83,
      "grad_norm": 4543.81689453125,
      "learning_rate": 7.818369453044376e-05,
      "loss": 13.6452,
      "step": 4828
    },
    {
      "epoch": 4.83,
      "grad_norm": 17311.138671875,
      "learning_rate": 7.817853457172343e-05,
      "loss": 12.2735,
      "step": 4829
    },
    {
      "epoch": 4.83,
      "grad_norm": 1949.6292724609375,
      "learning_rate": 7.81733746130031e-05,
      "loss": 14.578,
      "step": 4830
    },
    {
      "epoch": 4.84,
      "grad_norm": 5612.30029296875,
      "learning_rate": 7.816821465428277e-05,
      "loss": 15.1626,
      "step": 4831
    },
    {
      "epoch": 4.84,
      "grad_norm": 2135.02978515625,
      "learning_rate": 7.816305469556244e-05,
      "loss": 11.199,
      "step": 4832
    },
    {
      "epoch": 4.84,
      "grad_norm": 18462.580078125,
      "learning_rate": 7.81578947368421e-05,
      "loss": 13.4275,
      "step": 4833
    },
    {
      "epoch": 4.84,
      "grad_norm": 3651.80078125,
      "learning_rate": 7.815273477812179e-05,
      "loss": 16.2612,
      "step": 4834
    },
    {
      "epoch": 4.84,
      "grad_norm": 44516.890625,
      "learning_rate": 7.814757481940145e-05,
      "loss": 17.1741,
      "step": 4835
    },
    {
      "epoch": 4.84,
      "grad_norm": 50085.1015625,
      "learning_rate": 7.814241486068113e-05,
      "loss": 20.6805,
      "step": 4836
    },
    {
      "epoch": 4.84,
      "grad_norm": 9394.2529296875,
      "learning_rate": 7.813725490196079e-05,
      "loss": 14.3887,
      "step": 4837
    },
    {
      "epoch": 4.84,
      "grad_norm": 2326.991943359375,
      "learning_rate": 7.813209494324045e-05,
      "loss": 13.3866,
      "step": 4838
    },
    {
      "epoch": 4.84,
      "grad_norm": 7571.4228515625,
      "learning_rate": 7.812693498452012e-05,
      "loss": 14.4565,
      "step": 4839
    },
    {
      "epoch": 4.84,
      "grad_norm": 8230.71484375,
      "learning_rate": 7.81217750257998e-05,
      "loss": 14.6671,
      "step": 4840
    },
    {
      "epoch": 4.85,
      "grad_norm": 20192.4140625,
      "learning_rate": 7.811661506707947e-05,
      "loss": 12.4345,
      "step": 4841
    },
    {
      "epoch": 4.85,
      "grad_norm": 15626.419921875,
      "learning_rate": 7.811145510835913e-05,
      "loss": 13.5743,
      "step": 4842
    },
    {
      "epoch": 4.85,
      "grad_norm": 23999.5390625,
      "learning_rate": 7.81062951496388e-05,
      "loss": 19.0059,
      "step": 4843
    },
    {
      "epoch": 4.85,
      "grad_norm": 7517.63671875,
      "learning_rate": 7.810113519091847e-05,
      "loss": 14.7162,
      "step": 4844
    },
    {
      "epoch": 4.85,
      "grad_norm": 5588.00732421875,
      "learning_rate": 7.809597523219814e-05,
      "loss": 11.8421,
      "step": 4845
    },
    {
      "epoch": 4.85,
      "grad_norm": 10959.240234375,
      "learning_rate": 7.809081527347782e-05,
      "loss": 13.4913,
      "step": 4846
    },
    {
      "epoch": 4.85,
      "grad_norm": 41839.21875,
      "learning_rate": 7.808565531475749e-05,
      "loss": 21.463,
      "step": 4847
    },
    {
      "epoch": 4.85,
      "grad_norm": 4605.69384765625,
      "learning_rate": 7.808049535603715e-05,
      "loss": 20.1594,
      "step": 4848
    },
    {
      "epoch": 4.85,
      "grad_norm": 13395.4580078125,
      "learning_rate": 7.807533539731683e-05,
      "loss": 14.5273,
      "step": 4849
    },
    {
      "epoch": 4.85,
      "grad_norm": 3181.810302734375,
      "learning_rate": 7.807017543859649e-05,
      "loss": 18.9807,
      "step": 4850
    },
    {
      "epoch": 4.86,
      "grad_norm": 4242.08837890625,
      "learning_rate": 7.806501547987618e-05,
      "loss": 15.5188,
      "step": 4851
    },
    {
      "epoch": 4.86,
      "grad_norm": 12387.56640625,
      "learning_rate": 7.805985552115584e-05,
      "loss": 12.7168,
      "step": 4852
    },
    {
      "epoch": 4.86,
      "grad_norm": 1659.9200439453125,
      "learning_rate": 7.805469556243551e-05,
      "loss": 13.1837,
      "step": 4853
    },
    {
      "epoch": 4.86,
      "grad_norm": 5786.1142578125,
      "learning_rate": 7.804953560371517e-05,
      "loss": 13.5067,
      "step": 4854
    },
    {
      "epoch": 4.86,
      "grad_norm": 8934.515625,
      "learning_rate": 7.804437564499485e-05,
      "loss": 13.9973,
      "step": 4855
    },
    {
      "epoch": 4.86,
      "grad_norm": 53993.82421875,
      "learning_rate": 7.803921568627451e-05,
      "loss": 19.9325,
      "step": 4856
    },
    {
      "epoch": 4.86,
      "grad_norm": 2912.2392578125,
      "learning_rate": 7.803405572755418e-05,
      "loss": 12.3085,
      "step": 4857
    },
    {
      "epoch": 4.86,
      "grad_norm": 2520.1865234375,
      "learning_rate": 7.802889576883386e-05,
      "loss": 14.3842,
      "step": 4858
    },
    {
      "epoch": 4.86,
      "grad_norm": 1173.4171142578125,
      "learning_rate": 7.802373581011352e-05,
      "loss": 10.1755,
      "step": 4859
    },
    {
      "epoch": 4.86,
      "grad_norm": 3266.851806640625,
      "learning_rate": 7.801857585139319e-05,
      "loss": 13.9816,
      "step": 4860
    },
    {
      "epoch": 4.87,
      "grad_norm": 11660.6953125,
      "learning_rate": 7.801341589267285e-05,
      "loss": 17.8906,
      "step": 4861
    },
    {
      "epoch": 4.87,
      "grad_norm": 11349.876953125,
      "learning_rate": 7.800825593395254e-05,
      "loss": 11.7333,
      "step": 4862
    },
    {
      "epoch": 4.87,
      "grad_norm": 2169.08251953125,
      "learning_rate": 7.80030959752322e-05,
      "loss": 14.1857,
      "step": 4863
    },
    {
      "epoch": 4.87,
      "grad_norm": 13149.85546875,
      "learning_rate": 7.799793601651188e-05,
      "loss": 14.2989,
      "step": 4864
    },
    {
      "epoch": 4.87,
      "grad_norm": 11643.376953125,
      "learning_rate": 7.799277605779154e-05,
      "loss": 20.0513,
      "step": 4865
    },
    {
      "epoch": 4.87,
      "grad_norm": 2585.689453125,
      "learning_rate": 7.798761609907121e-05,
      "loss": 16.6994,
      "step": 4866
    },
    {
      "epoch": 4.87,
      "grad_norm": 2316.47314453125,
      "learning_rate": 7.798245614035087e-05,
      "loss": 12.9463,
      "step": 4867
    },
    {
      "epoch": 4.87,
      "grad_norm": 89579.8984375,
      "learning_rate": 7.797729618163056e-05,
      "loss": 14.7544,
      "step": 4868
    },
    {
      "epoch": 4.87,
      "grad_norm": 5639.64404296875,
      "learning_rate": 7.797213622291022e-05,
      "loss": 20.0163,
      "step": 4869
    },
    {
      "epoch": 4.87,
      "grad_norm": 4129.416015625,
      "learning_rate": 7.79669762641899e-05,
      "loss": 15.0613,
      "step": 4870
    },
    {
      "epoch": 4.88,
      "grad_norm": 6252.98828125,
      "learning_rate": 7.796181630546956e-05,
      "loss": 11.5285,
      "step": 4871
    },
    {
      "epoch": 4.88,
      "grad_norm": 5952.22265625,
      "learning_rate": 7.795665634674923e-05,
      "loss": 16.8838,
      "step": 4872
    },
    {
      "epoch": 4.88,
      "grad_norm": 1494.3468017578125,
      "learning_rate": 7.795149638802889e-05,
      "loss": 14.2355,
      "step": 4873
    },
    {
      "epoch": 4.88,
      "grad_norm": 18227.75390625,
      "learning_rate": 7.794633642930857e-05,
      "loss": 13.386,
      "step": 4874
    },
    {
      "epoch": 4.88,
      "grad_norm": 15196.5634765625,
      "learning_rate": 7.794117647058824e-05,
      "loss": 11.2364,
      "step": 4875
    },
    {
      "epoch": 4.88,
      "grad_norm": 13767.48046875,
      "learning_rate": 7.79360165118679e-05,
      "loss": 17.4192,
      "step": 4876
    },
    {
      "epoch": 4.88,
      "grad_norm": 19771.634765625,
      "learning_rate": 7.793085655314758e-05,
      "loss": 20.7919,
      "step": 4877
    },
    {
      "epoch": 4.88,
      "grad_norm": 4020.9013671875,
      "learning_rate": 7.792569659442724e-05,
      "loss": 13.8008,
      "step": 4878
    },
    {
      "epoch": 4.88,
      "grad_norm": 28257.1640625,
      "learning_rate": 7.792053663570693e-05,
      "loss": 15.0642,
      "step": 4879
    },
    {
      "epoch": 4.88,
      "grad_norm": 676.3651733398438,
      "learning_rate": 7.791537667698659e-05,
      "loss": 12.6814,
      "step": 4880
    },
    {
      "epoch": 4.89,
      "grad_norm": 8214.4716796875,
      "learning_rate": 7.791021671826626e-05,
      "loss": 23.202,
      "step": 4881
    },
    {
      "epoch": 4.89,
      "grad_norm": 859.0858764648438,
      "learning_rate": 7.790505675954592e-05,
      "loss": 13.8524,
      "step": 4882
    },
    {
      "epoch": 4.89,
      "grad_norm": 4491.4580078125,
      "learning_rate": 7.78998968008256e-05,
      "loss": 21.2582,
      "step": 4883
    },
    {
      "epoch": 4.89,
      "grad_norm": 8377.7138671875,
      "learning_rate": 7.789473684210526e-05,
      "loss": 16.1223,
      "step": 4884
    },
    {
      "epoch": 4.89,
      "grad_norm": 6181.99462890625,
      "learning_rate": 7.788957688338495e-05,
      "loss": 12.723,
      "step": 4885
    },
    {
      "epoch": 4.89,
      "grad_norm": 4787.53173828125,
      "learning_rate": 7.788441692466461e-05,
      "loss": 14.6952,
      "step": 4886
    },
    {
      "epoch": 4.89,
      "grad_norm": 9363.6884765625,
      "learning_rate": 7.787925696594428e-05,
      "loss": 18.2981,
      "step": 4887
    },
    {
      "epoch": 4.89,
      "grad_norm": 3444.525634765625,
      "learning_rate": 7.787409700722394e-05,
      "loss": 12.6697,
      "step": 4888
    },
    {
      "epoch": 4.89,
      "grad_norm": 11270.4814453125,
      "learning_rate": 7.786893704850362e-05,
      "loss": 17.1471,
      "step": 4889
    },
    {
      "epoch": 4.89,
      "grad_norm": 5033.89453125,
      "learning_rate": 7.786377708978328e-05,
      "loss": 17.2846,
      "step": 4890
    },
    {
      "epoch": 4.9,
      "grad_norm": 8886.7041015625,
      "learning_rate": 7.785861713106295e-05,
      "loss": 15.3759,
      "step": 4891
    },
    {
      "epoch": 4.9,
      "grad_norm": 7783.572265625,
      "learning_rate": 7.785345717234263e-05,
      "loss": 12.9487,
      "step": 4892
    },
    {
      "epoch": 4.9,
      "grad_norm": 1448.4659423828125,
      "learning_rate": 7.784829721362229e-05,
      "loss": 12.9164,
      "step": 4893
    },
    {
      "epoch": 4.9,
      "grad_norm": 5558.65283203125,
      "learning_rate": 7.784313725490196e-05,
      "loss": 14.1239,
      "step": 4894
    },
    {
      "epoch": 4.9,
      "grad_norm": 84807.34375,
      "learning_rate": 7.783797729618162e-05,
      "loss": 18.8535,
      "step": 4895
    },
    {
      "epoch": 4.9,
      "grad_norm": 4028.875732421875,
      "learning_rate": 7.783281733746131e-05,
      "loss": 14.4571,
      "step": 4896
    },
    {
      "epoch": 4.9,
      "grad_norm": 4142.26123046875,
      "learning_rate": 7.782765737874097e-05,
      "loss": 12.1637,
      "step": 4897
    },
    {
      "epoch": 4.9,
      "grad_norm": 1927.435302734375,
      "learning_rate": 7.782249742002065e-05,
      "loss": 14.2385,
      "step": 4898
    },
    {
      "epoch": 4.9,
      "grad_norm": 7208.7685546875,
      "learning_rate": 7.781733746130031e-05,
      "loss": 15.2726,
      "step": 4899
    },
    {
      "epoch": 4.9,
      "grad_norm": 1793.2830810546875,
      "learning_rate": 7.781217750257998e-05,
      "loss": 12.8795,
      "step": 4900
    },
    {
      "epoch": 4.91,
      "grad_norm": 889.003662109375,
      "learning_rate": 7.780701754385964e-05,
      "loss": 12.9095,
      "step": 4901
    },
    {
      "epoch": 4.91,
      "grad_norm": 4390.72265625,
      "learning_rate": 7.780185758513933e-05,
      "loss": 13.8423,
      "step": 4902
    },
    {
      "epoch": 4.91,
      "grad_norm": 8858.6328125,
      "learning_rate": 7.779669762641899e-05,
      "loss": 18.492,
      "step": 4903
    },
    {
      "epoch": 4.91,
      "grad_norm": 25305.064453125,
      "learning_rate": 7.779153766769867e-05,
      "loss": 19.058,
      "step": 4904
    },
    {
      "epoch": 4.91,
      "grad_norm": 3025.354248046875,
      "learning_rate": 7.778637770897833e-05,
      "loss": 11.8539,
      "step": 4905
    },
    {
      "epoch": 4.91,
      "grad_norm": 15618.802734375,
      "learning_rate": 7.7781217750258e-05,
      "loss": 13.6189,
      "step": 4906
    },
    {
      "epoch": 4.91,
      "grad_norm": 977.2045288085938,
      "learning_rate": 7.777605779153768e-05,
      "loss": 11.9494,
      "step": 4907
    },
    {
      "epoch": 4.91,
      "grad_norm": 7716.12890625,
      "learning_rate": 7.777089783281735e-05,
      "loss": 17.8257,
      "step": 4908
    },
    {
      "epoch": 4.91,
      "grad_norm": 4504.79150390625,
      "learning_rate": 7.776573787409701e-05,
      "loss": 12.8837,
      "step": 4909
    },
    {
      "epoch": 4.91,
      "grad_norm": 1855.495361328125,
      "learning_rate": 7.776057791537667e-05,
      "loss": 13.5818,
      "step": 4910
    },
    {
      "epoch": 4.92,
      "grad_norm": 4544.611328125,
      "learning_rate": 7.775541795665635e-05,
      "loss": 13.4736,
      "step": 4911
    },
    {
      "epoch": 4.92,
      "grad_norm": 7184.31787109375,
      "learning_rate": 7.775025799793601e-05,
      "loss": 13.6691,
      "step": 4912
    },
    {
      "epoch": 4.92,
      "grad_norm": 4876.84423828125,
      "learning_rate": 7.77450980392157e-05,
      "loss": 16.6864,
      "step": 4913
    },
    {
      "epoch": 4.92,
      "grad_norm": 3932.397705078125,
      "learning_rate": 7.773993808049536e-05,
      "loss": 17.0166,
      "step": 4914
    },
    {
      "epoch": 4.92,
      "grad_norm": 13598.4560546875,
      "learning_rate": 7.773477812177503e-05,
      "loss": 14.568,
      "step": 4915
    },
    {
      "epoch": 4.92,
      "grad_norm": 2168.343017578125,
      "learning_rate": 7.77296181630547e-05,
      "loss": 14.3328,
      "step": 4916
    },
    {
      "epoch": 4.92,
      "grad_norm": 8199.9365234375,
      "learning_rate": 7.772445820433437e-05,
      "loss": 10.584,
      "step": 4917
    },
    {
      "epoch": 4.92,
      "grad_norm": 3073.6494140625,
      "learning_rate": 7.771929824561403e-05,
      "loss": 14.0859,
      "step": 4918
    },
    {
      "epoch": 4.92,
      "grad_norm": 18146.607421875,
      "learning_rate": 7.771413828689372e-05,
      "loss": 18.583,
      "step": 4919
    },
    {
      "epoch": 4.92,
      "grad_norm": 2039.61572265625,
      "learning_rate": 7.770897832817338e-05,
      "loss": 13.5438,
      "step": 4920
    },
    {
      "epoch": 4.93,
      "grad_norm": 2067.6474609375,
      "learning_rate": 7.770381836945305e-05,
      "loss": 15.5341,
      "step": 4921
    },
    {
      "epoch": 4.93,
      "grad_norm": 2500.072265625,
      "learning_rate": 7.769865841073271e-05,
      "loss": 13.6109,
      "step": 4922
    },
    {
      "epoch": 4.93,
      "grad_norm": 2617.86962890625,
      "learning_rate": 7.769349845201239e-05,
      "loss": 10.5914,
      "step": 4923
    },
    {
      "epoch": 4.93,
      "grad_norm": 6183.5302734375,
      "learning_rate": 7.768833849329206e-05,
      "loss": 17.9197,
      "step": 4924
    },
    {
      "epoch": 4.93,
      "grad_norm": 12808.71484375,
      "learning_rate": 7.768317853457174e-05,
      "loss": 13.5989,
      "step": 4925
    },
    {
      "epoch": 4.93,
      "grad_norm": 11492.1767578125,
      "learning_rate": 7.76780185758514e-05,
      "loss": 21.5826,
      "step": 4926
    },
    {
      "epoch": 4.93,
      "grad_norm": 51584.6640625,
      "learning_rate": 7.767285861713106e-05,
      "loss": 29.8726,
      "step": 4927
    },
    {
      "epoch": 4.93,
      "grad_norm": 7789.20703125,
      "learning_rate": 7.766769865841073e-05,
      "loss": 27.7616,
      "step": 4928
    },
    {
      "epoch": 4.93,
      "grad_norm": 4423.1953125,
      "learning_rate": 7.76625386996904e-05,
      "loss": 17.5192,
      "step": 4929
    },
    {
      "epoch": 4.93,
      "grad_norm": 3983.162841796875,
      "learning_rate": 7.765737874097008e-05,
      "loss": 12.9007,
      "step": 4930
    },
    {
      "epoch": 4.94,
      "grad_norm": 5417.7529296875,
      "learning_rate": 7.765221878224974e-05,
      "loss": 12.2645,
      "step": 4931
    },
    {
      "epoch": 4.94,
      "grad_norm": 7343.9423828125,
      "learning_rate": 7.764705882352942e-05,
      "loss": 17.202,
      "step": 4932
    },
    {
      "epoch": 4.94,
      "grad_norm": 2042.5654296875,
      "learning_rate": 7.764189886480908e-05,
      "loss": 12.1622,
      "step": 4933
    },
    {
      "epoch": 4.94,
      "grad_norm": 6053.857421875,
      "learning_rate": 7.763673890608875e-05,
      "loss": 13.3574,
      "step": 4934
    },
    {
      "epoch": 4.94,
      "grad_norm": 4084.65380859375,
      "learning_rate": 7.763157894736843e-05,
      "loss": 13.6335,
      "step": 4935
    },
    {
      "epoch": 4.94,
      "grad_norm": 5675.27685546875,
      "learning_rate": 7.76264189886481e-05,
      "loss": 12.2217,
      "step": 4936
    },
    {
      "epoch": 4.94,
      "grad_norm": 3084.618408203125,
      "learning_rate": 7.762125902992776e-05,
      "loss": 12.8379,
      "step": 4937
    },
    {
      "epoch": 4.94,
      "grad_norm": 7669.93896484375,
      "learning_rate": 7.761609907120744e-05,
      "loss": 17.9957,
      "step": 4938
    },
    {
      "epoch": 4.94,
      "grad_norm": 6001.48291015625,
      "learning_rate": 7.76109391124871e-05,
      "loss": 25.7824,
      "step": 4939
    },
    {
      "epoch": 4.94,
      "grad_norm": 15078.9169921875,
      "learning_rate": 7.760577915376677e-05,
      "loss": 14.0616,
      "step": 4940
    },
    {
      "epoch": 4.95,
      "grad_norm": 5561.50732421875,
      "learning_rate": 7.760061919504645e-05,
      "loss": 14.6117,
      "step": 4941
    },
    {
      "epoch": 4.95,
      "grad_norm": 20689.279296875,
      "learning_rate": 7.759545923632612e-05,
      "loss": 13.3342,
      "step": 4942
    },
    {
      "epoch": 4.95,
      "grad_norm": 5363.07080078125,
      "learning_rate": 7.759029927760578e-05,
      "loss": 16.1236,
      "step": 4943
    },
    {
      "epoch": 4.95,
      "grad_norm": 9368.96875,
      "learning_rate": 7.758513931888546e-05,
      "loss": 13.5862,
      "step": 4944
    },
    {
      "epoch": 4.95,
      "grad_norm": 3332.78662109375,
      "learning_rate": 7.757997936016512e-05,
      "loss": 17.8161,
      "step": 4945
    },
    {
      "epoch": 4.95,
      "grad_norm": 35162.80859375,
      "learning_rate": 7.757481940144478e-05,
      "loss": 16.1241,
      "step": 4946
    },
    {
      "epoch": 4.95,
      "grad_norm": 30153.158203125,
      "learning_rate": 7.756965944272447e-05,
      "loss": 16.3711,
      "step": 4947
    },
    {
      "epoch": 4.95,
      "grad_norm": 2055.4306640625,
      "learning_rate": 7.756449948400413e-05,
      "loss": 13.4012,
      "step": 4948
    },
    {
      "epoch": 4.95,
      "grad_norm": 3666.776123046875,
      "learning_rate": 7.75593395252838e-05,
      "loss": 13.3456,
      "step": 4949
    },
    {
      "epoch": 4.95,
      "grad_norm": 1445.4322509765625,
      "learning_rate": 7.755417956656346e-05,
      "loss": 14.5136,
      "step": 4950
    },
    {
      "epoch": 4.96,
      "grad_norm": 2573.92333984375,
      "learning_rate": 7.754901960784314e-05,
      "loss": 14.9922,
      "step": 4951
    },
    {
      "epoch": 4.96,
      "grad_norm": 7892.2109375,
      "learning_rate": 7.754385964912281e-05,
      "loss": 12.2093,
      "step": 4952
    },
    {
      "epoch": 4.96,
      "grad_norm": 34003.453125,
      "learning_rate": 7.753869969040249e-05,
      "loss": 15.6381,
      "step": 4953
    },
    {
      "epoch": 4.96,
      "grad_norm": 2045.555908203125,
      "learning_rate": 7.753353973168215e-05,
      "loss": 15.6008,
      "step": 4954
    },
    {
      "epoch": 4.96,
      "grad_norm": 9946.6787109375,
      "learning_rate": 7.752837977296182e-05,
      "loss": 21.7748,
      "step": 4955
    },
    {
      "epoch": 4.96,
      "grad_norm": 7150.26611328125,
      "learning_rate": 7.752321981424148e-05,
      "loss": 16.9906,
      "step": 4956
    },
    {
      "epoch": 4.96,
      "grad_norm": 5785.5,
      "learning_rate": 7.751805985552116e-05,
      "loss": 11.0641,
      "step": 4957
    },
    {
      "epoch": 4.96,
      "grad_norm": 2591.847900390625,
      "learning_rate": 7.751289989680083e-05,
      "loss": 13.5279,
      "step": 4958
    },
    {
      "epoch": 4.96,
      "grad_norm": 1414.5052490234375,
      "learning_rate": 7.750773993808051e-05,
      "loss": 12.5628,
      "step": 4959
    },
    {
      "epoch": 4.96,
      "grad_norm": 4416.8046875,
      "learning_rate": 7.750257997936017e-05,
      "loss": 13.5669,
      "step": 4960
    },
    {
      "epoch": 4.97,
      "grad_norm": 11369.4580078125,
      "learning_rate": 7.749742002063984e-05,
      "loss": 19.8833,
      "step": 4961
    },
    {
      "epoch": 4.97,
      "grad_norm": 17492.4609375,
      "learning_rate": 7.74922600619195e-05,
      "loss": 15.5444,
      "step": 4962
    },
    {
      "epoch": 4.97,
      "grad_norm": 7921.78076171875,
      "learning_rate": 7.748710010319918e-05,
      "loss": 15.6872,
      "step": 4963
    },
    {
      "epoch": 4.97,
      "grad_norm": 35108.8125,
      "learning_rate": 7.748194014447885e-05,
      "loss": 18.9238,
      "step": 4964
    },
    {
      "epoch": 4.97,
      "grad_norm": 6508.025390625,
      "learning_rate": 7.747678018575851e-05,
      "loss": 13.5314,
      "step": 4965
    },
    {
      "epoch": 4.97,
      "grad_norm": 4311.9208984375,
      "learning_rate": 7.747162022703819e-05,
      "loss": 12.1346,
      "step": 4966
    },
    {
      "epoch": 4.97,
      "grad_norm": 7966.8779296875,
      "learning_rate": 7.746646026831785e-05,
      "loss": 17.1436,
      "step": 4967
    },
    {
      "epoch": 4.97,
      "grad_norm": 124167.609375,
      "learning_rate": 7.746130030959752e-05,
      "loss": 22.0004,
      "step": 4968
    },
    {
      "epoch": 4.97,
      "grad_norm": 36936.0859375,
      "learning_rate": 7.74561403508772e-05,
      "loss": 18.0695,
      "step": 4969
    },
    {
      "epoch": 4.97,
      "grad_norm": 1757.2042236328125,
      "learning_rate": 7.745098039215687e-05,
      "loss": 25.22,
      "step": 4970
    },
    {
      "epoch": 4.98,
      "grad_norm": 3542.843017578125,
      "learning_rate": 7.744582043343653e-05,
      "loss": 14.3182,
      "step": 4971
    },
    {
      "epoch": 4.98,
      "grad_norm": 1451.4066162109375,
      "learning_rate": 7.744066047471621e-05,
      "loss": 12.1312,
      "step": 4972
    },
    {
      "epoch": 4.98,
      "grad_norm": 25194.021484375,
      "learning_rate": 7.743550051599587e-05,
      "loss": 18.419,
      "step": 4973
    },
    {
      "epoch": 4.98,
      "grad_norm": 17361.9921875,
      "learning_rate": 7.743034055727554e-05,
      "loss": 15.7097,
      "step": 4974
    },
    {
      "epoch": 4.98,
      "grad_norm": 4005.442138671875,
      "learning_rate": 7.742518059855522e-05,
      "loss": 18.2825,
      "step": 4975
    },
    {
      "epoch": 4.98,
      "grad_norm": 2739.7392578125,
      "learning_rate": 7.74200206398349e-05,
      "loss": 16.2691,
      "step": 4976
    },
    {
      "epoch": 4.98,
      "grad_norm": 3755.878173828125,
      "learning_rate": 7.741486068111456e-05,
      "loss": 11.3706,
      "step": 4977
    },
    {
      "epoch": 4.98,
      "grad_norm": 9710.7255859375,
      "learning_rate": 7.740970072239423e-05,
      "loss": 19.9993,
      "step": 4978
    },
    {
      "epoch": 4.98,
      "grad_norm": 13954.5546875,
      "learning_rate": 7.740454076367389e-05,
      "loss": 12.5541,
      "step": 4979
    },
    {
      "epoch": 4.98,
      "grad_norm": 7510.9970703125,
      "learning_rate": 7.739938080495357e-05,
      "loss": 17.3715,
      "step": 4980
    },
    {
      "epoch": 4.99,
      "grad_norm": 5700.9853515625,
      "learning_rate": 7.739422084623324e-05,
      "loss": 15.5484,
      "step": 4981
    },
    {
      "epoch": 4.99,
      "grad_norm": 23800.71875,
      "learning_rate": 7.73890608875129e-05,
      "loss": 12.4372,
      "step": 4982
    },
    {
      "epoch": 4.99,
      "grad_norm": 16036.5380859375,
      "learning_rate": 7.738390092879258e-05,
      "loss": 14.3625,
      "step": 4983
    },
    {
      "epoch": 4.99,
      "grad_norm": 4288.21484375,
      "learning_rate": 7.737874097007224e-05,
      "loss": 13.9506,
      "step": 4984
    },
    {
      "epoch": 4.99,
      "grad_norm": 4175.44482421875,
      "learning_rate": 7.737358101135191e-05,
      "loss": 12.4642,
      "step": 4985
    },
    {
      "epoch": 4.99,
      "grad_norm": 2637.560546875,
      "learning_rate": 7.736842105263159e-05,
      "loss": 12.1772,
      "step": 4986
    },
    {
      "epoch": 4.99,
      "grad_norm": 2350.490478515625,
      "learning_rate": 7.736326109391126e-05,
      "loss": 11.8146,
      "step": 4987
    },
    {
      "epoch": 4.99,
      "grad_norm": 6008.0791015625,
      "learning_rate": 7.735810113519092e-05,
      "loss": 12.5694,
      "step": 4988
    },
    {
      "epoch": 4.99,
      "grad_norm": 35474.9296875,
      "learning_rate": 7.73529411764706e-05,
      "loss": 14.3503,
      "step": 4989
    },
    {
      "epoch": 4.99,
      "grad_norm": 875.72265625,
      "learning_rate": 7.734778121775026e-05,
      "loss": 11.4444,
      "step": 4990
    },
    {
      "epoch": 5.0,
      "grad_norm": 21406.310546875,
      "learning_rate": 7.734262125902993e-05,
      "loss": 17.6111,
      "step": 4991
    },
    {
      "epoch": 5.0,
      "grad_norm": 13233.638671875,
      "learning_rate": 7.73374613003096e-05,
      "loss": 22.3089,
      "step": 4992
    },
    {
      "epoch": 5.0,
      "grad_norm": 2388.962646484375,
      "learning_rate": 7.733230134158928e-05,
      "loss": 12.5976,
      "step": 4993
    },
    {
      "epoch": 5.0,
      "grad_norm": 21409.884765625,
      "learning_rate": 7.732714138286894e-05,
      "loss": 11.7507,
      "step": 4994
    },
    {
      "epoch": 5.0,
      "grad_norm": 135814.875,
      "learning_rate": 7.732198142414862e-05,
      "loss": 12.2299,
      "step": 4995
    },
    {
      "epoch": 5.0,
      "grad_norm": 4265.41650390625,
      "learning_rate": 7.731682146542828e-05,
      "loss": 13.111,
      "step": 4996
    },
    {
      "epoch": 5.0,
      "grad_norm": 2223.744140625,
      "learning_rate": 7.731166150670795e-05,
      "loss": 14.9293,
      "step": 4997
    },
    {
      "epoch": 5.0,
      "grad_norm": 4184.2275390625,
      "learning_rate": 7.730650154798763e-05,
      "loss": 12.4482,
      "step": 4998
    },
    {
      "epoch": 5.0,
      "grad_norm": 5450.64208984375,
      "learning_rate": 7.730134158926729e-05,
      "loss": 16.2968,
      "step": 4999
    },
    {
      "epoch": 5.01,
      "grad_norm": 1553.8929443359375,
      "learning_rate": 7.729618163054696e-05,
      "loss": 12.9139,
      "step": 5000
    },
    {
      "epoch": 5.01,
      "grad_norm": 7400.2666015625,
      "learning_rate": 7.729102167182662e-05,
      "loss": 14.0058,
      "step": 5001
    },
    {
      "epoch": 5.01,
      "grad_norm": 1547.4644775390625,
      "learning_rate": 7.72858617131063e-05,
      "loss": 13.6493,
      "step": 5002
    },
    {
      "epoch": 5.01,
      "grad_norm": 5623.77783203125,
      "learning_rate": 7.728070175438597e-05,
      "loss": 13.0538,
      "step": 5003
    },
    {
      "epoch": 5.01,
      "grad_norm": 2450.24658203125,
      "learning_rate": 7.727554179566565e-05,
      "loss": 13.1926,
      "step": 5004
    },
    {
      "epoch": 5.01,
      "grad_norm": 23174.91796875,
      "learning_rate": 7.72703818369453e-05,
      "loss": 13.892,
      "step": 5005
    },
    {
      "epoch": 5.01,
      "grad_norm": 2532.546875,
      "learning_rate": 7.726522187822498e-05,
      "loss": 11.5952,
      "step": 5006
    },
    {
      "epoch": 5.01,
      "grad_norm": 2630.084228515625,
      "learning_rate": 7.726006191950464e-05,
      "loss": 17.8991,
      "step": 5007
    },
    {
      "epoch": 5.01,
      "grad_norm": 12112.966796875,
      "learning_rate": 7.725490196078432e-05,
      "loss": 14.1522,
      "step": 5008
    },
    {
      "epoch": 5.01,
      "grad_norm": 11046.125,
      "learning_rate": 7.724974200206399e-05,
      "loss": 16.3695,
      "step": 5009
    },
    {
      "epoch": 5.02,
      "grad_norm": 1472.910888671875,
      "learning_rate": 7.724458204334367e-05,
      "loss": 10.6561,
      "step": 5010
    },
    {
      "epoch": 5.02,
      "grad_norm": 3032.68505859375,
      "learning_rate": 7.723942208462333e-05,
      "loss": 12.6518,
      "step": 5011
    },
    {
      "epoch": 5.02,
      "grad_norm": 765.94091796875,
      "learning_rate": 7.7234262125903e-05,
      "loss": 10.6511,
      "step": 5012
    },
    {
      "epoch": 5.02,
      "grad_norm": 5859.7939453125,
      "learning_rate": 7.722910216718266e-05,
      "loss": 17.7747,
      "step": 5013
    },
    {
      "epoch": 5.02,
      "grad_norm": 1051.2906494140625,
      "learning_rate": 7.722394220846234e-05,
      "loss": 14.1359,
      "step": 5014
    },
    {
      "epoch": 5.02,
      "grad_norm": 1617.3095703125,
      "learning_rate": 7.721878224974201e-05,
      "loss": 12.5262,
      "step": 5015
    },
    {
      "epoch": 5.02,
      "grad_norm": 2110.36865234375,
      "learning_rate": 7.721362229102167e-05,
      "loss": 13.7019,
      "step": 5016
    },
    {
      "epoch": 5.02,
      "grad_norm": 1537.9251708984375,
      "learning_rate": 7.720846233230135e-05,
      "loss": 13.0429,
      "step": 5017
    },
    {
      "epoch": 5.02,
      "grad_norm": 66947.59375,
      "learning_rate": 7.720330237358101e-05,
      "loss": 17.7887,
      "step": 5018
    },
    {
      "epoch": 5.02,
      "grad_norm": 5342.1044921875,
      "learning_rate": 7.719814241486068e-05,
      "loss": 15.4973,
      "step": 5019
    },
    {
      "epoch": 5.03,
      "grad_norm": 2405.48291015625,
      "learning_rate": 7.719298245614036e-05,
      "loss": 11.0785,
      "step": 5020
    },
    {
      "epoch": 5.03,
      "grad_norm": 8341.576171875,
      "learning_rate": 7.718782249742003e-05,
      "loss": 20.5037,
      "step": 5021
    },
    {
      "epoch": 5.03,
      "grad_norm": 2565.47509765625,
      "learning_rate": 7.718266253869969e-05,
      "loss": 15.7841,
      "step": 5022
    },
    {
      "epoch": 5.03,
      "grad_norm": 2841.11669921875,
      "learning_rate": 7.717750257997937e-05,
      "loss": 20.0635,
      "step": 5023
    },
    {
      "epoch": 5.03,
      "grad_norm": 3565.20166015625,
      "learning_rate": 7.717234262125903e-05,
      "loss": 10.7737,
      "step": 5024
    },
    {
      "epoch": 5.03,
      "grad_norm": 4422.83935546875,
      "learning_rate": 7.71671826625387e-05,
      "loss": 15.3677,
      "step": 5025
    },
    {
      "epoch": 5.03,
      "grad_norm": 5582.8466796875,
      "learning_rate": 7.716202270381838e-05,
      "loss": 12.0385,
      "step": 5026
    },
    {
      "epoch": 5.03,
      "grad_norm": 4537.80322265625,
      "learning_rate": 7.715686274509805e-05,
      "loss": 15.3154,
      "step": 5027
    },
    {
      "epoch": 5.03,
      "grad_norm": 5177.91015625,
      "learning_rate": 7.715170278637771e-05,
      "loss": 11.4036,
      "step": 5028
    },
    {
      "epoch": 5.03,
      "grad_norm": 2699.278564453125,
      "learning_rate": 7.714654282765739e-05,
      "loss": 13.9411,
      "step": 5029
    },
    {
      "epoch": 5.04,
      "grad_norm": 2699.239990234375,
      "learning_rate": 7.714138286893705e-05,
      "loss": 20.3725,
      "step": 5030
    },
    {
      "epoch": 5.04,
      "grad_norm": 41304.0703125,
      "learning_rate": 7.713622291021672e-05,
      "loss": 15.3728,
      "step": 5031
    },
    {
      "epoch": 5.04,
      "grad_norm": 8572.4150390625,
      "learning_rate": 7.71310629514964e-05,
      "loss": 13.5547,
      "step": 5032
    },
    {
      "epoch": 5.04,
      "grad_norm": 6264.3212890625,
      "learning_rate": 7.712590299277607e-05,
      "loss": 14.5247,
      "step": 5033
    },
    {
      "epoch": 5.04,
      "grad_norm": 1099.46484375,
      "learning_rate": 7.712074303405573e-05,
      "loss": 12.4625,
      "step": 5034
    },
    {
      "epoch": 5.04,
      "grad_norm": 1981.74609375,
      "learning_rate": 7.711558307533539e-05,
      "loss": 14.4874,
      "step": 5035
    },
    {
      "epoch": 5.04,
      "grad_norm": 6298.6416015625,
      "learning_rate": 7.711042311661507e-05,
      "loss": 15.8127,
      "step": 5036
    },
    {
      "epoch": 5.04,
      "grad_norm": 8775.1083984375,
      "learning_rate": 7.710526315789474e-05,
      "loss": 19.516,
      "step": 5037
    },
    {
      "epoch": 5.04,
      "grad_norm": 36118.140625,
      "learning_rate": 7.710010319917442e-05,
      "loss": 18.7884,
      "step": 5038
    },
    {
      "epoch": 5.04,
      "grad_norm": 15155.4091796875,
      "learning_rate": 7.709494324045408e-05,
      "loss": 14.3091,
      "step": 5039
    },
    {
      "epoch": 5.05,
      "grad_norm": 5325.96728515625,
      "learning_rate": 7.708978328173375e-05,
      "loss": 15.1145,
      "step": 5040
    },
    {
      "epoch": 5.05,
      "grad_norm": 22151.0546875,
      "learning_rate": 7.708462332301341e-05,
      "loss": 23.3572,
      "step": 5041
    },
    {
      "epoch": 5.05,
      "grad_norm": 4542.6689453125,
      "learning_rate": 7.707946336429309e-05,
      "loss": 17.7527,
      "step": 5042
    },
    {
      "epoch": 5.05,
      "grad_norm": 5285.8203125,
      "learning_rate": 7.707430340557276e-05,
      "loss": 11.4353,
      "step": 5043
    },
    {
      "epoch": 5.05,
      "grad_norm": 11530.837890625,
      "learning_rate": 7.706914344685244e-05,
      "loss": 14.321,
      "step": 5044
    },
    {
      "epoch": 5.05,
      "grad_norm": 5495.76025390625,
      "learning_rate": 7.70639834881321e-05,
      "loss": 13.8766,
      "step": 5045
    },
    {
      "epoch": 5.05,
      "grad_norm": 13784.1025390625,
      "learning_rate": 7.705882352941177e-05,
      "loss": 17.5338,
      "step": 5046
    },
    {
      "epoch": 5.05,
      "grad_norm": 6893.21875,
      "learning_rate": 7.705366357069143e-05,
      "loss": 12.4276,
      "step": 5047
    },
    {
      "epoch": 5.05,
      "grad_norm": 8536.0302734375,
      "learning_rate": 7.704850361197111e-05,
      "loss": 15.3223,
      "step": 5048
    },
    {
      "epoch": 5.05,
      "grad_norm": 15060.779296875,
      "learning_rate": 7.704334365325078e-05,
      "loss": 16.8876,
      "step": 5049
    },
    {
      "epoch": 5.06,
      "grad_norm": 3318.427978515625,
      "learning_rate": 7.703818369453046e-05,
      "loss": 13.6263,
      "step": 5050
    },
    {
      "epoch": 5.06,
      "grad_norm": 9427.318359375,
      "learning_rate": 7.703302373581012e-05,
      "loss": 14.845,
      "step": 5051
    },
    {
      "epoch": 5.06,
      "grad_norm": 8175.275390625,
      "learning_rate": 7.702786377708978e-05,
      "loss": 11.103,
      "step": 5052
    },
    {
      "epoch": 5.06,
      "grad_norm": 1773.725341796875,
      "learning_rate": 7.702270381836945e-05,
      "loss": 11.5607,
      "step": 5053
    },
    {
      "epoch": 5.06,
      "grad_norm": 6862.98046875,
      "learning_rate": 7.701754385964913e-05,
      "loss": 19.446,
      "step": 5054
    },
    {
      "epoch": 5.06,
      "grad_norm": 4842.49365234375,
      "learning_rate": 7.70123839009288e-05,
      "loss": 13.7859,
      "step": 5055
    },
    {
      "epoch": 5.06,
      "grad_norm": 15031.5234375,
      "learning_rate": 7.700722394220846e-05,
      "loss": 15.4542,
      "step": 5056
    },
    {
      "epoch": 5.06,
      "grad_norm": 2916.510986328125,
      "learning_rate": 7.700206398348814e-05,
      "loss": 11.3063,
      "step": 5057
    },
    {
      "epoch": 5.06,
      "grad_norm": 15488.244140625,
      "learning_rate": 7.69969040247678e-05,
      "loss": 15.1533,
      "step": 5058
    },
    {
      "epoch": 5.06,
      "grad_norm": 23578.3984375,
      "learning_rate": 7.699174406604747e-05,
      "loss": 17.1762,
      "step": 5059
    },
    {
      "epoch": 5.07,
      "grad_norm": 1739.4761962890625,
      "learning_rate": 7.698658410732715e-05,
      "loss": 11.8285,
      "step": 5060
    },
    {
      "epoch": 5.07,
      "grad_norm": 27804.48046875,
      "learning_rate": 7.698142414860682e-05,
      "loss": 12.7647,
      "step": 5061
    },
    {
      "epoch": 5.07,
      "grad_norm": 5809.64697265625,
      "learning_rate": 7.697626418988648e-05,
      "loss": 13.3652,
      "step": 5062
    },
    {
      "epoch": 5.07,
      "grad_norm": 12952.498046875,
      "learning_rate": 7.697110423116616e-05,
      "loss": 25.0303,
      "step": 5063
    },
    {
      "epoch": 5.07,
      "grad_norm": 11110.203125,
      "learning_rate": 7.696594427244582e-05,
      "loss": 16.8956,
      "step": 5064
    },
    {
      "epoch": 5.07,
      "grad_norm": 4589.5791015625,
      "learning_rate": 7.696078431372549e-05,
      "loss": 12.4995,
      "step": 5065
    },
    {
      "epoch": 5.07,
      "grad_norm": 17679.4453125,
      "learning_rate": 7.695562435500517e-05,
      "loss": 14.1273,
      "step": 5066
    },
    {
      "epoch": 5.07,
      "grad_norm": 2816.52490234375,
      "learning_rate": 7.695046439628484e-05,
      "loss": 13.6297,
      "step": 5067
    },
    {
      "epoch": 5.07,
      "grad_norm": 9146.9697265625,
      "learning_rate": 7.69453044375645e-05,
      "loss": 11.7106,
      "step": 5068
    },
    {
      "epoch": 5.07,
      "grad_norm": 40341.109375,
      "learning_rate": 7.694014447884418e-05,
      "loss": 15.7135,
      "step": 5069
    },
    {
      "epoch": 5.08,
      "grad_norm": 20160.771484375,
      "learning_rate": 7.693498452012384e-05,
      "loss": 16.1413,
      "step": 5070
    },
    {
      "epoch": 5.08,
      "grad_norm": 5632.97509765625,
      "learning_rate": 7.692982456140351e-05,
      "loss": 18.7003,
      "step": 5071
    },
    {
      "epoch": 5.08,
      "grad_norm": 4648.04541015625,
      "learning_rate": 7.692466460268319e-05,
      "loss": 15.1511,
      "step": 5072
    },
    {
      "epoch": 5.08,
      "grad_norm": 5000.29052734375,
      "learning_rate": 7.691950464396285e-05,
      "loss": 12.4274,
      "step": 5073
    },
    {
      "epoch": 5.08,
      "grad_norm": 2844.746337890625,
      "learning_rate": 7.691434468524252e-05,
      "loss": 14.0172,
      "step": 5074
    },
    {
      "epoch": 5.08,
      "grad_norm": 9834.6240234375,
      "learning_rate": 7.690918472652218e-05,
      "loss": 15.661,
      "step": 5075
    },
    {
      "epoch": 5.08,
      "grad_norm": 4073.32080078125,
      "learning_rate": 7.690402476780186e-05,
      "loss": 15.4436,
      "step": 5076
    },
    {
      "epoch": 5.08,
      "grad_norm": 4346.19091796875,
      "learning_rate": 7.689886480908153e-05,
      "loss": 11.1882,
      "step": 5077
    },
    {
      "epoch": 5.08,
      "grad_norm": 21149.9296875,
      "learning_rate": 7.689370485036121e-05,
      "loss": 12.1318,
      "step": 5078
    },
    {
      "epoch": 5.08,
      "grad_norm": 1015.1492919921875,
      "learning_rate": 7.688854489164087e-05,
      "loss": 11.7377,
      "step": 5079
    },
    {
      "epoch": 5.09,
      "grad_norm": 19042.5703125,
      "learning_rate": 7.688338493292054e-05,
      "loss": 18.997,
      "step": 5080
    },
    {
      "epoch": 5.09,
      "grad_norm": 2704.9345703125,
      "learning_rate": 7.68782249742002e-05,
      "loss": 13.7169,
      "step": 5081
    },
    {
      "epoch": 5.09,
      "grad_norm": 6273.4970703125,
      "learning_rate": 7.687306501547988e-05,
      "loss": 12.3932,
      "step": 5082
    },
    {
      "epoch": 5.09,
      "grad_norm": 3260.604736328125,
      "learning_rate": 7.686790505675955e-05,
      "loss": 13.0638,
      "step": 5083
    },
    {
      "epoch": 5.09,
      "grad_norm": 20745.869140625,
      "learning_rate": 7.686274509803923e-05,
      "loss": 12.5,
      "step": 5084
    },
    {
      "epoch": 5.09,
      "grad_norm": 2011.6575927734375,
      "learning_rate": 7.685758513931889e-05,
      "loss": 12.1389,
      "step": 5085
    },
    {
      "epoch": 5.09,
      "grad_norm": 769.9874267578125,
      "learning_rate": 7.685242518059856e-05,
      "loss": 11.2408,
      "step": 5086
    },
    {
      "epoch": 5.09,
      "grad_norm": 1796.7958984375,
      "learning_rate": 7.684726522187822e-05,
      "loss": 11.8502,
      "step": 5087
    },
    {
      "epoch": 5.09,
      "grad_norm": 10663.5166015625,
      "learning_rate": 7.68421052631579e-05,
      "loss": 14.4954,
      "step": 5088
    },
    {
      "epoch": 5.09,
      "grad_norm": 5279.96875,
      "learning_rate": 7.683694530443757e-05,
      "loss": 14.8316,
      "step": 5089
    },
    {
      "epoch": 5.1,
      "grad_norm": 1992.1424560546875,
      "learning_rate": 7.683178534571723e-05,
      "loss": 12.8164,
      "step": 5090
    },
    {
      "epoch": 5.1,
      "grad_norm": 9459.16796875,
      "learning_rate": 7.682662538699691e-05,
      "loss": 15.6997,
      "step": 5091
    },
    {
      "epoch": 5.1,
      "grad_norm": 1516.0670166015625,
      "learning_rate": 7.682146542827657e-05,
      "loss": 14.6015,
      "step": 5092
    },
    {
      "epoch": 5.1,
      "grad_norm": 10887.857421875,
      "learning_rate": 7.681630546955624e-05,
      "loss": 18.3773,
      "step": 5093
    },
    {
      "epoch": 5.1,
      "grad_norm": 7363.6533203125,
      "learning_rate": 7.681114551083592e-05,
      "loss": 15.5135,
      "step": 5094
    },
    {
      "epoch": 5.1,
      "grad_norm": 2315.5751953125,
      "learning_rate": 7.680598555211559e-05,
      "loss": 12.9893,
      "step": 5095
    },
    {
      "epoch": 5.1,
      "grad_norm": 2344.2216796875,
      "learning_rate": 7.680082559339525e-05,
      "loss": 15.259,
      "step": 5096
    },
    {
      "epoch": 5.1,
      "grad_norm": 40082.59375,
      "learning_rate": 7.679566563467493e-05,
      "loss": 19.4746,
      "step": 5097
    },
    {
      "epoch": 5.1,
      "grad_norm": 2823.039306640625,
      "learning_rate": 7.679050567595459e-05,
      "loss": 11.5807,
      "step": 5098
    },
    {
      "epoch": 5.1,
      "grad_norm": 82863.203125,
      "learning_rate": 7.678534571723426e-05,
      "loss": 17.4693,
      "step": 5099
    },
    {
      "epoch": 5.11,
      "grad_norm": 4191.5380859375,
      "learning_rate": 7.678018575851394e-05,
      "loss": 10.5568,
      "step": 5100
    },
    {
      "epoch": 5.11,
      "grad_norm": 998.3665161132812,
      "learning_rate": 7.677502579979361e-05,
      "loss": 11.2962,
      "step": 5101
    },
    {
      "epoch": 5.11,
      "grad_norm": 76974.2890625,
      "learning_rate": 7.676986584107327e-05,
      "loss": 15.0817,
      "step": 5102
    },
    {
      "epoch": 5.11,
      "grad_norm": 4135.9248046875,
      "learning_rate": 7.676470588235295e-05,
      "loss": 16.2881,
      "step": 5103
    },
    {
      "epoch": 5.11,
      "grad_norm": 3965.62158203125,
      "learning_rate": 7.675954592363261e-05,
      "loss": 15.5254,
      "step": 5104
    },
    {
      "epoch": 5.11,
      "grad_norm": 7147.0810546875,
      "learning_rate": 7.675438596491228e-05,
      "loss": 13.5033,
      "step": 5105
    },
    {
      "epoch": 5.11,
      "grad_norm": 52931.953125,
      "learning_rate": 7.674922600619196e-05,
      "loss": 21.0595,
      "step": 5106
    },
    {
      "epoch": 5.11,
      "grad_norm": 9763.9501953125,
      "learning_rate": 7.674406604747162e-05,
      "loss": 23.1709,
      "step": 5107
    },
    {
      "epoch": 5.11,
      "grad_norm": 5124.71240234375,
      "learning_rate": 7.67389060887513e-05,
      "loss": 32.5087,
      "step": 5108
    },
    {
      "epoch": 5.11,
      "grad_norm": 6286.31005859375,
      "learning_rate": 7.673374613003095e-05,
      "loss": 13.3421,
      "step": 5109
    },
    {
      "epoch": 5.12,
      "grad_norm": 3844.920166015625,
      "learning_rate": 7.672858617131063e-05,
      "loss": 13.5938,
      "step": 5110
    },
    {
      "epoch": 5.12,
      "grad_norm": 3519.061279296875,
      "learning_rate": 7.67234262125903e-05,
      "loss": 15.293,
      "step": 5111
    },
    {
      "epoch": 5.12,
      "grad_norm": 1562.180419921875,
      "learning_rate": 7.671826625386998e-05,
      "loss": 18.3109,
      "step": 5112
    },
    {
      "epoch": 5.12,
      "grad_norm": 2920.451904296875,
      "learning_rate": 7.671310629514964e-05,
      "loss": 13.1487,
      "step": 5113
    },
    {
      "epoch": 5.12,
      "grad_norm": 962.5162963867188,
      "learning_rate": 7.670794633642931e-05,
      "loss": 12.5214,
      "step": 5114
    },
    {
      "epoch": 5.12,
      "grad_norm": 1313.7901611328125,
      "learning_rate": 7.670278637770897e-05,
      "loss": 13.7662,
      "step": 5115
    },
    {
      "epoch": 5.12,
      "grad_norm": 12191.69921875,
      "learning_rate": 7.669762641898865e-05,
      "loss": 17.2271,
      "step": 5116
    },
    {
      "epoch": 5.12,
      "grad_norm": 3140.635498046875,
      "learning_rate": 7.669246646026832e-05,
      "loss": 10.6087,
      "step": 5117
    },
    {
      "epoch": 5.12,
      "grad_norm": 8512.3291015625,
      "learning_rate": 7.6687306501548e-05,
      "loss": 15.2433,
      "step": 5118
    },
    {
      "epoch": 5.12,
      "grad_norm": 5113.0244140625,
      "learning_rate": 7.668214654282766e-05,
      "loss": 16.5081,
      "step": 5119
    },
    {
      "epoch": 5.13,
      "grad_norm": 5787.32763671875,
      "learning_rate": 7.667698658410733e-05,
      "loss": 17.9931,
      "step": 5120
    },
    {
      "epoch": 5.13,
      "grad_norm": 1779.9560546875,
      "learning_rate": 7.6671826625387e-05,
      "loss": 10.7811,
      "step": 5121
    },
    {
      "epoch": 5.13,
      "grad_norm": 4397.5693359375,
      "learning_rate": 7.666666666666667e-05,
      "loss": 19.7702,
      "step": 5122
    },
    {
      "epoch": 5.13,
      "grad_norm": 14761.828125,
      "learning_rate": 7.666150670794634e-05,
      "loss": 19.3416,
      "step": 5123
    },
    {
      "epoch": 5.13,
      "grad_norm": 8289.775390625,
      "learning_rate": 7.6656346749226e-05,
      "loss": 20.4527,
      "step": 5124
    },
    {
      "epoch": 5.13,
      "grad_norm": 21334.03515625,
      "learning_rate": 7.665118679050568e-05,
      "loss": 19.3268,
      "step": 5125
    },
    {
      "epoch": 5.13,
      "grad_norm": 2474.900390625,
      "learning_rate": 7.664602683178534e-05,
      "loss": 13.7233,
      "step": 5126
    },
    {
      "epoch": 5.13,
      "grad_norm": 6955.8115234375,
      "learning_rate": 7.664086687306501e-05,
      "loss": 12.6976,
      "step": 5127
    },
    {
      "epoch": 5.13,
      "grad_norm": 6924.166015625,
      "learning_rate": 7.663570691434469e-05,
      "loss": 24.6009,
      "step": 5128
    },
    {
      "epoch": 5.13,
      "grad_norm": 1275.7943115234375,
      "learning_rate": 7.663054695562436e-05,
      "loss": 14.0041,
      "step": 5129
    },
    {
      "epoch": 5.14,
      "grad_norm": 19357.166015625,
      "learning_rate": 7.662538699690402e-05,
      "loss": 13.7356,
      "step": 5130
    },
    {
      "epoch": 5.14,
      "grad_norm": 3268.043212890625,
      "learning_rate": 7.66202270381837e-05,
      "loss": 13.5967,
      "step": 5131
    },
    {
      "epoch": 5.14,
      "grad_norm": 22204.55078125,
      "learning_rate": 7.661506707946336e-05,
      "loss": 24.371,
      "step": 5132
    },
    {
      "epoch": 5.14,
      "grad_norm": 19478.2890625,
      "learning_rate": 7.660990712074303e-05,
      "loss": 14.5506,
      "step": 5133
    },
    {
      "epoch": 5.14,
      "grad_norm": 5817.5576171875,
      "learning_rate": 7.660474716202271e-05,
      "loss": 24.8932,
      "step": 5134
    },
    {
      "epoch": 5.14,
      "grad_norm": 5167.2421875,
      "learning_rate": 7.659958720330238e-05,
      "loss": 16.4417,
      "step": 5135
    },
    {
      "epoch": 5.14,
      "grad_norm": 17322.869140625,
      "learning_rate": 7.659442724458204e-05,
      "loss": 18.3692,
      "step": 5136
    },
    {
      "epoch": 5.14,
      "grad_norm": 3009.9384765625,
      "learning_rate": 7.658926728586172e-05,
      "loss": 13.0965,
      "step": 5137
    },
    {
      "epoch": 5.14,
      "grad_norm": 11339.5498046875,
      "learning_rate": 7.658410732714138e-05,
      "loss": 12.8626,
      "step": 5138
    },
    {
      "epoch": 5.14,
      "grad_norm": 40110.1796875,
      "learning_rate": 7.657894736842105e-05,
      "loss": 18.2925,
      "step": 5139
    },
    {
      "epoch": 5.15,
      "grad_norm": 1310.656005859375,
      "learning_rate": 7.657378740970073e-05,
      "loss": 13.8699,
      "step": 5140
    },
    {
      "epoch": 5.15,
      "grad_norm": 3119.82666015625,
      "learning_rate": 7.65686274509804e-05,
      "loss": 12.6659,
      "step": 5141
    },
    {
      "epoch": 5.15,
      "grad_norm": 10630.8720703125,
      "learning_rate": 7.656346749226006e-05,
      "loss": 15.7828,
      "step": 5142
    },
    {
      "epoch": 5.15,
      "grad_norm": 6472.1005859375,
      "learning_rate": 7.655830753353973e-05,
      "loss": 15.9275,
      "step": 5143
    },
    {
      "epoch": 5.15,
      "grad_norm": 2377.933837890625,
      "learning_rate": 7.65531475748194e-05,
      "loss": 11.0952,
      "step": 5144
    },
    {
      "epoch": 5.15,
      "grad_norm": 7857.98779296875,
      "learning_rate": 7.654798761609907e-05,
      "loss": 17.4155,
      "step": 5145
    },
    {
      "epoch": 5.15,
      "grad_norm": 6538.52783203125,
      "learning_rate": 7.654282765737875e-05,
      "loss": 14.3944,
      "step": 5146
    },
    {
      "epoch": 5.15,
      "grad_norm": 14464.1484375,
      "learning_rate": 7.653766769865841e-05,
      "loss": 12.3628,
      "step": 5147
    },
    {
      "epoch": 5.15,
      "grad_norm": 5697.4638671875,
      "learning_rate": 7.653250773993808e-05,
      "loss": 12.7061,
      "step": 5148
    },
    {
      "epoch": 5.15,
      "grad_norm": 5445.791015625,
      "learning_rate": 7.652734778121775e-05,
      "loss": 13.2693,
      "step": 5149
    },
    {
      "epoch": 5.16,
      "grad_norm": 1532.989990234375,
      "learning_rate": 7.652218782249742e-05,
      "loss": 10.5407,
      "step": 5150
    },
    {
      "epoch": 5.16,
      "grad_norm": 6352.12255859375,
      "learning_rate": 7.65170278637771e-05,
      "loss": 14.2311,
      "step": 5151
    },
    {
      "epoch": 5.16,
      "grad_norm": 9236.9384765625,
      "learning_rate": 7.651186790505677e-05,
      "loss": 17.4472,
      "step": 5152
    },
    {
      "epoch": 5.16,
      "grad_norm": 4836.619140625,
      "learning_rate": 7.650670794633643e-05,
      "loss": 16.8158,
      "step": 5153
    },
    {
      "epoch": 5.16,
      "grad_norm": 3151.73291015625,
      "learning_rate": 7.65015479876161e-05,
      "loss": 13.7414,
      "step": 5154
    },
    {
      "epoch": 5.16,
      "grad_norm": 981.6922607421875,
      "learning_rate": 7.649638802889577e-05,
      "loss": 14.7615,
      "step": 5155
    },
    {
      "epoch": 5.16,
      "grad_norm": 21608.685546875,
      "learning_rate": 7.649122807017545e-05,
      "loss": 12.2141,
      "step": 5156
    },
    {
      "epoch": 5.16,
      "grad_norm": 4412.7294921875,
      "learning_rate": 7.648606811145511e-05,
      "loss": 13.9413,
      "step": 5157
    },
    {
      "epoch": 5.16,
      "grad_norm": 3836.585205078125,
      "learning_rate": 7.648090815273479e-05,
      "loss": 13.9925,
      "step": 5158
    },
    {
      "epoch": 5.16,
      "grad_norm": 2033.314697265625,
      "learning_rate": 7.647574819401445e-05,
      "loss": 12.9702,
      "step": 5159
    },
    {
      "epoch": 5.17,
      "grad_norm": 5483.6728515625,
      "learning_rate": 7.647058823529411e-05,
      "loss": 19.8062,
      "step": 5160
    },
    {
      "epoch": 5.17,
      "grad_norm": 5407.18359375,
      "learning_rate": 7.646542827657379e-05,
      "loss": 16.2178,
      "step": 5161
    },
    {
      "epoch": 5.17,
      "grad_norm": 902.8803100585938,
      "learning_rate": 7.646026831785346e-05,
      "loss": 11.7592,
      "step": 5162
    },
    {
      "epoch": 5.17,
      "grad_norm": 4215.013671875,
      "learning_rate": 7.645510835913313e-05,
      "loss": 13.9537,
      "step": 5163
    },
    {
      "epoch": 5.17,
      "grad_norm": 3750.79052734375,
      "learning_rate": 7.64499484004128e-05,
      "loss": 14.5599,
      "step": 5164
    },
    {
      "epoch": 5.17,
      "grad_norm": 9785.8291015625,
      "learning_rate": 7.644478844169247e-05,
      "loss": 17.6518,
      "step": 5165
    },
    {
      "epoch": 5.17,
      "grad_norm": 5850.6396484375,
      "learning_rate": 7.643962848297213e-05,
      "loss": 15.7482,
      "step": 5166
    },
    {
      "epoch": 5.17,
      "grad_norm": 11901.2783203125,
      "learning_rate": 7.64344685242518e-05,
      "loss": 13.4099,
      "step": 5167
    },
    {
      "epoch": 5.17,
      "grad_norm": 707.6355590820312,
      "learning_rate": 7.642930856553148e-05,
      "loss": 14.2063,
      "step": 5168
    },
    {
      "epoch": 5.17,
      "grad_norm": 2033.059814453125,
      "learning_rate": 7.642414860681115e-05,
      "loss": 15.2987,
      "step": 5169
    },
    {
      "epoch": 5.18,
      "grad_norm": 3536.371826171875,
      "learning_rate": 7.641898864809082e-05,
      "loss": 14.8113,
      "step": 5170
    },
    {
      "epoch": 5.18,
      "grad_norm": 15867.2841796875,
      "learning_rate": 7.641382868937049e-05,
      "loss": 16.7569,
      "step": 5171
    },
    {
      "epoch": 5.18,
      "grad_norm": 19684.169921875,
      "learning_rate": 7.640866873065015e-05,
      "loss": 15.3045,
      "step": 5172
    },
    {
      "epoch": 5.18,
      "grad_norm": 3736.7861328125,
      "learning_rate": 7.640350877192984e-05,
      "loss": 14.8998,
      "step": 5173
    },
    {
      "epoch": 5.18,
      "grad_norm": 2360.098876953125,
      "learning_rate": 7.63983488132095e-05,
      "loss": 12.85,
      "step": 5174
    },
    {
      "epoch": 5.18,
      "grad_norm": 3252.184326171875,
      "learning_rate": 7.639318885448917e-05,
      "loss": 10.1717,
      "step": 5175
    },
    {
      "epoch": 5.18,
      "grad_norm": 3163.3505859375,
      "learning_rate": 7.638802889576884e-05,
      "loss": 12.3287,
      "step": 5176
    },
    {
      "epoch": 5.18,
      "grad_norm": 20172.255859375,
      "learning_rate": 7.63828689370485e-05,
      "loss": 13.128,
      "step": 5177
    },
    {
      "epoch": 5.18,
      "grad_norm": 48761.5625,
      "learning_rate": 7.637770897832817e-05,
      "loss": 17.4404,
      "step": 5178
    },
    {
      "epoch": 5.18,
      "grad_norm": 8917.8505859375,
      "learning_rate": 7.637254901960785e-05,
      "loss": 13.3288,
      "step": 5179
    },
    {
      "epoch": 5.19,
      "grad_norm": 4559.3955078125,
      "learning_rate": 7.636738906088752e-05,
      "loss": 12.2883,
      "step": 5180
    },
    {
      "epoch": 5.19,
      "grad_norm": 2641.744384765625,
      "learning_rate": 7.636222910216718e-05,
      "loss": 15.0953,
      "step": 5181
    },
    {
      "epoch": 5.19,
      "grad_norm": 2385.256103515625,
      "learning_rate": 7.635706914344686e-05,
      "loss": 11.0841,
      "step": 5182
    },
    {
      "epoch": 5.19,
      "grad_norm": 14036.82421875,
      "learning_rate": 7.635190918472652e-05,
      "loss": 16.6331,
      "step": 5183
    },
    {
      "epoch": 5.19,
      "grad_norm": 1826.4996337890625,
      "learning_rate": 7.63467492260062e-05,
      "loss": 11.8287,
      "step": 5184
    },
    {
      "epoch": 5.19,
      "grad_norm": 12090.5283203125,
      "learning_rate": 7.634158926728587e-05,
      "loss": 16.1433,
      "step": 5185
    },
    {
      "epoch": 5.19,
      "grad_norm": 5651.046875,
      "learning_rate": 7.633642930856554e-05,
      "loss": 12.9429,
      "step": 5186
    },
    {
      "epoch": 5.19,
      "grad_norm": 3590.156494140625,
      "learning_rate": 7.63312693498452e-05,
      "loss": 13.0701,
      "step": 5187
    },
    {
      "epoch": 5.19,
      "grad_norm": 1499.446533203125,
      "learning_rate": 7.632610939112488e-05,
      "loss": 12.8868,
      "step": 5188
    },
    {
      "epoch": 5.19,
      "grad_norm": 10493.52734375,
      "learning_rate": 7.632094943240454e-05,
      "loss": 13.8686,
      "step": 5189
    },
    {
      "epoch": 5.2,
      "grad_norm": 6909.3671875,
      "learning_rate": 7.631578947368422e-05,
      "loss": 12.0846,
      "step": 5190
    },
    {
      "epoch": 5.2,
      "grad_norm": 883.0277709960938,
      "learning_rate": 7.631062951496389e-05,
      "loss": 12.9694,
      "step": 5191
    },
    {
      "epoch": 5.2,
      "grad_norm": 9834.4697265625,
      "learning_rate": 7.630546955624356e-05,
      "loss": 13.6738,
      "step": 5192
    },
    {
      "epoch": 5.2,
      "grad_norm": 3289.852783203125,
      "learning_rate": 7.630030959752322e-05,
      "loss": 11.7726,
      "step": 5193
    },
    {
      "epoch": 5.2,
      "grad_norm": 2788.83203125,
      "learning_rate": 7.62951496388029e-05,
      "loss": 12.9016,
      "step": 5194
    },
    {
      "epoch": 5.2,
      "grad_norm": 6021.88818359375,
      "learning_rate": 7.628998968008256e-05,
      "loss": 18.6362,
      "step": 5195
    },
    {
      "epoch": 5.2,
      "grad_norm": 4366.44384765625,
      "learning_rate": 7.628482972136223e-05,
      "loss": 15.7571,
      "step": 5196
    },
    {
      "epoch": 5.2,
      "grad_norm": 10548.13671875,
      "learning_rate": 7.62796697626419e-05,
      "loss": 22.2828,
      "step": 5197
    },
    {
      "epoch": 5.2,
      "grad_norm": 1176.377685546875,
      "learning_rate": 7.627450980392157e-05,
      "loss": 10.8024,
      "step": 5198
    },
    {
      "epoch": 5.2,
      "grad_norm": 489.3620910644531,
      "learning_rate": 7.626934984520124e-05,
      "loss": 11.7489,
      "step": 5199
    },
    {
      "epoch": 5.21,
      "grad_norm": 3871.5537109375,
      "learning_rate": 7.62641898864809e-05,
      "loss": 15.9331,
      "step": 5200
    },
    {
      "epoch": 5.21,
      "grad_norm": 5812.13232421875,
      "learning_rate": 7.625902992776059e-05,
      "loss": 12.9939,
      "step": 5201
    },
    {
      "epoch": 5.21,
      "grad_norm": 6997.47265625,
      "learning_rate": 7.625386996904025e-05,
      "loss": 21.0732,
      "step": 5202
    },
    {
      "epoch": 5.21,
      "grad_norm": 15136.2685546875,
      "learning_rate": 7.624871001031993e-05,
      "loss": 12.8465,
      "step": 5203
    },
    {
      "epoch": 5.21,
      "grad_norm": 2274.726318359375,
      "learning_rate": 7.624355005159959e-05,
      "loss": 12.1609,
      "step": 5204
    },
    {
      "epoch": 5.21,
      "grad_norm": 5607.01025390625,
      "learning_rate": 7.623839009287926e-05,
      "loss": 17.9436,
      "step": 5205
    },
    {
      "epoch": 5.21,
      "grad_norm": 11420.62890625,
      "learning_rate": 7.623323013415892e-05,
      "loss": 17.9393,
      "step": 5206
    },
    {
      "epoch": 5.21,
      "grad_norm": 1841.4815673828125,
      "learning_rate": 7.622807017543861e-05,
      "loss": 13.4391,
      "step": 5207
    },
    {
      "epoch": 5.21,
      "grad_norm": 4402.25537109375,
      "learning_rate": 7.622291021671827e-05,
      "loss": 13.02,
      "step": 5208
    },
    {
      "epoch": 5.21,
      "grad_norm": 1160.7705078125,
      "learning_rate": 7.621775025799795e-05,
      "loss": 10.8105,
      "step": 5209
    },
    {
      "epoch": 5.22,
      "grad_norm": 4614.3818359375,
      "learning_rate": 7.62125902992776e-05,
      "loss": 11.9197,
      "step": 5210
    },
    {
      "epoch": 5.22,
      "grad_norm": 4880.8515625,
      "learning_rate": 7.620743034055728e-05,
      "loss": 13.2265,
      "step": 5211
    },
    {
      "epoch": 5.22,
      "grad_norm": 25212.59375,
      "learning_rate": 7.620227038183696e-05,
      "loss": 13.3912,
      "step": 5212
    },
    {
      "epoch": 5.22,
      "grad_norm": 11857.310546875,
      "learning_rate": 7.619711042311662e-05,
      "loss": 12.2957,
      "step": 5213
    },
    {
      "epoch": 5.22,
      "grad_norm": 3247.136962890625,
      "learning_rate": 7.619195046439629e-05,
      "loss": 17.8276,
      "step": 5214
    },
    {
      "epoch": 5.22,
      "grad_norm": 351.4544982910156,
      "learning_rate": 7.618679050567595e-05,
      "loss": 16.1019,
      "step": 5215
    },
    {
      "epoch": 5.22,
      "grad_norm": 11988.2294921875,
      "learning_rate": 7.618163054695563e-05,
      "loss": 18.7636,
      "step": 5216
    },
    {
      "epoch": 5.22,
      "grad_norm": 3384.135009765625,
      "learning_rate": 7.617647058823529e-05,
      "loss": 13.6944,
      "step": 5217
    },
    {
      "epoch": 5.22,
      "grad_norm": 2488.51025390625,
      "learning_rate": 7.617131062951498e-05,
      "loss": 11.6702,
      "step": 5218
    },
    {
      "epoch": 5.22,
      "grad_norm": 444.4573669433594,
      "learning_rate": 7.616615067079464e-05,
      "loss": 10.8706,
      "step": 5219
    },
    {
      "epoch": 5.23,
      "grad_norm": 8436.4658203125,
      "learning_rate": 7.616099071207431e-05,
      "loss": 14.3832,
      "step": 5220
    },
    {
      "epoch": 5.23,
      "grad_norm": 4990.546875,
      "learning_rate": 7.615583075335397e-05,
      "loss": 22.1064,
      "step": 5221
    },
    {
      "epoch": 5.23,
      "grad_norm": 8177.82861328125,
      "learning_rate": 7.615067079463365e-05,
      "loss": 19.4587,
      "step": 5222
    },
    {
      "epoch": 5.23,
      "grad_norm": 4994.6572265625,
      "learning_rate": 7.614551083591331e-05,
      "loss": 12.7253,
      "step": 5223
    },
    {
      "epoch": 5.23,
      "grad_norm": 1413.7691650390625,
      "learning_rate": 7.6140350877193e-05,
      "loss": 10.8113,
      "step": 5224
    },
    {
      "epoch": 5.23,
      "grad_norm": 6992.06640625,
      "learning_rate": 7.613519091847266e-05,
      "loss": 11.4304,
      "step": 5225
    },
    {
      "epoch": 5.23,
      "grad_norm": 7239.939453125,
      "learning_rate": 7.613003095975233e-05,
      "loss": 12.61,
      "step": 5226
    },
    {
      "epoch": 5.23,
      "grad_norm": 13268.68359375,
      "learning_rate": 7.612487100103199e-05,
      "loss": 16.7206,
      "step": 5227
    },
    {
      "epoch": 5.23,
      "grad_norm": 6716.671875,
      "learning_rate": 7.611971104231167e-05,
      "loss": 12.8333,
      "step": 5228
    },
    {
      "epoch": 5.23,
      "grad_norm": 45692.51953125,
      "learning_rate": 7.611455108359134e-05,
      "loss": 14.0873,
      "step": 5229
    },
    {
      "epoch": 5.24,
      "grad_norm": 5622.73291015625,
      "learning_rate": 7.610939112487102e-05,
      "loss": 11.0673,
      "step": 5230
    },
    {
      "epoch": 5.24,
      "grad_norm": 4686.29541015625,
      "learning_rate": 7.610423116615068e-05,
      "loss": 12.3688,
      "step": 5231
    },
    {
      "epoch": 5.24,
      "grad_norm": 1977.9912109375,
      "learning_rate": 7.609907120743034e-05,
      "loss": 16.2629,
      "step": 5232
    },
    {
      "epoch": 5.24,
      "grad_norm": 797.1890258789062,
      "learning_rate": 7.609391124871001e-05,
      "loss": 10.6001,
      "step": 5233
    },
    {
      "epoch": 5.24,
      "grad_norm": 2777.36865234375,
      "learning_rate": 7.608875128998967e-05,
      "loss": 13.6103,
      "step": 5234
    },
    {
      "epoch": 5.24,
      "grad_norm": 4473.61962890625,
      "learning_rate": 7.608359133126936e-05,
      "loss": 16.7624,
      "step": 5235
    },
    {
      "epoch": 5.24,
      "grad_norm": 2122.66064453125,
      "learning_rate": 7.607843137254902e-05,
      "loss": 13.9005,
      "step": 5236
    },
    {
      "epoch": 5.24,
      "grad_norm": 14704.5068359375,
      "learning_rate": 7.60732714138287e-05,
      "loss": 15.3516,
      "step": 5237
    },
    {
      "epoch": 5.24,
      "grad_norm": 1161.229736328125,
      "learning_rate": 7.606811145510836e-05,
      "loss": 12.8085,
      "step": 5238
    },
    {
      "epoch": 5.24,
      "grad_norm": 6346.85009765625,
      "learning_rate": 7.606295149638803e-05,
      "loss": 12.3595,
      "step": 5239
    },
    {
      "epoch": 5.25,
      "grad_norm": 4430.236328125,
      "learning_rate": 7.605779153766769e-05,
      "loss": 13.3286,
      "step": 5240
    },
    {
      "epoch": 5.25,
      "grad_norm": 29306.783203125,
      "learning_rate": 7.605263157894738e-05,
      "loss": 15.8803,
      "step": 5241
    },
    {
      "epoch": 5.25,
      "grad_norm": 6493.98583984375,
      "learning_rate": 7.604747162022704e-05,
      "loss": 14.6493,
      "step": 5242
    },
    {
      "epoch": 5.25,
      "grad_norm": 7717.30908203125,
      "learning_rate": 7.604231166150672e-05,
      "loss": 16.7051,
      "step": 5243
    },
    {
      "epoch": 5.25,
      "grad_norm": 905.2557373046875,
      "learning_rate": 7.603715170278638e-05,
      "loss": 10.7922,
      "step": 5244
    },
    {
      "epoch": 5.25,
      "grad_norm": 2654.236083984375,
      "learning_rate": 7.603199174406605e-05,
      "loss": 19.3285,
      "step": 5245
    },
    {
      "epoch": 5.25,
      "grad_norm": 653.6224975585938,
      "learning_rate": 7.602683178534573e-05,
      "loss": 11.9304,
      "step": 5246
    },
    {
      "epoch": 5.25,
      "grad_norm": 9557.92578125,
      "learning_rate": 7.60216718266254e-05,
      "loss": 17.1465,
      "step": 5247
    },
    {
      "epoch": 5.25,
      "grad_norm": 7717.84033203125,
      "learning_rate": 7.601651186790506e-05,
      "loss": 14.2696,
      "step": 5248
    },
    {
      "epoch": 5.25,
      "grad_norm": 4895.90478515625,
      "learning_rate": 7.601135190918472e-05,
      "loss": 11.4736,
      "step": 5249
    },
    {
      "epoch": 5.26,
      "grad_norm": 8679.8955078125,
      "learning_rate": 7.60061919504644e-05,
      "loss": 14.9178,
      "step": 5250
    },
    {
      "epoch": 5.26,
      "grad_norm": 183724.125,
      "learning_rate": 7.600103199174406e-05,
      "loss": 29.2578,
      "step": 5251
    },
    {
      "epoch": 5.26,
      "grad_norm": 3374.025146484375,
      "learning_rate": 7.599587203302375e-05,
      "loss": 13.2389,
      "step": 5252
    },
    {
      "epoch": 5.26,
      "grad_norm": 4293.85107421875,
      "learning_rate": 7.599071207430341e-05,
      "loss": 17.1284,
      "step": 5253
    },
    {
      "epoch": 5.26,
      "grad_norm": 12962.4765625,
      "learning_rate": 7.598555211558308e-05,
      "loss": 13.0572,
      "step": 5254
    },
    {
      "epoch": 5.26,
      "grad_norm": 3462.71240234375,
      "learning_rate": 7.598039215686274e-05,
      "loss": 11.7353,
      "step": 5255
    },
    {
      "epoch": 5.26,
      "grad_norm": 4746.0322265625,
      "learning_rate": 7.597523219814242e-05,
      "loss": 18.175,
      "step": 5256
    },
    {
      "epoch": 5.26,
      "grad_norm": 4374.43115234375,
      "learning_rate": 7.597007223942209e-05,
      "loss": 11.1945,
      "step": 5257
    },
    {
      "epoch": 5.26,
      "grad_norm": 4722.06298828125,
      "learning_rate": 7.596491228070177e-05,
      "loss": 13.3884,
      "step": 5258
    },
    {
      "epoch": 5.26,
      "grad_norm": 13359.810546875,
      "learning_rate": 7.595975232198143e-05,
      "loss": 18.5385,
      "step": 5259
    },
    {
      "epoch": 5.27,
      "grad_norm": 6924.58447265625,
      "learning_rate": 7.59545923632611e-05,
      "loss": 15.0826,
      "step": 5260
    },
    {
      "epoch": 5.27,
      "grad_norm": 19409.17578125,
      "learning_rate": 7.594943240454076e-05,
      "loss": 19.9919,
      "step": 5261
    },
    {
      "epoch": 5.27,
      "grad_norm": 14029.4111328125,
      "learning_rate": 7.594427244582044e-05,
      "loss": 12.331,
      "step": 5262
    },
    {
      "epoch": 5.27,
      "grad_norm": 30841.740234375,
      "learning_rate": 7.593911248710011e-05,
      "loss": 19.761,
      "step": 5263
    },
    {
      "epoch": 5.27,
      "grad_norm": 7565.791015625,
      "learning_rate": 7.593395252837979e-05,
      "loss": 12.6063,
      "step": 5264
    },
    {
      "epoch": 5.27,
      "grad_norm": 6177.8642578125,
      "learning_rate": 7.592879256965945e-05,
      "loss": 13.6575,
      "step": 5265
    },
    {
      "epoch": 5.27,
      "grad_norm": 51747.71484375,
      "learning_rate": 7.592363261093912e-05,
      "loss": 15.6075,
      "step": 5266
    },
    {
      "epoch": 5.27,
      "grad_norm": 1910.4158935546875,
      "learning_rate": 7.591847265221878e-05,
      "loss": 15.0752,
      "step": 5267
    },
    {
      "epoch": 5.27,
      "grad_norm": 6874.484375,
      "learning_rate": 7.591331269349844e-05,
      "loss": 12.9925,
      "step": 5268
    },
    {
      "epoch": 5.27,
      "grad_norm": 2258.56591796875,
      "learning_rate": 7.590815273477813e-05,
      "loss": 14.6522,
      "step": 5269
    },
    {
      "epoch": 5.28,
      "grad_norm": 1756.4219970703125,
      "learning_rate": 7.590299277605779e-05,
      "loss": 12.4443,
      "step": 5270
    },
    {
      "epoch": 5.28,
      "grad_norm": 753.8267211914062,
      "learning_rate": 7.589783281733747e-05,
      "loss": 12.241,
      "step": 5271
    },
    {
      "epoch": 5.28,
      "grad_norm": 5806.470703125,
      "learning_rate": 7.589267285861713e-05,
      "loss": 15.1695,
      "step": 5272
    },
    {
      "epoch": 5.28,
      "grad_norm": 2307.47607421875,
      "learning_rate": 7.58875128998968e-05,
      "loss": 11.9259,
      "step": 5273
    },
    {
      "epoch": 5.28,
      "grad_norm": 7961.5947265625,
      "learning_rate": 7.588235294117648e-05,
      "loss": 15.4765,
      "step": 5274
    },
    {
      "epoch": 5.28,
      "grad_norm": 12621.22265625,
      "learning_rate": 7.587719298245615e-05,
      "loss": 14.748,
      "step": 5275
    },
    {
      "epoch": 5.28,
      "grad_norm": 4017.526611328125,
      "learning_rate": 7.587203302373581e-05,
      "loss": 17.3689,
      "step": 5276
    },
    {
      "epoch": 5.28,
      "grad_norm": 711.9181518554688,
      "learning_rate": 7.586687306501549e-05,
      "loss": 11.4456,
      "step": 5277
    },
    {
      "epoch": 5.28,
      "grad_norm": 2424.99072265625,
      "learning_rate": 7.586171310629515e-05,
      "loss": 13.8714,
      "step": 5278
    },
    {
      "epoch": 5.28,
      "grad_norm": 8767.9228515625,
      "learning_rate": 7.585655314757482e-05,
      "loss": 23.1618,
      "step": 5279
    },
    {
      "epoch": 5.29,
      "grad_norm": 11810.884765625,
      "learning_rate": 7.58513931888545e-05,
      "loss": 20.6447,
      "step": 5280
    },
    {
      "epoch": 5.29,
      "grad_norm": 25758.435546875,
      "learning_rate": 7.584623323013417e-05,
      "loss": 13.3083,
      "step": 5281
    },
    {
      "epoch": 5.29,
      "grad_norm": 2381.056884765625,
      "learning_rate": 7.584107327141383e-05,
      "loss": 21.1424,
      "step": 5282
    },
    {
      "epoch": 5.29,
      "grad_norm": 7821.67626953125,
      "learning_rate": 7.583591331269351e-05,
      "loss": 16.5426,
      "step": 5283
    },
    {
      "epoch": 5.29,
      "grad_norm": 4199.2392578125,
      "learning_rate": 7.583075335397317e-05,
      "loss": 16.4912,
      "step": 5284
    },
    {
      "epoch": 5.29,
      "grad_norm": 9925.7021484375,
      "learning_rate": 7.582559339525284e-05,
      "loss": 18.5443,
      "step": 5285
    },
    {
      "epoch": 5.29,
      "grad_norm": 1448.9658203125,
      "learning_rate": 7.582043343653252e-05,
      "loss": 11.5628,
      "step": 5286
    },
    {
      "epoch": 5.29,
      "grad_norm": 6211.9794921875,
      "learning_rate": 7.581527347781218e-05,
      "loss": 15.2735,
      "step": 5287
    },
    {
      "epoch": 5.29,
      "grad_norm": 10116.458984375,
      "learning_rate": 7.581011351909185e-05,
      "loss": 11.6645,
      "step": 5288
    },
    {
      "epoch": 5.29,
      "grad_norm": 12472.396484375,
      "learning_rate": 7.580495356037151e-05,
      "loss": 13.8561,
      "step": 5289
    },
    {
      "epoch": 5.3,
      "grad_norm": 5613.35693359375,
      "learning_rate": 7.579979360165119e-05,
      "loss": 18.7175,
      "step": 5290
    },
    {
      "epoch": 5.3,
      "grad_norm": 40978.19140625,
      "learning_rate": 7.579463364293086e-05,
      "loss": 11.9929,
      "step": 5291
    },
    {
      "epoch": 5.3,
      "grad_norm": 6769.9853515625,
      "learning_rate": 7.578947368421054e-05,
      "loss": 13.0214,
      "step": 5292
    },
    {
      "epoch": 5.3,
      "grad_norm": 39448.25390625,
      "learning_rate": 7.57843137254902e-05,
      "loss": 32.1965,
      "step": 5293
    },
    {
      "epoch": 5.3,
      "grad_norm": 45230.51953125,
      "learning_rate": 7.577915376676987e-05,
      "loss": 16.1765,
      "step": 5294
    },
    {
      "epoch": 5.3,
      "grad_norm": 1586.8284912109375,
      "learning_rate": 7.577399380804953e-05,
      "loss": 12.969,
      "step": 5295
    },
    {
      "epoch": 5.3,
      "grad_norm": 8298.5830078125,
      "learning_rate": 7.576883384932921e-05,
      "loss": 24.319,
      "step": 5296
    },
    {
      "epoch": 5.3,
      "grad_norm": 3806.301025390625,
      "learning_rate": 7.576367389060888e-05,
      "loss": 11.2697,
      "step": 5297
    },
    {
      "epoch": 5.3,
      "grad_norm": 3602.550537109375,
      "learning_rate": 7.575851393188856e-05,
      "loss": 13.8023,
      "step": 5298
    },
    {
      "epoch": 5.3,
      "grad_norm": 162097.859375,
      "learning_rate": 7.575335397316822e-05,
      "loss": 20.8519,
      "step": 5299
    },
    {
      "epoch": 5.31,
      "grad_norm": 12341.37890625,
      "learning_rate": 7.574819401444789e-05,
      "loss": 13.6188,
      "step": 5300
    },
    {
      "epoch": 5.31,
      "grad_norm": 1585.768310546875,
      "learning_rate": 7.574303405572755e-05,
      "loss": 13.3293,
      "step": 5301
    },
    {
      "epoch": 5.31,
      "grad_norm": 2390.10302734375,
      "learning_rate": 7.573787409700723e-05,
      "loss": 13.6439,
      "step": 5302
    },
    {
      "epoch": 5.31,
      "grad_norm": 1234.9146728515625,
      "learning_rate": 7.57327141382869e-05,
      "loss": 12.032,
      "step": 5303
    },
    {
      "epoch": 5.31,
      "grad_norm": 3150.982177734375,
      "learning_rate": 7.572755417956656e-05,
      "loss": 13.9089,
      "step": 5304
    },
    {
      "epoch": 5.31,
      "grad_norm": 18253.6015625,
      "learning_rate": 7.572239422084624e-05,
      "loss": 14.7603,
      "step": 5305
    },
    {
      "epoch": 5.31,
      "grad_norm": 8464.4931640625,
      "learning_rate": 7.57172342621259e-05,
      "loss": 12.726,
      "step": 5306
    },
    {
      "epoch": 5.31,
      "grad_norm": 5628.18994140625,
      "learning_rate": 7.571207430340557e-05,
      "loss": 12.3497,
      "step": 5307
    },
    {
      "epoch": 5.31,
      "grad_norm": 14518.705078125,
      "learning_rate": 7.570691434468525e-05,
      "loss": 13.738,
      "step": 5308
    },
    {
      "epoch": 5.31,
      "grad_norm": 3235.10595703125,
      "learning_rate": 7.570175438596492e-05,
      "loss": 14.1813,
      "step": 5309
    },
    {
      "epoch": 5.32,
      "grad_norm": 4003.1611328125,
      "learning_rate": 7.569659442724458e-05,
      "loss": 10.8704,
      "step": 5310
    },
    {
      "epoch": 5.32,
      "grad_norm": 8003.4326171875,
      "learning_rate": 7.569143446852426e-05,
      "loss": 16.5929,
      "step": 5311
    },
    {
      "epoch": 5.32,
      "grad_norm": 2435.50927734375,
      "learning_rate": 7.568627450980392e-05,
      "loss": 13.067,
      "step": 5312
    },
    {
      "epoch": 5.32,
      "grad_norm": 8633.3076171875,
      "learning_rate": 7.56811145510836e-05,
      "loss": 25.1933,
      "step": 5313
    },
    {
      "epoch": 5.32,
      "grad_norm": 9252.2744140625,
      "learning_rate": 7.567595459236327e-05,
      "loss": 20.2366,
      "step": 5314
    },
    {
      "epoch": 5.32,
      "grad_norm": 26568.087890625,
      "learning_rate": 7.567079463364294e-05,
      "loss": 21.205,
      "step": 5315
    },
    {
      "epoch": 5.32,
      "grad_norm": 24784.890625,
      "learning_rate": 7.56656346749226e-05,
      "loss": 17.3721,
      "step": 5316
    },
    {
      "epoch": 5.32,
      "grad_norm": 4060.3212890625,
      "learning_rate": 7.566047471620228e-05,
      "loss": 14.6785,
      "step": 5317
    },
    {
      "epoch": 5.32,
      "grad_norm": 32012.298828125,
      "learning_rate": 7.565531475748194e-05,
      "loss": 17.3816,
      "step": 5318
    },
    {
      "epoch": 5.32,
      "grad_norm": 1241.6920166015625,
      "learning_rate": 7.565015479876161e-05,
      "loss": 10.9852,
      "step": 5319
    },
    {
      "epoch": 5.33,
      "grad_norm": 6712.74267578125,
      "learning_rate": 7.564499484004129e-05,
      "loss": 15.4381,
      "step": 5320
    },
    {
      "epoch": 5.33,
      "grad_norm": 7807.44775390625,
      "learning_rate": 7.563983488132095e-05,
      "loss": 14.3034,
      "step": 5321
    },
    {
      "epoch": 5.33,
      "grad_norm": 6421.97314453125,
      "learning_rate": 7.563467492260062e-05,
      "loss": 15.735,
      "step": 5322
    },
    {
      "epoch": 5.33,
      "grad_norm": 3900.7060546875,
      "learning_rate": 7.562951496388028e-05,
      "loss": 13.7998,
      "step": 5323
    },
    {
      "epoch": 5.33,
      "grad_norm": 3051.959716796875,
      "learning_rate": 7.562435500515996e-05,
      "loss": 11.3855,
      "step": 5324
    },
    {
      "epoch": 5.33,
      "grad_norm": 17263.87109375,
      "learning_rate": 7.561919504643963e-05,
      "loss": 19.6176,
      "step": 5325
    },
    {
      "epoch": 5.33,
      "grad_norm": 496.3106994628906,
      "learning_rate": 7.561403508771931e-05,
      "loss": 11.2882,
      "step": 5326
    },
    {
      "epoch": 5.33,
      "grad_norm": 5448.76123046875,
      "learning_rate": 7.560887512899897e-05,
      "loss": 13.3508,
      "step": 5327
    },
    {
      "epoch": 5.33,
      "grad_norm": 2234.762451171875,
      "learning_rate": 7.560371517027864e-05,
      "loss": 14.2496,
      "step": 5328
    },
    {
      "epoch": 5.33,
      "grad_norm": 82634.25,
      "learning_rate": 7.55985552115583e-05,
      "loss": 14.9871,
      "step": 5329
    },
    {
      "epoch": 5.34,
      "grad_norm": 20057.826171875,
      "learning_rate": 7.559339525283798e-05,
      "loss": 14.6604,
      "step": 5330
    },
    {
      "epoch": 5.34,
      "grad_norm": 3544.73291015625,
      "learning_rate": 7.558823529411765e-05,
      "loss": 21.8726,
      "step": 5331
    },
    {
      "epoch": 5.34,
      "grad_norm": 5274.64306640625,
      "learning_rate": 7.558307533539733e-05,
      "loss": 14.0535,
      "step": 5332
    },
    {
      "epoch": 5.34,
      "grad_norm": 6359.48291015625,
      "learning_rate": 7.557791537667699e-05,
      "loss": 15.326,
      "step": 5333
    },
    {
      "epoch": 5.34,
      "grad_norm": 6617.47705078125,
      "learning_rate": 7.557275541795666e-05,
      "loss": 13.7438,
      "step": 5334
    },
    {
      "epoch": 5.34,
      "grad_norm": 2005.78515625,
      "learning_rate": 7.556759545923632e-05,
      "loss": 15.1592,
      "step": 5335
    },
    {
      "epoch": 5.34,
      "grad_norm": 6351.734375,
      "learning_rate": 7.5562435500516e-05,
      "loss": 10.8867,
      "step": 5336
    },
    {
      "epoch": 5.34,
      "grad_norm": 11545.4287109375,
      "learning_rate": 7.555727554179567e-05,
      "loss": 15.0875,
      "step": 5337
    },
    {
      "epoch": 5.34,
      "grad_norm": 2995.195556640625,
      "learning_rate": 7.555211558307533e-05,
      "loss": 14.499,
      "step": 5338
    },
    {
      "epoch": 5.34,
      "grad_norm": 6614.45361328125,
      "learning_rate": 7.554695562435501e-05,
      "loss": 13.1774,
      "step": 5339
    },
    {
      "epoch": 5.35,
      "grad_norm": 6721.6484375,
      "learning_rate": 7.554179566563467e-05,
      "loss": 13.3914,
      "step": 5340
    },
    {
      "epoch": 5.35,
      "grad_norm": 11577.9091796875,
      "learning_rate": 7.553663570691434e-05,
      "loss": 16.1312,
      "step": 5341
    },
    {
      "epoch": 5.35,
      "grad_norm": 1310.81298828125,
      "learning_rate": 7.553147574819402e-05,
      "loss": 12.3937,
      "step": 5342
    },
    {
      "epoch": 5.35,
      "grad_norm": 1918.7874755859375,
      "learning_rate": 7.55263157894737e-05,
      "loss": 13.2734,
      "step": 5343
    },
    {
      "epoch": 5.35,
      "grad_norm": 1727.2694091796875,
      "learning_rate": 7.552115583075335e-05,
      "loss": 12.3504,
      "step": 5344
    },
    {
      "epoch": 5.35,
      "grad_norm": 9939.1943359375,
      "learning_rate": 7.551599587203303e-05,
      "loss": 11.523,
      "step": 5345
    },
    {
      "epoch": 5.35,
      "grad_norm": 4794.76953125,
      "learning_rate": 7.551083591331269e-05,
      "loss": 12.4359,
      "step": 5346
    },
    {
      "epoch": 5.35,
      "grad_norm": 2513.922607421875,
      "learning_rate": 7.550567595459236e-05,
      "loss": 12.5416,
      "step": 5347
    },
    {
      "epoch": 5.35,
      "grad_norm": 3043.32568359375,
      "learning_rate": 7.550051599587204e-05,
      "loss": 14.626,
      "step": 5348
    },
    {
      "epoch": 5.35,
      "grad_norm": 2294.20654296875,
      "learning_rate": 7.549535603715171e-05,
      "loss": 14.6066,
      "step": 5349
    },
    {
      "epoch": 5.36,
      "grad_norm": 4016.810791015625,
      "learning_rate": 7.549019607843137e-05,
      "loss": 15.6864,
      "step": 5350
    },
    {
      "epoch": 5.36,
      "grad_norm": 16847.810546875,
      "learning_rate": 7.548503611971105e-05,
      "loss": 14.8238,
      "step": 5351
    },
    {
      "epoch": 5.36,
      "grad_norm": 24876.92578125,
      "learning_rate": 7.547987616099071e-05,
      "loss": 13.5185,
      "step": 5352
    },
    {
      "epoch": 5.36,
      "grad_norm": 7003.06298828125,
      "learning_rate": 7.547471620227038e-05,
      "loss": 16.8431,
      "step": 5353
    },
    {
      "epoch": 5.36,
      "grad_norm": 2018.18896484375,
      "learning_rate": 7.546955624355006e-05,
      "loss": 15.0943,
      "step": 5354
    },
    {
      "epoch": 5.36,
      "grad_norm": 11939.138671875,
      "learning_rate": 7.546439628482973e-05,
      "loss": 15.5061,
      "step": 5355
    },
    {
      "epoch": 5.36,
      "grad_norm": 24850.7265625,
      "learning_rate": 7.54592363261094e-05,
      "loss": 14.1424,
      "step": 5356
    },
    {
      "epoch": 5.36,
      "grad_norm": 7552.0673828125,
      "learning_rate": 7.545407636738906e-05,
      "loss": 11.826,
      "step": 5357
    },
    {
      "epoch": 5.36,
      "grad_norm": 6396.1875,
      "learning_rate": 7.544891640866873e-05,
      "loss": 14.6962,
      "step": 5358
    },
    {
      "epoch": 5.36,
      "grad_norm": 4840.12890625,
      "learning_rate": 7.54437564499484e-05,
      "loss": 13.5703,
      "step": 5359
    },
    {
      "epoch": 5.37,
      "grad_norm": 3669.96435546875,
      "learning_rate": 7.543859649122808e-05,
      "loss": 14.4134,
      "step": 5360
    },
    {
      "epoch": 5.37,
      "grad_norm": 15486.16015625,
      "learning_rate": 7.543343653250774e-05,
      "loss": 20.2388,
      "step": 5361
    },
    {
      "epoch": 5.37,
      "grad_norm": 8754.54296875,
      "learning_rate": 7.542827657378741e-05,
      "loss": 18.0899,
      "step": 5362
    },
    {
      "epoch": 5.37,
      "grad_norm": 1927.01806640625,
      "learning_rate": 7.542311661506708e-05,
      "loss": 16.7441,
      "step": 5363
    },
    {
      "epoch": 5.37,
      "grad_norm": 1846.088134765625,
      "learning_rate": 7.541795665634675e-05,
      "loss": 13.2505,
      "step": 5364
    },
    {
      "epoch": 5.37,
      "grad_norm": 17322.189453125,
      "learning_rate": 7.541279669762642e-05,
      "loss": 14.1265,
      "step": 5365
    },
    {
      "epoch": 5.37,
      "grad_norm": 3506.5771484375,
      "learning_rate": 7.54076367389061e-05,
      "loss": 11.1781,
      "step": 5366
    },
    {
      "epoch": 5.37,
      "grad_norm": 5199.1748046875,
      "learning_rate": 7.540247678018576e-05,
      "loss": 16.6606,
      "step": 5367
    },
    {
      "epoch": 5.37,
      "grad_norm": 22809.212890625,
      "learning_rate": 7.539731682146543e-05,
      "loss": 15.4958,
      "step": 5368
    },
    {
      "epoch": 5.37,
      "grad_norm": 8483.5302734375,
      "learning_rate": 7.53921568627451e-05,
      "loss": 21.2872,
      "step": 5369
    },
    {
      "epoch": 5.38,
      "grad_norm": 3359.598876953125,
      "learning_rate": 7.538699690402477e-05,
      "loss": 10.8931,
      "step": 5370
    },
    {
      "epoch": 5.38,
      "grad_norm": 11958.8271484375,
      "learning_rate": 7.538183694530444e-05,
      "loss": 15.4898,
      "step": 5371
    },
    {
      "epoch": 5.38,
      "grad_norm": 5537.9755859375,
      "learning_rate": 7.537667698658412e-05,
      "loss": 13.6966,
      "step": 5372
    },
    {
      "epoch": 5.38,
      "grad_norm": 2412.36865234375,
      "learning_rate": 7.537151702786378e-05,
      "loss": 13.7098,
      "step": 5373
    },
    {
      "epoch": 5.38,
      "grad_norm": 1411.1217041015625,
      "learning_rate": 7.536635706914344e-05,
      "loss": 13.5414,
      "step": 5374
    },
    {
      "epoch": 5.38,
      "grad_norm": 13828.4296875,
      "learning_rate": 7.536119711042312e-05,
      "loss": 17.2746,
      "step": 5375
    },
    {
      "epoch": 5.38,
      "grad_norm": 13487.78125,
      "learning_rate": 7.535603715170279e-05,
      "loss": 15.7759,
      "step": 5376
    },
    {
      "epoch": 5.38,
      "grad_norm": 3451.79541015625,
      "learning_rate": 7.535087719298246e-05,
      "loss": 13.0622,
      "step": 5377
    },
    {
      "epoch": 5.38,
      "grad_norm": 3319.832275390625,
      "learning_rate": 7.534571723426213e-05,
      "loss": 11.7079,
      "step": 5378
    },
    {
      "epoch": 5.38,
      "grad_norm": 9373.3701171875,
      "learning_rate": 7.53405572755418e-05,
      "loss": 14.199,
      "step": 5379
    },
    {
      "epoch": 5.39,
      "grad_norm": 8968.2626953125,
      "learning_rate": 7.533539731682146e-05,
      "loss": 15.9095,
      "step": 5380
    },
    {
      "epoch": 5.39,
      "grad_norm": 1556.730712890625,
      "learning_rate": 7.533023735810114e-05,
      "loss": 15.8872,
      "step": 5381
    },
    {
      "epoch": 5.39,
      "grad_norm": 3225.72998046875,
      "learning_rate": 7.532507739938081e-05,
      "loss": 14.647,
      "step": 5382
    },
    {
      "epoch": 5.39,
      "grad_norm": 5333.04248046875,
      "learning_rate": 7.531991744066048e-05,
      "loss": 13.5878,
      "step": 5383
    },
    {
      "epoch": 5.39,
      "grad_norm": 9003.2119140625,
      "learning_rate": 7.531475748194015e-05,
      "loss": 12.2071,
      "step": 5384
    },
    {
      "epoch": 5.39,
      "grad_norm": 6728.255859375,
      "learning_rate": 7.530959752321982e-05,
      "loss": 14.7711,
      "step": 5385
    },
    {
      "epoch": 5.39,
      "grad_norm": 46002.0,
      "learning_rate": 7.530443756449948e-05,
      "loss": 16.7493,
      "step": 5386
    },
    {
      "epoch": 5.39,
      "grad_norm": 3902.27099609375,
      "learning_rate": 7.529927760577916e-05,
      "loss": 11.6687,
      "step": 5387
    },
    {
      "epoch": 5.39,
      "grad_norm": 28565.947265625,
      "learning_rate": 7.529411764705883e-05,
      "loss": 15.2158,
      "step": 5388
    },
    {
      "epoch": 5.39,
      "grad_norm": 31668.33984375,
      "learning_rate": 7.52889576883385e-05,
      "loss": 14.9422,
      "step": 5389
    },
    {
      "epoch": 5.4,
      "grad_norm": 3255.37890625,
      "learning_rate": 7.528379772961817e-05,
      "loss": 24.444,
      "step": 5390
    },
    {
      "epoch": 5.4,
      "grad_norm": 8082.79443359375,
      "learning_rate": 7.527863777089784e-05,
      "loss": 28.6313,
      "step": 5391
    },
    {
      "epoch": 5.4,
      "grad_norm": 1766.3323974609375,
      "learning_rate": 7.52734778121775e-05,
      "loss": 16.906,
      "step": 5392
    },
    {
      "epoch": 5.4,
      "grad_norm": 7506.2451171875,
      "learning_rate": 7.526831785345718e-05,
      "loss": 15.4974,
      "step": 5393
    },
    {
      "epoch": 5.4,
      "grad_norm": 2870.154541015625,
      "learning_rate": 7.526315789473685e-05,
      "loss": 13.9401,
      "step": 5394
    },
    {
      "epoch": 5.4,
      "grad_norm": 3291.030029296875,
      "learning_rate": 7.525799793601651e-05,
      "loss": 14.7255,
      "step": 5395
    },
    {
      "epoch": 5.4,
      "grad_norm": 2883.455810546875,
      "learning_rate": 7.525283797729619e-05,
      "loss": 12.5737,
      "step": 5396
    },
    {
      "epoch": 5.4,
      "grad_norm": 4509.58056640625,
      "learning_rate": 7.524767801857585e-05,
      "loss": 14.2955,
      "step": 5397
    },
    {
      "epoch": 5.4,
      "grad_norm": 16022.1328125,
      "learning_rate": 7.524251805985552e-05,
      "loss": 16.4532,
      "step": 5398
    },
    {
      "epoch": 5.4,
      "grad_norm": 23227.298828125,
      "learning_rate": 7.52373581011352e-05,
      "loss": 12.7555,
      "step": 5399
    },
    {
      "epoch": 5.41,
      "grad_norm": 14841.21875,
      "learning_rate": 7.523219814241487e-05,
      "loss": 16.279,
      "step": 5400
    },
    {
      "epoch": 5.41,
      "grad_norm": 1232.8427734375,
      "learning_rate": 7.522703818369453e-05,
      "loss": 11.4697,
      "step": 5401
    },
    {
      "epoch": 5.41,
      "grad_norm": 2151.412841796875,
      "learning_rate": 7.52218782249742e-05,
      "loss": 11.894,
      "step": 5402
    },
    {
      "epoch": 5.41,
      "grad_norm": 14028.0009765625,
      "learning_rate": 7.521671826625387e-05,
      "loss": 18.405,
      "step": 5403
    },
    {
      "epoch": 5.41,
      "grad_norm": 11629.4462890625,
      "learning_rate": 7.521155830753354e-05,
      "loss": 17.8683,
      "step": 5404
    },
    {
      "epoch": 5.41,
      "grad_norm": 987.1012573242188,
      "learning_rate": 7.520639834881322e-05,
      "loss": 12.3221,
      "step": 5405
    },
    {
      "epoch": 5.41,
      "grad_norm": 15629.4521484375,
      "learning_rate": 7.520123839009289e-05,
      "loss": 18.2474,
      "step": 5406
    },
    {
      "epoch": 5.41,
      "grad_norm": 2742.230224609375,
      "learning_rate": 7.519607843137255e-05,
      "loss": 12.6432,
      "step": 5407
    },
    {
      "epoch": 5.41,
      "grad_norm": 1755.418701171875,
      "learning_rate": 7.519091847265223e-05,
      "loss": 17.6674,
      "step": 5408
    },
    {
      "epoch": 5.41,
      "grad_norm": 5457.25634765625,
      "learning_rate": 7.518575851393189e-05,
      "loss": 13.0173,
      "step": 5409
    },
    {
      "epoch": 5.42,
      "grad_norm": 6268.58740234375,
      "learning_rate": 7.518059855521156e-05,
      "loss": 11.9897,
      "step": 5410
    },
    {
      "epoch": 5.42,
      "grad_norm": 7362.50732421875,
      "learning_rate": 7.517543859649124e-05,
      "loss": 12.031,
      "step": 5411
    },
    {
      "epoch": 5.42,
      "grad_norm": 4668.77978515625,
      "learning_rate": 7.51702786377709e-05,
      "loss": 12.2198,
      "step": 5412
    },
    {
      "epoch": 5.42,
      "grad_norm": 5933.25537109375,
      "learning_rate": 7.516511867905057e-05,
      "loss": 14.1295,
      "step": 5413
    },
    {
      "epoch": 5.42,
      "grad_norm": 7361.1640625,
      "learning_rate": 7.515995872033023e-05,
      "loss": 18.3985,
      "step": 5414
    },
    {
      "epoch": 5.42,
      "grad_norm": 14538.193359375,
      "learning_rate": 7.515479876160991e-05,
      "loss": 14.141,
      "step": 5415
    },
    {
      "epoch": 5.42,
      "grad_norm": 11057.513671875,
      "learning_rate": 7.514963880288958e-05,
      "loss": 17.9838,
      "step": 5416
    },
    {
      "epoch": 5.42,
      "grad_norm": 15418.1044921875,
      "learning_rate": 7.514447884416926e-05,
      "loss": 16.7676,
      "step": 5417
    },
    {
      "epoch": 5.42,
      "grad_norm": 9452.421875,
      "learning_rate": 7.513931888544892e-05,
      "loss": 18.5079,
      "step": 5418
    },
    {
      "epoch": 5.42,
      "grad_norm": 16052.80078125,
      "learning_rate": 7.513415892672859e-05,
      "loss": 18.0842,
      "step": 5419
    },
    {
      "epoch": 5.43,
      "grad_norm": 15694.4716796875,
      "learning_rate": 7.512899896800825e-05,
      "loss": 13.8788,
      "step": 5420
    },
    {
      "epoch": 5.43,
      "grad_norm": 3790.82568359375,
      "learning_rate": 7.512383900928793e-05,
      "loss": 19.4351,
      "step": 5421
    },
    {
      "epoch": 5.43,
      "grad_norm": 7887.56494140625,
      "learning_rate": 7.51186790505676e-05,
      "loss": 12.483,
      "step": 5422
    },
    {
      "epoch": 5.43,
      "grad_norm": 13744.525390625,
      "learning_rate": 7.511351909184728e-05,
      "loss": 14.5713,
      "step": 5423
    },
    {
      "epoch": 5.43,
      "grad_norm": 798.2593994140625,
      "learning_rate": 7.510835913312694e-05,
      "loss": 15.9638,
      "step": 5424
    },
    {
      "epoch": 5.43,
      "grad_norm": 7892.4560546875,
      "learning_rate": 7.510319917440661e-05,
      "loss": 11.706,
      "step": 5425
    },
    {
      "epoch": 5.43,
      "grad_norm": 5485.4443359375,
      "learning_rate": 7.509803921568627e-05,
      "loss": 16.6908,
      "step": 5426
    },
    {
      "epoch": 5.43,
      "grad_norm": 57068.234375,
      "learning_rate": 7.509287925696595e-05,
      "loss": 20.4988,
      "step": 5427
    },
    {
      "epoch": 5.43,
      "grad_norm": 16977.625,
      "learning_rate": 7.508771929824562e-05,
      "loss": 14.0489,
      "step": 5428
    },
    {
      "epoch": 5.43,
      "grad_norm": 8804.724609375,
      "learning_rate": 7.508255933952528e-05,
      "loss": 21.6446,
      "step": 5429
    },
    {
      "epoch": 5.44,
      "grad_norm": 8318.83984375,
      "learning_rate": 7.507739938080496e-05,
      "loss": 14.1371,
      "step": 5430
    },
    {
      "epoch": 5.44,
      "grad_norm": 2897.84423828125,
      "learning_rate": 7.507223942208462e-05,
      "loss": 12.7697,
      "step": 5431
    },
    {
      "epoch": 5.44,
      "grad_norm": 10600.05859375,
      "learning_rate": 7.506707946336429e-05,
      "loss": 12.6436,
      "step": 5432
    },
    {
      "epoch": 5.44,
      "grad_norm": 6043.2783203125,
      "learning_rate": 7.506191950464397e-05,
      "loss": 20.0956,
      "step": 5433
    },
    {
      "epoch": 5.44,
      "grad_norm": 18894.88671875,
      "learning_rate": 7.505675954592364e-05,
      "loss": 17.0807,
      "step": 5434
    },
    {
      "epoch": 5.44,
      "grad_norm": 13619.52734375,
      "learning_rate": 7.50515995872033e-05,
      "loss": 19.8728,
      "step": 5435
    },
    {
      "epoch": 5.44,
      "grad_norm": 4166.20556640625,
      "learning_rate": 7.504643962848298e-05,
      "loss": 13.6931,
      "step": 5436
    },
    {
      "epoch": 5.44,
      "grad_norm": 6056.70751953125,
      "learning_rate": 7.504127966976264e-05,
      "loss": 12.1095,
      "step": 5437
    },
    {
      "epoch": 5.44,
      "grad_norm": 6710.56591796875,
      "learning_rate": 7.503611971104231e-05,
      "loss": 12.9316,
      "step": 5438
    },
    {
      "epoch": 5.44,
      "grad_norm": 4107.56494140625,
      "learning_rate": 7.503095975232199e-05,
      "loss": 17.6718,
      "step": 5439
    },
    {
      "epoch": 5.45,
      "grad_norm": 10063.1982421875,
      "learning_rate": 7.502579979360166e-05,
      "loss": 21.9992,
      "step": 5440
    },
    {
      "epoch": 5.45,
      "grad_norm": 5452.5,
      "learning_rate": 7.502063983488132e-05,
      "loss": 11.566,
      "step": 5441
    },
    {
      "epoch": 5.45,
      "grad_norm": 5261.91845703125,
      "learning_rate": 7.5015479876161e-05,
      "loss": 12.1515,
      "step": 5442
    },
    {
      "epoch": 5.45,
      "grad_norm": 16962.3125,
      "learning_rate": 7.501031991744066e-05,
      "loss": 27.2541,
      "step": 5443
    },
    {
      "epoch": 5.45,
      "grad_norm": 7021.0478515625,
      "learning_rate": 7.500515995872033e-05,
      "loss": 13.7193,
      "step": 5444
    },
    {
      "epoch": 5.45,
      "grad_norm": 17093.560546875,
      "learning_rate": 7.500000000000001e-05,
      "loss": 26.4205,
      "step": 5445
    },
    {
      "epoch": 5.45,
      "grad_norm": 3716.16015625,
      "learning_rate": 7.499484004127967e-05,
      "loss": 12.0833,
      "step": 5446
    },
    {
      "epoch": 5.45,
      "grad_norm": 34442.86328125,
      "learning_rate": 7.498968008255934e-05,
      "loss": 19.5774,
      "step": 5447
    },
    {
      "epoch": 5.45,
      "grad_norm": 1353.548095703125,
      "learning_rate": 7.4984520123839e-05,
      "loss": 10.733,
      "step": 5448
    },
    {
      "epoch": 5.45,
      "grad_norm": 3636.76123046875,
      "learning_rate": 7.497936016511868e-05,
      "loss": 11.8004,
      "step": 5449
    },
    {
      "epoch": 5.46,
      "grad_norm": 5981.27294921875,
      "learning_rate": 7.497420020639835e-05,
      "loss": 12.5823,
      "step": 5450
    },
    {
      "epoch": 5.46,
      "grad_norm": 8261.287109375,
      "learning_rate": 7.496904024767803e-05,
      "loss": 12.0881,
      "step": 5451
    },
    {
      "epoch": 5.46,
      "grad_norm": 5122.8857421875,
      "learning_rate": 7.496388028895769e-05,
      "loss": 13.2363,
      "step": 5452
    },
    {
      "epoch": 5.46,
      "grad_norm": 2863.301513671875,
      "learning_rate": 7.495872033023736e-05,
      "loss": 15.5281,
      "step": 5453
    },
    {
      "epoch": 5.46,
      "grad_norm": 5967.63916015625,
      "learning_rate": 7.495356037151702e-05,
      "loss": 24.8995,
      "step": 5454
    },
    {
      "epoch": 5.46,
      "grad_norm": 17135.30078125,
      "learning_rate": 7.49484004127967e-05,
      "loss": 24.7678,
      "step": 5455
    },
    {
      "epoch": 5.46,
      "grad_norm": 7624.646484375,
      "learning_rate": 7.494324045407637e-05,
      "loss": 14.5186,
      "step": 5456
    },
    {
      "epoch": 5.46,
      "grad_norm": 9594.47265625,
      "learning_rate": 7.493808049535605e-05,
      "loss": 14.4363,
      "step": 5457
    },
    {
      "epoch": 5.46,
      "grad_norm": 4228.22900390625,
      "learning_rate": 7.493292053663571e-05,
      "loss": 12.8166,
      "step": 5458
    },
    {
      "epoch": 5.46,
      "grad_norm": 6705.6533203125,
      "learning_rate": 7.492776057791538e-05,
      "loss": 15.1173,
      "step": 5459
    },
    {
      "epoch": 5.47,
      "grad_norm": 18449.0390625,
      "learning_rate": 7.492260061919504e-05,
      "loss": 17.4157,
      "step": 5460
    },
    {
      "epoch": 5.47,
      "grad_norm": 2463.12353515625,
      "learning_rate": 7.491744066047473e-05,
      "loss": 16.4342,
      "step": 5461
    },
    {
      "epoch": 5.47,
      "grad_norm": 1315.3406982421875,
      "learning_rate": 7.491228070175439e-05,
      "loss": 17.4876,
      "step": 5462
    },
    {
      "epoch": 5.47,
      "grad_norm": 6482.7744140625,
      "learning_rate": 7.490712074303405e-05,
      "loss": 15.2197,
      "step": 5463
    },
    {
      "epoch": 5.47,
      "grad_norm": 8335.962890625,
      "learning_rate": 7.490196078431373e-05,
      "loss": 16.4113,
      "step": 5464
    },
    {
      "epoch": 5.47,
      "grad_norm": 1934.87548828125,
      "learning_rate": 7.489680082559339e-05,
      "loss": 11.0003,
      "step": 5465
    },
    {
      "epoch": 5.47,
      "grad_norm": 55465.28125,
      "learning_rate": 7.489164086687306e-05,
      "loss": 28.1111,
      "step": 5466
    },
    {
      "epoch": 5.47,
      "grad_norm": 2443.61669921875,
      "learning_rate": 7.488648090815274e-05,
      "loss": 17.5256,
      "step": 5467
    },
    {
      "epoch": 5.47,
      "grad_norm": 865.145263671875,
      "learning_rate": 7.488132094943241e-05,
      "loss": 10.3465,
      "step": 5468
    },
    {
      "epoch": 5.47,
      "grad_norm": 4512.30419921875,
      "learning_rate": 7.487616099071207e-05,
      "loss": 16.0186,
      "step": 5469
    },
    {
      "epoch": 5.48,
      "grad_norm": 2990.3515625,
      "learning_rate": 7.487100103199175e-05,
      "loss": 12.1573,
      "step": 5470
    },
    {
      "epoch": 5.48,
      "grad_norm": 18982.83203125,
      "learning_rate": 7.486584107327141e-05,
      "loss": 12.4268,
      "step": 5471
    },
    {
      "epoch": 5.48,
      "grad_norm": 7062.103515625,
      "learning_rate": 7.486068111455108e-05,
      "loss": 12.7013,
      "step": 5472
    },
    {
      "epoch": 5.48,
      "grad_norm": 6305.822265625,
      "learning_rate": 7.485552115583076e-05,
      "loss": 13.5261,
      "step": 5473
    },
    {
      "epoch": 5.48,
      "grad_norm": 6998.49951171875,
      "learning_rate": 7.485036119711043e-05,
      "loss": 14.1084,
      "step": 5474
    },
    {
      "epoch": 5.48,
      "grad_norm": 3301.2548828125,
      "learning_rate": 7.48452012383901e-05,
      "loss": 13.0002,
      "step": 5475
    },
    {
      "epoch": 5.48,
      "grad_norm": 14218.5048828125,
      "learning_rate": 7.484004127966977e-05,
      "loss": 23.1356,
      "step": 5476
    },
    {
      "epoch": 5.48,
      "grad_norm": 5125.744140625,
      "learning_rate": 7.483488132094943e-05,
      "loss": 13.1454,
      "step": 5477
    },
    {
      "epoch": 5.48,
      "grad_norm": 13084.8193359375,
      "learning_rate": 7.482972136222912e-05,
      "loss": 18.8136,
      "step": 5478
    },
    {
      "epoch": 5.48,
      "grad_norm": 3882.62451171875,
      "learning_rate": 7.482456140350878e-05,
      "loss": 12.7486,
      "step": 5479
    },
    {
      "epoch": 5.49,
      "grad_norm": 15085.8935546875,
      "learning_rate": 7.481940144478845e-05,
      "loss": 14.2201,
      "step": 5480
    },
    {
      "epoch": 5.49,
      "grad_norm": 4407.70556640625,
      "learning_rate": 7.481424148606811e-05,
      "loss": 17.4116,
      "step": 5481
    },
    {
      "epoch": 5.49,
      "grad_norm": 4090.962890625,
      "learning_rate": 7.480908152734777e-05,
      "loss": 12.181,
      "step": 5482
    },
    {
      "epoch": 5.49,
      "grad_norm": 8760.580078125,
      "learning_rate": 7.480392156862745e-05,
      "loss": 13.4168,
      "step": 5483
    },
    {
      "epoch": 5.49,
      "grad_norm": 12180.046875,
      "learning_rate": 7.479876160990712e-05,
      "loss": 16.9772,
      "step": 5484
    },
    {
      "epoch": 5.49,
      "grad_norm": 3732.447021484375,
      "learning_rate": 7.47936016511868e-05,
      "loss": 14.2771,
      "step": 5485
    },
    {
      "epoch": 5.49,
      "grad_norm": 6545.50927734375,
      "learning_rate": 7.478844169246646e-05,
      "loss": 11.6548,
      "step": 5486
    },
    {
      "epoch": 5.49,
      "grad_norm": 12006.6689453125,
      "learning_rate": 7.478328173374613e-05,
      "loss": 20.588,
      "step": 5487
    },
    {
      "epoch": 5.49,
      "grad_norm": 2963.83251953125,
      "learning_rate": 7.47781217750258e-05,
      "loss": 14.3814,
      "step": 5488
    },
    {
      "epoch": 5.49,
      "grad_norm": 6740.19970703125,
      "learning_rate": 7.477296181630547e-05,
      "loss": 13.1458,
      "step": 5489
    },
    {
      "epoch": 5.5,
      "grad_norm": 1018.4514770507812,
      "learning_rate": 7.476780185758514e-05,
      "loss": 10.7518,
      "step": 5490
    },
    {
      "epoch": 5.5,
      "grad_norm": 3119.939453125,
      "learning_rate": 7.476264189886482e-05,
      "loss": 14.2312,
      "step": 5491
    },
    {
      "epoch": 5.5,
      "grad_norm": 19356.98828125,
      "learning_rate": 7.475748194014448e-05,
      "loss": 14.9615,
      "step": 5492
    },
    {
      "epoch": 5.5,
      "grad_norm": 5982.41650390625,
      "learning_rate": 7.475232198142415e-05,
      "loss": 14.7866,
      "step": 5493
    },
    {
      "epoch": 5.5,
      "grad_norm": 4028.16650390625,
      "learning_rate": 7.474716202270381e-05,
      "loss": 17.4883,
      "step": 5494
    },
    {
      "epoch": 5.5,
      "grad_norm": 6700.6796875,
      "learning_rate": 7.47420020639835e-05,
      "loss": 14.6326,
      "step": 5495
    },
    {
      "epoch": 5.5,
      "grad_norm": 2955.4443359375,
      "learning_rate": 7.473684210526316e-05,
      "loss": 15.0197,
      "step": 5496
    },
    {
      "epoch": 5.5,
      "grad_norm": 41020.23828125,
      "learning_rate": 7.473168214654284e-05,
      "loss": 19.1643,
      "step": 5497
    },
    {
      "epoch": 5.5,
      "grad_norm": 9868.7177734375,
      "learning_rate": 7.47265221878225e-05,
      "loss": 16.7174,
      "step": 5498
    },
    {
      "epoch": 5.5,
      "grad_norm": 3068.22314453125,
      "learning_rate": 7.472136222910216e-05,
      "loss": 18.845,
      "step": 5499
    },
    {
      "epoch": 5.51,
      "grad_norm": 16908.94921875,
      "learning_rate": 7.471620227038183e-05,
      "loss": 20.105,
      "step": 5500
    },
    {
      "epoch": 5.51,
      "grad_norm": 2141.3359375,
      "learning_rate": 7.471104231166151e-05,
      "loss": 12.3102,
      "step": 5501
    },
    {
      "epoch": 5.51,
      "grad_norm": 3300.865966796875,
      "learning_rate": 7.470588235294118e-05,
      "loss": 16.7519,
      "step": 5502
    },
    {
      "epoch": 5.51,
      "grad_norm": 9364.9326171875,
      "learning_rate": 7.470072239422084e-05,
      "loss": 13.3865,
      "step": 5503
    },
    {
      "epoch": 5.51,
      "grad_norm": 9854.294921875,
      "learning_rate": 7.469556243550052e-05,
      "loss": 11.7514,
      "step": 5504
    },
    {
      "epoch": 5.51,
      "grad_norm": 11521.9033203125,
      "learning_rate": 7.469040247678018e-05,
      "loss": 17.2325,
      "step": 5505
    },
    {
      "epoch": 5.51,
      "grad_norm": 14977.951171875,
      "learning_rate": 7.468524251805987e-05,
      "loss": 20.8853,
      "step": 5506
    },
    {
      "epoch": 5.51,
      "grad_norm": 4617.6259765625,
      "learning_rate": 7.468008255933953e-05,
      "loss": 18.9682,
      "step": 5507
    },
    {
      "epoch": 5.51,
      "grad_norm": 26571.8984375,
      "learning_rate": 7.46749226006192e-05,
      "loss": 15.5863,
      "step": 5508
    },
    {
      "epoch": 5.51,
      "grad_norm": 6100.9892578125,
      "learning_rate": 7.466976264189886e-05,
      "loss": 21.1233,
      "step": 5509
    },
    {
      "epoch": 5.52,
      "grad_norm": 23730.1328125,
      "learning_rate": 7.466460268317854e-05,
      "loss": 15.9564,
      "step": 5510
    },
    {
      "epoch": 5.52,
      "grad_norm": 5509.3583984375,
      "learning_rate": 7.46594427244582e-05,
      "loss": 15.3751,
      "step": 5511
    },
    {
      "epoch": 5.52,
      "grad_norm": 2883.3935546875,
      "learning_rate": 7.465428276573789e-05,
      "loss": 20.6454,
      "step": 5512
    },
    {
      "epoch": 5.52,
      "grad_norm": 2612.530517578125,
      "learning_rate": 7.464912280701755e-05,
      "loss": 12.4006,
      "step": 5513
    },
    {
      "epoch": 5.52,
      "grad_norm": 25258.306640625,
      "learning_rate": 7.464396284829722e-05,
      "loss": 20.2484,
      "step": 5514
    },
    {
      "epoch": 5.52,
      "grad_norm": 27111.103515625,
      "learning_rate": 7.463880288957688e-05,
      "loss": 17.487,
      "step": 5515
    },
    {
      "epoch": 5.52,
      "grad_norm": 3074.618896484375,
      "learning_rate": 7.463364293085656e-05,
      "loss": 16.0833,
      "step": 5516
    },
    {
      "epoch": 5.52,
      "grad_norm": 2132.40185546875,
      "learning_rate": 7.462848297213622e-05,
      "loss": 12.9766,
      "step": 5517
    },
    {
      "epoch": 5.52,
      "grad_norm": 65771.7578125,
      "learning_rate": 7.46233230134159e-05,
      "loss": 14.1671,
      "step": 5518
    },
    {
      "epoch": 5.52,
      "grad_norm": 11730.5986328125,
      "learning_rate": 7.461816305469557e-05,
      "loss": 17.6348,
      "step": 5519
    },
    {
      "epoch": 5.53,
      "grad_norm": 3097.2646484375,
      "learning_rate": 7.461300309597523e-05,
      "loss": 17.5012,
      "step": 5520
    },
    {
      "epoch": 5.53,
      "grad_norm": 4875.28564453125,
      "learning_rate": 7.46078431372549e-05,
      "loss": 15.1203,
      "step": 5521
    },
    {
      "epoch": 5.53,
      "grad_norm": 10715.1416015625,
      "learning_rate": 7.460268317853457e-05,
      "loss": 14.2962,
      "step": 5522
    },
    {
      "epoch": 5.53,
      "grad_norm": 8059.66796875,
      "learning_rate": 7.459752321981425e-05,
      "loss": 21.3229,
      "step": 5523
    },
    {
      "epoch": 5.53,
      "grad_norm": 4442.33837890625,
      "learning_rate": 7.459236326109391e-05,
      "loss": 13.3753,
      "step": 5524
    },
    {
      "epoch": 5.53,
      "grad_norm": 16060.78125,
      "learning_rate": 7.458720330237359e-05,
      "loss": 14.6435,
      "step": 5525
    },
    {
      "epoch": 5.53,
      "grad_norm": 7094.638671875,
      "learning_rate": 7.458204334365325e-05,
      "loss": 13.439,
      "step": 5526
    },
    {
      "epoch": 5.53,
      "grad_norm": 4495.7958984375,
      "learning_rate": 7.457688338493292e-05,
      "loss": 18.336,
      "step": 5527
    },
    {
      "epoch": 5.53,
      "grad_norm": 18145.78125,
      "learning_rate": 7.457172342621259e-05,
      "loss": 14.0159,
      "step": 5528
    },
    {
      "epoch": 5.53,
      "grad_norm": 7300.802734375,
      "learning_rate": 7.456656346749227e-05,
      "loss": 14.7779,
      "step": 5529
    },
    {
      "epoch": 5.54,
      "grad_norm": 22196.05078125,
      "learning_rate": 7.456140350877193e-05,
      "loss": 32.2958,
      "step": 5530
    },
    {
      "epoch": 5.54,
      "grad_norm": 4565.34765625,
      "learning_rate": 7.455624355005161e-05,
      "loss": 12.5984,
      "step": 5531
    },
    {
      "epoch": 5.54,
      "grad_norm": 18536.45703125,
      "learning_rate": 7.455108359133127e-05,
      "loss": 16.3578,
      "step": 5532
    },
    {
      "epoch": 5.54,
      "grad_norm": 4604.068359375,
      "learning_rate": 7.454592363261094e-05,
      "loss": 15.7285,
      "step": 5533
    },
    {
      "epoch": 5.54,
      "grad_norm": 2613.284912109375,
      "learning_rate": 7.454076367389062e-05,
      "loss": 12.7118,
      "step": 5534
    },
    {
      "epoch": 5.54,
      "grad_norm": 1316.4744873046875,
      "learning_rate": 7.453560371517028e-05,
      "loss": 13.4269,
      "step": 5535
    },
    {
      "epoch": 5.54,
      "grad_norm": 14258.39453125,
      "learning_rate": 7.453044375644995e-05,
      "loss": 18.0678,
      "step": 5536
    },
    {
      "epoch": 5.54,
      "grad_norm": 2520.844482421875,
      "learning_rate": 7.452528379772962e-05,
      "loss": 12.5521,
      "step": 5537
    },
    {
      "epoch": 5.54,
      "grad_norm": 143637.71875,
      "learning_rate": 7.452012383900929e-05,
      "loss": 19.9401,
      "step": 5538
    },
    {
      "epoch": 5.54,
      "grad_norm": 2679.41064453125,
      "learning_rate": 7.451496388028895e-05,
      "loss": 13.6367,
      "step": 5539
    },
    {
      "epoch": 5.55,
      "grad_norm": 2558.5810546875,
      "learning_rate": 7.450980392156864e-05,
      "loss": 11.5738,
      "step": 5540
    },
    {
      "epoch": 5.55,
      "grad_norm": 2696.846923828125,
      "learning_rate": 7.45046439628483e-05,
      "loss": 12.809,
      "step": 5541
    },
    {
      "epoch": 5.55,
      "grad_norm": 5495.06396484375,
      "learning_rate": 7.449948400412797e-05,
      "loss": 16.5096,
      "step": 5542
    },
    {
      "epoch": 5.55,
      "grad_norm": 6357.48046875,
      "learning_rate": 7.449432404540764e-05,
      "loss": 22.2663,
      "step": 5543
    },
    {
      "epoch": 5.55,
      "grad_norm": 38981.0859375,
      "learning_rate": 7.448916408668731e-05,
      "loss": 23.948,
      "step": 5544
    },
    {
      "epoch": 5.55,
      "grad_norm": 8989.904296875,
      "learning_rate": 7.448400412796697e-05,
      "loss": 12.1935,
      "step": 5545
    },
    {
      "epoch": 5.55,
      "grad_norm": 2486.89453125,
      "learning_rate": 7.447884416924666e-05,
      "loss": 14.1794,
      "step": 5546
    },
    {
      "epoch": 5.55,
      "grad_norm": 9997.01171875,
      "learning_rate": 7.447368421052632e-05,
      "loss": 12.0431,
      "step": 5547
    },
    {
      "epoch": 5.55,
      "grad_norm": 3932.118408203125,
      "learning_rate": 7.4468524251806e-05,
      "loss": 27.3841,
      "step": 5548
    },
    {
      "epoch": 5.55,
      "grad_norm": 15428.38671875,
      "learning_rate": 7.446336429308566e-05,
      "loss": 19.2029,
      "step": 5549
    },
    {
      "epoch": 5.56,
      "grad_norm": 18373.171875,
      "learning_rate": 7.445820433436533e-05,
      "loss": 21.2141,
      "step": 5550
    },
    {
      "epoch": 5.56,
      "grad_norm": 1734.3231201171875,
      "learning_rate": 7.4453044375645e-05,
      "loss": 11.8703,
      "step": 5551
    },
    {
      "epoch": 5.56,
      "grad_norm": 6225.27001953125,
      "learning_rate": 7.444788441692468e-05,
      "loss": 13.3368,
      "step": 5552
    },
    {
      "epoch": 5.56,
      "grad_norm": 8053.2939453125,
      "learning_rate": 7.444272445820434e-05,
      "loss": 13.4949,
      "step": 5553
    },
    {
      "epoch": 5.56,
      "grad_norm": 2064.482666015625,
      "learning_rate": 7.4437564499484e-05,
      "loss": 14.4404,
      "step": 5554
    },
    {
      "epoch": 5.56,
      "grad_norm": 14992.2001953125,
      "learning_rate": 7.443240454076368e-05,
      "loss": 13.9407,
      "step": 5555
    },
    {
      "epoch": 5.56,
      "grad_norm": 1089.3492431640625,
      "learning_rate": 7.442724458204334e-05,
      "loss": 14.8217,
      "step": 5556
    },
    {
      "epoch": 5.56,
      "grad_norm": 13459.19140625,
      "learning_rate": 7.442208462332302e-05,
      "loss": 13.4891,
      "step": 5557
    },
    {
      "epoch": 5.56,
      "grad_norm": 5764.88720703125,
      "learning_rate": 7.441692466460269e-05,
      "loss": 13.4548,
      "step": 5558
    },
    {
      "epoch": 5.56,
      "grad_norm": 2915.615234375,
      "learning_rate": 7.441176470588236e-05,
      "loss": 12.255,
      "step": 5559
    },
    {
      "epoch": 5.57,
      "grad_norm": 2924.028076171875,
      "learning_rate": 7.440660474716202e-05,
      "loss": 11.9678,
      "step": 5560
    },
    {
      "epoch": 5.57,
      "grad_norm": 24496.12109375,
      "learning_rate": 7.44014447884417e-05,
      "loss": 19.7934,
      "step": 5561
    },
    {
      "epoch": 5.57,
      "grad_norm": 33713.04296875,
      "learning_rate": 7.439628482972137e-05,
      "loss": 12.9994,
      "step": 5562
    },
    {
      "epoch": 5.57,
      "grad_norm": 2511.12939453125,
      "learning_rate": 7.439112487100104e-05,
      "loss": 11.9711,
      "step": 5563
    },
    {
      "epoch": 5.57,
      "grad_norm": 17560.240234375,
      "learning_rate": 7.43859649122807e-05,
      "loss": 15.298,
      "step": 5564
    },
    {
      "epoch": 5.57,
      "grad_norm": 5902.5595703125,
      "learning_rate": 7.438080495356038e-05,
      "loss": 15.9852,
      "step": 5565
    },
    {
      "epoch": 5.57,
      "grad_norm": 5462.140625,
      "learning_rate": 7.437564499484004e-05,
      "loss": 16.0018,
      "step": 5566
    },
    {
      "epoch": 5.57,
      "grad_norm": 6035.26513671875,
      "learning_rate": 7.437048503611972e-05,
      "loss": 13.6822,
      "step": 5567
    },
    {
      "epoch": 5.57,
      "grad_norm": 13155.0302734375,
      "learning_rate": 7.436532507739939e-05,
      "loss": 14.0716,
      "step": 5568
    },
    {
      "epoch": 5.57,
      "grad_norm": 3887.48974609375,
      "learning_rate": 7.436016511867906e-05,
      "loss": 11.4648,
      "step": 5569
    },
    {
      "epoch": 5.58,
      "grad_norm": 14904.75390625,
      "learning_rate": 7.435500515995873e-05,
      "loss": 16.9382,
      "step": 5570
    },
    {
      "epoch": 5.58,
      "grad_norm": 12906.5546875,
      "learning_rate": 7.434984520123839e-05,
      "loss": 14.0498,
      "step": 5571
    },
    {
      "epoch": 5.58,
      "grad_norm": 5498.59912109375,
      "learning_rate": 7.434468524251806e-05,
      "loss": 13.3685,
      "step": 5572
    },
    {
      "epoch": 5.58,
      "grad_norm": 1726.2247314453125,
      "learning_rate": 7.433952528379772e-05,
      "loss": 12.2569,
      "step": 5573
    },
    {
      "epoch": 5.58,
      "grad_norm": 11688.7421875,
      "learning_rate": 7.433436532507741e-05,
      "loss": 15.3647,
      "step": 5574
    },
    {
      "epoch": 5.58,
      "grad_norm": 1334.773193359375,
      "learning_rate": 7.432920536635707e-05,
      "loss": 11.5283,
      "step": 5575
    },
    {
      "epoch": 5.58,
      "grad_norm": 2055.983154296875,
      "learning_rate": 7.432404540763675e-05,
      "loss": 15.8459,
      "step": 5576
    },
    {
      "epoch": 5.58,
      "grad_norm": 6139.62890625,
      "learning_rate": 7.43188854489164e-05,
      "loss": 14.7714,
      "step": 5577
    },
    {
      "epoch": 5.58,
      "grad_norm": 6619.6083984375,
      "learning_rate": 7.431372549019608e-05,
      "loss": 17.1977,
      "step": 5578
    },
    {
      "epoch": 5.58,
      "grad_norm": 9985.111328125,
      "learning_rate": 7.430856553147576e-05,
      "loss": 13.5333,
      "step": 5579
    },
    {
      "epoch": 5.59,
      "grad_norm": 23967.921875,
      "learning_rate": 7.430340557275543e-05,
      "loss": 16.2394,
      "step": 5580
    },
    {
      "epoch": 5.59,
      "grad_norm": 4870.99951171875,
      "learning_rate": 7.429824561403509e-05,
      "loss": 12.741,
      "step": 5581
    },
    {
      "epoch": 5.59,
      "grad_norm": 23249.96875,
      "learning_rate": 7.429308565531477e-05,
      "loss": 17.0081,
      "step": 5582
    },
    {
      "epoch": 5.59,
      "grad_norm": 6975.6591796875,
      "learning_rate": 7.428792569659443e-05,
      "loss": 16.7747,
      "step": 5583
    },
    {
      "epoch": 5.59,
      "grad_norm": 2707.238525390625,
      "learning_rate": 7.42827657378741e-05,
      "loss": 12.7376,
      "step": 5584
    },
    {
      "epoch": 5.59,
      "grad_norm": 7349.0810546875,
      "learning_rate": 7.427760577915378e-05,
      "loss": 15.5752,
      "step": 5585
    },
    {
      "epoch": 5.59,
      "grad_norm": 2689.236572265625,
      "learning_rate": 7.427244582043345e-05,
      "loss": 19.5899,
      "step": 5586
    },
    {
      "epoch": 5.59,
      "grad_norm": 7412.845703125,
      "learning_rate": 7.426728586171311e-05,
      "loss": 13.3953,
      "step": 5587
    },
    {
      "epoch": 5.59,
      "grad_norm": 3210.537841796875,
      "learning_rate": 7.426212590299279e-05,
      "loss": 11.9403,
      "step": 5588
    },
    {
      "epoch": 5.59,
      "grad_norm": 5466.46240234375,
      "learning_rate": 7.425696594427245e-05,
      "loss": 14.8096,
      "step": 5589
    },
    {
      "epoch": 5.6,
      "grad_norm": 5678.86083984375,
      "learning_rate": 7.425180598555212e-05,
      "loss": 14.2718,
      "step": 5590
    },
    {
      "epoch": 5.6,
      "grad_norm": 10356.6591796875,
      "learning_rate": 7.42466460268318e-05,
      "loss": 16.3485,
      "step": 5591
    },
    {
      "epoch": 5.6,
      "grad_norm": 1883.8763427734375,
      "learning_rate": 7.424148606811146e-05,
      "loss": 14.7054,
      "step": 5592
    },
    {
      "epoch": 5.6,
      "grad_norm": 8461.68359375,
      "learning_rate": 7.423632610939113e-05,
      "loss": 17.6368,
      "step": 5593
    },
    {
      "epoch": 5.6,
      "grad_norm": 2353.06103515625,
      "learning_rate": 7.423116615067079e-05,
      "loss": 13.1344,
      "step": 5594
    },
    {
      "epoch": 5.6,
      "grad_norm": 3194.1123046875,
      "learning_rate": 7.422600619195047e-05,
      "loss": 30.0831,
      "step": 5595
    },
    {
      "epoch": 5.6,
      "grad_norm": 2563.755126953125,
      "learning_rate": 7.422084623323014e-05,
      "loss": 13.8731,
      "step": 5596
    },
    {
      "epoch": 5.6,
      "grad_norm": 2541.245849609375,
      "learning_rate": 7.421568627450982e-05,
      "loss": 17.8448,
      "step": 5597
    },
    {
      "epoch": 5.6,
      "grad_norm": 1138.571044921875,
      "learning_rate": 7.421052631578948e-05,
      "loss": 12.5645,
      "step": 5598
    },
    {
      "epoch": 5.6,
      "grad_norm": 4838.3662109375,
      "learning_rate": 7.420536635706915e-05,
      "loss": 15.4807,
      "step": 5599
    },
    {
      "epoch": 5.61,
      "grad_norm": 45498.31640625,
      "learning_rate": 7.420020639834881e-05,
      "loss": 14.078,
      "step": 5600
    },
    {
      "epoch": 5.61,
      "grad_norm": 8813.3203125,
      "learning_rate": 7.419504643962849e-05,
      "loss": 14.4693,
      "step": 5601
    },
    {
      "epoch": 5.61,
      "grad_norm": 1512.0618896484375,
      "learning_rate": 7.418988648090816e-05,
      "loss": 11.1384,
      "step": 5602
    },
    {
      "epoch": 5.61,
      "grad_norm": 3406.55810546875,
      "learning_rate": 7.418472652218784e-05,
      "loss": 14.9823,
      "step": 5603
    },
    {
      "epoch": 5.61,
      "grad_norm": 2890.6728515625,
      "learning_rate": 7.41795665634675e-05,
      "loss": 13.7204,
      "step": 5604
    },
    {
      "epoch": 5.61,
      "grad_norm": 3031.629150390625,
      "learning_rate": 7.417440660474717e-05,
      "loss": 12.2405,
      "step": 5605
    },
    {
      "epoch": 5.61,
      "grad_norm": 15914.837890625,
      "learning_rate": 7.416924664602683e-05,
      "loss": 14.1106,
      "step": 5606
    },
    {
      "epoch": 5.61,
      "grad_norm": 4979.74853515625,
      "learning_rate": 7.41640866873065e-05,
      "loss": 11.185,
      "step": 5607
    },
    {
      "epoch": 5.61,
      "grad_norm": 1134.7130126953125,
      "learning_rate": 7.415892672858618e-05,
      "loss": 13.7827,
      "step": 5608
    },
    {
      "epoch": 5.61,
      "grad_norm": 3822.23828125,
      "learning_rate": 7.415376676986584e-05,
      "loss": 15.9193,
      "step": 5609
    },
    {
      "epoch": 5.62,
      "grad_norm": 23571.880859375,
      "learning_rate": 7.414860681114552e-05,
      "loss": 22.3332,
      "step": 5610
    },
    {
      "epoch": 5.62,
      "grad_norm": 1788.6396484375,
      "learning_rate": 7.414344685242518e-05,
      "loss": 12.1567,
      "step": 5611
    },
    {
      "epoch": 5.62,
      "grad_norm": 8870.625,
      "learning_rate": 7.413828689370485e-05,
      "loss": 13.3126,
      "step": 5612
    },
    {
      "epoch": 5.62,
      "grad_norm": 1255.7872314453125,
      "learning_rate": 7.413312693498453e-05,
      "loss": 14.8518,
      "step": 5613
    },
    {
      "epoch": 5.62,
      "grad_norm": 7836.7646484375,
      "learning_rate": 7.41279669762642e-05,
      "loss": 21.7454,
      "step": 5614
    },
    {
      "epoch": 5.62,
      "grad_norm": 2939.66845703125,
      "learning_rate": 7.412280701754386e-05,
      "loss": 13.8865,
      "step": 5615
    },
    {
      "epoch": 5.62,
      "grad_norm": 3292.181396484375,
      "learning_rate": 7.411764705882354e-05,
      "loss": 15.7699,
      "step": 5616
    },
    {
      "epoch": 5.62,
      "grad_norm": 8167.109375,
      "learning_rate": 7.41124871001032e-05,
      "loss": 13.3492,
      "step": 5617
    },
    {
      "epoch": 5.62,
      "grad_norm": 6480.72412109375,
      "learning_rate": 7.410732714138287e-05,
      "loss": 11.9379,
      "step": 5618
    },
    {
      "epoch": 5.62,
      "grad_norm": 19850.33984375,
      "learning_rate": 7.410216718266255e-05,
      "loss": 12.9755,
      "step": 5619
    },
    {
      "epoch": 5.63,
      "grad_norm": 7838.47265625,
      "learning_rate": 7.409700722394222e-05,
      "loss": 14.754,
      "step": 5620
    },
    {
      "epoch": 5.63,
      "grad_norm": 26598.01953125,
      "learning_rate": 7.409184726522188e-05,
      "loss": 13.8928,
      "step": 5621
    },
    {
      "epoch": 5.63,
      "grad_norm": 3174.849853515625,
      "learning_rate": 7.408668730650156e-05,
      "loss": 14.9037,
      "step": 5622
    },
    {
      "epoch": 5.63,
      "grad_norm": 4632.11767578125,
      "learning_rate": 7.408152734778122e-05,
      "loss": 14.1558,
      "step": 5623
    },
    {
      "epoch": 5.63,
      "grad_norm": 5155.02734375,
      "learning_rate": 7.407636738906089e-05,
      "loss": 11.6353,
      "step": 5624
    },
    {
      "epoch": 5.63,
      "grad_norm": 5684.9970703125,
      "learning_rate": 7.407120743034057e-05,
      "loss": 16.7034,
      "step": 5625
    },
    {
      "epoch": 5.63,
      "grad_norm": 7383.19677734375,
      "learning_rate": 7.406604747162023e-05,
      "loss": 18.7698,
      "step": 5626
    },
    {
      "epoch": 5.63,
      "grad_norm": 3729.8330078125,
      "learning_rate": 7.40608875128999e-05,
      "loss": 11.5312,
      "step": 5627
    },
    {
      "epoch": 5.63,
      "grad_norm": 1636.761474609375,
      "learning_rate": 7.405572755417956e-05,
      "loss": 12.4806,
      "step": 5628
    },
    {
      "epoch": 5.63,
      "grad_norm": 4249.7109375,
      "learning_rate": 7.405056759545924e-05,
      "loss": 14.4923,
      "step": 5629
    },
    {
      "epoch": 5.64,
      "grad_norm": 610.3734741210938,
      "learning_rate": 7.404540763673891e-05,
      "loss": 13.4936,
      "step": 5630
    },
    {
      "epoch": 5.64,
      "grad_norm": 2367.912841796875,
      "learning_rate": 7.404024767801859e-05,
      "loss": 12.8465,
      "step": 5631
    },
    {
      "epoch": 5.64,
      "grad_norm": 12553.947265625,
      "learning_rate": 7.403508771929825e-05,
      "loss": 11.1882,
      "step": 5632
    },
    {
      "epoch": 5.64,
      "grad_norm": 47740.046875,
      "learning_rate": 7.402992776057792e-05,
      "loss": 16.5096,
      "step": 5633
    },
    {
      "epoch": 5.64,
      "grad_norm": 4513.115234375,
      "learning_rate": 7.402476780185758e-05,
      "loss": 11.1857,
      "step": 5634
    },
    {
      "epoch": 5.64,
      "grad_norm": 17734.263671875,
      "learning_rate": 7.401960784313726e-05,
      "loss": 21.849,
      "step": 5635
    },
    {
      "epoch": 5.64,
      "grad_norm": 9240.3662109375,
      "learning_rate": 7.401444788441693e-05,
      "loss": 15.0285,
      "step": 5636
    },
    {
      "epoch": 5.64,
      "grad_norm": 10267.736328125,
      "learning_rate": 7.40092879256966e-05,
      "loss": 11.6837,
      "step": 5637
    },
    {
      "epoch": 5.64,
      "grad_norm": 1518.596923828125,
      "learning_rate": 7.400412796697627e-05,
      "loss": 13.061,
      "step": 5638
    },
    {
      "epoch": 5.64,
      "grad_norm": 53297.953125,
      "learning_rate": 7.399896800825594e-05,
      "loss": 17.8638,
      "step": 5639
    },
    {
      "epoch": 5.65,
      "grad_norm": 4207.2080078125,
      "learning_rate": 7.39938080495356e-05,
      "loss": 11.6494,
      "step": 5640
    },
    {
      "epoch": 5.65,
      "grad_norm": 3451.3408203125,
      "learning_rate": 7.398864809081528e-05,
      "loss": 10.4713,
      "step": 5641
    },
    {
      "epoch": 5.65,
      "grad_norm": 9467.3984375,
      "learning_rate": 7.398348813209495e-05,
      "loss": 17.2566,
      "step": 5642
    },
    {
      "epoch": 5.65,
      "grad_norm": 4525.06494140625,
      "learning_rate": 7.397832817337461e-05,
      "loss": 13.237,
      "step": 5643
    },
    {
      "epoch": 5.65,
      "grad_norm": 1033.73095703125,
      "learning_rate": 7.397316821465429e-05,
      "loss": 15.942,
      "step": 5644
    },
    {
      "epoch": 5.65,
      "grad_norm": 9123.59375,
      "learning_rate": 7.396800825593395e-05,
      "loss": 14.0547,
      "step": 5645
    },
    {
      "epoch": 5.65,
      "grad_norm": 10056.8359375,
      "learning_rate": 7.396284829721362e-05,
      "loss": 15.3313,
      "step": 5646
    },
    {
      "epoch": 5.65,
      "grad_norm": 1278.349853515625,
      "learning_rate": 7.39576883384933e-05,
      "loss": 13.499,
      "step": 5647
    },
    {
      "epoch": 5.65,
      "grad_norm": 12849.1640625,
      "learning_rate": 7.395252837977297e-05,
      "loss": 15.6084,
      "step": 5648
    },
    {
      "epoch": 5.65,
      "grad_norm": 5391.732421875,
      "learning_rate": 7.394736842105263e-05,
      "loss": 12.1934,
      "step": 5649
    },
    {
      "epoch": 5.66,
      "grad_norm": 2117.84619140625,
      "learning_rate": 7.394220846233231e-05,
      "loss": 13.257,
      "step": 5650
    },
    {
      "epoch": 5.66,
      "grad_norm": 256.68621826171875,
      "learning_rate": 7.393704850361197e-05,
      "loss": 12.4517,
      "step": 5651
    },
    {
      "epoch": 5.66,
      "grad_norm": 4353.96875,
      "learning_rate": 7.393188854489164e-05,
      "loss": 14.8137,
      "step": 5652
    },
    {
      "epoch": 5.66,
      "grad_norm": 9349.7265625,
      "learning_rate": 7.392672858617132e-05,
      "loss": 14.9253,
      "step": 5653
    },
    {
      "epoch": 5.66,
      "grad_norm": 18248.392578125,
      "learning_rate": 7.392156862745099e-05,
      "loss": 11.9819,
      "step": 5654
    },
    {
      "epoch": 5.66,
      "grad_norm": 1761.9765625,
      "learning_rate": 7.391640866873065e-05,
      "loss": 13.5682,
      "step": 5655
    },
    {
      "epoch": 5.66,
      "grad_norm": 1095.5794677734375,
      "learning_rate": 7.391124871001033e-05,
      "loss": 10.8837,
      "step": 5656
    },
    {
      "epoch": 5.66,
      "grad_norm": 3372.619140625,
      "learning_rate": 7.390608875128999e-05,
      "loss": 15.4588,
      "step": 5657
    },
    {
      "epoch": 5.66,
      "grad_norm": 21773.59765625,
      "learning_rate": 7.390092879256966e-05,
      "loss": 14.6811,
      "step": 5658
    },
    {
      "epoch": 5.66,
      "grad_norm": 1073.350341796875,
      "learning_rate": 7.389576883384934e-05,
      "loss": 11.8837,
      "step": 5659
    },
    {
      "epoch": 5.67,
      "grad_norm": 30741.400390625,
      "learning_rate": 7.3890608875129e-05,
      "loss": 13.9381,
      "step": 5660
    },
    {
      "epoch": 5.67,
      "grad_norm": 5705.0146484375,
      "learning_rate": 7.388544891640867e-05,
      "loss": 13.3118,
      "step": 5661
    },
    {
      "epoch": 5.67,
      "grad_norm": 18293.5703125,
      "learning_rate": 7.388028895768833e-05,
      "loss": 16.0651,
      "step": 5662
    },
    {
      "epoch": 5.67,
      "grad_norm": 5328.92919921875,
      "learning_rate": 7.387512899896801e-05,
      "loss": 11.9216,
      "step": 5663
    },
    {
      "epoch": 5.67,
      "grad_norm": 35245.1953125,
      "learning_rate": 7.386996904024768e-05,
      "loss": 18.6542,
      "step": 5664
    },
    {
      "epoch": 5.67,
      "grad_norm": 3564.39501953125,
      "learning_rate": 7.386480908152736e-05,
      "loss": 12.4135,
      "step": 5665
    },
    {
      "epoch": 5.67,
      "grad_norm": 7696.56591796875,
      "learning_rate": 7.385964912280702e-05,
      "loss": 14.6094,
      "step": 5666
    },
    {
      "epoch": 5.67,
      "grad_norm": 3381.076416015625,
      "learning_rate": 7.385448916408669e-05,
      "loss": 13.7049,
      "step": 5667
    },
    {
      "epoch": 5.67,
      "grad_norm": 1848.9583740234375,
      "learning_rate": 7.384932920536635e-05,
      "loss": 9.9709,
      "step": 5668
    },
    {
      "epoch": 5.67,
      "grad_norm": 4844.41796875,
      "learning_rate": 7.384416924664603e-05,
      "loss": 28.5477,
      "step": 5669
    },
    {
      "epoch": 5.68,
      "grad_norm": 6480.6513671875,
      "learning_rate": 7.38390092879257e-05,
      "loss": 12.6961,
      "step": 5670
    },
    {
      "epoch": 5.68,
      "grad_norm": 4946.54541015625,
      "learning_rate": 7.383384932920538e-05,
      "loss": 14.303,
      "step": 5671
    },
    {
      "epoch": 5.68,
      "grad_norm": 10094.3115234375,
      "learning_rate": 7.382868937048504e-05,
      "loss": 20.6334,
      "step": 5672
    },
    {
      "epoch": 5.68,
      "grad_norm": 3295.29052734375,
      "learning_rate": 7.382352941176471e-05,
      "loss": 14.0044,
      "step": 5673
    },
    {
      "epoch": 5.68,
      "grad_norm": 8980.93359375,
      "learning_rate": 7.381836945304437e-05,
      "loss": 13.0163,
      "step": 5674
    },
    {
      "epoch": 5.68,
      "grad_norm": 15923.46484375,
      "learning_rate": 7.381320949432405e-05,
      "loss": 11.1642,
      "step": 5675
    },
    {
      "epoch": 5.68,
      "grad_norm": 2493.344482421875,
      "learning_rate": 7.380804953560372e-05,
      "loss": 11.0588,
      "step": 5676
    },
    {
      "epoch": 5.68,
      "grad_norm": 3342.720947265625,
      "learning_rate": 7.38028895768834e-05,
      "loss": 15.2807,
      "step": 5677
    },
    {
      "epoch": 5.68,
      "grad_norm": 2464.08837890625,
      "learning_rate": 7.379772961816306e-05,
      "loss": 16.6024,
      "step": 5678
    },
    {
      "epoch": 5.68,
      "grad_norm": 5436.98828125,
      "learning_rate": 7.379256965944272e-05,
      "loss": 12.8942,
      "step": 5679
    },
    {
      "epoch": 5.69,
      "grad_norm": 3839.848388671875,
      "learning_rate": 7.37874097007224e-05,
      "loss": 12.5104,
      "step": 5680
    },
    {
      "epoch": 5.69,
      "grad_norm": 3866.13525390625,
      "learning_rate": 7.378224974200207e-05,
      "loss": 14.0734,
      "step": 5681
    },
    {
      "epoch": 5.69,
      "grad_norm": 21374.283203125,
      "learning_rate": 7.377708978328174e-05,
      "loss": 16.1432,
      "step": 5682
    },
    {
      "epoch": 5.69,
      "grad_norm": 2147.09326171875,
      "learning_rate": 7.37719298245614e-05,
      "loss": 13.1307,
      "step": 5683
    },
    {
      "epoch": 5.69,
      "grad_norm": 3411.595703125,
      "learning_rate": 7.376676986584108e-05,
      "loss": 11.9906,
      "step": 5684
    },
    {
      "epoch": 5.69,
      "grad_norm": 2966.722412109375,
      "learning_rate": 7.376160990712074e-05,
      "loss": 14.3693,
      "step": 5685
    },
    {
      "epoch": 5.69,
      "grad_norm": 51077.4765625,
      "learning_rate": 7.375644994840041e-05,
      "loss": 19.1697,
      "step": 5686
    },
    {
      "epoch": 5.69,
      "grad_norm": 3799.78662109375,
      "learning_rate": 7.375128998968009e-05,
      "loss": 12.987,
      "step": 5687
    },
    {
      "epoch": 5.69,
      "grad_norm": 12936.3837890625,
      "learning_rate": 7.374613003095976e-05,
      "loss": 23.2555,
      "step": 5688
    },
    {
      "epoch": 5.69,
      "grad_norm": 72168.28125,
      "learning_rate": 7.374097007223942e-05,
      "loss": 14.3485,
      "step": 5689
    },
    {
      "epoch": 5.7,
      "grad_norm": 888.4307250976562,
      "learning_rate": 7.37358101135191e-05,
      "loss": 12.1331,
      "step": 5690
    },
    {
      "epoch": 5.7,
      "grad_norm": 1896.008056640625,
      "learning_rate": 7.373065015479876e-05,
      "loss": 11.5785,
      "step": 5691
    },
    {
      "epoch": 5.7,
      "grad_norm": 5229.1572265625,
      "learning_rate": 7.372549019607843e-05,
      "loss": 12.9047,
      "step": 5692
    },
    {
      "epoch": 5.7,
      "grad_norm": 2915.776611328125,
      "learning_rate": 7.372033023735811e-05,
      "loss": 15.2875,
      "step": 5693
    },
    {
      "epoch": 5.7,
      "grad_norm": 2498.886962890625,
      "learning_rate": 7.371517027863778e-05,
      "loss": 11.3604,
      "step": 5694
    },
    {
      "epoch": 5.7,
      "grad_norm": 8220.9599609375,
      "learning_rate": 7.371001031991744e-05,
      "loss": 14.6343,
      "step": 5695
    },
    {
      "epoch": 5.7,
      "grad_norm": 6867.91943359375,
      "learning_rate": 7.37048503611971e-05,
      "loss": 12.3872,
      "step": 5696
    },
    {
      "epoch": 5.7,
      "grad_norm": 2456.566162109375,
      "learning_rate": 7.369969040247678e-05,
      "loss": 14.7041,
      "step": 5697
    },
    {
      "epoch": 5.7,
      "grad_norm": 15302.05078125,
      "learning_rate": 7.369453044375645e-05,
      "loss": 13.2711,
      "step": 5698
    },
    {
      "epoch": 5.7,
      "grad_norm": 3833.280517578125,
      "learning_rate": 7.368937048503613e-05,
      "loss": 14.8393,
      "step": 5699
    },
    {
      "epoch": 5.71,
      "grad_norm": 19341.392578125,
      "learning_rate": 7.368421052631579e-05,
      "loss": 13.3203,
      "step": 5700
    },
    {
      "epoch": 5.71,
      "grad_norm": 5603.228515625,
      "learning_rate": 7.367905056759546e-05,
      "loss": 13.7476,
      "step": 5701
    },
    {
      "epoch": 5.71,
      "grad_norm": 2125.360107421875,
      "learning_rate": 7.367389060887512e-05,
      "loss": 12.2077,
      "step": 5702
    },
    {
      "epoch": 5.71,
      "grad_norm": 1919.2471923828125,
      "learning_rate": 7.36687306501548e-05,
      "loss": 13.5717,
      "step": 5703
    },
    {
      "epoch": 5.71,
      "grad_norm": 13459.322265625,
      "learning_rate": 7.366357069143447e-05,
      "loss": 14.8033,
      "step": 5704
    },
    {
      "epoch": 5.71,
      "grad_norm": 8819.6474609375,
      "learning_rate": 7.365841073271415e-05,
      "loss": 14.6891,
      "step": 5705
    },
    {
      "epoch": 5.71,
      "grad_norm": 5142.96630859375,
      "learning_rate": 7.365325077399381e-05,
      "loss": 19.4677,
      "step": 5706
    },
    {
      "epoch": 5.71,
      "grad_norm": 11745.52734375,
      "learning_rate": 7.364809081527348e-05,
      "loss": 13.2912,
      "step": 5707
    },
    {
      "epoch": 5.71,
      "grad_norm": 1464.077880859375,
      "learning_rate": 7.364293085655314e-05,
      "loss": 11.917,
      "step": 5708
    },
    {
      "epoch": 5.71,
      "grad_norm": 9307.0830078125,
      "learning_rate": 7.363777089783282e-05,
      "loss": 18.8479,
      "step": 5709
    },
    {
      "epoch": 5.72,
      "grad_norm": 6438.48486328125,
      "learning_rate": 7.36326109391125e-05,
      "loss": 14.4913,
      "step": 5710
    },
    {
      "epoch": 5.72,
      "grad_norm": 1034.2166748046875,
      "learning_rate": 7.362745098039217e-05,
      "loss": 12.2863,
      "step": 5711
    },
    {
      "epoch": 5.72,
      "grad_norm": 3285.271240234375,
      "learning_rate": 7.362229102167183e-05,
      "loss": 13.8362,
      "step": 5712
    },
    {
      "epoch": 5.72,
      "grad_norm": 4328.09326171875,
      "learning_rate": 7.36171310629515e-05,
      "loss": 14.4424,
      "step": 5713
    },
    {
      "epoch": 5.72,
      "grad_norm": 9042.0947265625,
      "learning_rate": 7.361197110423116e-05,
      "loss": 11.6861,
      "step": 5714
    },
    {
      "epoch": 5.72,
      "grad_norm": 1713.7528076171875,
      "learning_rate": 7.360681114551084e-05,
      "loss": 11.6658,
      "step": 5715
    },
    {
      "epoch": 5.72,
      "grad_norm": 2654.959716796875,
      "learning_rate": 7.360165118679051e-05,
      "loss": 19.9516,
      "step": 5716
    },
    {
      "epoch": 5.72,
      "grad_norm": 1091.042236328125,
      "learning_rate": 7.359649122807017e-05,
      "loss": 16.0459,
      "step": 5717
    },
    {
      "epoch": 5.72,
      "grad_norm": 1503.4034423828125,
      "learning_rate": 7.359133126934985e-05,
      "loss": 10.8415,
      "step": 5718
    },
    {
      "epoch": 5.72,
      "grad_norm": 2665.76953125,
      "learning_rate": 7.358617131062951e-05,
      "loss": 14.1182,
      "step": 5719
    },
    {
      "epoch": 5.73,
      "grad_norm": 21629.927734375,
      "learning_rate": 7.358101135190918e-05,
      "loss": 13.9011,
      "step": 5720
    },
    {
      "epoch": 5.73,
      "grad_norm": 5702.9150390625,
      "learning_rate": 7.357585139318886e-05,
      "loss": 11.8832,
      "step": 5721
    },
    {
      "epoch": 5.73,
      "grad_norm": 1103.8397216796875,
      "learning_rate": 7.357069143446853e-05,
      "loss": 11.6071,
      "step": 5722
    },
    {
      "epoch": 5.73,
      "grad_norm": 2256.201416015625,
      "learning_rate": 7.35655314757482e-05,
      "loss": 11.7137,
      "step": 5723
    },
    {
      "epoch": 5.73,
      "grad_norm": 24552.55078125,
      "learning_rate": 7.356037151702787e-05,
      "loss": 16.1724,
      "step": 5724
    },
    {
      "epoch": 5.73,
      "grad_norm": 7121.1455078125,
      "learning_rate": 7.355521155830753e-05,
      "loss": 13.3737,
      "step": 5725
    },
    {
      "epoch": 5.73,
      "grad_norm": 3381.237548828125,
      "learning_rate": 7.35500515995872e-05,
      "loss": 12.2709,
      "step": 5726
    },
    {
      "epoch": 5.73,
      "grad_norm": 58812.05078125,
      "learning_rate": 7.354489164086688e-05,
      "loss": 16.8073,
      "step": 5727
    },
    {
      "epoch": 5.73,
      "grad_norm": 11313.998046875,
      "learning_rate": 7.353973168214655e-05,
      "loss": 16.953,
      "step": 5728
    },
    {
      "epoch": 5.73,
      "grad_norm": 10193.751953125,
      "learning_rate": 7.353457172342621e-05,
      "loss": 12.6903,
      "step": 5729
    },
    {
      "epoch": 5.74,
      "grad_norm": 3731.5888671875,
      "learning_rate": 7.352941176470589e-05,
      "loss": 11.0066,
      "step": 5730
    },
    {
      "epoch": 5.74,
      "grad_norm": 2949.603515625,
      "learning_rate": 7.352425180598555e-05,
      "loss": 16.0367,
      "step": 5731
    },
    {
      "epoch": 5.74,
      "grad_norm": 10031.0927734375,
      "learning_rate": 7.351909184726522e-05,
      "loss": 14.6512,
      "step": 5732
    },
    {
      "epoch": 5.74,
      "grad_norm": 12153.7783203125,
      "learning_rate": 7.35139318885449e-05,
      "loss": 19.9281,
      "step": 5733
    },
    {
      "epoch": 5.74,
      "grad_norm": 10355.1640625,
      "learning_rate": 7.350877192982456e-05,
      "loss": 17.9546,
      "step": 5734
    },
    {
      "epoch": 5.74,
      "grad_norm": 5312.93310546875,
      "learning_rate": 7.350361197110423e-05,
      "loss": 13.8071,
      "step": 5735
    },
    {
      "epoch": 5.74,
      "grad_norm": 3113.9443359375,
      "learning_rate": 7.34984520123839e-05,
      "loss": 13.2557,
      "step": 5736
    },
    {
      "epoch": 5.74,
      "grad_norm": 2417.81298828125,
      "learning_rate": 7.349329205366357e-05,
      "loss": 13.4534,
      "step": 5737
    },
    {
      "epoch": 5.74,
      "grad_norm": 16311.2509765625,
      "learning_rate": 7.348813209494324e-05,
      "loss": 15.2283,
      "step": 5738
    },
    {
      "epoch": 5.74,
      "grad_norm": 49721.35546875,
      "learning_rate": 7.348297213622292e-05,
      "loss": 15.2558,
      "step": 5739
    },
    {
      "epoch": 5.75,
      "grad_norm": 3735.513427734375,
      "learning_rate": 7.347781217750258e-05,
      "loss": 12.3743,
      "step": 5740
    },
    {
      "epoch": 5.75,
      "grad_norm": 10163.2841796875,
      "learning_rate": 7.347265221878225e-05,
      "loss": 19.2316,
      "step": 5741
    },
    {
      "epoch": 5.75,
      "grad_norm": 7105.9775390625,
      "learning_rate": 7.346749226006192e-05,
      "loss": 13.3505,
      "step": 5742
    },
    {
      "epoch": 5.75,
      "grad_norm": 3652.85302734375,
      "learning_rate": 7.346233230134159e-05,
      "loss": 13.8312,
      "step": 5743
    },
    {
      "epoch": 5.75,
      "grad_norm": 4600.0185546875,
      "learning_rate": 7.345717234262126e-05,
      "loss": 13.9942,
      "step": 5744
    },
    {
      "epoch": 5.75,
      "grad_norm": 1559.3778076171875,
      "learning_rate": 7.345201238390094e-05,
      "loss": 13.7949,
      "step": 5745
    },
    {
      "epoch": 5.75,
      "grad_norm": 5355.1611328125,
      "learning_rate": 7.34468524251806e-05,
      "loss": 13.361,
      "step": 5746
    },
    {
      "epoch": 5.75,
      "grad_norm": 4373.5107421875,
      "learning_rate": 7.344169246646027e-05,
      "loss": 16.5078,
      "step": 5747
    },
    {
      "epoch": 5.75,
      "grad_norm": 2219.20166015625,
      "learning_rate": 7.343653250773994e-05,
      "loss": 12.1834,
      "step": 5748
    },
    {
      "epoch": 5.75,
      "grad_norm": 2993.7138671875,
      "learning_rate": 7.343137254901961e-05,
      "loss": 12.8134,
      "step": 5749
    },
    {
      "epoch": 5.76,
      "grad_norm": 26125.4609375,
      "learning_rate": 7.342621259029928e-05,
      "loss": 13.6187,
      "step": 5750
    },
    {
      "epoch": 5.76,
      "grad_norm": 3758.13330078125,
      "learning_rate": 7.342105263157895e-05,
      "loss": 13.5224,
      "step": 5751
    },
    {
      "epoch": 5.76,
      "grad_norm": 9236.92578125,
      "learning_rate": 7.341589267285862e-05,
      "loss": 13.372,
      "step": 5752
    },
    {
      "epoch": 5.76,
      "grad_norm": 21702.029296875,
      "learning_rate": 7.341073271413828e-05,
      "loss": 14.1606,
      "step": 5753
    },
    {
      "epoch": 5.76,
      "grad_norm": 3115.140625,
      "learning_rate": 7.340557275541796e-05,
      "loss": 13.6502,
      "step": 5754
    },
    {
      "epoch": 5.76,
      "grad_norm": 9321.619140625,
      "learning_rate": 7.340041279669763e-05,
      "loss": 16.9835,
      "step": 5755
    },
    {
      "epoch": 5.76,
      "grad_norm": 6156.01220703125,
      "learning_rate": 7.33952528379773e-05,
      "loss": 13.2278,
      "step": 5756
    },
    {
      "epoch": 5.76,
      "grad_norm": 21244.5078125,
      "learning_rate": 7.339009287925697e-05,
      "loss": 13.6917,
      "step": 5757
    },
    {
      "epoch": 5.76,
      "grad_norm": 9021.1826171875,
      "learning_rate": 7.338493292053664e-05,
      "loss": 13.5675,
      "step": 5758
    },
    {
      "epoch": 5.76,
      "grad_norm": 943.7390747070312,
      "learning_rate": 7.33797729618163e-05,
      "loss": 13.0059,
      "step": 5759
    },
    {
      "epoch": 5.77,
      "grad_norm": 42626.37890625,
      "learning_rate": 7.337461300309598e-05,
      "loss": 15.1235,
      "step": 5760
    },
    {
      "epoch": 5.77,
      "grad_norm": 6213.75830078125,
      "learning_rate": 7.336945304437565e-05,
      "loss": 14.5292,
      "step": 5761
    },
    {
      "epoch": 5.77,
      "grad_norm": 1699.323974609375,
      "learning_rate": 7.336429308565532e-05,
      "loss": 28.4999,
      "step": 5762
    },
    {
      "epoch": 5.77,
      "grad_norm": 4768.5205078125,
      "learning_rate": 7.335913312693499e-05,
      "loss": 10.7955,
      "step": 5763
    },
    {
      "epoch": 5.77,
      "grad_norm": 2611.61376953125,
      "learning_rate": 7.335397316821466e-05,
      "loss": 12.0591,
      "step": 5764
    },
    {
      "epoch": 5.77,
      "grad_norm": 3641.756591796875,
      "learning_rate": 7.334881320949432e-05,
      "loss": 12.4858,
      "step": 5765
    },
    {
      "epoch": 5.77,
      "grad_norm": 758.2694091796875,
      "learning_rate": 7.3343653250774e-05,
      "loss": 10.9987,
      "step": 5766
    },
    {
      "epoch": 5.77,
      "grad_norm": 3770.538330078125,
      "learning_rate": 7.333849329205367e-05,
      "loss": 12.7772,
      "step": 5767
    },
    {
      "epoch": 5.77,
      "grad_norm": 5565.171875,
      "learning_rate": 7.333333333333333e-05,
      "loss": 18.3371,
      "step": 5768
    },
    {
      "epoch": 5.77,
      "grad_norm": 11976.5517578125,
      "learning_rate": 7.3328173374613e-05,
      "loss": 12.2082,
      "step": 5769
    },
    {
      "epoch": 5.78,
      "grad_norm": 24450.859375,
      "learning_rate": 7.332301341589267e-05,
      "loss": 12.9946,
      "step": 5770
    },
    {
      "epoch": 5.78,
      "grad_norm": 4233.7060546875,
      "learning_rate": 7.331785345717234e-05,
      "loss": 12.1691,
      "step": 5771
    },
    {
      "epoch": 5.78,
      "grad_norm": 5043.369140625,
      "learning_rate": 7.331269349845202e-05,
      "loss": 17.0488,
      "step": 5772
    },
    {
      "epoch": 5.78,
      "grad_norm": 139799.546875,
      "learning_rate": 7.330753353973169e-05,
      "loss": 13.6428,
      "step": 5773
    },
    {
      "epoch": 5.78,
      "grad_norm": 1839.93359375,
      "learning_rate": 7.330237358101135e-05,
      "loss": 12.3194,
      "step": 5774
    },
    {
      "epoch": 5.78,
      "grad_norm": 7470.7744140625,
      "learning_rate": 7.329721362229103e-05,
      "loss": 15.2487,
      "step": 5775
    },
    {
      "epoch": 5.78,
      "grad_norm": 5102.44384765625,
      "learning_rate": 7.329205366357069e-05,
      "loss": 11.2303,
      "step": 5776
    },
    {
      "epoch": 5.78,
      "grad_norm": 9671.654296875,
      "learning_rate": 7.328689370485036e-05,
      "loss": 16.6089,
      "step": 5777
    },
    {
      "epoch": 5.78,
      "grad_norm": 7756.81787109375,
      "learning_rate": 7.328173374613004e-05,
      "loss": 12.552,
      "step": 5778
    },
    {
      "epoch": 5.78,
      "grad_norm": 2707.520751953125,
      "learning_rate": 7.327657378740971e-05,
      "loss": 20.0733,
      "step": 5779
    },
    {
      "epoch": 5.79,
      "grad_norm": 5370.1904296875,
      "learning_rate": 7.327141382868937e-05,
      "loss": 13.8399,
      "step": 5780
    },
    {
      "epoch": 5.79,
      "grad_norm": 9319.4296875,
      "learning_rate": 7.326625386996905e-05,
      "loss": 11.4937,
      "step": 5781
    },
    {
      "epoch": 5.79,
      "grad_norm": 8787.93359375,
      "learning_rate": 7.326109391124871e-05,
      "loss": 14.1219,
      "step": 5782
    },
    {
      "epoch": 5.79,
      "grad_norm": 1064.2554931640625,
      "learning_rate": 7.32559339525284e-05,
      "loss": 11.9915,
      "step": 5783
    },
    {
      "epoch": 5.79,
      "grad_norm": 17995.955078125,
      "learning_rate": 7.325077399380806e-05,
      "loss": 15.7276,
      "step": 5784
    },
    {
      "epoch": 5.79,
      "grad_norm": 8528.4736328125,
      "learning_rate": 7.324561403508772e-05,
      "loss": 11.4018,
      "step": 5785
    },
    {
      "epoch": 5.79,
      "grad_norm": 2381.072265625,
      "learning_rate": 7.324045407636739e-05,
      "loss": 18.5094,
      "step": 5786
    },
    {
      "epoch": 5.79,
      "grad_norm": 4691.80615234375,
      "learning_rate": 7.323529411764705e-05,
      "loss": 15.5436,
      "step": 5787
    },
    {
      "epoch": 5.79,
      "grad_norm": 1071.13916015625,
      "learning_rate": 7.323013415892673e-05,
      "loss": 11.9489,
      "step": 5788
    },
    {
      "epoch": 5.79,
      "grad_norm": 1911.49951171875,
      "learning_rate": 7.32249742002064e-05,
      "loss": 11.9356,
      "step": 5789
    },
    {
      "epoch": 5.8,
      "grad_norm": 28510.490234375,
      "learning_rate": 7.321981424148608e-05,
      "loss": 16.4865,
      "step": 5790
    },
    {
      "epoch": 5.8,
      "grad_norm": 5303.73974609375,
      "learning_rate": 7.321465428276574e-05,
      "loss": 13.2961,
      "step": 5791
    },
    {
      "epoch": 5.8,
      "grad_norm": 16433.126953125,
      "learning_rate": 7.320949432404541e-05,
      "loss": 16.1135,
      "step": 5792
    },
    {
      "epoch": 5.8,
      "grad_norm": 4485.94970703125,
      "learning_rate": 7.320433436532507e-05,
      "loss": 17.9509,
      "step": 5793
    },
    {
      "epoch": 5.8,
      "grad_norm": 4344.09619140625,
      "learning_rate": 7.319917440660475e-05,
      "loss": 13.4473,
      "step": 5794
    },
    {
      "epoch": 5.8,
      "grad_norm": 5259.23583984375,
      "learning_rate": 7.319401444788442e-05,
      "loss": 16.2504,
      "step": 5795
    },
    {
      "epoch": 5.8,
      "grad_norm": 136688.40625,
      "learning_rate": 7.31888544891641e-05,
      "loss": 20.3651,
      "step": 5796
    },
    {
      "epoch": 5.8,
      "grad_norm": 11497.2490234375,
      "learning_rate": 7.318369453044376e-05,
      "loss": 14.848,
      "step": 5797
    },
    {
      "epoch": 5.8,
      "grad_norm": 10690.69921875,
      "learning_rate": 7.317853457172343e-05,
      "loss": 12.3678,
      "step": 5798
    },
    {
      "epoch": 5.8,
      "grad_norm": 3612.73779296875,
      "learning_rate": 7.317337461300309e-05,
      "loss": 13.5066,
      "step": 5799
    },
    {
      "epoch": 5.81,
      "grad_norm": 14234.3828125,
      "learning_rate": 7.316821465428278e-05,
      "loss": 17.4375,
      "step": 5800
    },
    {
      "epoch": 5.81,
      "grad_norm": 9054.5166015625,
      "learning_rate": 7.316305469556244e-05,
      "loss": 12.5872,
      "step": 5801
    },
    {
      "epoch": 5.81,
      "grad_norm": 3373.051025390625,
      "learning_rate": 7.315789473684212e-05,
      "loss": 14.8636,
      "step": 5802
    },
    {
      "epoch": 5.81,
      "grad_norm": 12262.7060546875,
      "learning_rate": 7.315273477812178e-05,
      "loss": 12.8615,
      "step": 5803
    },
    {
      "epoch": 5.81,
      "grad_norm": 3016.369384765625,
      "learning_rate": 7.314757481940144e-05,
      "loss": 15.1562,
      "step": 5804
    },
    {
      "epoch": 5.81,
      "grad_norm": 2759.333984375,
      "learning_rate": 7.314241486068111e-05,
      "loss": 14.4875,
      "step": 5805
    },
    {
      "epoch": 5.81,
      "grad_norm": 19661.9453125,
      "learning_rate": 7.313725490196079e-05,
      "loss": 15.3601,
      "step": 5806
    },
    {
      "epoch": 5.81,
      "grad_norm": 18592.1484375,
      "learning_rate": 7.313209494324046e-05,
      "loss": 12.9566,
      "step": 5807
    },
    {
      "epoch": 5.81,
      "grad_norm": 2433.6416015625,
      "learning_rate": 7.312693498452012e-05,
      "loss": 16.0415,
      "step": 5808
    },
    {
      "epoch": 5.81,
      "grad_norm": 2925.197998046875,
      "learning_rate": 7.31217750257998e-05,
      "loss": 17.8617,
      "step": 5809
    },
    {
      "epoch": 5.82,
      "grad_norm": 2018.8160400390625,
      "learning_rate": 7.311661506707946e-05,
      "loss": 14.0772,
      "step": 5810
    },
    {
      "epoch": 5.82,
      "grad_norm": 8795.822265625,
      "learning_rate": 7.311145510835915e-05,
      "loss": 12.0264,
      "step": 5811
    },
    {
      "epoch": 5.82,
      "grad_norm": 8693.490234375,
      "learning_rate": 7.310629514963881e-05,
      "loss": 15.428,
      "step": 5812
    },
    {
      "epoch": 5.82,
      "grad_norm": 4206.998046875,
      "learning_rate": 7.310113519091848e-05,
      "loss": 14.6578,
      "step": 5813
    },
    {
      "epoch": 5.82,
      "grad_norm": 3429.732177734375,
      "learning_rate": 7.309597523219814e-05,
      "loss": 14.0784,
      "step": 5814
    },
    {
      "epoch": 5.82,
      "grad_norm": 2915.8916015625,
      "learning_rate": 7.309081527347782e-05,
      "loss": 13.1449,
      "step": 5815
    },
    {
      "epoch": 5.82,
      "grad_norm": 1255.5714111328125,
      "learning_rate": 7.308565531475748e-05,
      "loss": 13.0654,
      "step": 5816
    },
    {
      "epoch": 5.82,
      "grad_norm": 3478.125732421875,
      "learning_rate": 7.308049535603717e-05,
      "loss": 14.0431,
      "step": 5817
    },
    {
      "epoch": 5.82,
      "grad_norm": 22741.662109375,
      "learning_rate": 7.307533539731683e-05,
      "loss": 17.8873,
      "step": 5818
    },
    {
      "epoch": 5.82,
      "grad_norm": 1373.7601318359375,
      "learning_rate": 7.30701754385965e-05,
      "loss": 12.9543,
      "step": 5819
    },
    {
      "epoch": 5.83,
      "grad_norm": 2671.56494140625,
      "learning_rate": 7.306501547987616e-05,
      "loss": 12.5608,
      "step": 5820
    },
    {
      "epoch": 5.83,
      "grad_norm": 18282.806640625,
      "learning_rate": 7.305985552115582e-05,
      "loss": 25.1137,
      "step": 5821
    },
    {
      "epoch": 5.83,
      "grad_norm": 5863.68505859375,
      "learning_rate": 7.30546955624355e-05,
      "loss": 12.0531,
      "step": 5822
    },
    {
      "epoch": 5.83,
      "grad_norm": 6376.494140625,
      "learning_rate": 7.304953560371517e-05,
      "loss": 13.0638,
      "step": 5823
    },
    {
      "epoch": 5.83,
      "grad_norm": 15679.3759765625,
      "learning_rate": 7.304437564499485e-05,
      "loss": 13.0935,
      "step": 5824
    },
    {
      "epoch": 5.83,
      "grad_norm": 6641.74365234375,
      "learning_rate": 7.303921568627451e-05,
      "loss": 14.3477,
      "step": 5825
    },
    {
      "epoch": 5.83,
      "grad_norm": 3411.805419921875,
      "learning_rate": 7.303405572755418e-05,
      "loss": 12.3527,
      "step": 5826
    },
    {
      "epoch": 5.83,
      "grad_norm": 3466.168701171875,
      "learning_rate": 7.302889576883384e-05,
      "loss": 15.6967,
      "step": 5827
    },
    {
      "epoch": 5.83,
      "grad_norm": 2037.1103515625,
      "learning_rate": 7.302373581011353e-05,
      "loss": 11.4341,
      "step": 5828
    },
    {
      "epoch": 5.83,
      "grad_norm": 20720.150390625,
      "learning_rate": 7.301857585139319e-05,
      "loss": 19.8172,
      "step": 5829
    },
    {
      "epoch": 5.84,
      "grad_norm": 1915.539794921875,
      "learning_rate": 7.301341589267287e-05,
      "loss": 13.1301,
      "step": 5830
    },
    {
      "epoch": 5.84,
      "grad_norm": 3388.49560546875,
      "learning_rate": 7.300825593395253e-05,
      "loss": 13.0949,
      "step": 5831
    },
    {
      "epoch": 5.84,
      "grad_norm": 1517.265625,
      "learning_rate": 7.30030959752322e-05,
      "loss": 14.6539,
      "step": 5832
    },
    {
      "epoch": 5.84,
      "grad_norm": 3950.439208984375,
      "learning_rate": 7.299793601651186e-05,
      "loss": 14.1119,
      "step": 5833
    },
    {
      "epoch": 5.84,
      "grad_norm": 5748.92236328125,
      "learning_rate": 7.299277605779155e-05,
      "loss": 13.0089,
      "step": 5834
    },
    {
      "epoch": 5.84,
      "grad_norm": 11765.8916015625,
      "learning_rate": 7.298761609907121e-05,
      "loss": 12.7623,
      "step": 5835
    },
    {
      "epoch": 5.84,
      "grad_norm": 15821.021484375,
      "learning_rate": 7.298245614035089e-05,
      "loss": 12.5738,
      "step": 5836
    },
    {
      "epoch": 5.84,
      "grad_norm": 16139.96875,
      "learning_rate": 7.297729618163055e-05,
      "loss": 21.1199,
      "step": 5837
    },
    {
      "epoch": 5.84,
      "grad_norm": 26086.65234375,
      "learning_rate": 7.297213622291022e-05,
      "loss": 14.3527,
      "step": 5838
    },
    {
      "epoch": 5.84,
      "grad_norm": 1140.0211181640625,
      "learning_rate": 7.296697626418988e-05,
      "loss": 10.5093,
      "step": 5839
    },
    {
      "epoch": 5.85,
      "grad_norm": 1691.6661376953125,
      "learning_rate": 7.296181630546956e-05,
      "loss": 12.2007,
      "step": 5840
    },
    {
      "epoch": 5.85,
      "grad_norm": 1177.55810546875,
      "learning_rate": 7.295665634674923e-05,
      "loss": 14.1983,
      "step": 5841
    },
    {
      "epoch": 5.85,
      "grad_norm": 4252.5380859375,
      "learning_rate": 7.29514963880289e-05,
      "loss": 13.8324,
      "step": 5842
    },
    {
      "epoch": 5.85,
      "grad_norm": 2624.63037109375,
      "learning_rate": 7.294633642930857e-05,
      "loss": 13.0487,
      "step": 5843
    },
    {
      "epoch": 5.85,
      "grad_norm": 5049.19140625,
      "learning_rate": 7.294117647058823e-05,
      "loss": 11.2552,
      "step": 5844
    },
    {
      "epoch": 5.85,
      "grad_norm": 26163.771484375,
      "learning_rate": 7.293601651186792e-05,
      "loss": 15.9315,
      "step": 5845
    },
    {
      "epoch": 5.85,
      "grad_norm": 1458.4044189453125,
      "learning_rate": 7.293085655314758e-05,
      "loss": 12.1976,
      "step": 5846
    },
    {
      "epoch": 5.85,
      "grad_norm": 3059.609375,
      "learning_rate": 7.292569659442725e-05,
      "loss": 11.3884,
      "step": 5847
    },
    {
      "epoch": 5.85,
      "grad_norm": 4412.35791015625,
      "learning_rate": 7.292053663570691e-05,
      "loss": 16.3673,
      "step": 5848
    },
    {
      "epoch": 5.85,
      "grad_norm": 8271.3115234375,
      "learning_rate": 7.291537667698659e-05,
      "loss": 14.6628,
      "step": 5849
    },
    {
      "epoch": 5.86,
      "grad_norm": 3704.251708984375,
      "learning_rate": 7.291021671826625e-05,
      "loss": 16.9333,
      "step": 5850
    },
    {
      "epoch": 5.86,
      "grad_norm": 18101.6171875,
      "learning_rate": 7.290505675954594e-05,
      "loss": 16.2128,
      "step": 5851
    },
    {
      "epoch": 5.86,
      "grad_norm": 8814.7392578125,
      "learning_rate": 7.28998968008256e-05,
      "loss": 17.8929,
      "step": 5852
    },
    {
      "epoch": 5.86,
      "grad_norm": 18429.443359375,
      "learning_rate": 7.289473684210527e-05,
      "loss": 19.0995,
      "step": 5853
    },
    {
      "epoch": 5.86,
      "grad_norm": 4528.94775390625,
      "learning_rate": 7.288957688338493e-05,
      "loss": 13.0046,
      "step": 5854
    },
    {
      "epoch": 5.86,
      "grad_norm": 1650.338134765625,
      "learning_rate": 7.288441692466461e-05,
      "loss": 13.1776,
      "step": 5855
    },
    {
      "epoch": 5.86,
      "grad_norm": 4416.3740234375,
      "learning_rate": 7.287925696594428e-05,
      "loss": 14.3458,
      "step": 5856
    },
    {
      "epoch": 5.86,
      "grad_norm": 2978.86279296875,
      "learning_rate": 7.287409700722394e-05,
      "loss": 12.0192,
      "step": 5857
    },
    {
      "epoch": 5.86,
      "grad_norm": 2602.037353515625,
      "learning_rate": 7.286893704850362e-05,
      "loss": 17.0312,
      "step": 5858
    },
    {
      "epoch": 5.86,
      "grad_norm": 1611.7264404296875,
      "learning_rate": 7.286377708978328e-05,
      "loss": 11.8346,
      "step": 5859
    },
    {
      "epoch": 5.87,
      "grad_norm": 11568.69140625,
      "learning_rate": 7.285861713106295e-05,
      "loss": 22.2936,
      "step": 5860
    },
    {
      "epoch": 5.87,
      "grad_norm": 5532.38916015625,
      "learning_rate": 7.285345717234261e-05,
      "loss": 11.4992,
      "step": 5861
    },
    {
      "epoch": 5.87,
      "grad_norm": 3253.20068359375,
      "learning_rate": 7.28482972136223e-05,
      "loss": 18.174,
      "step": 5862
    },
    {
      "epoch": 5.87,
      "grad_norm": 7431.033203125,
      "learning_rate": 7.284313725490196e-05,
      "loss": 13.8434,
      "step": 5863
    },
    {
      "epoch": 5.87,
      "grad_norm": 1015.21142578125,
      "learning_rate": 7.283797729618164e-05,
      "loss": 13.7615,
      "step": 5864
    },
    {
      "epoch": 5.87,
      "grad_norm": 5842.12060546875,
      "learning_rate": 7.28328173374613e-05,
      "loss": 15.6773,
      "step": 5865
    },
    {
      "epoch": 5.87,
      "grad_norm": 22490.921875,
      "learning_rate": 7.282765737874097e-05,
      "loss": 13.9748,
      "step": 5866
    },
    {
      "epoch": 5.87,
      "grad_norm": 5575.5673828125,
      "learning_rate": 7.282249742002063e-05,
      "loss": 14.1736,
      "step": 5867
    },
    {
      "epoch": 5.87,
      "grad_norm": 6395.78076171875,
      "learning_rate": 7.281733746130032e-05,
      "loss": 13.6631,
      "step": 5868
    },
    {
      "epoch": 5.87,
      "grad_norm": 616.6455688476562,
      "learning_rate": 7.281217750257998e-05,
      "loss": 10.8816,
      "step": 5869
    },
    {
      "epoch": 5.88,
      "grad_norm": 7404.24462890625,
      "learning_rate": 7.280701754385966e-05,
      "loss": 12.9043,
      "step": 5870
    },
    {
      "epoch": 5.88,
      "grad_norm": 16636.951171875,
      "learning_rate": 7.280185758513932e-05,
      "loss": 14.1637,
      "step": 5871
    },
    {
      "epoch": 5.88,
      "grad_norm": 2774.453857421875,
      "learning_rate": 7.2796697626419e-05,
      "loss": 11.4048,
      "step": 5872
    },
    {
      "epoch": 5.88,
      "grad_norm": 1971.3741455078125,
      "learning_rate": 7.279153766769867e-05,
      "loss": 12.2595,
      "step": 5873
    },
    {
      "epoch": 5.88,
      "grad_norm": 330.4285888671875,
      "learning_rate": 7.278637770897834e-05,
      "loss": 12.1956,
      "step": 5874
    },
    {
      "epoch": 5.88,
      "grad_norm": 10145.41015625,
      "learning_rate": 7.2781217750258e-05,
      "loss": 15.0885,
      "step": 5875
    },
    {
      "epoch": 5.88,
      "grad_norm": 509.76593017578125,
      "learning_rate": 7.277605779153766e-05,
      "loss": 10.7511,
      "step": 5876
    },
    {
      "epoch": 5.88,
      "grad_norm": 9525.8837890625,
      "learning_rate": 7.277089783281734e-05,
      "loss": 11.7278,
      "step": 5877
    },
    {
      "epoch": 5.88,
      "grad_norm": 4714.2119140625,
      "learning_rate": 7.2765737874097e-05,
      "loss": 13.5379,
      "step": 5878
    },
    {
      "epoch": 5.88,
      "grad_norm": 13641.6455078125,
      "learning_rate": 7.276057791537669e-05,
      "loss": 12.3507,
      "step": 5879
    },
    {
      "epoch": 5.89,
      "grad_norm": 3588.920654296875,
      "learning_rate": 7.275541795665635e-05,
      "loss": 12.2104,
      "step": 5880
    },
    {
      "epoch": 5.89,
      "grad_norm": 1823.7603759765625,
      "learning_rate": 7.275025799793602e-05,
      "loss": 13.9294,
      "step": 5881
    },
    {
      "epoch": 5.89,
      "grad_norm": 3383.270263671875,
      "learning_rate": 7.274509803921568e-05,
      "loss": 12.2686,
      "step": 5882
    },
    {
      "epoch": 5.89,
      "grad_norm": 5655.6689453125,
      "learning_rate": 7.273993808049536e-05,
      "loss": 13.5026,
      "step": 5883
    },
    {
      "epoch": 5.89,
      "grad_norm": 1126.967529296875,
      "learning_rate": 7.273477812177503e-05,
      "loss": 15.4973,
      "step": 5884
    },
    {
      "epoch": 5.89,
      "grad_norm": 4039.575439453125,
      "learning_rate": 7.272961816305471e-05,
      "loss": 10.6156,
      "step": 5885
    },
    {
      "epoch": 5.89,
      "grad_norm": 25881.140625,
      "learning_rate": 7.272445820433437e-05,
      "loss": 13.1287,
      "step": 5886
    },
    {
      "epoch": 5.89,
      "grad_norm": 14807.638671875,
      "learning_rate": 7.271929824561404e-05,
      "loss": 16.2313,
      "step": 5887
    },
    {
      "epoch": 5.89,
      "grad_norm": 2795.762939453125,
      "learning_rate": 7.27141382868937e-05,
      "loss": 11.3519,
      "step": 5888
    },
    {
      "epoch": 5.89,
      "grad_norm": 1217.0269775390625,
      "learning_rate": 7.270897832817338e-05,
      "loss": 12.1702,
      "step": 5889
    },
    {
      "epoch": 5.9,
      "grad_norm": 15148.7734375,
      "learning_rate": 7.270381836945305e-05,
      "loss": 21.6961,
      "step": 5890
    },
    {
      "epoch": 5.9,
      "grad_norm": 2379.65869140625,
      "learning_rate": 7.269865841073273e-05,
      "loss": 13.522,
      "step": 5891
    },
    {
      "epoch": 5.9,
      "grad_norm": 4897.0693359375,
      "learning_rate": 7.269349845201239e-05,
      "loss": 13.3208,
      "step": 5892
    },
    {
      "epoch": 5.9,
      "grad_norm": 17466.90625,
      "learning_rate": 7.268833849329205e-05,
      "loss": 15.7272,
      "step": 5893
    },
    {
      "epoch": 5.9,
      "grad_norm": 11591.90625,
      "learning_rate": 7.268317853457172e-05,
      "loss": 13.8379,
      "step": 5894
    },
    {
      "epoch": 5.9,
      "grad_norm": 3695.53271484375,
      "learning_rate": 7.267801857585139e-05,
      "loss": 12.4304,
      "step": 5895
    },
    {
      "epoch": 5.9,
      "grad_norm": 3573.7158203125,
      "learning_rate": 7.267285861713107e-05,
      "loss": 11.6947,
      "step": 5896
    },
    {
      "epoch": 5.9,
      "grad_norm": 1874.1932373046875,
      "learning_rate": 7.266769865841073e-05,
      "loss": 12.5923,
      "step": 5897
    },
    {
      "epoch": 5.9,
      "grad_norm": 2292.199462890625,
      "learning_rate": 7.266253869969041e-05,
      "loss": 13.1867,
      "step": 5898
    },
    {
      "epoch": 5.9,
      "grad_norm": 5773.3037109375,
      "learning_rate": 7.265737874097007e-05,
      "loss": 12.1779,
      "step": 5899
    },
    {
      "epoch": 5.91,
      "grad_norm": 2783.123779296875,
      "learning_rate": 7.265221878224974e-05,
      "loss": 15.0914,
      "step": 5900
    },
    {
      "epoch": 5.91,
      "grad_norm": 465.8470458984375,
      "learning_rate": 7.264705882352942e-05,
      "loss": 9.8021,
      "step": 5901
    },
    {
      "epoch": 5.91,
      "grad_norm": 11910.126953125,
      "learning_rate": 7.26418988648091e-05,
      "loss": 12.6009,
      "step": 5902
    },
    {
      "epoch": 5.91,
      "grad_norm": 8398.1552734375,
      "learning_rate": 7.263673890608875e-05,
      "loss": 16.0472,
      "step": 5903
    },
    {
      "epoch": 5.91,
      "grad_norm": 3233.838623046875,
      "learning_rate": 7.263157894736843e-05,
      "loss": 11.901,
      "step": 5904
    },
    {
      "epoch": 5.91,
      "grad_norm": 2666.438232421875,
      "learning_rate": 7.262641898864809e-05,
      "loss": 12.8131,
      "step": 5905
    },
    {
      "epoch": 5.91,
      "grad_norm": 3162.494384765625,
      "learning_rate": 7.262125902992776e-05,
      "loss": 14.118,
      "step": 5906
    },
    {
      "epoch": 5.91,
      "grad_norm": 16025.7734375,
      "learning_rate": 7.261609907120744e-05,
      "loss": 17.6501,
      "step": 5907
    },
    {
      "epoch": 5.91,
      "grad_norm": 4240.734375,
      "learning_rate": 7.261093911248711e-05,
      "loss": 15.0166,
      "step": 5908
    },
    {
      "epoch": 5.91,
      "grad_norm": 2292.599853515625,
      "learning_rate": 7.260577915376677e-05,
      "loss": 13.4355,
      "step": 5909
    },
    {
      "epoch": 5.92,
      "grad_norm": 1173.214599609375,
      "learning_rate": 7.260061919504644e-05,
      "loss": 11.498,
      "step": 5910
    },
    {
      "epoch": 5.92,
      "grad_norm": 6428.3759765625,
      "learning_rate": 7.259545923632611e-05,
      "loss": 17.4275,
      "step": 5911
    },
    {
      "epoch": 5.92,
      "grad_norm": 9200.68359375,
      "learning_rate": 7.259029927760578e-05,
      "loss": 13.5518,
      "step": 5912
    },
    {
      "epoch": 5.92,
      "grad_norm": 65134.43359375,
      "learning_rate": 7.258513931888546e-05,
      "loss": 15.3357,
      "step": 5913
    },
    {
      "epoch": 5.92,
      "grad_norm": 7306.3408203125,
      "learning_rate": 7.257997936016512e-05,
      "loss": 12.8325,
      "step": 5914
    },
    {
      "epoch": 5.92,
      "grad_norm": 4892.51513671875,
      "learning_rate": 7.25748194014448e-05,
      "loss": 14.5166,
      "step": 5915
    },
    {
      "epoch": 5.92,
      "grad_norm": 1125.2105712890625,
      "learning_rate": 7.256965944272446e-05,
      "loss": 17.966,
      "step": 5916
    },
    {
      "epoch": 5.92,
      "grad_norm": 8405.9599609375,
      "learning_rate": 7.256449948400413e-05,
      "loss": 12.6907,
      "step": 5917
    },
    {
      "epoch": 5.92,
      "grad_norm": 1632.537841796875,
      "learning_rate": 7.25593395252838e-05,
      "loss": 16.4389,
      "step": 5918
    },
    {
      "epoch": 5.92,
      "grad_norm": 2509.83984375,
      "learning_rate": 7.255417956656348e-05,
      "loss": 16.1634,
      "step": 5919
    },
    {
      "epoch": 5.93,
      "grad_norm": 4434.333984375,
      "learning_rate": 7.254901960784314e-05,
      "loss": 11.801,
      "step": 5920
    },
    {
      "epoch": 5.93,
      "grad_norm": 2640.925048828125,
      "learning_rate": 7.254385964912281e-05,
      "loss": 14.4244,
      "step": 5921
    },
    {
      "epoch": 5.93,
      "grad_norm": 8927.6650390625,
      "learning_rate": 7.253869969040248e-05,
      "loss": 12.8548,
      "step": 5922
    },
    {
      "epoch": 5.93,
      "grad_norm": 3924.359130859375,
      "learning_rate": 7.253353973168215e-05,
      "loss": 11.8873,
      "step": 5923
    },
    {
      "epoch": 5.93,
      "grad_norm": 1314.1275634765625,
      "learning_rate": 7.252837977296182e-05,
      "loss": 9.6969,
      "step": 5924
    },
    {
      "epoch": 5.93,
      "grad_norm": 2684.6474609375,
      "learning_rate": 7.25232198142415e-05,
      "loss": 15.215,
      "step": 5925
    },
    {
      "epoch": 5.93,
      "grad_norm": 3048.444580078125,
      "learning_rate": 7.251805985552116e-05,
      "loss": 11.3423,
      "step": 5926
    },
    {
      "epoch": 5.93,
      "grad_norm": 1847.65576171875,
      "learning_rate": 7.251289989680083e-05,
      "loss": 11.0907,
      "step": 5927
    },
    {
      "epoch": 5.93,
      "grad_norm": 4685.06298828125,
      "learning_rate": 7.25077399380805e-05,
      "loss": 12.9683,
      "step": 5928
    },
    {
      "epoch": 5.93,
      "grad_norm": 1728.658203125,
      "learning_rate": 7.250257997936017e-05,
      "loss": 19.9421,
      "step": 5929
    },
    {
      "epoch": 5.94,
      "grad_norm": 4749.5283203125,
      "learning_rate": 7.249742002063984e-05,
      "loss": 10.9328,
      "step": 5930
    },
    {
      "epoch": 5.94,
      "grad_norm": 1738.898193359375,
      "learning_rate": 7.24922600619195e-05,
      "loss": 12.0866,
      "step": 5931
    },
    {
      "epoch": 5.94,
      "grad_norm": 1405.4383544921875,
      "learning_rate": 7.248710010319918e-05,
      "loss": 11.4144,
      "step": 5932
    },
    {
      "epoch": 5.94,
      "grad_norm": 1824.8460693359375,
      "learning_rate": 7.248194014447884e-05,
      "loss": 17.7849,
      "step": 5933
    },
    {
      "epoch": 5.94,
      "grad_norm": 2148.684326171875,
      "learning_rate": 7.247678018575852e-05,
      "loss": 14.0309,
      "step": 5934
    },
    {
      "epoch": 5.94,
      "grad_norm": 2424.529052734375,
      "learning_rate": 7.247162022703819e-05,
      "loss": 11.9679,
      "step": 5935
    },
    {
      "epoch": 5.94,
      "grad_norm": 1522.52783203125,
      "learning_rate": 7.246646026831786e-05,
      "loss": 11.555,
      "step": 5936
    },
    {
      "epoch": 5.94,
      "grad_norm": 2762.561767578125,
      "learning_rate": 7.246130030959753e-05,
      "loss": 12.0358,
      "step": 5937
    },
    {
      "epoch": 5.94,
      "grad_norm": 235.95669555664062,
      "learning_rate": 7.24561403508772e-05,
      "loss": 10.6804,
      "step": 5938
    },
    {
      "epoch": 5.94,
      "grad_norm": 7378.28271484375,
      "learning_rate": 7.245098039215686e-05,
      "loss": 16.3308,
      "step": 5939
    },
    {
      "epoch": 5.95,
      "grad_norm": 16387.9453125,
      "learning_rate": 7.244582043343654e-05,
      "loss": 15.4121,
      "step": 5940
    },
    {
      "epoch": 5.95,
      "grad_norm": 1339.794189453125,
      "learning_rate": 7.244066047471621e-05,
      "loss": 11.5444,
      "step": 5941
    },
    {
      "epoch": 5.95,
      "grad_norm": 4249.37890625,
      "learning_rate": 7.243550051599588e-05,
      "loss": 14.0552,
      "step": 5942
    },
    {
      "epoch": 5.95,
      "grad_norm": 11377.087890625,
      "learning_rate": 7.243034055727555e-05,
      "loss": 12.7266,
      "step": 5943
    },
    {
      "epoch": 5.95,
      "grad_norm": 5875.6533203125,
      "learning_rate": 7.242518059855522e-05,
      "loss": 14.9984,
      "step": 5944
    },
    {
      "epoch": 5.95,
      "grad_norm": 36588.27734375,
      "learning_rate": 7.242002063983488e-05,
      "loss": 16.5891,
      "step": 5945
    },
    {
      "epoch": 5.95,
      "grad_norm": 3838.45703125,
      "learning_rate": 7.241486068111456e-05,
      "loss": 11.184,
      "step": 5946
    },
    {
      "epoch": 5.95,
      "grad_norm": 1343.33154296875,
      "learning_rate": 7.240970072239423e-05,
      "loss": 11.486,
      "step": 5947
    },
    {
      "epoch": 5.95,
      "grad_norm": 2855.327880859375,
      "learning_rate": 7.240454076367389e-05,
      "loss": 10.8364,
      "step": 5948
    },
    {
      "epoch": 5.95,
      "grad_norm": 20603.583984375,
      "learning_rate": 7.239938080495357e-05,
      "loss": 13.4003,
      "step": 5949
    },
    {
      "epoch": 5.96,
      "grad_norm": 9637.0068359375,
      "learning_rate": 7.239422084623323e-05,
      "loss": 25.8409,
      "step": 5950
    },
    {
      "epoch": 5.96,
      "grad_norm": 1335.6656494140625,
      "learning_rate": 7.23890608875129e-05,
      "loss": 12.729,
      "step": 5951
    },
    {
      "epoch": 5.96,
      "grad_norm": 1226.331787109375,
      "learning_rate": 7.238390092879258e-05,
      "loss": 14.5341,
      "step": 5952
    },
    {
      "epoch": 5.96,
      "grad_norm": 101524.4453125,
      "learning_rate": 7.237874097007225e-05,
      "loss": 13.9652,
      "step": 5953
    },
    {
      "epoch": 5.96,
      "grad_norm": 3220.75439453125,
      "learning_rate": 7.237358101135191e-05,
      "loss": 13.0783,
      "step": 5954
    },
    {
      "epoch": 5.96,
      "grad_norm": 544.7110595703125,
      "learning_rate": 7.236842105263159e-05,
      "loss": 10.4839,
      "step": 5955
    },
    {
      "epoch": 5.96,
      "grad_norm": 768.4398803710938,
      "learning_rate": 7.236326109391125e-05,
      "loss": 10.7573,
      "step": 5956
    },
    {
      "epoch": 5.96,
      "grad_norm": 878.3892211914062,
      "learning_rate": 7.235810113519092e-05,
      "loss": 10.6947,
      "step": 5957
    },
    {
      "epoch": 5.96,
      "grad_norm": 3408.508544921875,
      "learning_rate": 7.23529411764706e-05,
      "loss": 14.2509,
      "step": 5958
    },
    {
      "epoch": 5.96,
      "grad_norm": 51074.359375,
      "learning_rate": 7.234778121775027e-05,
      "loss": 14.3967,
      "step": 5959
    },
    {
      "epoch": 5.97,
      "grad_norm": 3972.347900390625,
      "learning_rate": 7.234262125902993e-05,
      "loss": 14.667,
      "step": 5960
    },
    {
      "epoch": 5.97,
      "grad_norm": 722.64794921875,
      "learning_rate": 7.23374613003096e-05,
      "loss": 12.0227,
      "step": 5961
    },
    {
      "epoch": 5.97,
      "grad_norm": 12729.5380859375,
      "learning_rate": 7.233230134158927e-05,
      "loss": 12.8564,
      "step": 5962
    },
    {
      "epoch": 5.97,
      "grad_norm": 1917.5399169921875,
      "learning_rate": 7.232714138286894e-05,
      "loss": 10.7126,
      "step": 5963
    },
    {
      "epoch": 5.97,
      "grad_norm": 56301.19140625,
      "learning_rate": 7.232198142414862e-05,
      "loss": 13.2425,
      "step": 5964
    },
    {
      "epoch": 5.97,
      "grad_norm": 3281.819580078125,
      "learning_rate": 7.231682146542828e-05,
      "loss": 16.5949,
      "step": 5965
    },
    {
      "epoch": 5.97,
      "grad_norm": 5685.904296875,
      "learning_rate": 7.231166150670795e-05,
      "loss": 12.1884,
      "step": 5966
    },
    {
      "epoch": 5.97,
      "grad_norm": 4617.673828125,
      "learning_rate": 7.230650154798761e-05,
      "loss": 18.261,
      "step": 5967
    },
    {
      "epoch": 5.97,
      "grad_norm": 2022.4058837890625,
      "learning_rate": 7.230134158926729e-05,
      "loss": 13.0486,
      "step": 5968
    },
    {
      "epoch": 5.97,
      "grad_norm": 3063.290283203125,
      "learning_rate": 7.229618163054696e-05,
      "loss": 14.1423,
      "step": 5969
    },
    {
      "epoch": 5.98,
      "grad_norm": 7542.27587890625,
      "learning_rate": 7.229102167182664e-05,
      "loss": 13.0866,
      "step": 5970
    },
    {
      "epoch": 5.98,
      "grad_norm": 13516.0595703125,
      "learning_rate": 7.22858617131063e-05,
      "loss": 13.3012,
      "step": 5971
    },
    {
      "epoch": 5.98,
      "grad_norm": 1712.876708984375,
      "learning_rate": 7.228070175438597e-05,
      "loss": 13.5132,
      "step": 5972
    },
    {
      "epoch": 5.98,
      "grad_norm": 38092.49609375,
      "learning_rate": 7.227554179566563e-05,
      "loss": 13.9391,
      "step": 5973
    },
    {
      "epoch": 5.98,
      "grad_norm": 18830.439453125,
      "learning_rate": 7.22703818369453e-05,
      "loss": 12.859,
      "step": 5974
    },
    {
      "epoch": 5.98,
      "grad_norm": 3154.41259765625,
      "learning_rate": 7.226522187822498e-05,
      "loss": 11.7468,
      "step": 5975
    },
    {
      "epoch": 5.98,
      "grad_norm": 1059.5875244140625,
      "learning_rate": 7.226006191950466e-05,
      "loss": 13.5055,
      "step": 5976
    },
    {
      "epoch": 5.98,
      "grad_norm": 1929.8243408203125,
      "learning_rate": 7.225490196078432e-05,
      "loss": 14.9869,
      "step": 5977
    },
    {
      "epoch": 5.98,
      "grad_norm": 8141.92578125,
      "learning_rate": 7.224974200206399e-05,
      "loss": 13.0782,
      "step": 5978
    },
    {
      "epoch": 5.98,
      "grad_norm": 6051.8408203125,
      "learning_rate": 7.224458204334365e-05,
      "loss": 10.4738,
      "step": 5979
    },
    {
      "epoch": 5.99,
      "grad_norm": 2609.559326171875,
      "learning_rate": 7.223942208462333e-05,
      "loss": 11.7266,
      "step": 5980
    },
    {
      "epoch": 5.99,
      "grad_norm": 2822.925048828125,
      "learning_rate": 7.2234262125903e-05,
      "loss": 16.7505,
      "step": 5981
    },
    {
      "epoch": 5.99,
      "grad_norm": 2697.704833984375,
      "learning_rate": 7.222910216718266e-05,
      "loss": 15.1416,
      "step": 5982
    },
    {
      "epoch": 5.99,
      "grad_norm": 3877.41455078125,
      "learning_rate": 7.222394220846234e-05,
      "loss": 10.8409,
      "step": 5983
    },
    {
      "epoch": 5.99,
      "grad_norm": 4129.16796875,
      "learning_rate": 7.2218782249742e-05,
      "loss": 15.8324,
      "step": 5984
    },
    {
      "epoch": 5.99,
      "grad_norm": 3992.30859375,
      "learning_rate": 7.221362229102167e-05,
      "loss": 11.6634,
      "step": 5985
    },
    {
      "epoch": 5.99,
      "grad_norm": 43302.3203125,
      "learning_rate": 7.220846233230135e-05,
      "loss": 12.8854,
      "step": 5986
    },
    {
      "epoch": 5.99,
      "grad_norm": 15241.0009765625,
      "learning_rate": 7.220330237358102e-05,
      "loss": 15.801,
      "step": 5987
    },
    {
      "epoch": 5.99,
      "grad_norm": 3060.9208984375,
      "learning_rate": 7.219814241486068e-05,
      "loss": 14.4067,
      "step": 5988
    },
    {
      "epoch": 5.99,
      "grad_norm": 12143.6201171875,
      "learning_rate": 7.219298245614036e-05,
      "loss": 15.1038,
      "step": 5989
    },
    {
      "epoch": 6.0,
      "grad_norm": 2380.656982421875,
      "learning_rate": 7.218782249742002e-05,
      "loss": 16.1215,
      "step": 5990
    },
    {
      "epoch": 6.0,
      "grad_norm": 4057.877685546875,
      "learning_rate": 7.218266253869969e-05,
      "loss": 13.2322,
      "step": 5991
    },
    {
      "epoch": 6.0,
      "grad_norm": 8628.7255859375,
      "learning_rate": 7.217750257997937e-05,
      "loss": 14.7546,
      "step": 5992
    },
    {
      "epoch": 6.0,
      "grad_norm": 1428.4229736328125,
      "learning_rate": 7.217234262125904e-05,
      "loss": 11.2263,
      "step": 5993
    },
    {
      "epoch": 6.0,
      "grad_norm": 12569.55859375,
      "learning_rate": 7.21671826625387e-05,
      "loss": 16.4283,
      "step": 5994
    },
    {
      "epoch": 6.0,
      "grad_norm": 2296.64892578125,
      "learning_rate": 7.216202270381838e-05,
      "loss": 17.355,
      "step": 5995
    },
    {
      "epoch": 6.0,
      "grad_norm": 5798.68212890625,
      "learning_rate": 7.215686274509804e-05,
      "loss": 14.7144,
      "step": 5996
    },
    {
      "epoch": 6.0,
      "grad_norm": 6077.2431640625,
      "learning_rate": 7.215170278637771e-05,
      "loss": 12.0634,
      "step": 5997
    },
    {
      "epoch": 6.0,
      "grad_norm": 2674.385009765625,
      "learning_rate": 7.214654282765739e-05,
      "loss": 10.6372,
      "step": 5998
    },
    {
      "epoch": 6.01,
      "grad_norm": 4786.7841796875,
      "learning_rate": 7.214138286893706e-05,
      "loss": 13.1613,
      "step": 5999
    },
    {
      "epoch": 6.01,
      "grad_norm": 4708.00244140625,
      "learning_rate": 7.213622291021672e-05,
      "loss": 12.8522,
      "step": 6000
    },
    {
      "epoch": 6.01,
      "grad_norm": 1308.606689453125,
      "learning_rate": 7.213106295149638e-05,
      "loss": 13.6949,
      "step": 6001
    },
    {
      "epoch": 6.01,
      "grad_norm": 838.9725341796875,
      "learning_rate": 7.212590299277606e-05,
      "loss": 13.0425,
      "step": 6002
    },
    {
      "epoch": 6.01,
      "grad_norm": 1096.614013671875,
      "learning_rate": 7.212074303405573e-05,
      "loss": 16.4894,
      "step": 6003
    },
    {
      "epoch": 6.01,
      "grad_norm": 1085.6256103515625,
      "learning_rate": 7.21155830753354e-05,
      "loss": 12.4623,
      "step": 6004
    },
    {
      "epoch": 6.01,
      "grad_norm": 4670.234375,
      "learning_rate": 7.211042311661507e-05,
      "loss": 14.8832,
      "step": 6005
    },
    {
      "epoch": 6.01,
      "grad_norm": 786.3147583007812,
      "learning_rate": 7.210526315789474e-05,
      "loss": 8.6883,
      "step": 6006
    },
    {
      "epoch": 6.01,
      "grad_norm": 6850.45556640625,
      "learning_rate": 7.21001031991744e-05,
      "loss": 14.1973,
      "step": 6007
    },
    {
      "epoch": 6.01,
      "grad_norm": 1122.777587890625,
      "learning_rate": 7.209494324045408e-05,
      "loss": 10.8975,
      "step": 6008
    },
    {
      "epoch": 6.02,
      "grad_norm": 2133.310302734375,
      "learning_rate": 7.208978328173375e-05,
      "loss": 11.8336,
      "step": 6009
    },
    {
      "epoch": 6.02,
      "grad_norm": 7114.06103515625,
      "learning_rate": 7.208462332301343e-05,
      "loss": 18.1965,
      "step": 6010
    },
    {
      "epoch": 6.02,
      "grad_norm": 4792.93798828125,
      "learning_rate": 7.207946336429309e-05,
      "loss": 14.0263,
      "step": 6011
    },
    {
      "epoch": 6.02,
      "grad_norm": 1895.513427734375,
      "learning_rate": 7.207430340557276e-05,
      "loss": 11.2778,
      "step": 6012
    },
    {
      "epoch": 6.02,
      "grad_norm": 14436.013671875,
      "learning_rate": 7.206914344685242e-05,
      "loss": 15.3736,
      "step": 6013
    },
    {
      "epoch": 6.02,
      "grad_norm": 6291.76220703125,
      "learning_rate": 7.20639834881321e-05,
      "loss": 14.908,
      "step": 6014
    },
    {
      "epoch": 6.02,
      "grad_norm": 17213.712890625,
      "learning_rate": 7.205882352941177e-05,
      "loss": 18.4068,
      "step": 6015
    },
    {
      "epoch": 6.02,
      "grad_norm": 15380.3076171875,
      "learning_rate": 7.205366357069145e-05,
      "loss": 13.9401,
      "step": 6016
    },
    {
      "epoch": 6.02,
      "grad_norm": 12772.2802734375,
      "learning_rate": 7.204850361197111e-05,
      "loss": 14.7255,
      "step": 6017
    },
    {
      "epoch": 6.02,
      "grad_norm": 3418.4833984375,
      "learning_rate": 7.204334365325077e-05,
      "loss": 14.1019,
      "step": 6018
    },
    {
      "epoch": 6.03,
      "grad_norm": 6047.9140625,
      "learning_rate": 7.203818369453044e-05,
      "loss": 12.6975,
      "step": 6019
    },
    {
      "epoch": 6.03,
      "grad_norm": 6006.16064453125,
      "learning_rate": 7.203302373581012e-05,
      "loss": 15.7509,
      "step": 6020
    },
    {
      "epoch": 6.03,
      "grad_norm": 1574.1119384765625,
      "learning_rate": 7.202786377708979e-05,
      "loss": 11.8436,
      "step": 6021
    },
    {
      "epoch": 6.03,
      "grad_norm": 1467.7449951171875,
      "learning_rate": 7.202270381836945e-05,
      "loss": 15.7703,
      "step": 6022
    },
    {
      "epoch": 6.03,
      "grad_norm": 2561.778076171875,
      "learning_rate": 7.201754385964913e-05,
      "loss": 12.2922,
      "step": 6023
    },
    {
      "epoch": 6.03,
      "grad_norm": 19305.95703125,
      "learning_rate": 7.201238390092879e-05,
      "loss": 15.0928,
      "step": 6024
    },
    {
      "epoch": 6.03,
      "grad_norm": 7136.1357421875,
      "learning_rate": 7.200722394220846e-05,
      "loss": 14.0186,
      "step": 6025
    },
    {
      "epoch": 6.03,
      "grad_norm": 5950.91748046875,
      "learning_rate": 7.200206398348814e-05,
      "loss": 12.1552,
      "step": 6026
    },
    {
      "epoch": 6.03,
      "grad_norm": 1983.3873291015625,
      "learning_rate": 7.199690402476781e-05,
      "loss": 11.6445,
      "step": 6027
    },
    {
      "epoch": 6.03,
      "grad_norm": 2825.751953125,
      "learning_rate": 7.199174406604747e-05,
      "loss": 13.6449,
      "step": 6028
    },
    {
      "epoch": 6.04,
      "grad_norm": 234.69747924804688,
      "learning_rate": 7.198658410732715e-05,
      "loss": 11.7203,
      "step": 6029
    },
    {
      "epoch": 6.04,
      "grad_norm": 10983.28125,
      "learning_rate": 7.198142414860681e-05,
      "loss": 18.008,
      "step": 6030
    },
    {
      "epoch": 6.04,
      "grad_norm": 18192.400390625,
      "learning_rate": 7.197626418988648e-05,
      "loss": 15.4759,
      "step": 6031
    },
    {
      "epoch": 6.04,
      "grad_norm": 51626.9609375,
      "learning_rate": 7.197110423116616e-05,
      "loss": 17.9849,
      "step": 6032
    },
    {
      "epoch": 6.04,
      "grad_norm": 12722.6923828125,
      "learning_rate": 7.196594427244583e-05,
      "loss": 14.6611,
      "step": 6033
    },
    {
      "epoch": 6.04,
      "grad_norm": 2279.619873046875,
      "learning_rate": 7.196078431372549e-05,
      "loss": 12.7625,
      "step": 6034
    },
    {
      "epoch": 6.04,
      "grad_norm": 3391.60205078125,
      "learning_rate": 7.195562435500515e-05,
      "loss": 18.2138,
      "step": 6035
    },
    {
      "epoch": 6.04,
      "grad_norm": 5216.93017578125,
      "learning_rate": 7.195046439628483e-05,
      "loss": 13.9414,
      "step": 6036
    },
    {
      "epoch": 6.04,
      "grad_norm": 8320.6689453125,
      "learning_rate": 7.19453044375645e-05,
      "loss": 13.069,
      "step": 6037
    },
    {
      "epoch": 6.04,
      "grad_norm": 6672.4873046875,
      "learning_rate": 7.194014447884418e-05,
      "loss": 12.7537,
      "step": 6038
    },
    {
      "epoch": 6.05,
      "grad_norm": 7192.0908203125,
      "learning_rate": 7.193498452012384e-05,
      "loss": 17.541,
      "step": 6039
    },
    {
      "epoch": 6.05,
      "grad_norm": 6585.07275390625,
      "learning_rate": 7.192982456140351e-05,
      "loss": 13.767,
      "step": 6040
    },
    {
      "epoch": 6.05,
      "grad_norm": 2727.046875,
      "learning_rate": 7.192466460268317e-05,
      "loss": 13.0895,
      "step": 6041
    },
    {
      "epoch": 6.05,
      "grad_norm": 912.2862548828125,
      "learning_rate": 7.191950464396285e-05,
      "loss": 17.6926,
      "step": 6042
    },
    {
      "epoch": 6.05,
      "grad_norm": 4773.53955078125,
      "learning_rate": 7.191434468524252e-05,
      "loss": 18.0263,
      "step": 6043
    },
    {
      "epoch": 6.05,
      "grad_norm": 2898.8310546875,
      "learning_rate": 7.19091847265222e-05,
      "loss": 11.9843,
      "step": 6044
    },
    {
      "epoch": 6.05,
      "grad_norm": 283.1396484375,
      "learning_rate": 7.190402476780186e-05,
      "loss": 10.5376,
      "step": 6045
    },
    {
      "epoch": 6.05,
      "grad_norm": 5441.9658203125,
      "learning_rate": 7.189886480908153e-05,
      "loss": 12.7237,
      "step": 6046
    },
    {
      "epoch": 6.05,
      "grad_norm": 2675.24853515625,
      "learning_rate": 7.18937048503612e-05,
      "loss": 12.0831,
      "step": 6047
    },
    {
      "epoch": 6.05,
      "grad_norm": 3435.06982421875,
      "learning_rate": 7.188854489164087e-05,
      "loss": 12.3889,
      "step": 6048
    },
    {
      "epoch": 6.06,
      "grad_norm": 11132.2294921875,
      "learning_rate": 7.188338493292054e-05,
      "loss": 18.3137,
      "step": 6049
    },
    {
      "epoch": 6.06,
      "grad_norm": 3159.768310546875,
      "learning_rate": 7.187822497420022e-05,
      "loss": 13.3539,
      "step": 6050
    },
    {
      "epoch": 6.06,
      "grad_norm": 849.5033569335938,
      "learning_rate": 7.187306501547988e-05,
      "loss": 12.0512,
      "step": 6051
    },
    {
      "epoch": 6.06,
      "grad_norm": 3225.1513671875,
      "learning_rate": 7.186790505675955e-05,
      "loss": 14.7546,
      "step": 6052
    },
    {
      "epoch": 6.06,
      "grad_norm": 1446.6246337890625,
      "learning_rate": 7.186274509803921e-05,
      "loss": 10.9956,
      "step": 6053
    },
    {
      "epoch": 6.06,
      "grad_norm": 18785.384765625,
      "learning_rate": 7.185758513931889e-05,
      "loss": 14.9432,
      "step": 6054
    },
    {
      "epoch": 6.06,
      "grad_norm": 5432.48291015625,
      "learning_rate": 7.185242518059856e-05,
      "loss": 11.2503,
      "step": 6055
    },
    {
      "epoch": 6.06,
      "grad_norm": 6743.123046875,
      "learning_rate": 7.184726522187822e-05,
      "loss": 10.2473,
      "step": 6056
    },
    {
      "epoch": 6.06,
      "grad_norm": 708.8643188476562,
      "learning_rate": 7.18421052631579e-05,
      "loss": 11.8667,
      "step": 6057
    },
    {
      "epoch": 6.06,
      "grad_norm": 2185.90869140625,
      "learning_rate": 7.183694530443756e-05,
      "loss": 12.6653,
      "step": 6058
    },
    {
      "epoch": 6.07,
      "grad_norm": 2906.1171875,
      "learning_rate": 7.183178534571723e-05,
      "loss": 11.9379,
      "step": 6059
    },
    {
      "epoch": 6.07,
      "grad_norm": 2170.715087890625,
      "learning_rate": 7.182662538699691e-05,
      "loss": 11.8854,
      "step": 6060
    },
    {
      "epoch": 6.07,
      "grad_norm": 6092.46435546875,
      "learning_rate": 7.182146542827658e-05,
      "loss": 11.9561,
      "step": 6061
    },
    {
      "epoch": 6.07,
      "grad_norm": 2475.151611328125,
      "learning_rate": 7.181630546955624e-05,
      "loss": 12.8791,
      "step": 6062
    },
    {
      "epoch": 6.07,
      "grad_norm": 19472.18359375,
      "learning_rate": 7.181114551083592e-05,
      "loss": 14.5547,
      "step": 6063
    },
    {
      "epoch": 6.07,
      "grad_norm": 8237.65234375,
      "learning_rate": 7.180598555211558e-05,
      "loss": 13.313,
      "step": 6064
    },
    {
      "epoch": 6.07,
      "grad_norm": 12035.75,
      "learning_rate": 7.180082559339525e-05,
      "loss": 14.5583,
      "step": 6065
    },
    {
      "epoch": 6.07,
      "grad_norm": 7805.3447265625,
      "learning_rate": 7.179566563467493e-05,
      "loss": 12.3959,
      "step": 6066
    },
    {
      "epoch": 6.07,
      "grad_norm": 17687.845703125,
      "learning_rate": 7.17905056759546e-05,
      "loss": 24.534,
      "step": 6067
    },
    {
      "epoch": 6.07,
      "grad_norm": 13977.703125,
      "learning_rate": 7.178534571723426e-05,
      "loss": 17.9151,
      "step": 6068
    },
    {
      "epoch": 6.08,
      "grad_norm": 7151.1669921875,
      "learning_rate": 7.178018575851394e-05,
      "loss": 13.4503,
      "step": 6069
    },
    {
      "epoch": 6.08,
      "grad_norm": 253.231201171875,
      "learning_rate": 7.17750257997936e-05,
      "loss": 11.09,
      "step": 6070
    },
    {
      "epoch": 6.08,
      "grad_norm": 2022.8560791015625,
      "learning_rate": 7.176986584107327e-05,
      "loss": 18.0382,
      "step": 6071
    },
    {
      "epoch": 6.08,
      "grad_norm": 10487.3369140625,
      "learning_rate": 7.176470588235295e-05,
      "loss": 11.7464,
      "step": 6072
    },
    {
      "epoch": 6.08,
      "grad_norm": 2995.5947265625,
      "learning_rate": 7.175954592363261e-05,
      "loss": 12.7167,
      "step": 6073
    },
    {
      "epoch": 6.08,
      "grad_norm": 7608.72412109375,
      "learning_rate": 7.175438596491228e-05,
      "loss": 15.3987,
      "step": 6074
    },
    {
      "epoch": 6.08,
      "grad_norm": 1238.2772216796875,
      "learning_rate": 7.174922600619194e-05,
      "loss": 12.5565,
      "step": 6075
    },
    {
      "epoch": 6.08,
      "grad_norm": 1837.5601806640625,
      "learning_rate": 7.174406604747162e-05,
      "loss": 12.9376,
      "step": 6076
    },
    {
      "epoch": 6.08,
      "grad_norm": 3418.8544921875,
      "learning_rate": 7.17389060887513e-05,
      "loss": 16.5247,
      "step": 6077
    },
    {
      "epoch": 6.08,
      "grad_norm": 2230.058837890625,
      "learning_rate": 7.173374613003097e-05,
      "loss": 12.9244,
      "step": 6078
    },
    {
      "epoch": 6.09,
      "grad_norm": 30423.197265625,
      "learning_rate": 7.172858617131063e-05,
      "loss": 16.7831,
      "step": 6079
    },
    {
      "epoch": 6.09,
      "grad_norm": 3363.357666015625,
      "learning_rate": 7.17234262125903e-05,
      "loss": 13.4173,
      "step": 6080
    },
    {
      "epoch": 6.09,
      "grad_norm": 2210.46875,
      "learning_rate": 7.171826625386996e-05,
      "loss": 13.9433,
      "step": 6081
    },
    {
      "epoch": 6.09,
      "grad_norm": 3916.579345703125,
      "learning_rate": 7.171310629514964e-05,
      "loss": 11.9052,
      "step": 6082
    },
    {
      "epoch": 6.09,
      "grad_norm": 11892.2275390625,
      "learning_rate": 7.170794633642931e-05,
      "loss": 12.5265,
      "step": 6083
    },
    {
      "epoch": 6.09,
      "grad_norm": 4028.95947265625,
      "learning_rate": 7.170278637770899e-05,
      "loss": 14.0819,
      "step": 6084
    },
    {
      "epoch": 6.09,
      "grad_norm": 6678.3720703125,
      "learning_rate": 7.169762641898865e-05,
      "loss": 15.4826,
      "step": 6085
    },
    {
      "epoch": 6.09,
      "grad_norm": 6883.134765625,
      "learning_rate": 7.169246646026832e-05,
      "loss": 13.7318,
      "step": 6086
    },
    {
      "epoch": 6.09,
      "grad_norm": 33479.3125,
      "learning_rate": 7.168730650154798e-05,
      "loss": 12.9592,
      "step": 6087
    },
    {
      "epoch": 6.09,
      "grad_norm": 24890.66015625,
      "learning_rate": 7.168214654282766e-05,
      "loss": 15.5143,
      "step": 6088
    },
    {
      "epoch": 6.1,
      "grad_norm": 1040.107421875,
      "learning_rate": 7.167698658410733e-05,
      "loss": 11.4849,
      "step": 6089
    },
    {
      "epoch": 6.1,
      "grad_norm": 1740.4649658203125,
      "learning_rate": 7.1671826625387e-05,
      "loss": 13.4458,
      "step": 6090
    },
    {
      "epoch": 6.1,
      "grad_norm": 2647.2421875,
      "learning_rate": 7.166666666666667e-05,
      "loss": 12.5627,
      "step": 6091
    },
    {
      "epoch": 6.1,
      "grad_norm": 4423.03271484375,
      "learning_rate": 7.166150670794633e-05,
      "loss": 16.4361,
      "step": 6092
    },
    {
      "epoch": 6.1,
      "grad_norm": 540.1174926757812,
      "learning_rate": 7.1656346749226e-05,
      "loss": 11.118,
      "step": 6093
    },
    {
      "epoch": 6.1,
      "grad_norm": 9788.388671875,
      "learning_rate": 7.165118679050568e-05,
      "loss": 15.1266,
      "step": 6094
    },
    {
      "epoch": 6.1,
      "grad_norm": 1845.34619140625,
      "learning_rate": 7.164602683178535e-05,
      "loss": 11.4783,
      "step": 6095
    },
    {
      "epoch": 6.1,
      "grad_norm": 2545.885986328125,
      "learning_rate": 7.164086687306501e-05,
      "loss": 13.2014,
      "step": 6096
    },
    {
      "epoch": 6.1,
      "grad_norm": 46988.03125,
      "learning_rate": 7.163570691434469e-05,
      "loss": 15.6588,
      "step": 6097
    },
    {
      "epoch": 6.1,
      "grad_norm": 8197.3076171875,
      "learning_rate": 7.163054695562435e-05,
      "loss": 17.0594,
      "step": 6098
    },
    {
      "epoch": 6.11,
      "grad_norm": 1909.99560546875,
      "learning_rate": 7.162538699690402e-05,
      "loss": 10.6086,
      "step": 6099
    },
    {
      "epoch": 6.11,
      "grad_norm": 2434.312744140625,
      "learning_rate": 7.16202270381837e-05,
      "loss": 11.4232,
      "step": 6100
    },
    {
      "epoch": 6.11,
      "grad_norm": 8460.3779296875,
      "learning_rate": 7.161506707946337e-05,
      "loss": 14.0671,
      "step": 6101
    },
    {
      "epoch": 6.11,
      "grad_norm": 3454.34912109375,
      "learning_rate": 7.160990712074303e-05,
      "loss": 11.2515,
      "step": 6102
    },
    {
      "epoch": 6.11,
      "grad_norm": 8726.3779296875,
      "learning_rate": 7.160474716202271e-05,
      "loss": 15.4325,
      "step": 6103
    },
    {
      "epoch": 6.11,
      "grad_norm": 6937.08837890625,
      "learning_rate": 7.159958720330237e-05,
      "loss": 11.4465,
      "step": 6104
    },
    {
      "epoch": 6.11,
      "grad_norm": 11487.0048828125,
      "learning_rate": 7.159442724458206e-05,
      "loss": 12.1541,
      "step": 6105
    },
    {
      "epoch": 6.11,
      "grad_norm": 4735.9619140625,
      "learning_rate": 7.158926728586172e-05,
      "loss": 13.2092,
      "step": 6106
    },
    {
      "epoch": 6.11,
      "grad_norm": 1781.3203125,
      "learning_rate": 7.158410732714138e-05,
      "loss": 14.204,
      "step": 6107
    },
    {
      "epoch": 6.11,
      "grad_norm": 674.5192260742188,
      "learning_rate": 7.157894736842105e-05,
      "loss": 14.8659,
      "step": 6108
    },
    {
      "epoch": 6.12,
      "grad_norm": 1834.1717529296875,
      "learning_rate": 7.157378740970072e-05,
      "loss": 15.9817,
      "step": 6109
    },
    {
      "epoch": 6.12,
      "grad_norm": 10418.4072265625,
      "learning_rate": 7.156862745098039e-05,
      "loss": 13.5473,
      "step": 6110
    },
    {
      "epoch": 6.12,
      "grad_norm": 919.3976440429688,
      "learning_rate": 7.156346749226006e-05,
      "loss": 16.6006,
      "step": 6111
    },
    {
      "epoch": 6.12,
      "grad_norm": 13834.2568359375,
      "learning_rate": 7.155830753353974e-05,
      "loss": 13.1913,
      "step": 6112
    },
    {
      "epoch": 6.12,
      "grad_norm": 10784.072265625,
      "learning_rate": 7.15531475748194e-05,
      "loss": 12.6778,
      "step": 6113
    },
    {
      "epoch": 6.12,
      "grad_norm": 17173.5078125,
      "learning_rate": 7.154798761609907e-05,
      "loss": 22.3967,
      "step": 6114
    },
    {
      "epoch": 6.12,
      "grad_norm": 10830.021484375,
      "learning_rate": 7.154282765737874e-05,
      "loss": 14.1437,
      "step": 6115
    },
    {
      "epoch": 6.12,
      "grad_norm": 1766.599609375,
      "learning_rate": 7.153766769865841e-05,
      "loss": 14.7139,
      "step": 6116
    },
    {
      "epoch": 6.12,
      "grad_norm": 1973.538330078125,
      "learning_rate": 7.153250773993808e-05,
      "loss": 13.8362,
      "step": 6117
    },
    {
      "epoch": 6.12,
      "grad_norm": 14020.2919921875,
      "learning_rate": 7.152734778121776e-05,
      "loss": 14.7915,
      "step": 6118
    },
    {
      "epoch": 6.13,
      "grad_norm": 9627.759765625,
      "learning_rate": 7.152218782249742e-05,
      "loss": 12.3649,
      "step": 6119
    },
    {
      "epoch": 6.13,
      "grad_norm": 26997.546875,
      "learning_rate": 7.15170278637771e-05,
      "loss": 14.1036,
      "step": 6120
    },
    {
      "epoch": 6.13,
      "grad_norm": 5862.6123046875,
      "learning_rate": 7.151186790505676e-05,
      "loss": 10.4179,
      "step": 6121
    },
    {
      "epoch": 6.13,
      "grad_norm": 8957.4560546875,
      "learning_rate": 7.150670794633644e-05,
      "loss": 13.3834,
      "step": 6122
    },
    {
      "epoch": 6.13,
      "grad_norm": 2273.49072265625,
      "learning_rate": 7.15015479876161e-05,
      "loss": 13.4501,
      "step": 6123
    },
    {
      "epoch": 6.13,
      "grad_norm": 3985.99365234375,
      "learning_rate": 7.149638802889578e-05,
      "loss": 12.6978,
      "step": 6124
    },
    {
      "epoch": 6.13,
      "grad_norm": 7685.26220703125,
      "learning_rate": 7.149122807017544e-05,
      "loss": 12.9999,
      "step": 6125
    },
    {
      "epoch": 6.13,
      "grad_norm": 22287.37890625,
      "learning_rate": 7.14860681114551e-05,
      "loss": 13.6843,
      "step": 6126
    },
    {
      "epoch": 6.13,
      "grad_norm": 5029.10986328125,
      "learning_rate": 7.148090815273478e-05,
      "loss": 11.6179,
      "step": 6127
    },
    {
      "epoch": 6.13,
      "grad_norm": 3755.92138671875,
      "learning_rate": 7.147574819401445e-05,
      "loss": 11.9559,
      "step": 6128
    },
    {
      "epoch": 6.14,
      "grad_norm": 1989.9244384765625,
      "learning_rate": 7.147058823529412e-05,
      "loss": 10.6795,
      "step": 6129
    },
    {
      "epoch": 6.14,
      "grad_norm": 2533.859130859375,
      "learning_rate": 7.146542827657379e-05,
      "loss": 12.5114,
      "step": 6130
    },
    {
      "epoch": 6.14,
      "grad_norm": 22779.458984375,
      "learning_rate": 7.146026831785346e-05,
      "loss": 20.6332,
      "step": 6131
    },
    {
      "epoch": 6.14,
      "grad_norm": 12537.21484375,
      "learning_rate": 7.145510835913312e-05,
      "loss": 10.9579,
      "step": 6132
    },
    {
      "epoch": 6.14,
      "grad_norm": 45998.2734375,
      "learning_rate": 7.144994840041281e-05,
      "loss": 15.7519,
      "step": 6133
    },
    {
      "epoch": 6.14,
      "grad_norm": 1092.0611572265625,
      "learning_rate": 7.144478844169247e-05,
      "loss": 11.6516,
      "step": 6134
    },
    {
      "epoch": 6.14,
      "grad_norm": 2754.256591796875,
      "learning_rate": 7.143962848297214e-05,
      "loss": 15.0965,
      "step": 6135
    },
    {
      "epoch": 6.14,
      "grad_norm": 1468.8480224609375,
      "learning_rate": 7.14344685242518e-05,
      "loss": 12.9809,
      "step": 6136
    },
    {
      "epoch": 6.14,
      "grad_norm": 14011.0712890625,
      "learning_rate": 7.142930856553148e-05,
      "loss": 14.5165,
      "step": 6137
    },
    {
      "epoch": 6.14,
      "grad_norm": 1751.5013427734375,
      "learning_rate": 7.142414860681114e-05,
      "loss": 11.9392,
      "step": 6138
    },
    {
      "epoch": 6.15,
      "grad_norm": 35742.38671875,
      "learning_rate": 7.141898864809083e-05,
      "loss": 16.7346,
      "step": 6139
    },
    {
      "epoch": 6.15,
      "grad_norm": 6635.7705078125,
      "learning_rate": 7.141382868937049e-05,
      "loss": 13.2528,
      "step": 6140
    },
    {
      "epoch": 6.15,
      "grad_norm": 1052.8543701171875,
      "learning_rate": 7.140866873065016e-05,
      "loss": 11.9187,
      "step": 6141
    },
    {
      "epoch": 6.15,
      "grad_norm": 5469.7998046875,
      "learning_rate": 7.140350877192983e-05,
      "loss": 11.7852,
      "step": 6142
    },
    {
      "epoch": 6.15,
      "grad_norm": 3057.727783203125,
      "learning_rate": 7.139834881320949e-05,
      "loss": 13.3809,
      "step": 6143
    },
    {
      "epoch": 6.15,
      "grad_norm": 729.40576171875,
      "learning_rate": 7.139318885448916e-05,
      "loss": 14.7442,
      "step": 6144
    },
    {
      "epoch": 6.15,
      "grad_norm": 2428.76953125,
      "learning_rate": 7.138802889576884e-05,
      "loss": 14.5765,
      "step": 6145
    },
    {
      "epoch": 6.15,
      "grad_norm": 773.2638549804688,
      "learning_rate": 7.138286893704851e-05,
      "loss": 10.7981,
      "step": 6146
    },
    {
      "epoch": 6.15,
      "grad_norm": 2528.366943359375,
      "learning_rate": 7.137770897832817e-05,
      "loss": 16.9864,
      "step": 6147
    },
    {
      "epoch": 6.15,
      "grad_norm": 2487.63134765625,
      "learning_rate": 7.137254901960785e-05,
      "loss": 13.2464,
      "step": 6148
    },
    {
      "epoch": 6.16,
      "grad_norm": 4973.6865234375,
      "learning_rate": 7.136738906088751e-05,
      "loss": 13.0597,
      "step": 6149
    },
    {
      "epoch": 6.16,
      "grad_norm": 1789.52734375,
      "learning_rate": 7.13622291021672e-05,
      "loss": 14.4927,
      "step": 6150
    },
    {
      "epoch": 6.16,
      "grad_norm": 891.7008666992188,
      "learning_rate": 7.135706914344686e-05,
      "loss": 12.7224,
      "step": 6151
    },
    {
      "epoch": 6.16,
      "grad_norm": 3256.2021484375,
      "learning_rate": 7.135190918472653e-05,
      "loss": 13.5112,
      "step": 6152
    },
    {
      "epoch": 6.16,
      "grad_norm": 51512.44140625,
      "learning_rate": 7.134674922600619e-05,
      "loss": 17.9407,
      "step": 6153
    },
    {
      "epoch": 6.16,
      "grad_norm": 7025.91455078125,
      "learning_rate": 7.134158926728587e-05,
      "loss": 23.7869,
      "step": 6154
    },
    {
      "epoch": 6.16,
      "grad_norm": 2323.097412109375,
      "learning_rate": 7.133642930856553e-05,
      "loss": 12.0954,
      "step": 6155
    },
    {
      "epoch": 6.16,
      "grad_norm": 2542.669677734375,
      "learning_rate": 7.133126934984521e-05,
      "loss": 13.585,
      "step": 6156
    },
    {
      "epoch": 6.16,
      "grad_norm": 4385.548828125,
      "learning_rate": 7.132610939112488e-05,
      "loss": 13.3286,
      "step": 6157
    },
    {
      "epoch": 6.16,
      "grad_norm": 4440.6279296875,
      "learning_rate": 7.132094943240455e-05,
      "loss": 13.8748,
      "step": 6158
    },
    {
      "epoch": 6.17,
      "grad_norm": 1836.9647216796875,
      "learning_rate": 7.131578947368421e-05,
      "loss": 12.9901,
      "step": 6159
    },
    {
      "epoch": 6.17,
      "grad_norm": 2149.237060546875,
      "learning_rate": 7.131062951496389e-05,
      "loss": 13.4448,
      "step": 6160
    },
    {
      "epoch": 6.17,
      "grad_norm": 16223.1357421875,
      "learning_rate": 7.130546955624356e-05,
      "loss": 15.7981,
      "step": 6161
    },
    {
      "epoch": 6.17,
      "grad_norm": 768.7001342773438,
      "learning_rate": 7.130030959752322e-05,
      "loss": 12.4289,
      "step": 6162
    },
    {
      "epoch": 6.17,
      "grad_norm": 2753.0615234375,
      "learning_rate": 7.12951496388029e-05,
      "loss": 11.3043,
      "step": 6163
    },
    {
      "epoch": 6.17,
      "grad_norm": 2115.06201171875,
      "learning_rate": 7.128998968008256e-05,
      "loss": 15.4523,
      "step": 6164
    },
    {
      "epoch": 6.17,
      "grad_norm": 3408.99658203125,
      "learning_rate": 7.128482972136223e-05,
      "loss": 17.6958,
      "step": 6165
    },
    {
      "epoch": 6.17,
      "grad_norm": 11112.7763671875,
      "learning_rate": 7.127966976264189e-05,
      "loss": 15.5332,
      "step": 6166
    },
    {
      "epoch": 6.17,
      "grad_norm": 1591.6317138671875,
      "learning_rate": 7.127450980392158e-05,
      "loss": 12.1851,
      "step": 6167
    },
    {
      "epoch": 6.17,
      "grad_norm": 18903.361328125,
      "learning_rate": 7.126934984520124e-05,
      "loss": 20.0817,
      "step": 6168
    },
    {
      "epoch": 6.18,
      "grad_norm": 4043.211669921875,
      "learning_rate": 7.126418988648092e-05,
      "loss": 14.6727,
      "step": 6169
    },
    {
      "epoch": 6.18,
      "grad_norm": 6295.41357421875,
      "learning_rate": 7.125902992776058e-05,
      "loss": 10.9018,
      "step": 6170
    },
    {
      "epoch": 6.18,
      "grad_norm": 7900.31787109375,
      "learning_rate": 7.125386996904025e-05,
      "loss": 17.0686,
      "step": 6171
    },
    {
      "epoch": 6.18,
      "grad_norm": 7929.58740234375,
      "learning_rate": 7.124871001031991e-05,
      "loss": 14.5976,
      "step": 6172
    },
    {
      "epoch": 6.18,
      "grad_norm": 5978.349609375,
      "learning_rate": 7.12435500515996e-05,
      "loss": 14.5067,
      "step": 6173
    },
    {
      "epoch": 6.18,
      "grad_norm": 491.32135009765625,
      "learning_rate": 7.123839009287926e-05,
      "loss": 14.5024,
      "step": 6174
    },
    {
      "epoch": 6.18,
      "grad_norm": 6336.1220703125,
      "learning_rate": 7.123323013415894e-05,
      "loss": 15.194,
      "step": 6175
    },
    {
      "epoch": 6.18,
      "grad_norm": 15877.8466796875,
      "learning_rate": 7.12280701754386e-05,
      "loss": 11.8536,
      "step": 6176
    },
    {
      "epoch": 6.18,
      "grad_norm": 18816.951171875,
      "learning_rate": 7.122291021671827e-05,
      "loss": 12.3412,
      "step": 6177
    },
    {
      "epoch": 6.18,
      "grad_norm": 20392.560546875,
      "learning_rate": 7.121775025799795e-05,
      "loss": 15.5814,
      "step": 6178
    },
    {
      "epoch": 6.19,
      "grad_norm": 7148.7998046875,
      "learning_rate": 7.121259029927761e-05,
      "loss": 13.0937,
      "step": 6179
    },
    {
      "epoch": 6.19,
      "grad_norm": 1762.111328125,
      "learning_rate": 7.120743034055728e-05,
      "loss": 12.6184,
      "step": 6180
    },
    {
      "epoch": 6.19,
      "grad_norm": 7583.396484375,
      "learning_rate": 7.120227038183694e-05,
      "loss": 14.5252,
      "step": 6181
    },
    {
      "epoch": 6.19,
      "grad_norm": 25625.564453125,
      "learning_rate": 7.119711042311662e-05,
      "loss": 13.2654,
      "step": 6182
    },
    {
      "epoch": 6.19,
      "grad_norm": 7444.3125,
      "learning_rate": 7.119195046439628e-05,
      "loss": 16.227,
      "step": 6183
    },
    {
      "epoch": 6.19,
      "grad_norm": 26545.79296875,
      "learning_rate": 7.118679050567597e-05,
      "loss": 16.3892,
      "step": 6184
    },
    {
      "epoch": 6.19,
      "grad_norm": 8179.771484375,
      "learning_rate": 7.118163054695563e-05,
      "loss": 16.4135,
      "step": 6185
    },
    {
      "epoch": 6.19,
      "grad_norm": 8187.134765625,
      "learning_rate": 7.11764705882353e-05,
      "loss": 13.9383,
      "step": 6186
    },
    {
      "epoch": 6.19,
      "grad_norm": 4702.34814453125,
      "learning_rate": 7.117131062951496e-05,
      "loss": 11.0142,
      "step": 6187
    },
    {
      "epoch": 6.19,
      "grad_norm": 43468.875,
      "learning_rate": 7.116615067079464e-05,
      "loss": 19.3147,
      "step": 6188
    },
    {
      "epoch": 6.2,
      "grad_norm": 4798.326171875,
      "learning_rate": 7.116099071207431e-05,
      "loss": 11.7293,
      "step": 6189
    },
    {
      "epoch": 6.2,
      "grad_norm": 9632.466796875,
      "learning_rate": 7.115583075335399e-05,
      "loss": 18.84,
      "step": 6190
    },
    {
      "epoch": 6.2,
      "grad_norm": 2425.452880859375,
      "learning_rate": 7.115067079463365e-05,
      "loss": 12.392,
      "step": 6191
    },
    {
      "epoch": 6.2,
      "grad_norm": 1244.9229736328125,
      "learning_rate": 7.114551083591332e-05,
      "loss": 11.9417,
      "step": 6192
    },
    {
      "epoch": 6.2,
      "grad_norm": 36256.8125,
      "learning_rate": 7.114035087719298e-05,
      "loss": 12.9121,
      "step": 6193
    },
    {
      "epoch": 6.2,
      "grad_norm": 3098.6875,
      "learning_rate": 7.113519091847266e-05,
      "loss": 14.3207,
      "step": 6194
    },
    {
      "epoch": 6.2,
      "grad_norm": 61330.484375,
      "learning_rate": 7.113003095975233e-05,
      "loss": 12.103,
      "step": 6195
    },
    {
      "epoch": 6.2,
      "grad_norm": 11980.2158203125,
      "learning_rate": 7.112487100103199e-05,
      "loss": 17.9605,
      "step": 6196
    },
    {
      "epoch": 6.2,
      "grad_norm": 6497.87451171875,
      "learning_rate": 7.111971104231167e-05,
      "loss": 17.5003,
      "step": 6197
    },
    {
      "epoch": 6.2,
      "grad_norm": 5290.07861328125,
      "learning_rate": 7.111455108359133e-05,
      "loss": 13.8141,
      "step": 6198
    },
    {
      "epoch": 6.21,
      "grad_norm": 1161.38671875,
      "learning_rate": 7.1109391124871e-05,
      "loss": 12.9428,
      "step": 6199
    },
    {
      "epoch": 6.21,
      "grad_norm": 7985.37158203125,
      "learning_rate": 7.110423116615066e-05,
      "loss": 11.7956,
      "step": 6200
    },
    {
      "epoch": 6.21,
      "grad_norm": 1711.7293701171875,
      "learning_rate": 7.109907120743035e-05,
      "loss": 13.1817,
      "step": 6201
    },
    {
      "epoch": 6.21,
      "grad_norm": 1017.3939819335938,
      "learning_rate": 7.109391124871001e-05,
      "loss": 13.0296,
      "step": 6202
    },
    {
      "epoch": 6.21,
      "grad_norm": 3603.40625,
      "learning_rate": 7.108875128998969e-05,
      "loss": 14.5183,
      "step": 6203
    },
    {
      "epoch": 6.21,
      "grad_norm": 850.6472778320312,
      "learning_rate": 7.108359133126935e-05,
      "loss": 11.6869,
      "step": 6204
    },
    {
      "epoch": 6.21,
      "grad_norm": 3578.246337890625,
      "learning_rate": 7.107843137254902e-05,
      "loss": 16.6215,
      "step": 6205
    },
    {
      "epoch": 6.21,
      "grad_norm": 144128.03125,
      "learning_rate": 7.10732714138287e-05,
      "loss": 16.6502,
      "step": 6206
    },
    {
      "epoch": 6.21,
      "grad_norm": 6340.8125,
      "learning_rate": 7.106811145510837e-05,
      "loss": 16.9573,
      "step": 6207
    },
    {
      "epoch": 6.21,
      "grad_norm": 3891.741455078125,
      "learning_rate": 7.106295149638803e-05,
      "loss": 15.4635,
      "step": 6208
    },
    {
      "epoch": 6.22,
      "grad_norm": 3671.410400390625,
      "learning_rate": 7.105779153766771e-05,
      "loss": 12.1697,
      "step": 6209
    },
    {
      "epoch": 6.22,
      "grad_norm": 857.5960693359375,
      "learning_rate": 7.105263157894737e-05,
      "loss": 10.8972,
      "step": 6210
    },
    {
      "epoch": 6.22,
      "grad_norm": 3054.653076171875,
      "learning_rate": 7.104747162022704e-05,
      "loss": 13.5693,
      "step": 6211
    },
    {
      "epoch": 6.22,
      "grad_norm": 2598.130615234375,
      "learning_rate": 7.104231166150672e-05,
      "loss": 14.5664,
      "step": 6212
    },
    {
      "epoch": 6.22,
      "grad_norm": 4292.78515625,
      "learning_rate": 7.103715170278639e-05,
      "loss": 11.9846,
      "step": 6213
    },
    {
      "epoch": 6.22,
      "grad_norm": 17567.4296875,
      "learning_rate": 7.103199174406605e-05,
      "loss": 16.2628,
      "step": 6214
    },
    {
      "epoch": 6.22,
      "grad_norm": 2121.90771484375,
      "learning_rate": 7.102683178534571e-05,
      "loss": 17.335,
      "step": 6215
    },
    {
      "epoch": 6.22,
      "grad_norm": 2480.5517578125,
      "learning_rate": 7.102167182662539e-05,
      "loss": 18.0996,
      "step": 6216
    },
    {
      "epoch": 6.22,
      "grad_norm": 11708.37890625,
      "learning_rate": 7.101651186790505e-05,
      "loss": 17.6754,
      "step": 6217
    },
    {
      "epoch": 6.22,
      "grad_norm": 3788.9033203125,
      "learning_rate": 7.101135190918474e-05,
      "loss": 12.0506,
      "step": 6218
    },
    {
      "epoch": 6.23,
      "grad_norm": 4612.279296875,
      "learning_rate": 7.10061919504644e-05,
      "loss": 18.3761,
      "step": 6219
    },
    {
      "epoch": 6.23,
      "grad_norm": 4979.20703125,
      "learning_rate": 7.100103199174407e-05,
      "loss": 17.5331,
      "step": 6220
    },
    {
      "epoch": 6.23,
      "grad_norm": 1944.4971923828125,
      "learning_rate": 7.099587203302373e-05,
      "loss": 11.9879,
      "step": 6221
    },
    {
      "epoch": 6.23,
      "grad_norm": 7154.47265625,
      "learning_rate": 7.099071207430341e-05,
      "loss": 16.9056,
      "step": 6222
    },
    {
      "epoch": 6.23,
      "grad_norm": 4925.12060546875,
      "learning_rate": 7.098555211558308e-05,
      "loss": 16.456,
      "step": 6223
    },
    {
      "epoch": 6.23,
      "grad_norm": 1481.4703369140625,
      "learning_rate": 7.098039215686276e-05,
      "loss": 12.4007,
      "step": 6224
    },
    {
      "epoch": 6.23,
      "grad_norm": 19356.267578125,
      "learning_rate": 7.097523219814242e-05,
      "loss": 13.5958,
      "step": 6225
    },
    {
      "epoch": 6.23,
      "grad_norm": 22206.5546875,
      "learning_rate": 7.097007223942209e-05,
      "loss": 15.4377,
      "step": 6226
    },
    {
      "epoch": 6.23,
      "grad_norm": 5198.57373046875,
      "learning_rate": 7.096491228070175e-05,
      "loss": 11.5001,
      "step": 6227
    },
    {
      "epoch": 6.23,
      "grad_norm": 10073.0986328125,
      "learning_rate": 7.095975232198143e-05,
      "loss": 13.006,
      "step": 6228
    },
    {
      "epoch": 6.24,
      "grad_norm": 701.261474609375,
      "learning_rate": 7.09545923632611e-05,
      "loss": 11.717,
      "step": 6229
    },
    {
      "epoch": 6.24,
      "grad_norm": 15086.3671875,
      "learning_rate": 7.094943240454078e-05,
      "loss": 10.6204,
      "step": 6230
    },
    {
      "epoch": 6.24,
      "grad_norm": 3459.764404296875,
      "learning_rate": 7.094427244582044e-05,
      "loss": 15.0283,
      "step": 6231
    },
    {
      "epoch": 6.24,
      "grad_norm": 739.107421875,
      "learning_rate": 7.09391124871001e-05,
      "loss": 12.46,
      "step": 6232
    },
    {
      "epoch": 6.24,
      "grad_norm": 11578.4912109375,
      "learning_rate": 7.093395252837977e-05,
      "loss": 14.1447,
      "step": 6233
    },
    {
      "epoch": 6.24,
      "grad_norm": 3480.48828125,
      "learning_rate": 7.092879256965945e-05,
      "loss": 16.6074,
      "step": 6234
    },
    {
      "epoch": 6.24,
      "grad_norm": 6919.92919921875,
      "learning_rate": 7.092363261093912e-05,
      "loss": 15.8474,
      "step": 6235
    },
    {
      "epoch": 6.24,
      "grad_norm": 5128.81787109375,
      "learning_rate": 7.091847265221878e-05,
      "loss": 17.3469,
      "step": 6236
    },
    {
      "epoch": 6.24,
      "grad_norm": 2693.32421875,
      "learning_rate": 7.091331269349846e-05,
      "loss": 15.7185,
      "step": 6237
    },
    {
      "epoch": 6.24,
      "grad_norm": 22204.896484375,
      "learning_rate": 7.090815273477812e-05,
      "loss": 13.3926,
      "step": 6238
    },
    {
      "epoch": 6.25,
      "grad_norm": 51294.72265625,
      "learning_rate": 7.09029927760578e-05,
      "loss": 19.1703,
      "step": 6239
    },
    {
      "epoch": 6.25,
      "grad_norm": 6845.62939453125,
      "learning_rate": 7.089783281733747e-05,
      "loss": 13.9605,
      "step": 6240
    },
    {
      "epoch": 6.25,
      "grad_norm": 6149.4765625,
      "learning_rate": 7.089267285861714e-05,
      "loss": 9.8715,
      "step": 6241
    },
    {
      "epoch": 6.25,
      "grad_norm": 7195.619140625,
      "learning_rate": 7.08875128998968e-05,
      "loss": 13.2166,
      "step": 6242
    },
    {
      "epoch": 6.25,
      "grad_norm": 2766.615478515625,
      "learning_rate": 7.088235294117648e-05,
      "loss": 13.4779,
      "step": 6243
    },
    {
      "epoch": 6.25,
      "grad_norm": 11201.2294921875,
      "learning_rate": 7.087719298245614e-05,
      "loss": 13.1101,
      "step": 6244
    },
    {
      "epoch": 6.25,
      "grad_norm": 14000.8095703125,
      "learning_rate": 7.087203302373581e-05,
      "loss": 15.3659,
      "step": 6245
    },
    {
      "epoch": 6.25,
      "grad_norm": 1136.73681640625,
      "learning_rate": 7.086687306501549e-05,
      "loss": 10.9595,
      "step": 6246
    },
    {
      "epoch": 6.25,
      "grad_norm": 1809.515625,
      "learning_rate": 7.086171310629516e-05,
      "loss": 11.5976,
      "step": 6247
    },
    {
      "epoch": 6.25,
      "grad_norm": 6254.2373046875,
      "learning_rate": 7.085655314757482e-05,
      "loss": 15.4069,
      "step": 6248
    },
    {
      "epoch": 6.26,
      "grad_norm": 1316.3653564453125,
      "learning_rate": 7.08513931888545e-05,
      "loss": 13.8749,
      "step": 6249
    },
    {
      "epoch": 6.26,
      "grad_norm": 16186.9677734375,
      "learning_rate": 7.084623323013416e-05,
      "loss": 12.6629,
      "step": 6250
    },
    {
      "epoch": 6.26,
      "grad_norm": 376.4715576171875,
      "learning_rate": 7.084107327141383e-05,
      "loss": 12.6895,
      "step": 6251
    },
    {
      "epoch": 6.26,
      "grad_norm": 30941.587890625,
      "learning_rate": 7.083591331269351e-05,
      "loss": 12.3461,
      "step": 6252
    },
    {
      "epoch": 6.26,
      "grad_norm": 2775.82080078125,
      "learning_rate": 7.083075335397317e-05,
      "loss": 12.1122,
      "step": 6253
    },
    {
      "epoch": 6.26,
      "grad_norm": 745.8867797851562,
      "learning_rate": 7.082559339525284e-05,
      "loss": 12.8681,
      "step": 6254
    },
    {
      "epoch": 6.26,
      "grad_norm": 5660.14794921875,
      "learning_rate": 7.08204334365325e-05,
      "loss": 13.4243,
      "step": 6255
    },
    {
      "epoch": 6.26,
      "grad_norm": 1490.2945556640625,
      "learning_rate": 7.081527347781218e-05,
      "loss": 13.3077,
      "step": 6256
    },
    {
      "epoch": 6.26,
      "grad_norm": 1714.50634765625,
      "learning_rate": 7.081011351909185e-05,
      "loss": 11.7811,
      "step": 6257
    },
    {
      "epoch": 6.26,
      "grad_norm": 3147.005126953125,
      "learning_rate": 7.080495356037153e-05,
      "loss": 11.0322,
      "step": 6258
    },
    {
      "epoch": 6.27,
      "grad_norm": 2992.6552734375,
      "learning_rate": 7.079979360165119e-05,
      "loss": 13.1673,
      "step": 6259
    },
    {
      "epoch": 6.27,
      "grad_norm": 29847.962890625,
      "learning_rate": 7.079463364293086e-05,
      "loss": 17.461,
      "step": 6260
    },
    {
      "epoch": 6.27,
      "grad_norm": 2166.907958984375,
      "learning_rate": 7.078947368421052e-05,
      "loss": 18.2071,
      "step": 6261
    },
    {
      "epoch": 6.27,
      "grad_norm": 1957.1536865234375,
      "learning_rate": 7.07843137254902e-05,
      "loss": 11.3932,
      "step": 6262
    },
    {
      "epoch": 6.27,
      "grad_norm": 5653.3984375,
      "learning_rate": 7.077915376676987e-05,
      "loss": 15.196,
      "step": 6263
    },
    {
      "epoch": 6.27,
      "grad_norm": 1309.0711669921875,
      "learning_rate": 7.077399380804955e-05,
      "loss": 11.3348,
      "step": 6264
    },
    {
      "epoch": 6.27,
      "grad_norm": 6692.26611328125,
      "learning_rate": 7.076883384932921e-05,
      "loss": 12.526,
      "step": 6265
    },
    {
      "epoch": 6.27,
      "grad_norm": 1225.4981689453125,
      "learning_rate": 7.076367389060888e-05,
      "loss": 11.8083,
      "step": 6266
    },
    {
      "epoch": 6.27,
      "grad_norm": 3709.794921875,
      "learning_rate": 7.075851393188854e-05,
      "loss": 12.1295,
      "step": 6267
    },
    {
      "epoch": 6.27,
      "grad_norm": 6505.2998046875,
      "learning_rate": 7.075335397316822e-05,
      "loss": 11.1626,
      "step": 6268
    },
    {
      "epoch": 6.28,
      "grad_norm": 2721.177978515625,
      "learning_rate": 7.07481940144479e-05,
      "loss": 12.4654,
      "step": 6269
    },
    {
      "epoch": 6.28,
      "grad_norm": 8540.9306640625,
      "learning_rate": 7.074303405572755e-05,
      "loss": 16.8594,
      "step": 6270
    },
    {
      "epoch": 6.28,
      "grad_norm": 1864.17431640625,
      "learning_rate": 7.073787409700723e-05,
      "loss": 13.0357,
      "step": 6271
    },
    {
      "epoch": 6.28,
      "grad_norm": 1626.6175537109375,
      "learning_rate": 7.073271413828689e-05,
      "loss": 11.986,
      "step": 6272
    },
    {
      "epoch": 6.28,
      "grad_norm": 1837.450439453125,
      "learning_rate": 7.072755417956656e-05,
      "loss": 15.0937,
      "step": 6273
    },
    {
      "epoch": 6.28,
      "grad_norm": 29000.45703125,
      "learning_rate": 7.072239422084624e-05,
      "loss": 13.407,
      "step": 6274
    },
    {
      "epoch": 6.28,
      "grad_norm": 4436.75537109375,
      "learning_rate": 7.071723426212591e-05,
      "loss": 12.303,
      "step": 6275
    },
    {
      "epoch": 6.28,
      "grad_norm": 2345.109619140625,
      "learning_rate": 7.071207430340557e-05,
      "loss": 17.9594,
      "step": 6276
    },
    {
      "epoch": 6.28,
      "grad_norm": 5734.3193359375,
      "learning_rate": 7.070691434468525e-05,
      "loss": 17.0018,
      "step": 6277
    },
    {
      "epoch": 6.28,
      "grad_norm": 451.1159362792969,
      "learning_rate": 7.070175438596491e-05,
      "loss": 12.77,
      "step": 6278
    },
    {
      "epoch": 6.29,
      "grad_norm": 11328.5693359375,
      "learning_rate": 7.069659442724458e-05,
      "loss": 12.4122,
      "step": 6279
    },
    {
      "epoch": 6.29,
      "grad_norm": 1810.8572998046875,
      "learning_rate": 7.069143446852426e-05,
      "loss": 12.0642,
      "step": 6280
    },
    {
      "epoch": 6.29,
      "grad_norm": 12583.240234375,
      "learning_rate": 7.068627450980393e-05,
      "loss": 19.4414,
      "step": 6281
    },
    {
      "epoch": 6.29,
      "grad_norm": 4648.986328125,
      "learning_rate": 7.06811145510836e-05,
      "loss": 13.8447,
      "step": 6282
    },
    {
      "epoch": 6.29,
      "grad_norm": 6388.19677734375,
      "learning_rate": 7.067595459236327e-05,
      "loss": 14.7026,
      "step": 6283
    },
    {
      "epoch": 6.29,
      "grad_norm": 5633.244140625,
      "learning_rate": 7.067079463364293e-05,
      "loss": 14.0248,
      "step": 6284
    },
    {
      "epoch": 6.29,
      "grad_norm": 5226.00146484375,
      "learning_rate": 7.06656346749226e-05,
      "loss": 13.9163,
      "step": 6285
    },
    {
      "epoch": 6.29,
      "grad_norm": 3428.746337890625,
      "learning_rate": 7.066047471620228e-05,
      "loss": 14.6645,
      "step": 6286
    },
    {
      "epoch": 6.29,
      "grad_norm": 3986.900634765625,
      "learning_rate": 7.065531475748194e-05,
      "loss": 17.0583,
      "step": 6287
    },
    {
      "epoch": 6.29,
      "grad_norm": 4292.07568359375,
      "learning_rate": 7.065015479876161e-05,
      "loss": 12.5422,
      "step": 6288
    },
    {
      "epoch": 6.3,
      "grad_norm": 58957.60546875,
      "learning_rate": 7.064499484004128e-05,
      "loss": 18.0484,
      "step": 6289
    },
    {
      "epoch": 6.3,
      "grad_norm": 908.5712890625,
      "learning_rate": 7.063983488132095e-05,
      "loss": 13.5248,
      "step": 6290
    },
    {
      "epoch": 6.3,
      "grad_norm": 2315.337646484375,
      "learning_rate": 7.063467492260062e-05,
      "loss": 13.4384,
      "step": 6291
    },
    {
      "epoch": 6.3,
      "grad_norm": 109890.640625,
      "learning_rate": 7.06295149638803e-05,
      "loss": 19.9136,
      "step": 6292
    },
    {
      "epoch": 6.3,
      "grad_norm": 6139.1435546875,
      "learning_rate": 7.062435500515996e-05,
      "loss": 14.6957,
      "step": 6293
    },
    {
      "epoch": 6.3,
      "grad_norm": 12087.6025390625,
      "learning_rate": 7.061919504643963e-05,
      "loss": 11.6682,
      "step": 6294
    },
    {
      "epoch": 6.3,
      "grad_norm": 2839.604736328125,
      "learning_rate": 7.06140350877193e-05,
      "loss": 11.6089,
      "step": 6295
    },
    {
      "epoch": 6.3,
      "grad_norm": 3129.234130859375,
      "learning_rate": 7.060887512899897e-05,
      "loss": 11.6144,
      "step": 6296
    },
    {
      "epoch": 6.3,
      "grad_norm": 5526.537109375,
      "learning_rate": 7.060371517027864e-05,
      "loss": 14.2659,
      "step": 6297
    },
    {
      "epoch": 6.3,
      "grad_norm": 6542.5908203125,
      "learning_rate": 7.059855521155832e-05,
      "loss": 15.191,
      "step": 6298
    },
    {
      "epoch": 6.31,
      "grad_norm": 8426.953125,
      "learning_rate": 7.059339525283798e-05,
      "loss": 14.8557,
      "step": 6299
    },
    {
      "epoch": 6.31,
      "grad_norm": 3653.506591796875,
      "learning_rate": 7.058823529411765e-05,
      "loss": 13.616,
      "step": 6300
    },
    {
      "epoch": 6.31,
      "grad_norm": 16437.3984375,
      "learning_rate": 7.058307533539732e-05,
      "loss": 25.2604,
      "step": 6301
    },
    {
      "epoch": 6.31,
      "grad_norm": 26631.986328125,
      "learning_rate": 7.057791537667699e-05,
      "loss": 22.6069,
      "step": 6302
    },
    {
      "epoch": 6.31,
      "grad_norm": 13298.9208984375,
      "learning_rate": 7.057275541795666e-05,
      "loss": 12.1689,
      "step": 6303
    },
    {
      "epoch": 6.31,
      "grad_norm": 7811.11865234375,
      "learning_rate": 7.056759545923633e-05,
      "loss": 13.9356,
      "step": 6304
    },
    {
      "epoch": 6.31,
      "grad_norm": 895.1427612304688,
      "learning_rate": 7.0562435500516e-05,
      "loss": 12.8964,
      "step": 6305
    },
    {
      "epoch": 6.31,
      "grad_norm": 3588.88330078125,
      "learning_rate": 7.055727554179566e-05,
      "loss": 13.1335,
      "step": 6306
    },
    {
      "epoch": 6.31,
      "grad_norm": 12058.2861328125,
      "learning_rate": 7.055211558307534e-05,
      "loss": 21.1293,
      "step": 6307
    },
    {
      "epoch": 6.31,
      "grad_norm": 11362.4150390625,
      "learning_rate": 7.054695562435501e-05,
      "loss": 11.4979,
      "step": 6308
    },
    {
      "epoch": 6.32,
      "grad_norm": 3050.70849609375,
      "learning_rate": 7.054179566563468e-05,
      "loss": 12.0896,
      "step": 6309
    },
    {
      "epoch": 6.32,
      "grad_norm": 1811.3248291015625,
      "learning_rate": 7.053663570691435e-05,
      "loss": 10.1154,
      "step": 6310
    },
    {
      "epoch": 6.32,
      "grad_norm": 327.496337890625,
      "learning_rate": 7.053147574819402e-05,
      "loss": 10.6761,
      "step": 6311
    },
    {
      "epoch": 6.32,
      "grad_norm": 11817.94140625,
      "learning_rate": 7.052631578947368e-05,
      "loss": 13.575,
      "step": 6312
    },
    {
      "epoch": 6.32,
      "grad_norm": 5506.82177734375,
      "learning_rate": 7.052115583075336e-05,
      "loss": 12.959,
      "step": 6313
    },
    {
      "epoch": 6.32,
      "grad_norm": 7541.96923828125,
      "learning_rate": 7.051599587203303e-05,
      "loss": 14.8917,
      "step": 6314
    },
    {
      "epoch": 6.32,
      "grad_norm": 6650.41650390625,
      "learning_rate": 7.05108359133127e-05,
      "loss": 11.0522,
      "step": 6315
    },
    {
      "epoch": 6.32,
      "grad_norm": 6072.44287109375,
      "learning_rate": 7.050567595459237e-05,
      "loss": 16.0193,
      "step": 6316
    },
    {
      "epoch": 6.32,
      "grad_norm": 29039.693359375,
      "learning_rate": 7.050051599587204e-05,
      "loss": 18.1018,
      "step": 6317
    },
    {
      "epoch": 6.32,
      "grad_norm": 3325.28955078125,
      "learning_rate": 7.04953560371517e-05,
      "loss": 16.007,
      "step": 6318
    },
    {
      "epoch": 6.33,
      "grad_norm": 10312.3359375,
      "learning_rate": 7.049019607843138e-05,
      "loss": 15.9544,
      "step": 6319
    },
    {
      "epoch": 6.33,
      "grad_norm": 7754.92578125,
      "learning_rate": 7.048503611971105e-05,
      "loss": 12.5763,
      "step": 6320
    },
    {
      "epoch": 6.33,
      "grad_norm": 2198.252197265625,
      "learning_rate": 7.047987616099072e-05,
      "loss": 12.1862,
      "step": 6321
    },
    {
      "epoch": 6.33,
      "grad_norm": 43475.6328125,
      "learning_rate": 7.047471620227039e-05,
      "loss": 16.222,
      "step": 6322
    },
    {
      "epoch": 6.33,
      "grad_norm": 716.9581298828125,
      "learning_rate": 7.046955624355005e-05,
      "loss": 10.2554,
      "step": 6323
    },
    {
      "epoch": 6.33,
      "grad_norm": 30548.75,
      "learning_rate": 7.046439628482972e-05,
      "loss": 16.7024,
      "step": 6324
    },
    {
      "epoch": 6.33,
      "grad_norm": 2166.85302734375,
      "learning_rate": 7.04592363261094e-05,
      "loss": 11.9622,
      "step": 6325
    },
    {
      "epoch": 6.33,
      "grad_norm": 1127.21875,
      "learning_rate": 7.045407636738907e-05,
      "loss": 18.1787,
      "step": 6326
    },
    {
      "epoch": 6.33,
      "grad_norm": 5941.1435546875,
      "learning_rate": 7.044891640866873e-05,
      "loss": 12.5486,
      "step": 6327
    },
    {
      "epoch": 6.33,
      "grad_norm": 1683.99169921875,
      "learning_rate": 7.04437564499484e-05,
      "loss": 15.9115,
      "step": 6328
    },
    {
      "epoch": 6.34,
      "grad_norm": 418.6175231933594,
      "learning_rate": 7.043859649122807e-05,
      "loss": 10.6647,
      "step": 6329
    },
    {
      "epoch": 6.34,
      "grad_norm": 1037.239013671875,
      "learning_rate": 7.043343653250774e-05,
      "loss": 10.8719,
      "step": 6330
    },
    {
      "epoch": 6.34,
      "grad_norm": 4095.97412109375,
      "learning_rate": 7.042827657378742e-05,
      "loss": 15.4192,
      "step": 6331
    },
    {
      "epoch": 6.34,
      "grad_norm": 14451.1591796875,
      "learning_rate": 7.042311661506709e-05,
      "loss": 26.4505,
      "step": 6332
    },
    {
      "epoch": 6.34,
      "grad_norm": 1138.7677001953125,
      "learning_rate": 7.041795665634675e-05,
      "loss": 12.6429,
      "step": 6333
    },
    {
      "epoch": 6.34,
      "grad_norm": 2102.8681640625,
      "learning_rate": 7.041279669762643e-05,
      "loss": 25.6268,
      "step": 6334
    },
    {
      "epoch": 6.34,
      "grad_norm": 7618.98388671875,
      "learning_rate": 7.040763673890609e-05,
      "loss": 18.3477,
      "step": 6335
    },
    {
      "epoch": 6.34,
      "grad_norm": 6872.2451171875,
      "learning_rate": 7.040247678018576e-05,
      "loss": 13.4369,
      "step": 6336
    },
    {
      "epoch": 6.34,
      "grad_norm": 1896.3009033203125,
      "learning_rate": 7.039731682146544e-05,
      "loss": 12.78,
      "step": 6337
    },
    {
      "epoch": 6.34,
      "grad_norm": 40598.703125,
      "learning_rate": 7.039215686274511e-05,
      "loss": 18.8579,
      "step": 6338
    },
    {
      "epoch": 6.35,
      "grad_norm": 13051.27734375,
      "learning_rate": 7.038699690402477e-05,
      "loss": 11.1528,
      "step": 6339
    },
    {
      "epoch": 6.35,
      "grad_norm": 2509.339111328125,
      "learning_rate": 7.038183694530443e-05,
      "loss": 14.6804,
      "step": 6340
    },
    {
      "epoch": 6.35,
      "grad_norm": 35481.45703125,
      "learning_rate": 7.03766769865841e-05,
      "loss": 22.5183,
      "step": 6341
    },
    {
      "epoch": 6.35,
      "grad_norm": 2942.48095703125,
      "learning_rate": 7.037151702786378e-05,
      "loss": 11.5,
      "step": 6342
    },
    {
      "epoch": 6.35,
      "grad_norm": 8513.8310546875,
      "learning_rate": 7.036635706914346e-05,
      "loss": 12.7548,
      "step": 6343
    },
    {
      "epoch": 6.35,
      "grad_norm": 6332.2421875,
      "learning_rate": 7.036119711042312e-05,
      "loss": 12.8341,
      "step": 6344
    },
    {
      "epoch": 6.35,
      "grad_norm": 2761.18505859375,
      "learning_rate": 7.035603715170279e-05,
      "loss": 14.8322,
      "step": 6345
    },
    {
      "epoch": 6.35,
      "grad_norm": 12813.3984375,
      "learning_rate": 7.035087719298245e-05,
      "loss": 13.8308,
      "step": 6346
    },
    {
      "epoch": 6.35,
      "grad_norm": 8802.18359375,
      "learning_rate": 7.034571723426213e-05,
      "loss": 19.1069,
      "step": 6347
    },
    {
      "epoch": 6.35,
      "grad_norm": 529734.0625,
      "learning_rate": 7.03405572755418e-05,
      "loss": 20.6276,
      "step": 6348
    },
    {
      "epoch": 6.36,
      "grad_norm": 10040.6708984375,
      "learning_rate": 7.033539731682148e-05,
      "loss": 13.3892,
      "step": 6349
    },
    {
      "epoch": 6.36,
      "grad_norm": 55329.8125,
      "learning_rate": 7.033023735810114e-05,
      "loss": 13.2599,
      "step": 6350
    },
    {
      "epoch": 6.36,
      "grad_norm": 34299.7265625,
      "learning_rate": 7.032507739938081e-05,
      "loss": 15.7422,
      "step": 6351
    },
    {
      "epoch": 6.36,
      "grad_norm": 1056.0821533203125,
      "learning_rate": 7.031991744066047e-05,
      "loss": 12.3767,
      "step": 6352
    },
    {
      "epoch": 6.36,
      "grad_norm": 4021.750244140625,
      "learning_rate": 7.031475748194015e-05,
      "loss": 12.0107,
      "step": 6353
    },
    {
      "epoch": 6.36,
      "grad_norm": 571.518310546875,
      "learning_rate": 7.030959752321982e-05,
      "loss": 11.3829,
      "step": 6354
    },
    {
      "epoch": 6.36,
      "grad_norm": 1265.0108642578125,
      "learning_rate": 7.03044375644995e-05,
      "loss": 11.9661,
      "step": 6355
    },
    {
      "epoch": 6.36,
      "grad_norm": 5598.1669921875,
      "learning_rate": 7.029927760577916e-05,
      "loss": 14.0743,
      "step": 6356
    },
    {
      "epoch": 6.36,
      "grad_norm": 4927.6611328125,
      "learning_rate": 7.029411764705882e-05,
      "loss": 14.9835,
      "step": 6357
    },
    {
      "epoch": 6.36,
      "grad_norm": 1445.7296142578125,
      "learning_rate": 7.028895768833849e-05,
      "loss": 11.7363,
      "step": 6358
    },
    {
      "epoch": 6.37,
      "grad_norm": 12437.5107421875,
      "learning_rate": 7.028379772961817e-05,
      "loss": 18.1616,
      "step": 6359
    },
    {
      "epoch": 6.37,
      "grad_norm": 5648.81103515625,
      "learning_rate": 7.027863777089784e-05,
      "loss": 18.5156,
      "step": 6360
    },
    {
      "epoch": 6.37,
      "grad_norm": 7198.0205078125,
      "learning_rate": 7.02734778121775e-05,
      "loss": 11.0996,
      "step": 6361
    },
    {
      "epoch": 6.37,
      "grad_norm": 3472.2666015625,
      "learning_rate": 7.026831785345718e-05,
      "loss": 11.603,
      "step": 6362
    },
    {
      "epoch": 6.37,
      "grad_norm": 1781.1605224609375,
      "learning_rate": 7.026315789473684e-05,
      "loss": 10.5797,
      "step": 6363
    },
    {
      "epoch": 6.37,
      "grad_norm": 44932.9765625,
      "learning_rate": 7.025799793601651e-05,
      "loss": 15.4719,
      "step": 6364
    },
    {
      "epoch": 6.37,
      "grad_norm": 3577.5595703125,
      "learning_rate": 7.025283797729619e-05,
      "loss": 12.6022,
      "step": 6365
    },
    {
      "epoch": 6.37,
      "grad_norm": 2219.852783203125,
      "learning_rate": 7.024767801857586e-05,
      "loss": 12.5239,
      "step": 6366
    },
    {
      "epoch": 6.37,
      "grad_norm": 17941.4296875,
      "learning_rate": 7.024251805985552e-05,
      "loss": 17.7538,
      "step": 6367
    },
    {
      "epoch": 6.37,
      "grad_norm": 5408.3095703125,
      "learning_rate": 7.02373581011352e-05,
      "loss": 12.8919,
      "step": 6368
    },
    {
      "epoch": 6.38,
      "grad_norm": 11685.14453125,
      "learning_rate": 7.023219814241486e-05,
      "loss": 13.2165,
      "step": 6369
    },
    {
      "epoch": 6.38,
      "grad_norm": 3432.965087890625,
      "learning_rate": 7.022703818369453e-05,
      "loss": 16.2435,
      "step": 6370
    },
    {
      "epoch": 6.38,
      "grad_norm": 1845.798583984375,
      "learning_rate": 7.02218782249742e-05,
      "loss": 15.2797,
      "step": 6371
    },
    {
      "epoch": 6.38,
      "grad_norm": 9011.8837890625,
      "learning_rate": 7.021671826625388e-05,
      "loss": 14.9255,
      "step": 6372
    },
    {
      "epoch": 6.38,
      "grad_norm": 1310.4583740234375,
      "learning_rate": 7.021155830753354e-05,
      "loss": 12.3247,
      "step": 6373
    },
    {
      "epoch": 6.38,
      "grad_norm": 5448.20458984375,
      "learning_rate": 7.020639834881322e-05,
      "loss": 13.8441,
      "step": 6374
    },
    {
      "epoch": 6.38,
      "grad_norm": 3452.440673828125,
      "learning_rate": 7.020123839009288e-05,
      "loss": 11.8959,
      "step": 6375
    },
    {
      "epoch": 6.38,
      "grad_norm": 517.1951904296875,
      "learning_rate": 7.019607843137255e-05,
      "loss": 10.5966,
      "step": 6376
    },
    {
      "epoch": 6.38,
      "grad_norm": 3327.566162109375,
      "learning_rate": 7.019091847265223e-05,
      "loss": 14.37,
      "step": 6377
    },
    {
      "epoch": 6.38,
      "grad_norm": 43405.76953125,
      "learning_rate": 7.018575851393189e-05,
      "loss": 19.6918,
      "step": 6378
    },
    {
      "epoch": 6.39,
      "grad_norm": 7043.455078125,
      "learning_rate": 7.018059855521156e-05,
      "loss": 16.4613,
      "step": 6379
    },
    {
      "epoch": 6.39,
      "grad_norm": 1372.117919921875,
      "learning_rate": 7.017543859649122e-05,
      "loss": 13.4138,
      "step": 6380
    },
    {
      "epoch": 6.39,
      "grad_norm": 4421.2080078125,
      "learning_rate": 7.01702786377709e-05,
      "loss": 11.5216,
      "step": 6381
    },
    {
      "epoch": 6.39,
      "grad_norm": 3478.37646484375,
      "learning_rate": 7.016511867905057e-05,
      "loss": 17.7984,
      "step": 6382
    },
    {
      "epoch": 6.39,
      "grad_norm": 2935.52783203125,
      "learning_rate": 7.015995872033025e-05,
      "loss": 11.5495,
      "step": 6383
    },
    {
      "epoch": 6.39,
      "grad_norm": 50935.02734375,
      "learning_rate": 7.015479876160991e-05,
      "loss": 14.1925,
      "step": 6384
    },
    {
      "epoch": 6.39,
      "grad_norm": 3075.40576171875,
      "learning_rate": 7.014963880288958e-05,
      "loss": 15.4773,
      "step": 6385
    },
    {
      "epoch": 6.39,
      "grad_norm": 5323.1650390625,
      "learning_rate": 7.014447884416924e-05,
      "loss": 13.1781,
      "step": 6386
    },
    {
      "epoch": 6.39,
      "grad_norm": 4422.115234375,
      "learning_rate": 7.013931888544892e-05,
      "loss": 13.0856,
      "step": 6387
    },
    {
      "epoch": 6.39,
      "grad_norm": 21982.392578125,
      "learning_rate": 7.013415892672859e-05,
      "loss": 12.739,
      "step": 6388
    },
    {
      "epoch": 6.4,
      "grad_norm": 10522.5361328125,
      "learning_rate": 7.012899896800827e-05,
      "loss": 12.9641,
      "step": 6389
    },
    {
      "epoch": 6.4,
      "grad_norm": 14525.4853515625,
      "learning_rate": 7.012383900928793e-05,
      "loss": 13.3956,
      "step": 6390
    },
    {
      "epoch": 6.4,
      "grad_norm": 4442.89306640625,
      "learning_rate": 7.01186790505676e-05,
      "loss": 12.9169,
      "step": 6391
    },
    {
      "epoch": 6.4,
      "grad_norm": 1840.7806396484375,
      "learning_rate": 7.011351909184726e-05,
      "loss": 14.2083,
      "step": 6392
    },
    {
      "epoch": 6.4,
      "grad_norm": 35294.37890625,
      "learning_rate": 7.010835913312694e-05,
      "loss": 14.1733,
      "step": 6393
    },
    {
      "epoch": 6.4,
      "grad_norm": 5954.84814453125,
      "learning_rate": 7.010319917440661e-05,
      "loss": 25.2775,
      "step": 6394
    },
    {
      "epoch": 6.4,
      "grad_norm": 1098.1529541015625,
      "learning_rate": 7.009803921568627e-05,
      "loss": 10.6698,
      "step": 6395
    },
    {
      "epoch": 6.4,
      "grad_norm": 4707.27587890625,
      "learning_rate": 7.009287925696595e-05,
      "loss": 13.5154,
      "step": 6396
    },
    {
      "epoch": 6.4,
      "grad_norm": 4022.934814453125,
      "learning_rate": 7.008771929824561e-05,
      "loss": 12.5642,
      "step": 6397
    },
    {
      "epoch": 6.4,
      "grad_norm": 8305.2333984375,
      "learning_rate": 7.008255933952528e-05,
      "loss": 19.6158,
      "step": 6398
    },
    {
      "epoch": 6.41,
      "grad_norm": 1370.61865234375,
      "learning_rate": 7.007739938080496e-05,
      "loss": 12.7495,
      "step": 6399
    },
    {
      "epoch": 6.41,
      "grad_norm": 3800.176513671875,
      "learning_rate": 7.007223942208463e-05,
      "loss": 13.7998,
      "step": 6400
    },
    {
      "epoch": 6.41,
      "grad_norm": 7051.5380859375,
      "learning_rate": 7.006707946336429e-05,
      "loss": 12.6222,
      "step": 6401
    },
    {
      "epoch": 6.41,
      "grad_norm": 3516.7822265625,
      "learning_rate": 7.006191950464397e-05,
      "loss": 14.0972,
      "step": 6402
    },
    {
      "epoch": 6.41,
      "grad_norm": 4364.72119140625,
      "learning_rate": 7.005675954592363e-05,
      "loss": 13.1669,
      "step": 6403
    },
    {
      "epoch": 6.41,
      "grad_norm": 5066.275390625,
      "learning_rate": 7.00515995872033e-05,
      "loss": 13.7406,
      "step": 6404
    },
    {
      "epoch": 6.41,
      "grad_norm": 1671.0147705078125,
      "learning_rate": 7.004643962848298e-05,
      "loss": 11.9821,
      "step": 6405
    },
    {
      "epoch": 6.41,
      "grad_norm": 13465.8837890625,
      "learning_rate": 7.004127966976265e-05,
      "loss": 13.1949,
      "step": 6406
    },
    {
      "epoch": 6.41,
      "grad_norm": 1841.578369140625,
      "learning_rate": 7.003611971104231e-05,
      "loss": 14.6487,
      "step": 6407
    },
    {
      "epoch": 6.41,
      "grad_norm": 1447.3291015625,
      "learning_rate": 7.003095975232199e-05,
      "loss": 14.1045,
      "step": 6408
    },
    {
      "epoch": 6.42,
      "grad_norm": 56687.26171875,
      "learning_rate": 7.002579979360165e-05,
      "loss": 17.4186,
      "step": 6409
    },
    {
      "epoch": 6.42,
      "grad_norm": 19981.876953125,
      "learning_rate": 7.002063983488134e-05,
      "loss": 14.3939,
      "step": 6410
    },
    {
      "epoch": 6.42,
      "grad_norm": 23637.947265625,
      "learning_rate": 7.0015479876161e-05,
      "loss": 21.6503,
      "step": 6411
    },
    {
      "epoch": 6.42,
      "grad_norm": 8149.18115234375,
      "learning_rate": 7.001031991744066e-05,
      "loss": 12.3963,
      "step": 6412
    },
    {
      "epoch": 6.42,
      "grad_norm": 16141.5185546875,
      "learning_rate": 7.000515995872033e-05,
      "loss": 21.1511,
      "step": 6413
    },
    {
      "epoch": 6.42,
      "grad_norm": 2567.360107421875,
      "learning_rate": 7e-05,
      "loss": 14.5449,
      "step": 6414
    },
    {
      "epoch": 6.42,
      "grad_norm": 10397.4560546875,
      "learning_rate": 6.999484004127967e-05,
      "loss": 13.3262,
      "step": 6415
    },
    {
      "epoch": 6.42,
      "grad_norm": 900.8095092773438,
      "learning_rate": 6.998968008255934e-05,
      "loss": 11.8633,
      "step": 6416
    },
    {
      "epoch": 6.42,
      "grad_norm": 1257.29248046875,
      "learning_rate": 6.998452012383902e-05,
      "loss": 15.3017,
      "step": 6417
    },
    {
      "epoch": 6.42,
      "grad_norm": 2492.128662109375,
      "learning_rate": 6.997936016511868e-05,
      "loss": 17.2822,
      "step": 6418
    },
    {
      "epoch": 6.43,
      "grad_norm": 14559.150390625,
      "learning_rate": 6.997420020639835e-05,
      "loss": 18.9123,
      "step": 6419
    },
    {
      "epoch": 6.43,
      "grad_norm": 8176.8935546875,
      "learning_rate": 6.996904024767801e-05,
      "loss": 13.0997,
      "step": 6420
    },
    {
      "epoch": 6.43,
      "grad_norm": 1748.22900390625,
      "learning_rate": 6.996388028895769e-05,
      "loss": 12.2258,
      "step": 6421
    },
    {
      "epoch": 6.43,
      "grad_norm": 4893.25048828125,
      "learning_rate": 6.995872033023736e-05,
      "loss": 19.7513,
      "step": 6422
    },
    {
      "epoch": 6.43,
      "grad_norm": 16253.1962890625,
      "learning_rate": 6.995356037151704e-05,
      "loss": 14.8818,
      "step": 6423
    },
    {
      "epoch": 6.43,
      "grad_norm": 5466.27392578125,
      "learning_rate": 6.99484004127967e-05,
      "loss": 16.544,
      "step": 6424
    },
    {
      "epoch": 6.43,
      "grad_norm": 3392.8486328125,
      "learning_rate": 6.994324045407637e-05,
      "loss": 14.482,
      "step": 6425
    },
    {
      "epoch": 6.43,
      "grad_norm": 11811.115234375,
      "learning_rate": 6.993808049535603e-05,
      "loss": 18.5598,
      "step": 6426
    },
    {
      "epoch": 6.43,
      "grad_norm": 522.7116088867188,
      "learning_rate": 6.993292053663572e-05,
      "loss": 13.1395,
      "step": 6427
    },
    {
      "epoch": 6.43,
      "grad_norm": 1714.3873291015625,
      "learning_rate": 6.992776057791538e-05,
      "loss": 13.5614,
      "step": 6428
    },
    {
      "epoch": 6.44,
      "grad_norm": 8140.751953125,
      "learning_rate": 6.992260061919504e-05,
      "loss": 16.6924,
      "step": 6429
    },
    {
      "epoch": 6.44,
      "grad_norm": 659.0464477539062,
      "learning_rate": 6.991744066047472e-05,
      "loss": 12.3219,
      "step": 6430
    },
    {
      "epoch": 6.44,
      "grad_norm": 814.8134765625,
      "learning_rate": 6.991228070175438e-05,
      "loss": 10.4835,
      "step": 6431
    },
    {
      "epoch": 6.44,
      "grad_norm": 5705.4619140625,
      "learning_rate": 6.990712074303405e-05,
      "loss": 12.6558,
      "step": 6432
    },
    {
      "epoch": 6.44,
      "grad_norm": 6631.3330078125,
      "learning_rate": 6.990196078431373e-05,
      "loss": 24.4721,
      "step": 6433
    },
    {
      "epoch": 6.44,
      "grad_norm": 6695.9189453125,
      "learning_rate": 6.98968008255934e-05,
      "loss": 11.1457,
      "step": 6434
    },
    {
      "epoch": 6.44,
      "grad_norm": 1154.3017578125,
      "learning_rate": 6.989164086687306e-05,
      "loss": 11.6955,
      "step": 6435
    },
    {
      "epoch": 6.44,
      "grad_norm": 9854.2119140625,
      "learning_rate": 6.988648090815274e-05,
      "loss": 14.8583,
      "step": 6436
    },
    {
      "epoch": 6.44,
      "grad_norm": 7312.400390625,
      "learning_rate": 6.98813209494324e-05,
      "loss": 16.2643,
      "step": 6437
    },
    {
      "epoch": 6.44,
      "grad_norm": 2903.645751953125,
      "learning_rate": 6.987616099071207e-05,
      "loss": 12.1957,
      "step": 6438
    },
    {
      "epoch": 6.45,
      "grad_norm": 987.6326293945312,
      "learning_rate": 6.987100103199175e-05,
      "loss": 11.0982,
      "step": 6439
    },
    {
      "epoch": 6.45,
      "grad_norm": 5217.19775390625,
      "learning_rate": 6.986584107327142e-05,
      "loss": 13.638,
      "step": 6440
    },
    {
      "epoch": 6.45,
      "grad_norm": 2992.364990234375,
      "learning_rate": 6.986068111455108e-05,
      "loss": 12.3306,
      "step": 6441
    },
    {
      "epoch": 6.45,
      "grad_norm": 20091.330078125,
      "learning_rate": 6.985552115583076e-05,
      "loss": 14.0182,
      "step": 6442
    },
    {
      "epoch": 6.45,
      "grad_norm": 11385.2158203125,
      "learning_rate": 6.985036119711042e-05,
      "loss": 14.0917,
      "step": 6443
    },
    {
      "epoch": 6.45,
      "grad_norm": 5941.2578125,
      "learning_rate": 6.984520123839011e-05,
      "loss": 17.5261,
      "step": 6444
    },
    {
      "epoch": 6.45,
      "grad_norm": 884.6678466796875,
      "learning_rate": 6.984004127966977e-05,
      "loss": 11.1766,
      "step": 6445
    },
    {
      "epoch": 6.45,
      "grad_norm": 3786.39453125,
      "learning_rate": 6.983488132094944e-05,
      "loss": 14.1382,
      "step": 6446
    },
    {
      "epoch": 6.45,
      "grad_norm": 5089.12060546875,
      "learning_rate": 6.98297213622291e-05,
      "loss": 11.9838,
      "step": 6447
    },
    {
      "epoch": 6.45,
      "grad_norm": 3569.65478515625,
      "learning_rate": 6.982456140350876e-05,
      "loss": 11.8813,
      "step": 6448
    },
    {
      "epoch": 6.46,
      "grad_norm": 2370.588134765625,
      "learning_rate": 6.981940144478844e-05,
      "loss": 12.1701,
      "step": 6449
    },
    {
      "epoch": 6.46,
      "grad_norm": 10963.5673828125,
      "learning_rate": 6.981424148606811e-05,
      "loss": 12.7833,
      "step": 6450
    },
    {
      "epoch": 6.46,
      "grad_norm": 3194.958251953125,
      "learning_rate": 6.980908152734779e-05,
      "loss": 10.1282,
      "step": 6451
    },
    {
      "epoch": 6.46,
      "grad_norm": 3272.675048828125,
      "learning_rate": 6.980392156862745e-05,
      "loss": 11.939,
      "step": 6452
    },
    {
      "epoch": 6.46,
      "grad_norm": 3952.578857421875,
      "learning_rate": 6.979876160990712e-05,
      "loss": 13.7288,
      "step": 6453
    },
    {
      "epoch": 6.46,
      "grad_norm": 43401.9453125,
      "learning_rate": 6.979360165118678e-05,
      "loss": 14.6009,
      "step": 6454
    },
    {
      "epoch": 6.46,
      "grad_norm": 1789.3624267578125,
      "learning_rate": 6.978844169246647e-05,
      "loss": 11.595,
      "step": 6455
    },
    {
      "epoch": 6.46,
      "grad_norm": 3666.04736328125,
      "learning_rate": 6.978328173374613e-05,
      "loss": 14.04,
      "step": 6456
    },
    {
      "epoch": 6.46,
      "grad_norm": 4178.59619140625,
      "learning_rate": 6.977812177502581e-05,
      "loss": 16.0288,
      "step": 6457
    },
    {
      "epoch": 6.46,
      "grad_norm": 22868.46484375,
      "learning_rate": 6.977296181630547e-05,
      "loss": 12.4736,
      "step": 6458
    },
    {
      "epoch": 6.47,
      "grad_norm": 3737.7099609375,
      "learning_rate": 6.976780185758514e-05,
      "loss": 15.1766,
      "step": 6459
    },
    {
      "epoch": 6.47,
      "grad_norm": 9609.92578125,
      "learning_rate": 6.97626418988648e-05,
      "loss": 13.803,
      "step": 6460
    },
    {
      "epoch": 6.47,
      "grad_norm": 4332.5576171875,
      "learning_rate": 6.975748194014449e-05,
      "loss": 16.5573,
      "step": 6461
    },
    {
      "epoch": 6.47,
      "grad_norm": 7435.08056640625,
      "learning_rate": 6.975232198142415e-05,
      "loss": 22.0137,
      "step": 6462
    },
    {
      "epoch": 6.47,
      "grad_norm": 9560.287109375,
      "learning_rate": 6.974716202270383e-05,
      "loss": 13.5549,
      "step": 6463
    },
    {
      "epoch": 6.47,
      "grad_norm": 6852.755859375,
      "learning_rate": 6.974200206398349e-05,
      "loss": 17.2662,
      "step": 6464
    },
    {
      "epoch": 6.47,
      "grad_norm": 4967.60986328125,
      "learning_rate": 6.973684210526315e-05,
      "loss": 20.693,
      "step": 6465
    },
    {
      "epoch": 6.47,
      "grad_norm": 22256.9296875,
      "learning_rate": 6.973168214654282e-05,
      "loss": 13.453,
      "step": 6466
    },
    {
      "epoch": 6.47,
      "grad_norm": 10659.11328125,
      "learning_rate": 6.97265221878225e-05,
      "loss": 15.0433,
      "step": 6467
    },
    {
      "epoch": 6.47,
      "grad_norm": 3163.427001953125,
      "learning_rate": 6.972136222910217e-05,
      "loss": 17.414,
      "step": 6468
    },
    {
      "epoch": 6.48,
      "grad_norm": 1356.568359375,
      "learning_rate": 6.971620227038183e-05,
      "loss": 14.6514,
      "step": 6469
    },
    {
      "epoch": 6.48,
      "grad_norm": 41528.859375,
      "learning_rate": 6.971104231166151e-05,
      "loss": 21.3738,
      "step": 6470
    },
    {
      "epoch": 6.48,
      "grad_norm": 3732.988525390625,
      "learning_rate": 6.970588235294117e-05,
      "loss": 11.2068,
      "step": 6471
    },
    {
      "epoch": 6.48,
      "grad_norm": 25762.9765625,
      "learning_rate": 6.970072239422086e-05,
      "loss": 14.423,
      "step": 6472
    },
    {
      "epoch": 6.48,
      "grad_norm": 2903.5703125,
      "learning_rate": 6.969556243550052e-05,
      "loss": 14.043,
      "step": 6473
    },
    {
      "epoch": 6.48,
      "grad_norm": 15440.2568359375,
      "learning_rate": 6.96904024767802e-05,
      "loss": 12.6508,
      "step": 6474
    },
    {
      "epoch": 6.48,
      "grad_norm": 18048.388671875,
      "learning_rate": 6.968524251805985e-05,
      "loss": 10.9946,
      "step": 6475
    },
    {
      "epoch": 6.48,
      "grad_norm": 2091.943115234375,
      "learning_rate": 6.968008255933953e-05,
      "loss": 12.3049,
      "step": 6476
    },
    {
      "epoch": 6.48,
      "grad_norm": 1948.010498046875,
      "learning_rate": 6.967492260061919e-05,
      "loss": 11.1733,
      "step": 6477
    },
    {
      "epoch": 6.48,
      "grad_norm": 2046.977294921875,
      "learning_rate": 6.966976264189888e-05,
      "loss": 10.7072,
      "step": 6478
    },
    {
      "epoch": 6.49,
      "grad_norm": 549.11328125,
      "learning_rate": 6.966460268317854e-05,
      "loss": 11.3074,
      "step": 6479
    },
    {
      "epoch": 6.49,
      "grad_norm": 2804.488037109375,
      "learning_rate": 6.965944272445821e-05,
      "loss": 12.9721,
      "step": 6480
    },
    {
      "epoch": 6.49,
      "grad_norm": 653.8092651367188,
      "learning_rate": 6.965428276573787e-05,
      "loss": 11.8468,
      "step": 6481
    },
    {
      "epoch": 6.49,
      "grad_norm": 27975.689453125,
      "learning_rate": 6.964912280701754e-05,
      "loss": 11.7384,
      "step": 6482
    },
    {
      "epoch": 6.49,
      "grad_norm": 21036.0703125,
      "learning_rate": 6.964396284829722e-05,
      "loss": 14.1359,
      "step": 6483
    },
    {
      "epoch": 6.49,
      "grad_norm": 18120.521484375,
      "learning_rate": 6.963880288957688e-05,
      "loss": 12.8805,
      "step": 6484
    },
    {
      "epoch": 6.49,
      "grad_norm": 4513.09033203125,
      "learning_rate": 6.963364293085656e-05,
      "loss": 16.1355,
      "step": 6485
    },
    {
      "epoch": 6.49,
      "grad_norm": 2160.580810546875,
      "learning_rate": 6.962848297213622e-05,
      "loss": 13.2348,
      "step": 6486
    },
    {
      "epoch": 6.49,
      "grad_norm": 1937.6893310546875,
      "learning_rate": 6.96233230134159e-05,
      "loss": 15.0901,
      "step": 6487
    },
    {
      "epoch": 6.49,
      "grad_norm": 2238.16845703125,
      "learning_rate": 6.961816305469556e-05,
      "loss": 15.5856,
      "step": 6488
    },
    {
      "epoch": 6.5,
      "grad_norm": 1716.951416015625,
      "learning_rate": 6.961300309597524e-05,
      "loss": 14.0873,
      "step": 6489
    },
    {
      "epoch": 6.5,
      "grad_norm": 12226.4033203125,
      "learning_rate": 6.96078431372549e-05,
      "loss": 11.3925,
      "step": 6490
    },
    {
      "epoch": 6.5,
      "grad_norm": 6625.703125,
      "learning_rate": 6.960268317853458e-05,
      "loss": 21.2207,
      "step": 6491
    },
    {
      "epoch": 6.5,
      "grad_norm": 7067.18603515625,
      "learning_rate": 6.959752321981424e-05,
      "loss": 12.1811,
      "step": 6492
    },
    {
      "epoch": 6.5,
      "grad_norm": 2047.6309814453125,
      "learning_rate": 6.959236326109391e-05,
      "loss": 11.6546,
      "step": 6493
    },
    {
      "epoch": 6.5,
      "grad_norm": 4499.5107421875,
      "learning_rate": 6.958720330237358e-05,
      "loss": 21.207,
      "step": 6494
    },
    {
      "epoch": 6.5,
      "grad_norm": 19182.384765625,
      "learning_rate": 6.958204334365326e-05,
      "loss": 19.2032,
      "step": 6495
    },
    {
      "epoch": 6.5,
      "grad_norm": 2727.712646484375,
      "learning_rate": 6.957688338493292e-05,
      "loss": 12.334,
      "step": 6496
    },
    {
      "epoch": 6.5,
      "grad_norm": 3095.467041015625,
      "learning_rate": 6.95717234262126e-05,
      "loss": 17.5879,
      "step": 6497
    },
    {
      "epoch": 6.5,
      "grad_norm": 29968.55859375,
      "learning_rate": 6.956656346749226e-05,
      "loss": 17.029,
      "step": 6498
    },
    {
      "epoch": 6.51,
      "grad_norm": 6118.822265625,
      "learning_rate": 6.956140350877193e-05,
      "loss": 10.8361,
      "step": 6499
    },
    {
      "epoch": 6.51,
      "grad_norm": 6986.60595703125,
      "learning_rate": 6.955624355005161e-05,
      "loss": 14.703,
      "step": 6500
    },
    {
      "epoch": 6.51,
      "grad_norm": 1439.6417236328125,
      "learning_rate": 6.955108359133127e-05,
      "loss": 13.008,
      "step": 6501
    },
    {
      "epoch": 6.51,
      "grad_norm": 4258.66650390625,
      "learning_rate": 6.954592363261094e-05,
      "loss": 12.767,
      "step": 6502
    },
    {
      "epoch": 6.51,
      "grad_norm": 1706.5406494140625,
      "learning_rate": 6.95407636738906e-05,
      "loss": 11.7152,
      "step": 6503
    },
    {
      "epoch": 6.51,
      "grad_norm": 7641.79052734375,
      "learning_rate": 6.953560371517028e-05,
      "loss": 11.0347,
      "step": 6504
    },
    {
      "epoch": 6.51,
      "grad_norm": 3212.12451171875,
      "learning_rate": 6.953044375644994e-05,
      "loss": 14.5353,
      "step": 6505
    },
    {
      "epoch": 6.51,
      "grad_norm": 26806.900390625,
      "learning_rate": 6.952528379772963e-05,
      "loss": 18.5169,
      "step": 6506
    },
    {
      "epoch": 6.51,
      "grad_norm": 5469.53271484375,
      "learning_rate": 6.952012383900929e-05,
      "loss": 11.9977,
      "step": 6507
    },
    {
      "epoch": 6.51,
      "grad_norm": 10195.5185546875,
      "learning_rate": 6.951496388028896e-05,
      "loss": 12.5987,
      "step": 6508
    },
    {
      "epoch": 6.52,
      "grad_norm": 7713.3662109375,
      "learning_rate": 6.950980392156863e-05,
      "loss": 18.8047,
      "step": 6509
    },
    {
      "epoch": 6.52,
      "grad_norm": 1742.2381591796875,
      "learning_rate": 6.95046439628483e-05,
      "loss": 12.8249,
      "step": 6510
    },
    {
      "epoch": 6.52,
      "grad_norm": 1150.5281982421875,
      "learning_rate": 6.949948400412797e-05,
      "loss": 13.6113,
      "step": 6511
    },
    {
      "epoch": 6.52,
      "grad_norm": 28287.5546875,
      "learning_rate": 6.949432404540765e-05,
      "loss": 12.3593,
      "step": 6512
    },
    {
      "epoch": 6.52,
      "grad_norm": 2687.339111328125,
      "learning_rate": 6.948916408668731e-05,
      "loss": 13.5594,
      "step": 6513
    },
    {
      "epoch": 6.52,
      "grad_norm": 17582.1953125,
      "learning_rate": 6.948400412796698e-05,
      "loss": 16.4636,
      "step": 6514
    },
    {
      "epoch": 6.52,
      "grad_norm": 9887.810546875,
      "learning_rate": 6.947884416924665e-05,
      "loss": 13.6244,
      "step": 6515
    },
    {
      "epoch": 6.52,
      "grad_norm": 29236.681640625,
      "learning_rate": 6.947368421052632e-05,
      "loss": 12.8282,
      "step": 6516
    },
    {
      "epoch": 6.52,
      "grad_norm": 2770.492919921875,
      "learning_rate": 6.9468524251806e-05,
      "loss": 12.6748,
      "step": 6517
    },
    {
      "epoch": 6.52,
      "grad_norm": 10274.115234375,
      "learning_rate": 6.946336429308566e-05,
      "loss": 11.917,
      "step": 6518
    },
    {
      "epoch": 6.53,
      "grad_norm": 2581.763916015625,
      "learning_rate": 6.945820433436533e-05,
      "loss": 13.2069,
      "step": 6519
    },
    {
      "epoch": 6.53,
      "grad_norm": 8788.30078125,
      "learning_rate": 6.945304437564499e-05,
      "loss": 14.144,
      "step": 6520
    },
    {
      "epoch": 6.53,
      "grad_norm": 1028.6993408203125,
      "learning_rate": 6.944788441692467e-05,
      "loss": 11.0844,
      "step": 6521
    },
    {
      "epoch": 6.53,
      "grad_norm": 19105.23046875,
      "learning_rate": 6.944272445820433e-05,
      "loss": 15.4111,
      "step": 6522
    },
    {
      "epoch": 6.53,
      "grad_norm": 2778.009033203125,
      "learning_rate": 6.943756449948401e-05,
      "loss": 11.1445,
      "step": 6523
    },
    {
      "epoch": 6.53,
      "grad_norm": 8597.1103515625,
      "learning_rate": 6.943240454076368e-05,
      "loss": 24.3184,
      "step": 6524
    },
    {
      "epoch": 6.53,
      "grad_norm": 2178.47607421875,
      "learning_rate": 6.942724458204335e-05,
      "loss": 11.3027,
      "step": 6525
    },
    {
      "epoch": 6.53,
      "grad_norm": 2233.872314453125,
      "learning_rate": 6.942208462332301e-05,
      "loss": 14.502,
      "step": 6526
    },
    {
      "epoch": 6.53,
      "grad_norm": 10474.8974609375,
      "learning_rate": 6.941692466460269e-05,
      "loss": 16.263,
      "step": 6527
    },
    {
      "epoch": 6.53,
      "grad_norm": 3081.955810546875,
      "learning_rate": 6.941176470588236e-05,
      "loss": 13.7769,
      "step": 6528
    },
    {
      "epoch": 6.54,
      "grad_norm": 1111.905517578125,
      "learning_rate": 6.940660474716203e-05,
      "loss": 12.3835,
      "step": 6529
    },
    {
      "epoch": 6.54,
      "grad_norm": 5887.26025390625,
      "learning_rate": 6.94014447884417e-05,
      "loss": 14.9146,
      "step": 6530
    },
    {
      "epoch": 6.54,
      "grad_norm": 5381.46728515625,
      "learning_rate": 6.939628482972137e-05,
      "loss": 16.2681,
      "step": 6531
    },
    {
      "epoch": 6.54,
      "grad_norm": 3305.947265625,
      "learning_rate": 6.939112487100103e-05,
      "loss": 13.0556,
      "step": 6532
    },
    {
      "epoch": 6.54,
      "grad_norm": 2926.296142578125,
      "learning_rate": 6.93859649122807e-05,
      "loss": 13.1891,
      "step": 6533
    },
    {
      "epoch": 6.54,
      "grad_norm": 6451.34326171875,
      "learning_rate": 6.938080495356038e-05,
      "loss": 9.8599,
      "step": 6534
    },
    {
      "epoch": 6.54,
      "grad_norm": 1131.6650390625,
      "learning_rate": 6.937564499484005e-05,
      "loss": 13.9232,
      "step": 6535
    },
    {
      "epoch": 6.54,
      "grad_norm": 2072.82763671875,
      "learning_rate": 6.937048503611972e-05,
      "loss": 12.3897,
      "step": 6536
    },
    {
      "epoch": 6.54,
      "grad_norm": 4772.01953125,
      "learning_rate": 6.936532507739938e-05,
      "loss": 15.4181,
      "step": 6537
    },
    {
      "epoch": 6.54,
      "grad_norm": 3349.070068359375,
      "learning_rate": 6.936016511867905e-05,
      "loss": 15.1775,
      "step": 6538
    },
    {
      "epoch": 6.55,
      "grad_norm": 1925.0926513671875,
      "learning_rate": 6.935500515995873e-05,
      "loss": 11.7467,
      "step": 6539
    },
    {
      "epoch": 6.55,
      "grad_norm": 21581.125,
      "learning_rate": 6.93498452012384e-05,
      "loss": 12.5524,
      "step": 6540
    },
    {
      "epoch": 6.55,
      "grad_norm": 4934.64306640625,
      "learning_rate": 6.934468524251806e-05,
      "loss": 12.7546,
      "step": 6541
    },
    {
      "epoch": 6.55,
      "grad_norm": 7096.64892578125,
      "learning_rate": 6.933952528379774e-05,
      "loss": 13.3337,
      "step": 6542
    },
    {
      "epoch": 6.55,
      "grad_norm": 2714.765380859375,
      "learning_rate": 6.93343653250774e-05,
      "loss": 13.1531,
      "step": 6543
    },
    {
      "epoch": 6.55,
      "grad_norm": 1450.8094482421875,
      "learning_rate": 6.932920536635707e-05,
      "loss": 13.7794,
      "step": 6544
    },
    {
      "epoch": 6.55,
      "grad_norm": 7308.86865234375,
      "learning_rate": 6.932404540763675e-05,
      "loss": 16.4022,
      "step": 6545
    },
    {
      "epoch": 6.55,
      "grad_norm": 675.7506713867188,
      "learning_rate": 6.931888544891642e-05,
      "loss": 12.3139,
      "step": 6546
    },
    {
      "epoch": 6.55,
      "grad_norm": 1043.965576171875,
      "learning_rate": 6.931372549019608e-05,
      "loss": 12.5756,
      "step": 6547
    },
    {
      "epoch": 6.55,
      "grad_norm": 13745.7802734375,
      "learning_rate": 6.930856553147576e-05,
      "loss": 18.5627,
      "step": 6548
    },
    {
      "epoch": 6.56,
      "grad_norm": 3151.5341796875,
      "learning_rate": 6.930340557275542e-05,
      "loss": 15.3034,
      "step": 6549
    },
    {
      "epoch": 6.56,
      "grad_norm": 5046.4775390625,
      "learning_rate": 6.929824561403509e-05,
      "loss": 13.7733,
      "step": 6550
    },
    {
      "epoch": 6.56,
      "grad_norm": 9102.80859375,
      "learning_rate": 6.929308565531477e-05,
      "loss": 38.2829,
      "step": 6551
    },
    {
      "epoch": 6.56,
      "grad_norm": 9449.5380859375,
      "learning_rate": 6.928792569659444e-05,
      "loss": 14.0571,
      "step": 6552
    },
    {
      "epoch": 6.56,
      "grad_norm": 4221.84326171875,
      "learning_rate": 6.92827657378741e-05,
      "loss": 22.1943,
      "step": 6553
    },
    {
      "epoch": 6.56,
      "grad_norm": 17046.43359375,
      "learning_rate": 6.927760577915376e-05,
      "loss": 14.9009,
      "step": 6554
    },
    {
      "epoch": 6.56,
      "grad_norm": 4336.14208984375,
      "learning_rate": 6.927244582043344e-05,
      "loss": 11.6205,
      "step": 6555
    },
    {
      "epoch": 6.56,
      "grad_norm": 3859.261962890625,
      "learning_rate": 6.926728586171311e-05,
      "loss": 12.9984,
      "step": 6556
    },
    {
      "epoch": 6.56,
      "grad_norm": 30184.95703125,
      "learning_rate": 6.926212590299279e-05,
      "loss": 15.8905,
      "step": 6557
    },
    {
      "epoch": 6.56,
      "grad_norm": 18674.150390625,
      "learning_rate": 6.925696594427245e-05,
      "loss": 15.2724,
      "step": 6558
    },
    {
      "epoch": 6.57,
      "grad_norm": 4287.78857421875,
      "learning_rate": 6.925180598555212e-05,
      "loss": 14.4802,
      "step": 6559
    },
    {
      "epoch": 6.57,
      "grad_norm": 1188.1673583984375,
      "learning_rate": 6.924664602683178e-05,
      "loss": 11.0186,
      "step": 6560
    },
    {
      "epoch": 6.57,
      "grad_norm": 223.92164611816406,
      "learning_rate": 6.924148606811146e-05,
      "loss": 10.8442,
      "step": 6561
    },
    {
      "epoch": 6.57,
      "grad_norm": 7401.8095703125,
      "learning_rate": 6.923632610939113e-05,
      "loss": 12.5051,
      "step": 6562
    },
    {
      "epoch": 6.57,
      "grad_norm": 7189.8154296875,
      "learning_rate": 6.92311661506708e-05,
      "loss": 12.9672,
      "step": 6563
    },
    {
      "epoch": 6.57,
      "grad_norm": 5088.94873046875,
      "learning_rate": 6.922600619195047e-05,
      "loss": 14.7869,
      "step": 6564
    },
    {
      "epoch": 6.57,
      "grad_norm": 22570.353515625,
      "learning_rate": 6.922084623323014e-05,
      "loss": 13.5146,
      "step": 6565
    },
    {
      "epoch": 6.57,
      "grad_norm": 11256.806640625,
      "learning_rate": 6.92156862745098e-05,
      "loss": 16.7494,
      "step": 6566
    },
    {
      "epoch": 6.57,
      "grad_norm": 22047.15234375,
      "learning_rate": 6.921052631578948e-05,
      "loss": 17.4878,
      "step": 6567
    },
    {
      "epoch": 6.57,
      "grad_norm": 3727.908203125,
      "learning_rate": 6.920536635706915e-05,
      "loss": 19.108,
      "step": 6568
    },
    {
      "epoch": 6.58,
      "grad_norm": 894.562744140625,
      "learning_rate": 6.920020639834883e-05,
      "loss": 13.121,
      "step": 6569
    },
    {
      "epoch": 6.58,
      "grad_norm": 18313.58984375,
      "learning_rate": 6.919504643962849e-05,
      "loss": 14.5412,
      "step": 6570
    },
    {
      "epoch": 6.58,
      "grad_norm": 3757.591064453125,
      "learning_rate": 6.918988648090816e-05,
      "loss": 16.3851,
      "step": 6571
    },
    {
      "epoch": 6.58,
      "grad_norm": 7325.25244140625,
      "learning_rate": 6.918472652218782e-05,
      "loss": 17.5496,
      "step": 6572
    },
    {
      "epoch": 6.58,
      "grad_norm": 8916.2783203125,
      "learning_rate": 6.91795665634675e-05,
      "loss": 12.9547,
      "step": 6573
    },
    {
      "epoch": 6.58,
      "grad_norm": 3108.62646484375,
      "learning_rate": 6.917440660474717e-05,
      "loss": 11.6462,
      "step": 6574
    },
    {
      "epoch": 6.58,
      "grad_norm": 3953.889892578125,
      "learning_rate": 6.916924664602683e-05,
      "loss": 12.4611,
      "step": 6575
    },
    {
      "epoch": 6.58,
      "grad_norm": 14378.9267578125,
      "learning_rate": 6.916408668730651e-05,
      "loss": 14.6687,
      "step": 6576
    },
    {
      "epoch": 6.58,
      "grad_norm": 24136.01953125,
      "learning_rate": 6.915892672858617e-05,
      "loss": 21.6419,
      "step": 6577
    },
    {
      "epoch": 6.58,
      "grad_norm": 9256.2578125,
      "learning_rate": 6.915376676986584e-05,
      "loss": 14.9433,
      "step": 6578
    },
    {
      "epoch": 6.59,
      "grad_norm": 1911.5506591796875,
      "learning_rate": 6.914860681114552e-05,
      "loss": 15.5185,
      "step": 6579
    },
    {
      "epoch": 6.59,
      "grad_norm": 7940.92431640625,
      "learning_rate": 6.914344685242519e-05,
      "loss": 15.4354,
      "step": 6580
    },
    {
      "epoch": 6.59,
      "grad_norm": 5085.82421875,
      "learning_rate": 6.913828689370485e-05,
      "loss": 11.3091,
      "step": 6581
    },
    {
      "epoch": 6.59,
      "grad_norm": 9481.3251953125,
      "learning_rate": 6.913312693498453e-05,
      "loss": 13.7895,
      "step": 6582
    },
    {
      "epoch": 6.59,
      "grad_norm": 6440.7001953125,
      "learning_rate": 6.912796697626419e-05,
      "loss": 13.2887,
      "step": 6583
    },
    {
      "epoch": 6.59,
      "grad_norm": 10914.05859375,
      "learning_rate": 6.912280701754386e-05,
      "loss": 12.6243,
      "step": 6584
    },
    {
      "epoch": 6.59,
      "grad_norm": 3596.5546875,
      "learning_rate": 6.911764705882354e-05,
      "loss": 13.2006,
      "step": 6585
    },
    {
      "epoch": 6.59,
      "grad_norm": 16264.298828125,
      "learning_rate": 6.911248710010321e-05,
      "loss": 13.2496,
      "step": 6586
    },
    {
      "epoch": 6.59,
      "grad_norm": 5057.61083984375,
      "learning_rate": 6.910732714138287e-05,
      "loss": 11.8146,
      "step": 6587
    },
    {
      "epoch": 6.59,
      "grad_norm": 147905.734375,
      "learning_rate": 6.910216718266255e-05,
      "loss": 19.8994,
      "step": 6588
    },
    {
      "epoch": 6.6,
      "grad_norm": 9763.337890625,
      "learning_rate": 6.909700722394221e-05,
      "loss": 21.5725,
      "step": 6589
    },
    {
      "epoch": 6.6,
      "grad_norm": 561.4481201171875,
      "learning_rate": 6.909184726522188e-05,
      "loss": 11.9409,
      "step": 6590
    },
    {
      "epoch": 6.6,
      "grad_norm": 2025.6488037109375,
      "learning_rate": 6.908668730650156e-05,
      "loss": 11.3811,
      "step": 6591
    },
    {
      "epoch": 6.6,
      "grad_norm": 2253.73974609375,
      "learning_rate": 6.908152734778122e-05,
      "loss": 11.8631,
      "step": 6592
    },
    {
      "epoch": 6.6,
      "grad_norm": 7292.2861328125,
      "learning_rate": 6.907636738906089e-05,
      "loss": 16.4284,
      "step": 6593
    },
    {
      "epoch": 6.6,
      "grad_norm": 1119.294921875,
      "learning_rate": 6.907120743034055e-05,
      "loss": 10.5386,
      "step": 6594
    },
    {
      "epoch": 6.6,
      "grad_norm": 3161.5732421875,
      "learning_rate": 6.906604747162023e-05,
      "loss": 11.1825,
      "step": 6595
    },
    {
      "epoch": 6.6,
      "grad_norm": 1027.83837890625,
      "learning_rate": 6.90608875128999e-05,
      "loss": 12.2502,
      "step": 6596
    },
    {
      "epoch": 6.6,
      "grad_norm": 24125.791015625,
      "learning_rate": 6.905572755417958e-05,
      "loss": 12.9203,
      "step": 6597
    },
    {
      "epoch": 6.6,
      "grad_norm": 29893.498046875,
      "learning_rate": 6.905056759545924e-05,
      "loss": 16.1046,
      "step": 6598
    },
    {
      "epoch": 6.61,
      "grad_norm": 6332.29638671875,
      "learning_rate": 6.904540763673891e-05,
      "loss": 11.5983,
      "step": 6599
    },
    {
      "epoch": 6.61,
      "grad_norm": 1216.8486328125,
      "learning_rate": 6.904024767801857e-05,
      "loss": 11.1679,
      "step": 6600
    },
    {
      "epoch": 6.61,
      "grad_norm": 4256.0361328125,
      "learning_rate": 6.903508771929825e-05,
      "loss": 12.5566,
      "step": 6601
    },
    {
      "epoch": 6.61,
      "grad_norm": 700.3070678710938,
      "learning_rate": 6.902992776057792e-05,
      "loss": 15.1761,
      "step": 6602
    },
    {
      "epoch": 6.61,
      "grad_norm": 1455.4039306640625,
      "learning_rate": 6.90247678018576e-05,
      "loss": 11.8246,
      "step": 6603
    },
    {
      "epoch": 6.61,
      "grad_norm": 18008.234375,
      "learning_rate": 6.901960784313726e-05,
      "loss": 13.9344,
      "step": 6604
    },
    {
      "epoch": 6.61,
      "grad_norm": 3535.50244140625,
      "learning_rate": 6.901444788441693e-05,
      "loss": 14.4561,
      "step": 6605
    },
    {
      "epoch": 6.61,
      "grad_norm": 8015.7021484375,
      "learning_rate": 6.90092879256966e-05,
      "loss": 17.1959,
      "step": 6606
    },
    {
      "epoch": 6.61,
      "grad_norm": 32868.9765625,
      "learning_rate": 6.900412796697627e-05,
      "loss": 13.2748,
      "step": 6607
    },
    {
      "epoch": 6.61,
      "grad_norm": 4507.0009765625,
      "learning_rate": 6.899896800825594e-05,
      "loss": 18.7193,
      "step": 6608
    },
    {
      "epoch": 6.62,
      "grad_norm": 2869.0947265625,
      "learning_rate": 6.89938080495356e-05,
      "loss": 12.015,
      "step": 6609
    },
    {
      "epoch": 6.62,
      "grad_norm": 1371.5797119140625,
      "learning_rate": 6.898864809081528e-05,
      "loss": 13.4394,
      "step": 6610
    },
    {
      "epoch": 6.62,
      "grad_norm": 7612.23828125,
      "learning_rate": 6.898348813209494e-05,
      "loss": 12.4107,
      "step": 6611
    },
    {
      "epoch": 6.62,
      "grad_norm": 3507.409912109375,
      "learning_rate": 6.897832817337461e-05,
      "loss": 14.9398,
      "step": 6612
    },
    {
      "epoch": 6.62,
      "grad_norm": 8112.6640625,
      "learning_rate": 6.897316821465429e-05,
      "loss": 11.328,
      "step": 6613
    },
    {
      "epoch": 6.62,
      "grad_norm": 44742.91796875,
      "learning_rate": 6.896800825593396e-05,
      "loss": 15.5243,
      "step": 6614
    },
    {
      "epoch": 6.62,
      "grad_norm": 3196.876220703125,
      "learning_rate": 6.896284829721362e-05,
      "loss": 16.1712,
      "step": 6615
    },
    {
      "epoch": 6.62,
      "grad_norm": 6811.76025390625,
      "learning_rate": 6.89576883384933e-05,
      "loss": 16.7941,
      "step": 6616
    },
    {
      "epoch": 6.62,
      "grad_norm": 5116.13720703125,
      "learning_rate": 6.895252837977296e-05,
      "loss": 16.0268,
      "step": 6617
    },
    {
      "epoch": 6.62,
      "grad_norm": 4392.6298828125,
      "learning_rate": 6.894736842105263e-05,
      "loss": 13.9173,
      "step": 6618
    },
    {
      "epoch": 6.63,
      "grad_norm": 2267.775634765625,
      "learning_rate": 6.894220846233231e-05,
      "loss": 17.9357,
      "step": 6619
    },
    {
      "epoch": 6.63,
      "grad_norm": 19409.201171875,
      "learning_rate": 6.893704850361198e-05,
      "loss": 10.9955,
      "step": 6620
    },
    {
      "epoch": 6.63,
      "grad_norm": 5400.908203125,
      "learning_rate": 6.893188854489164e-05,
      "loss": 18.0746,
      "step": 6621
    },
    {
      "epoch": 6.63,
      "grad_norm": 32801.2265625,
      "learning_rate": 6.892672858617132e-05,
      "loss": 14.2154,
      "step": 6622
    },
    {
      "epoch": 6.63,
      "grad_norm": 575.2958374023438,
      "learning_rate": 6.892156862745098e-05,
      "loss": 12.1663,
      "step": 6623
    },
    {
      "epoch": 6.63,
      "grad_norm": 3517.8095703125,
      "learning_rate": 6.891640866873065e-05,
      "loss": 15.4108,
      "step": 6624
    },
    {
      "epoch": 6.63,
      "grad_norm": 4933.0029296875,
      "learning_rate": 6.891124871001033e-05,
      "loss": 15.5364,
      "step": 6625
    },
    {
      "epoch": 6.63,
      "grad_norm": 14563.224609375,
      "learning_rate": 6.890608875128999e-05,
      "loss": 12.3952,
      "step": 6626
    },
    {
      "epoch": 6.63,
      "grad_norm": 1472.93115234375,
      "learning_rate": 6.890092879256966e-05,
      "loss": 12.6476,
      "step": 6627
    },
    {
      "epoch": 6.63,
      "grad_norm": 7945.1259765625,
      "learning_rate": 6.889576883384932e-05,
      "loss": 13.0977,
      "step": 6628
    },
    {
      "epoch": 6.64,
      "grad_norm": 1755.9932861328125,
      "learning_rate": 6.8890608875129e-05,
      "loss": 13.7788,
      "step": 6629
    },
    {
      "epoch": 6.64,
      "grad_norm": 8158.61962890625,
      "learning_rate": 6.888544891640867e-05,
      "loss": 13.0811,
      "step": 6630
    },
    {
      "epoch": 6.64,
      "grad_norm": 13006.599609375,
      "learning_rate": 6.888028895768835e-05,
      "loss": 13.1994,
      "step": 6631
    },
    {
      "epoch": 6.64,
      "grad_norm": 10842.2119140625,
      "learning_rate": 6.887512899896801e-05,
      "loss": 10.8089,
      "step": 6632
    },
    {
      "epoch": 6.64,
      "grad_norm": 3346.48779296875,
      "learning_rate": 6.886996904024768e-05,
      "loss": 12.0242,
      "step": 6633
    },
    {
      "epoch": 6.64,
      "grad_norm": 17100.744140625,
      "learning_rate": 6.886480908152734e-05,
      "loss": 11.4682,
      "step": 6634
    },
    {
      "epoch": 6.64,
      "grad_norm": 3201.37158203125,
      "learning_rate": 6.885964912280702e-05,
      "loss": 15.2423,
      "step": 6635
    },
    {
      "epoch": 6.64,
      "grad_norm": 24298.884765625,
      "learning_rate": 6.88544891640867e-05,
      "loss": 15.6155,
      "step": 6636
    },
    {
      "epoch": 6.64,
      "grad_norm": 7080.140625,
      "learning_rate": 6.884932920536637e-05,
      "loss": 12.795,
      "step": 6637
    },
    {
      "epoch": 6.64,
      "grad_norm": 13360.29296875,
      "learning_rate": 6.884416924664603e-05,
      "loss": 11.2885,
      "step": 6638
    },
    {
      "epoch": 6.65,
      "grad_norm": 1127.20361328125,
      "learning_rate": 6.88390092879257e-05,
      "loss": 15.8469,
      "step": 6639
    },
    {
      "epoch": 6.65,
      "grad_norm": 607.0708618164062,
      "learning_rate": 6.883384932920536e-05,
      "loss": 12.2485,
      "step": 6640
    },
    {
      "epoch": 6.65,
      "grad_norm": 982.7974853515625,
      "learning_rate": 6.882868937048504e-05,
      "loss": 13.9353,
      "step": 6641
    },
    {
      "epoch": 6.65,
      "grad_norm": 3589.088623046875,
      "learning_rate": 6.882352941176471e-05,
      "loss": 15.4511,
      "step": 6642
    },
    {
      "epoch": 6.65,
      "grad_norm": 3508.729736328125,
      "learning_rate": 6.881836945304437e-05,
      "loss": 12.5289,
      "step": 6643
    },
    {
      "epoch": 6.65,
      "grad_norm": 563.1956787109375,
      "learning_rate": 6.881320949432405e-05,
      "loss": 13.2962,
      "step": 6644
    },
    {
      "epoch": 6.65,
      "grad_norm": 6772.82958984375,
      "learning_rate": 6.880804953560371e-05,
      "loss": 18.2663,
      "step": 6645
    },
    {
      "epoch": 6.65,
      "grad_norm": 1323.6375732421875,
      "learning_rate": 6.880288957688338e-05,
      "loss": 11.5767,
      "step": 6646
    },
    {
      "epoch": 6.65,
      "grad_norm": 3007.047607421875,
      "learning_rate": 6.879772961816306e-05,
      "loss": 10.3143,
      "step": 6647
    },
    {
      "epoch": 6.65,
      "grad_norm": 812.4993286132812,
      "learning_rate": 6.879256965944273e-05,
      "loss": 11.7977,
      "step": 6648
    },
    {
      "epoch": 6.66,
      "grad_norm": 5323.45703125,
      "learning_rate": 6.87874097007224e-05,
      "loss": 11.6337,
      "step": 6649
    },
    {
      "epoch": 6.66,
      "grad_norm": 1459.60693359375,
      "learning_rate": 6.878224974200207e-05,
      "loss": 11.771,
      "step": 6650
    },
    {
      "epoch": 6.66,
      "grad_norm": 27160.369140625,
      "learning_rate": 6.877708978328173e-05,
      "loss": 11.4054,
      "step": 6651
    },
    {
      "epoch": 6.66,
      "grad_norm": 12198.048828125,
      "learning_rate": 6.87719298245614e-05,
      "loss": 14.5873,
      "step": 6652
    },
    {
      "epoch": 6.66,
      "grad_norm": 12267.833984375,
      "learning_rate": 6.876676986584108e-05,
      "loss": 13.4288,
      "step": 6653
    },
    {
      "epoch": 6.66,
      "grad_norm": 1222.2296142578125,
      "learning_rate": 6.876160990712075e-05,
      "loss": 12.0919,
      "step": 6654
    },
    {
      "epoch": 6.66,
      "grad_norm": 5941.49853515625,
      "learning_rate": 6.875644994840041e-05,
      "loss": 12.7352,
      "step": 6655
    },
    {
      "epoch": 6.66,
      "grad_norm": 2224.815673828125,
      "learning_rate": 6.875128998968009e-05,
      "loss": 10.7352,
      "step": 6656
    },
    {
      "epoch": 6.66,
      "grad_norm": 5046.123046875,
      "learning_rate": 6.874613003095975e-05,
      "loss": 15.5266,
      "step": 6657
    },
    {
      "epoch": 6.66,
      "grad_norm": 40676.32421875,
      "learning_rate": 6.874097007223942e-05,
      "loss": 15.5155,
      "step": 6658
    },
    {
      "epoch": 6.67,
      "grad_norm": 21367.802734375,
      "learning_rate": 6.87358101135191e-05,
      "loss": 14.9496,
      "step": 6659
    },
    {
      "epoch": 6.67,
      "grad_norm": 3805.28759765625,
      "learning_rate": 6.873065015479877e-05,
      "loss": 12.7071,
      "step": 6660
    },
    {
      "epoch": 6.67,
      "grad_norm": 3260.857177734375,
      "learning_rate": 6.872549019607843e-05,
      "loss": 12.9177,
      "step": 6661
    },
    {
      "epoch": 6.67,
      "grad_norm": 1932.595703125,
      "learning_rate": 6.87203302373581e-05,
      "loss": 11.6223,
      "step": 6662
    },
    {
      "epoch": 6.67,
      "grad_norm": 5140.3974609375,
      "learning_rate": 6.871517027863777e-05,
      "loss": 19.4592,
      "step": 6663
    },
    {
      "epoch": 6.67,
      "grad_norm": 6640.72607421875,
      "learning_rate": 6.871001031991744e-05,
      "loss": 16.5185,
      "step": 6664
    },
    {
      "epoch": 6.67,
      "grad_norm": 12626.78125,
      "learning_rate": 6.870485036119712e-05,
      "loss": 14.7904,
      "step": 6665
    },
    {
      "epoch": 6.67,
      "grad_norm": 4472.75390625,
      "learning_rate": 6.869969040247678e-05,
      "loss": 12.5027,
      "step": 6666
    },
    {
      "epoch": 6.67,
      "grad_norm": 690.3479614257812,
      "learning_rate": 6.869453044375645e-05,
      "loss": 13.29,
      "step": 6667
    },
    {
      "epoch": 6.67,
      "grad_norm": 3417.544189453125,
      "learning_rate": 6.868937048503612e-05,
      "loss": 16.4317,
      "step": 6668
    },
    {
      "epoch": 6.68,
      "grad_norm": 5620.29443359375,
      "learning_rate": 6.868421052631579e-05,
      "loss": 10.3889,
      "step": 6669
    },
    {
      "epoch": 6.68,
      "grad_norm": 7633.86083984375,
      "learning_rate": 6.867905056759546e-05,
      "loss": 10.535,
      "step": 6670
    },
    {
      "epoch": 6.68,
      "grad_norm": 2744.11376953125,
      "learning_rate": 6.867389060887514e-05,
      "loss": 13.5004,
      "step": 6671
    },
    {
      "epoch": 6.68,
      "grad_norm": 6248.2900390625,
      "learning_rate": 6.86687306501548e-05,
      "loss": 17.5288,
      "step": 6672
    },
    {
      "epoch": 6.68,
      "grad_norm": 10519.1806640625,
      "learning_rate": 6.866357069143447e-05,
      "loss": 17.7671,
      "step": 6673
    },
    {
      "epoch": 6.68,
      "grad_norm": 2887.661376953125,
      "learning_rate": 6.865841073271414e-05,
      "loss": 13.809,
      "step": 6674
    },
    {
      "epoch": 6.68,
      "grad_norm": 10919.5888671875,
      "learning_rate": 6.865325077399381e-05,
      "loss": 13.702,
      "step": 6675
    },
    {
      "epoch": 6.68,
      "grad_norm": 4110.14794921875,
      "learning_rate": 6.864809081527348e-05,
      "loss": 11.4136,
      "step": 6676
    },
    {
      "epoch": 6.68,
      "grad_norm": 45116.44140625,
      "learning_rate": 6.864293085655316e-05,
      "loss": 12.5511,
      "step": 6677
    },
    {
      "epoch": 6.68,
      "grad_norm": 18256.1953125,
      "learning_rate": 6.863777089783282e-05,
      "loss": 14.3168,
      "step": 6678
    },
    {
      "epoch": 6.69,
      "grad_norm": 10380.3994140625,
      "learning_rate": 6.863261093911248e-05,
      "loss": 15.2778,
      "step": 6679
    },
    {
      "epoch": 6.69,
      "grad_norm": 3034.39208984375,
      "learning_rate": 6.862745098039216e-05,
      "loss": 11.3717,
      "step": 6680
    },
    {
      "epoch": 6.69,
      "grad_norm": 2969.660888671875,
      "learning_rate": 6.862229102167183e-05,
      "loss": 15.4265,
      "step": 6681
    },
    {
      "epoch": 6.69,
      "grad_norm": 215224.8125,
      "learning_rate": 6.86171310629515e-05,
      "loss": 22.9776,
      "step": 6682
    },
    {
      "epoch": 6.69,
      "grad_norm": 31270.5,
      "learning_rate": 6.861197110423117e-05,
      "loss": 16.6115,
      "step": 6683
    },
    {
      "epoch": 6.69,
      "grad_norm": 9159.7333984375,
      "learning_rate": 6.860681114551084e-05,
      "loss": 22.1575,
      "step": 6684
    },
    {
      "epoch": 6.69,
      "grad_norm": 4549.5546875,
      "learning_rate": 6.86016511867905e-05,
      "loss": 19.1817,
      "step": 6685
    },
    {
      "epoch": 6.69,
      "grad_norm": 494.4908447265625,
      "learning_rate": 6.859649122807018e-05,
      "loss": 11.7103,
      "step": 6686
    },
    {
      "epoch": 6.69,
      "grad_norm": 21759.240234375,
      "learning_rate": 6.859133126934985e-05,
      "loss": 13.207,
      "step": 6687
    },
    {
      "epoch": 6.69,
      "grad_norm": 12208.4970703125,
      "learning_rate": 6.858617131062952e-05,
      "loss": 17.4087,
      "step": 6688
    },
    {
      "epoch": 6.7,
      "grad_norm": 955.9166870117188,
      "learning_rate": 6.858101135190919e-05,
      "loss": 10.7413,
      "step": 6689
    },
    {
      "epoch": 6.7,
      "grad_norm": 15247.6328125,
      "learning_rate": 6.857585139318886e-05,
      "loss": 13.2321,
      "step": 6690
    },
    {
      "epoch": 6.7,
      "grad_norm": 10874.8779296875,
      "learning_rate": 6.857069143446852e-05,
      "loss": 15.9677,
      "step": 6691
    },
    {
      "epoch": 6.7,
      "grad_norm": 29835.7734375,
      "learning_rate": 6.85655314757482e-05,
      "loss": 25.5667,
      "step": 6692
    },
    {
      "epoch": 6.7,
      "grad_norm": 5822.0712890625,
      "learning_rate": 6.856037151702787e-05,
      "loss": 19.6125,
      "step": 6693
    },
    {
      "epoch": 6.7,
      "grad_norm": 2141.552734375,
      "learning_rate": 6.855521155830754e-05,
      "loss": 12.025,
      "step": 6694
    },
    {
      "epoch": 6.7,
      "grad_norm": 2477.865478515625,
      "learning_rate": 6.85500515995872e-05,
      "loss": 12.4701,
      "step": 6695
    },
    {
      "epoch": 6.7,
      "grad_norm": 7374.29248046875,
      "learning_rate": 6.854489164086688e-05,
      "loss": 19.0477,
      "step": 6696
    },
    {
      "epoch": 6.7,
      "grad_norm": 8174.4404296875,
      "learning_rate": 6.853973168214654e-05,
      "loss": 16.8187,
      "step": 6697
    },
    {
      "epoch": 6.7,
      "grad_norm": 4271.22607421875,
      "learning_rate": 6.853457172342622e-05,
      "loss": 13.8965,
      "step": 6698
    },
    {
      "epoch": 6.71,
      "grad_norm": 6800.8193359375,
      "learning_rate": 6.852941176470589e-05,
      "loss": 17.5532,
      "step": 6699
    },
    {
      "epoch": 6.71,
      "grad_norm": 4042.7236328125,
      "learning_rate": 6.852425180598555e-05,
      "loss": 19.2688,
      "step": 6700
    },
    {
      "epoch": 6.71,
      "grad_norm": 4769.53955078125,
      "learning_rate": 6.851909184726523e-05,
      "loss": 14.5001,
      "step": 6701
    },
    {
      "epoch": 6.71,
      "grad_norm": 12068.3251953125,
      "learning_rate": 6.851393188854489e-05,
      "loss": 12.9101,
      "step": 6702
    },
    {
      "epoch": 6.71,
      "grad_norm": 1408.1683349609375,
      "learning_rate": 6.850877192982456e-05,
      "loss": 10.9151,
      "step": 6703
    },
    {
      "epoch": 6.71,
      "grad_norm": 4709.375,
      "learning_rate": 6.850361197110424e-05,
      "loss": 11.6512,
      "step": 6704
    },
    {
      "epoch": 6.71,
      "grad_norm": 13215.9697265625,
      "learning_rate": 6.849845201238391e-05,
      "loss": 11.3777,
      "step": 6705
    },
    {
      "epoch": 6.71,
      "grad_norm": 25561.0,
      "learning_rate": 6.849329205366357e-05,
      "loss": 17.9542,
      "step": 6706
    },
    {
      "epoch": 6.71,
      "grad_norm": 4599.3857421875,
      "learning_rate": 6.848813209494325e-05,
      "loss": 12.032,
      "step": 6707
    },
    {
      "epoch": 6.71,
      "grad_norm": 3588.81396484375,
      "learning_rate": 6.84829721362229e-05,
      "loss": 15.2178,
      "step": 6708
    },
    {
      "epoch": 6.72,
      "grad_norm": 3812.90966796875,
      "learning_rate": 6.847781217750258e-05,
      "loss": 17.1216,
      "step": 6709
    },
    {
      "epoch": 6.72,
      "grad_norm": 4382.36669921875,
      "learning_rate": 6.847265221878226e-05,
      "loss": 14.4892,
      "step": 6710
    },
    {
      "epoch": 6.72,
      "grad_norm": 1625.2779541015625,
      "learning_rate": 6.846749226006193e-05,
      "loss": 14.0895,
      "step": 6711
    },
    {
      "epoch": 6.72,
      "grad_norm": 6865.365234375,
      "learning_rate": 6.846233230134159e-05,
      "loss": 20.4241,
      "step": 6712
    },
    {
      "epoch": 6.72,
      "grad_norm": 9446.703125,
      "learning_rate": 6.845717234262127e-05,
      "loss": 22.7635,
      "step": 6713
    },
    {
      "epoch": 6.72,
      "grad_norm": 4297.60693359375,
      "learning_rate": 6.845201238390093e-05,
      "loss": 15.3598,
      "step": 6714
    },
    {
      "epoch": 6.72,
      "grad_norm": 21213.412109375,
      "learning_rate": 6.84468524251806e-05,
      "loss": 13.083,
      "step": 6715
    },
    {
      "epoch": 6.72,
      "grad_norm": 8531.876953125,
      "learning_rate": 6.844169246646028e-05,
      "loss": 13.095,
      "step": 6716
    },
    {
      "epoch": 6.72,
      "grad_norm": 24073.298828125,
      "learning_rate": 6.843653250773994e-05,
      "loss": 11.8211,
      "step": 6717
    },
    {
      "epoch": 6.72,
      "grad_norm": 3774.612548828125,
      "learning_rate": 6.843137254901961e-05,
      "loss": 11.773,
      "step": 6718
    },
    {
      "epoch": 6.73,
      "grad_norm": 749.796875,
      "learning_rate": 6.842621259029927e-05,
      "loss": 12.6474,
      "step": 6719
    },
    {
      "epoch": 6.73,
      "grad_norm": 4157.50048828125,
      "learning_rate": 6.842105263157895e-05,
      "loss": 15.3746,
      "step": 6720
    },
    {
      "epoch": 6.73,
      "grad_norm": 25654.583984375,
      "learning_rate": 6.841589267285862e-05,
      "loss": 14.506,
      "step": 6721
    },
    {
      "epoch": 6.73,
      "grad_norm": 2692.01123046875,
      "learning_rate": 6.84107327141383e-05,
      "loss": 11.6826,
      "step": 6722
    },
    {
      "epoch": 6.73,
      "grad_norm": 5200.75390625,
      "learning_rate": 6.840557275541796e-05,
      "loss": 17.3663,
      "step": 6723
    },
    {
      "epoch": 6.73,
      "grad_norm": 3871.86083984375,
      "learning_rate": 6.840041279669763e-05,
      "loss": 11.6418,
      "step": 6724
    },
    {
      "epoch": 6.73,
      "grad_norm": 10951.1572265625,
      "learning_rate": 6.839525283797729e-05,
      "loss": 12.3085,
      "step": 6725
    },
    {
      "epoch": 6.73,
      "grad_norm": 814.4000244140625,
      "learning_rate": 6.839009287925697e-05,
      "loss": 14.2553,
      "step": 6726
    },
    {
      "epoch": 6.73,
      "grad_norm": 308.8004150390625,
      "learning_rate": 6.838493292053664e-05,
      "loss": 13.297,
      "step": 6727
    },
    {
      "epoch": 6.73,
      "grad_norm": 5127.18310546875,
      "learning_rate": 6.837977296181632e-05,
      "loss": 18.6136,
      "step": 6728
    },
    {
      "epoch": 6.74,
      "grad_norm": 102430.7421875,
      "learning_rate": 6.837461300309598e-05,
      "loss": 25.1295,
      "step": 6729
    },
    {
      "epoch": 6.74,
      "grad_norm": 1845.700439453125,
      "learning_rate": 6.836945304437565e-05,
      "loss": 12.0093,
      "step": 6730
    },
    {
      "epoch": 6.74,
      "grad_norm": 3041.361572265625,
      "learning_rate": 6.836429308565531e-05,
      "loss": 12.5595,
      "step": 6731
    },
    {
      "epoch": 6.74,
      "grad_norm": 3378.69287109375,
      "learning_rate": 6.8359133126935e-05,
      "loss": 14.7393,
      "step": 6732
    },
    {
      "epoch": 6.74,
      "grad_norm": 8773.046875,
      "learning_rate": 6.835397316821466e-05,
      "loss": 19.7292,
      "step": 6733
    },
    {
      "epoch": 6.74,
      "grad_norm": 27430.439453125,
      "learning_rate": 6.834881320949432e-05,
      "loss": 24.3553,
      "step": 6734
    },
    {
      "epoch": 6.74,
      "grad_norm": 4297.3984375,
      "learning_rate": 6.8343653250774e-05,
      "loss": 14.4545,
      "step": 6735
    },
    {
      "epoch": 6.74,
      "grad_norm": 3210.887939453125,
      "learning_rate": 6.833849329205366e-05,
      "loss": 14.4658,
      "step": 6736
    },
    {
      "epoch": 6.74,
      "grad_norm": 1240.1014404296875,
      "learning_rate": 6.833333333333333e-05,
      "loss": 12.2956,
      "step": 6737
    },
    {
      "epoch": 6.74,
      "grad_norm": 1275.5369873046875,
      "learning_rate": 6.8328173374613e-05,
      "loss": 11.3494,
      "step": 6738
    },
    {
      "epoch": 6.75,
      "grad_norm": 69437.78125,
      "learning_rate": 6.832301341589268e-05,
      "loss": 16.2993,
      "step": 6739
    },
    {
      "epoch": 6.75,
      "grad_norm": 25826.927734375,
      "learning_rate": 6.831785345717234e-05,
      "loss": 11.8763,
      "step": 6740
    },
    {
      "epoch": 6.75,
      "grad_norm": 34450.40625,
      "learning_rate": 6.831269349845202e-05,
      "loss": 19.7332,
      "step": 6741
    },
    {
      "epoch": 6.75,
      "grad_norm": 1117.4779052734375,
      "learning_rate": 6.830753353973168e-05,
      "loss": 16.0142,
      "step": 6742
    },
    {
      "epoch": 6.75,
      "grad_norm": 2268.474853515625,
      "learning_rate": 6.830237358101135e-05,
      "loss": 15.9617,
      "step": 6743
    },
    {
      "epoch": 6.75,
      "grad_norm": 1311.605224609375,
      "learning_rate": 6.829721362229103e-05,
      "loss": 10.211,
      "step": 6744
    },
    {
      "epoch": 6.75,
      "grad_norm": 14235.9287109375,
      "learning_rate": 6.82920536635707e-05,
      "loss": 11.97,
      "step": 6745
    },
    {
      "epoch": 6.75,
      "grad_norm": 7489.86572265625,
      "learning_rate": 6.828689370485036e-05,
      "loss": 12.4154,
      "step": 6746
    },
    {
      "epoch": 6.75,
      "grad_norm": 25032.501953125,
      "learning_rate": 6.828173374613004e-05,
      "loss": 18.9423,
      "step": 6747
    },
    {
      "epoch": 6.75,
      "grad_norm": 4938.66015625,
      "learning_rate": 6.82765737874097e-05,
      "loss": 15.4513,
      "step": 6748
    },
    {
      "epoch": 6.76,
      "grad_norm": 5500.9619140625,
      "learning_rate": 6.827141382868939e-05,
      "loss": 18.021,
      "step": 6749
    },
    {
      "epoch": 6.76,
      "grad_norm": 8207.97265625,
      "learning_rate": 6.826625386996905e-05,
      "loss": 16.469,
      "step": 6750
    },
    {
      "epoch": 6.76,
      "grad_norm": 1308.3173828125,
      "learning_rate": 6.826109391124871e-05,
      "loss": 13.8811,
      "step": 6751
    },
    {
      "epoch": 6.76,
      "grad_norm": 10411.54296875,
      "learning_rate": 6.825593395252838e-05,
      "loss": 20.3332,
      "step": 6752
    },
    {
      "epoch": 6.76,
      "grad_norm": 1363.07763671875,
      "learning_rate": 6.825077399380804e-05,
      "loss": 18.3383,
      "step": 6753
    },
    {
      "epoch": 6.76,
      "grad_norm": 2134.9375,
      "learning_rate": 6.824561403508772e-05,
      "loss": 14.0879,
      "step": 6754
    },
    {
      "epoch": 6.76,
      "grad_norm": 62298.53125,
      "learning_rate": 6.824045407636739e-05,
      "loss": 12.8958,
      "step": 6755
    },
    {
      "epoch": 6.76,
      "grad_norm": 22532.890625,
      "learning_rate": 6.823529411764707e-05,
      "loss": 17.4614,
      "step": 6756
    },
    {
      "epoch": 6.76,
      "grad_norm": 1036.5230712890625,
      "learning_rate": 6.823013415892673e-05,
      "loss": 13.9501,
      "step": 6757
    },
    {
      "epoch": 6.76,
      "grad_norm": 4015.350830078125,
      "learning_rate": 6.82249742002064e-05,
      "loss": 12.7682,
      "step": 6758
    },
    {
      "epoch": 6.77,
      "grad_norm": 3541.243896484375,
      "learning_rate": 6.821981424148606e-05,
      "loss": 12.4295,
      "step": 6759
    },
    {
      "epoch": 6.77,
      "grad_norm": 3554.165771484375,
      "learning_rate": 6.821465428276575e-05,
      "loss": 13.672,
      "step": 6760
    },
    {
      "epoch": 6.77,
      "grad_norm": 4897.0966796875,
      "learning_rate": 6.820949432404541e-05,
      "loss": 16.3448,
      "step": 6761
    },
    {
      "epoch": 6.77,
      "grad_norm": 8217.501953125,
      "learning_rate": 6.820433436532509e-05,
      "loss": 14.5137,
      "step": 6762
    },
    {
      "epoch": 6.77,
      "grad_norm": 1460.944580078125,
      "learning_rate": 6.819917440660475e-05,
      "loss": 11.5807,
      "step": 6763
    },
    {
      "epoch": 6.77,
      "grad_norm": 8876.5927734375,
      "learning_rate": 6.819401444788442e-05,
      "loss": 18.1869,
      "step": 6764
    },
    {
      "epoch": 6.77,
      "grad_norm": 32664.720703125,
      "learning_rate": 6.818885448916408e-05,
      "loss": 13.8219,
      "step": 6765
    },
    {
      "epoch": 6.77,
      "grad_norm": 820.6538696289062,
      "learning_rate": 6.818369453044377e-05,
      "loss": 16.3861,
      "step": 6766
    },
    {
      "epoch": 6.77,
      "grad_norm": 14284.3486328125,
      "learning_rate": 6.817853457172343e-05,
      "loss": 10.4661,
      "step": 6767
    },
    {
      "epoch": 6.77,
      "grad_norm": 10592.705078125,
      "learning_rate": 6.817337461300309e-05,
      "loss": 14.9731,
      "step": 6768
    },
    {
      "epoch": 6.78,
      "grad_norm": 16568.587890625,
      "learning_rate": 6.816821465428277e-05,
      "loss": 13.905,
      "step": 6769
    },
    {
      "epoch": 6.78,
      "grad_norm": 4773.71923828125,
      "learning_rate": 6.816305469556243e-05,
      "loss": 12.6369,
      "step": 6770
    },
    {
      "epoch": 6.78,
      "grad_norm": 48862.48828125,
      "learning_rate": 6.81578947368421e-05,
      "loss": 15.616,
      "step": 6771
    },
    {
      "epoch": 6.78,
      "grad_norm": 18888.326171875,
      "learning_rate": 6.815273477812178e-05,
      "loss": 14.9829,
      "step": 6772
    },
    {
      "epoch": 6.78,
      "grad_norm": 2021.698486328125,
      "learning_rate": 6.814757481940145e-05,
      "loss": 15.2463,
      "step": 6773
    },
    {
      "epoch": 6.78,
      "grad_norm": 31684.98828125,
      "learning_rate": 6.814241486068111e-05,
      "loss": 16.9574,
      "step": 6774
    },
    {
      "epoch": 6.78,
      "grad_norm": 852.6495361328125,
      "learning_rate": 6.813725490196079e-05,
      "loss": 11.4137,
      "step": 6775
    },
    {
      "epoch": 6.78,
      "grad_norm": 7238.16259765625,
      "learning_rate": 6.813209494324045e-05,
      "loss": 15.6479,
      "step": 6776
    },
    {
      "epoch": 6.78,
      "grad_norm": 5364.3046875,
      "learning_rate": 6.812693498452014e-05,
      "loss": 15.341,
      "step": 6777
    },
    {
      "epoch": 6.78,
      "grad_norm": 126041.15625,
      "learning_rate": 6.81217750257998e-05,
      "loss": 16.0529,
      "step": 6778
    },
    {
      "epoch": 6.79,
      "grad_norm": 19137.328125,
      "learning_rate": 6.811661506707947e-05,
      "loss": 11.2814,
      "step": 6779
    },
    {
      "epoch": 6.79,
      "grad_norm": 1982.7801513671875,
      "learning_rate": 6.811145510835913e-05,
      "loss": 15.0582,
      "step": 6780
    },
    {
      "epoch": 6.79,
      "grad_norm": 2507.4189453125,
      "learning_rate": 6.810629514963881e-05,
      "loss": 12.0477,
      "step": 6781
    },
    {
      "epoch": 6.79,
      "grad_norm": 4834.736328125,
      "learning_rate": 6.810113519091847e-05,
      "loss": 12.2356,
      "step": 6782
    },
    {
      "epoch": 6.79,
      "grad_norm": 5972.7587890625,
      "learning_rate": 6.809597523219816e-05,
      "loss": 19.1532,
      "step": 6783
    },
    {
      "epoch": 6.79,
      "grad_norm": 28841.20703125,
      "learning_rate": 6.809081527347782e-05,
      "loss": 13.2192,
      "step": 6784
    },
    {
      "epoch": 6.79,
      "grad_norm": 9592.38671875,
      "learning_rate": 6.808565531475749e-05,
      "loss": 15.4132,
      "step": 6785
    },
    {
      "epoch": 6.79,
      "grad_norm": 6426.84912109375,
      "learning_rate": 6.808049535603715e-05,
      "loss": 13.6163,
      "step": 6786
    },
    {
      "epoch": 6.79,
      "grad_norm": 1916.0247802734375,
      "learning_rate": 6.807533539731681e-05,
      "loss": 11.4504,
      "step": 6787
    },
    {
      "epoch": 6.79,
      "grad_norm": 4171.5078125,
      "learning_rate": 6.80701754385965e-05,
      "loss": 30.3966,
      "step": 6788
    },
    {
      "epoch": 6.8,
      "grad_norm": 2941.177001953125,
      "learning_rate": 6.806501547987616e-05,
      "loss": 12.0317,
      "step": 6789
    },
    {
      "epoch": 6.8,
      "grad_norm": 14392.126953125,
      "learning_rate": 6.805985552115584e-05,
      "loss": 11.1666,
      "step": 6790
    },
    {
      "epoch": 6.8,
      "grad_norm": 2413.462890625,
      "learning_rate": 6.80546955624355e-05,
      "loss": 17.1621,
      "step": 6791
    },
    {
      "epoch": 6.8,
      "grad_norm": 10470.947265625,
      "learning_rate": 6.804953560371517e-05,
      "loss": 13.1157,
      "step": 6792
    },
    {
      "epoch": 6.8,
      "grad_norm": 16971.9609375,
      "learning_rate": 6.804437564499483e-05,
      "loss": 12.9473,
      "step": 6793
    },
    {
      "epoch": 6.8,
      "grad_norm": 4593.0615234375,
      "learning_rate": 6.803921568627452e-05,
      "loss": 13.3492,
      "step": 6794
    },
    {
      "epoch": 6.8,
      "grad_norm": 1169.187744140625,
      "learning_rate": 6.803405572755418e-05,
      "loss": 13.712,
      "step": 6795
    },
    {
      "epoch": 6.8,
      "grad_norm": 10963.2548828125,
      "learning_rate": 6.802889576883386e-05,
      "loss": 14.9755,
      "step": 6796
    },
    {
      "epoch": 6.8,
      "grad_norm": 11436.109375,
      "learning_rate": 6.802373581011352e-05,
      "loss": 12.9568,
      "step": 6797
    },
    {
      "epoch": 6.8,
      "grad_norm": 7668.986328125,
      "learning_rate": 6.801857585139319e-05,
      "loss": 16.4596,
      "step": 6798
    },
    {
      "epoch": 6.81,
      "grad_norm": 1876.4141845703125,
      "learning_rate": 6.801341589267285e-05,
      "loss": 13.7716,
      "step": 6799
    },
    {
      "epoch": 6.81,
      "grad_norm": 823.4993896484375,
      "learning_rate": 6.800825593395254e-05,
      "loss": 14.0359,
      "step": 6800
    },
    {
      "epoch": 6.81,
      "grad_norm": 4723.376953125,
      "learning_rate": 6.80030959752322e-05,
      "loss": 15.3234,
      "step": 6801
    },
    {
      "epoch": 6.81,
      "grad_norm": 4418.3515625,
      "learning_rate": 6.799793601651188e-05,
      "loss": 14.1487,
      "step": 6802
    },
    {
      "epoch": 6.81,
      "grad_norm": 2253.22705078125,
      "learning_rate": 6.799277605779154e-05,
      "loss": 15.6591,
      "step": 6803
    },
    {
      "epoch": 6.81,
      "grad_norm": 1233.8570556640625,
      "learning_rate": 6.79876160990712e-05,
      "loss": 11.1819,
      "step": 6804
    },
    {
      "epoch": 6.81,
      "grad_norm": 14017.5888671875,
      "learning_rate": 6.798245614035089e-05,
      "loss": 14.1203,
      "step": 6805
    },
    {
      "epoch": 6.81,
      "grad_norm": 4802.7138671875,
      "learning_rate": 6.797729618163055e-05,
      "loss": 12.1625,
      "step": 6806
    },
    {
      "epoch": 6.81,
      "grad_norm": 7465.25537109375,
      "learning_rate": 6.797213622291022e-05,
      "loss": 14.3005,
      "step": 6807
    },
    {
      "epoch": 6.81,
      "grad_norm": 1404.613037109375,
      "learning_rate": 6.796697626418988e-05,
      "loss": 12.0899,
      "step": 6808
    },
    {
      "epoch": 6.82,
      "grad_norm": 6243.35546875,
      "learning_rate": 6.796181630546956e-05,
      "loss": 11.8125,
      "step": 6809
    },
    {
      "epoch": 6.82,
      "grad_norm": 6510.44140625,
      "learning_rate": 6.795665634674922e-05,
      "loss": 14.6467,
      "step": 6810
    },
    {
      "epoch": 6.82,
      "grad_norm": 7103.58984375,
      "learning_rate": 6.795149638802891e-05,
      "loss": 17.8327,
      "step": 6811
    },
    {
      "epoch": 6.82,
      "grad_norm": 6487.75732421875,
      "learning_rate": 6.794633642930857e-05,
      "loss": 14.4285,
      "step": 6812
    },
    {
      "epoch": 6.82,
      "grad_norm": 2525.493408203125,
      "learning_rate": 6.794117647058824e-05,
      "loss": 12.354,
      "step": 6813
    },
    {
      "epoch": 6.82,
      "grad_norm": 6495.2109375,
      "learning_rate": 6.79360165118679e-05,
      "loss": 16.5585,
      "step": 6814
    },
    {
      "epoch": 6.82,
      "grad_norm": 11150.5986328125,
      "learning_rate": 6.793085655314758e-05,
      "loss": 16.7509,
      "step": 6815
    },
    {
      "epoch": 6.82,
      "grad_norm": 20742.431640625,
      "learning_rate": 6.792569659442724e-05,
      "loss": 12.5031,
      "step": 6816
    },
    {
      "epoch": 6.82,
      "grad_norm": 6797.43017578125,
      "learning_rate": 6.792053663570693e-05,
      "loss": 12.3761,
      "step": 6817
    },
    {
      "epoch": 6.82,
      "grad_norm": 5084.5927734375,
      "learning_rate": 6.791537667698659e-05,
      "loss": 13.3167,
      "step": 6818
    },
    {
      "epoch": 6.83,
      "grad_norm": 1281.3812255859375,
      "learning_rate": 6.791021671826626e-05,
      "loss": 14.8912,
      "step": 6819
    },
    {
      "epoch": 6.83,
      "grad_norm": 2127.081298828125,
      "learning_rate": 6.790505675954592e-05,
      "loss": 17.6026,
      "step": 6820
    },
    {
      "epoch": 6.83,
      "grad_norm": 2461.549560546875,
      "learning_rate": 6.78998968008256e-05,
      "loss": 11.8986,
      "step": 6821
    },
    {
      "epoch": 6.83,
      "grad_norm": 3509.605712890625,
      "learning_rate": 6.789473684210527e-05,
      "loss": 12.0626,
      "step": 6822
    },
    {
      "epoch": 6.83,
      "grad_norm": 1968.39501953125,
      "learning_rate": 6.788957688338493e-05,
      "loss": 14.9374,
      "step": 6823
    },
    {
      "epoch": 6.83,
      "grad_norm": 3678.981689453125,
      "learning_rate": 6.788441692466461e-05,
      "loss": 10.6947,
      "step": 6824
    },
    {
      "epoch": 6.83,
      "grad_norm": 4156.72119140625,
      "learning_rate": 6.787925696594427e-05,
      "loss": 14.019,
      "step": 6825
    },
    {
      "epoch": 6.83,
      "grad_norm": 943.1458740234375,
      "learning_rate": 6.787409700722394e-05,
      "loss": 11.6522,
      "step": 6826
    },
    {
      "epoch": 6.83,
      "grad_norm": 4733.61962890625,
      "learning_rate": 6.78689370485036e-05,
      "loss": 16.4568,
      "step": 6827
    },
    {
      "epoch": 6.83,
      "grad_norm": 4395.77099609375,
      "learning_rate": 6.786377708978329e-05,
      "loss": 12.9843,
      "step": 6828
    },
    {
      "epoch": 6.84,
      "grad_norm": 7722.2470703125,
      "learning_rate": 6.785861713106295e-05,
      "loss": 13.3167,
      "step": 6829
    },
    {
      "epoch": 6.84,
      "grad_norm": 1328.646484375,
      "learning_rate": 6.785345717234263e-05,
      "loss": 12.5262,
      "step": 6830
    },
    {
      "epoch": 6.84,
      "grad_norm": 3898.290283203125,
      "learning_rate": 6.784829721362229e-05,
      "loss": 17.4687,
      "step": 6831
    },
    {
      "epoch": 6.84,
      "grad_norm": 5142.99853515625,
      "learning_rate": 6.784313725490196e-05,
      "loss": 12.2321,
      "step": 6832
    },
    {
      "epoch": 6.84,
      "grad_norm": 21279.884765625,
      "learning_rate": 6.783797729618164e-05,
      "loss": 18.0328,
      "step": 6833
    },
    {
      "epoch": 6.84,
      "grad_norm": 4437.04345703125,
      "learning_rate": 6.783281733746131e-05,
      "loss": 13.2817,
      "step": 6834
    },
    {
      "epoch": 6.84,
      "grad_norm": 5403.80126953125,
      "learning_rate": 6.782765737874097e-05,
      "loss": 12.464,
      "step": 6835
    },
    {
      "epoch": 6.84,
      "grad_norm": 2854.42041015625,
      "learning_rate": 6.782249742002065e-05,
      "loss": 11.4702,
      "step": 6836
    },
    {
      "epoch": 6.84,
      "grad_norm": 754.885986328125,
      "learning_rate": 6.781733746130031e-05,
      "loss": 10.4359,
      "step": 6837
    },
    {
      "epoch": 6.84,
      "grad_norm": 14575.744140625,
      "learning_rate": 6.781217750257998e-05,
      "loss": 10.767,
      "step": 6838
    },
    {
      "epoch": 6.85,
      "grad_norm": 30695.634765625,
      "learning_rate": 6.780701754385966e-05,
      "loss": 19.4373,
      "step": 6839
    },
    {
      "epoch": 6.85,
      "grad_norm": 9417.6708984375,
      "learning_rate": 6.780185758513932e-05,
      "loss": 13.3351,
      "step": 6840
    },
    {
      "epoch": 6.85,
      "grad_norm": 5158.6015625,
      "learning_rate": 6.7796697626419e-05,
      "loss": 13.2413,
      "step": 6841
    },
    {
      "epoch": 6.85,
      "grad_norm": 8182.4208984375,
      "learning_rate": 6.779153766769865e-05,
      "loss": 13.2073,
      "step": 6842
    },
    {
      "epoch": 6.85,
      "grad_norm": 3256.53955078125,
      "learning_rate": 6.778637770897833e-05,
      "loss": 12.4574,
      "step": 6843
    },
    {
      "epoch": 6.85,
      "grad_norm": 2144.38232421875,
      "learning_rate": 6.778121775025799e-05,
      "loss": 19.161,
      "step": 6844
    },
    {
      "epoch": 6.85,
      "grad_norm": 5157.74853515625,
      "learning_rate": 6.777605779153768e-05,
      "loss": 19.5143,
      "step": 6845
    },
    {
      "epoch": 6.85,
      "grad_norm": 4766.1533203125,
      "learning_rate": 6.777089783281734e-05,
      "loss": 16.6798,
      "step": 6846
    },
    {
      "epoch": 6.85,
      "grad_norm": 5110.04345703125,
      "learning_rate": 6.776573787409701e-05,
      "loss": 17.2663,
      "step": 6847
    },
    {
      "epoch": 6.85,
      "grad_norm": 4130.4970703125,
      "learning_rate": 6.776057791537667e-05,
      "loss": 13.2327,
      "step": 6848
    },
    {
      "epoch": 6.86,
      "grad_norm": 3820.746337890625,
      "learning_rate": 6.775541795665635e-05,
      "loss": 13.9151,
      "step": 6849
    },
    {
      "epoch": 6.86,
      "grad_norm": 3793.498046875,
      "learning_rate": 6.775025799793602e-05,
      "loss": 11.4331,
      "step": 6850
    },
    {
      "epoch": 6.86,
      "grad_norm": 7348.02392578125,
      "learning_rate": 6.77450980392157e-05,
      "loss": 14.0058,
      "step": 6851
    },
    {
      "epoch": 6.86,
      "grad_norm": 2241.87646484375,
      "learning_rate": 6.773993808049536e-05,
      "loss": 12.8407,
      "step": 6852
    },
    {
      "epoch": 6.86,
      "grad_norm": 8683.767578125,
      "learning_rate": 6.773477812177503e-05,
      "loss": 11.7366,
      "step": 6853
    },
    {
      "epoch": 6.86,
      "grad_norm": 1152.7230224609375,
      "learning_rate": 6.77296181630547e-05,
      "loss": 13.0589,
      "step": 6854
    },
    {
      "epoch": 6.86,
      "grad_norm": 2719.823974609375,
      "learning_rate": 6.772445820433437e-05,
      "loss": 14.0384,
      "step": 6855
    },
    {
      "epoch": 6.86,
      "grad_norm": 2254.223388671875,
      "learning_rate": 6.771929824561404e-05,
      "loss": 12.1666,
      "step": 6856
    },
    {
      "epoch": 6.86,
      "grad_norm": 13602.9208984375,
      "learning_rate": 6.771413828689372e-05,
      "loss": 13.7809,
      "step": 6857
    },
    {
      "epoch": 6.86,
      "grad_norm": 6687.8291015625,
      "learning_rate": 6.770897832817338e-05,
      "loss": 15.0319,
      "step": 6858
    },
    {
      "epoch": 6.87,
      "grad_norm": 25778.15234375,
      "learning_rate": 6.770381836945304e-05,
      "loss": 16.2411,
      "step": 6859
    },
    {
      "epoch": 6.87,
      "grad_norm": 14833.611328125,
      "learning_rate": 6.769865841073271e-05,
      "loss": 18.1552,
      "step": 6860
    },
    {
      "epoch": 6.87,
      "grad_norm": 1468.9228515625,
      "learning_rate": 6.769349845201239e-05,
      "loss": 13.4952,
      "step": 6861
    },
    {
      "epoch": 6.87,
      "grad_norm": 3620.343017578125,
      "learning_rate": 6.768833849329206e-05,
      "loss": 11.7159,
      "step": 6862
    },
    {
      "epoch": 6.87,
      "grad_norm": 1044.5450439453125,
      "learning_rate": 6.768317853457172e-05,
      "loss": 11.3831,
      "step": 6863
    },
    {
      "epoch": 6.87,
      "grad_norm": 15702.9150390625,
      "learning_rate": 6.76780185758514e-05,
      "loss": 15.0026,
      "step": 6864
    },
    {
      "epoch": 6.87,
      "grad_norm": 3191.886962890625,
      "learning_rate": 6.767285861713106e-05,
      "loss": 15.5028,
      "step": 6865
    },
    {
      "epoch": 6.87,
      "grad_norm": 1269.0416259765625,
      "learning_rate": 6.766769865841073e-05,
      "loss": 12.8785,
      "step": 6866
    },
    {
      "epoch": 6.87,
      "grad_norm": 2667.078125,
      "learning_rate": 6.766253869969041e-05,
      "loss": 16.4017,
      "step": 6867
    },
    {
      "epoch": 6.87,
      "grad_norm": 1479.7413330078125,
      "learning_rate": 6.765737874097008e-05,
      "loss": 15.3816,
      "step": 6868
    },
    {
      "epoch": 6.88,
      "grad_norm": 7578.49169921875,
      "learning_rate": 6.765221878224974e-05,
      "loss": 18.5105,
      "step": 6869
    },
    {
      "epoch": 6.88,
      "grad_norm": 4255.9072265625,
      "learning_rate": 6.764705882352942e-05,
      "loss": 20.0638,
      "step": 6870
    },
    {
      "epoch": 6.88,
      "grad_norm": 6410.85009765625,
      "learning_rate": 6.764189886480908e-05,
      "loss": 17.4936,
      "step": 6871
    },
    {
      "epoch": 6.88,
      "grad_norm": 470.0202331542969,
      "learning_rate": 6.763673890608875e-05,
      "loss": 12.3743,
      "step": 6872
    },
    {
      "epoch": 6.88,
      "grad_norm": 16138.1513671875,
      "learning_rate": 6.763157894736843e-05,
      "loss": 13.9692,
      "step": 6873
    },
    {
      "epoch": 6.88,
      "grad_norm": 3935.3017578125,
      "learning_rate": 6.76264189886481e-05,
      "loss": 11.7855,
      "step": 6874
    },
    {
      "epoch": 6.88,
      "grad_norm": 1600.1971435546875,
      "learning_rate": 6.762125902992776e-05,
      "loss": 13.1336,
      "step": 6875
    },
    {
      "epoch": 6.88,
      "grad_norm": 2910.56005859375,
      "learning_rate": 6.761609907120743e-05,
      "loss": 16.3514,
      "step": 6876
    },
    {
      "epoch": 6.88,
      "grad_norm": 7180.3388671875,
      "learning_rate": 6.76109391124871e-05,
      "loss": 22.4207,
      "step": 6877
    },
    {
      "epoch": 6.88,
      "grad_norm": 853.2576293945312,
      "learning_rate": 6.760577915376677e-05,
      "loss": 14.9874,
      "step": 6878
    },
    {
      "epoch": 6.89,
      "grad_norm": 6108.11865234375,
      "learning_rate": 6.760061919504645e-05,
      "loss": 13.4952,
      "step": 6879
    },
    {
      "epoch": 6.89,
      "grad_norm": 24622.943359375,
      "learning_rate": 6.759545923632611e-05,
      "loss": 15.7394,
      "step": 6880
    },
    {
      "epoch": 6.89,
      "grad_norm": 2760.520751953125,
      "learning_rate": 6.759029927760578e-05,
      "loss": 11.709,
      "step": 6881
    },
    {
      "epoch": 6.89,
      "grad_norm": 1433.931884765625,
      "learning_rate": 6.758513931888545e-05,
      "loss": 10.0712,
      "step": 6882
    },
    {
      "epoch": 6.89,
      "grad_norm": 1932.05224609375,
      "learning_rate": 6.757997936016512e-05,
      "loss": 12.457,
      "step": 6883
    },
    {
      "epoch": 6.89,
      "grad_norm": 22163.0234375,
      "learning_rate": 6.75748194014448e-05,
      "loss": 20.7817,
      "step": 6884
    },
    {
      "epoch": 6.89,
      "grad_norm": 1093.90478515625,
      "learning_rate": 6.756965944272447e-05,
      "loss": 13.2556,
      "step": 6885
    },
    {
      "epoch": 6.89,
      "grad_norm": 3402.218994140625,
      "learning_rate": 6.756449948400413e-05,
      "loss": 12.7143,
      "step": 6886
    },
    {
      "epoch": 6.89,
      "grad_norm": 2298.2880859375,
      "learning_rate": 6.75593395252838e-05,
      "loss": 12.2436,
      "step": 6887
    },
    {
      "epoch": 6.89,
      "grad_norm": 5757.8544921875,
      "learning_rate": 6.755417956656347e-05,
      "loss": 12.3411,
      "step": 6888
    },
    {
      "epoch": 6.9,
      "grad_norm": 686.93359375,
      "learning_rate": 6.754901960784314e-05,
      "loss": 11.2329,
      "step": 6889
    },
    {
      "epoch": 6.9,
      "grad_norm": 8520.1806640625,
      "learning_rate": 6.754385964912281e-05,
      "loss": 23.0245,
      "step": 6890
    },
    {
      "epoch": 6.9,
      "grad_norm": 5258.0576171875,
      "learning_rate": 6.753869969040249e-05,
      "loss": 13.603,
      "step": 6891
    },
    {
      "epoch": 6.9,
      "grad_norm": 4535.8759765625,
      "learning_rate": 6.753353973168215e-05,
      "loss": 20.5622,
      "step": 6892
    },
    {
      "epoch": 6.9,
      "grad_norm": 6127.4716796875,
      "learning_rate": 6.752837977296182e-05,
      "loss": 13.3014,
      "step": 6893
    },
    {
      "epoch": 6.9,
      "grad_norm": 17601.833984375,
      "learning_rate": 6.752321981424149e-05,
      "loss": 15.8886,
      "step": 6894
    },
    {
      "epoch": 6.9,
      "grad_norm": 2505.184326171875,
      "learning_rate": 6.751805985552116e-05,
      "loss": 16.0865,
      "step": 6895
    },
    {
      "epoch": 6.9,
      "grad_norm": 22251.162109375,
      "learning_rate": 6.751289989680083e-05,
      "loss": 25.5856,
      "step": 6896
    },
    {
      "epoch": 6.9,
      "grad_norm": 2220.52734375,
      "learning_rate": 6.75077399380805e-05,
      "loss": 12.7443,
      "step": 6897
    },
    {
      "epoch": 6.9,
      "grad_norm": 9587.1259765625,
      "learning_rate": 6.750257997936017e-05,
      "loss": 13.4594,
      "step": 6898
    },
    {
      "epoch": 6.91,
      "grad_norm": 6116.685546875,
      "learning_rate": 6.749742002063983e-05,
      "loss": 13.4706,
      "step": 6899
    },
    {
      "epoch": 6.91,
      "grad_norm": 22053.689453125,
      "learning_rate": 6.74922600619195e-05,
      "loss": 15.6517,
      "step": 6900
    },
    {
      "epoch": 6.91,
      "grad_norm": 1122.4708251953125,
      "learning_rate": 6.748710010319918e-05,
      "loss": 17.4729,
      "step": 6901
    },
    {
      "epoch": 6.91,
      "grad_norm": 548.614990234375,
      "learning_rate": 6.748194014447885e-05,
      "loss": 10.8597,
      "step": 6902
    },
    {
      "epoch": 6.91,
      "grad_norm": 7581.40185546875,
      "learning_rate": 6.747678018575852e-05,
      "loss": 15.4605,
      "step": 6903
    },
    {
      "epoch": 6.91,
      "grad_norm": 4565.02880859375,
      "learning_rate": 6.747162022703819e-05,
      "loss": 11.6136,
      "step": 6904
    },
    {
      "epoch": 6.91,
      "grad_norm": 4677.33447265625,
      "learning_rate": 6.746646026831785e-05,
      "loss": 13.8255,
      "step": 6905
    },
    {
      "epoch": 6.91,
      "grad_norm": 21265.4921875,
      "learning_rate": 6.746130030959753e-05,
      "loss": 14.2305,
      "step": 6906
    },
    {
      "epoch": 6.91,
      "grad_norm": 4657.396484375,
      "learning_rate": 6.74561403508772e-05,
      "loss": 16.7137,
      "step": 6907
    },
    {
      "epoch": 6.91,
      "grad_norm": 10362.31640625,
      "learning_rate": 6.745098039215687e-05,
      "loss": 13.6372,
      "step": 6908
    },
    {
      "epoch": 6.92,
      "grad_norm": 2342.728271484375,
      "learning_rate": 6.744582043343654e-05,
      "loss": 12.0812,
      "step": 6909
    },
    {
      "epoch": 6.92,
      "grad_norm": 13283.994140625,
      "learning_rate": 6.744066047471621e-05,
      "loss": 13.2556,
      "step": 6910
    },
    {
      "epoch": 6.92,
      "grad_norm": 5998.353515625,
      "learning_rate": 6.743550051599587e-05,
      "loss": 14.3859,
      "step": 6911
    },
    {
      "epoch": 6.92,
      "grad_norm": 425.82525634765625,
      "learning_rate": 6.743034055727555e-05,
      "loss": 11.4218,
      "step": 6912
    },
    {
      "epoch": 6.92,
      "grad_norm": 2673.45458984375,
      "learning_rate": 6.742518059855522e-05,
      "loss": 11.5901,
      "step": 6913
    },
    {
      "epoch": 6.92,
      "grad_norm": 3714.685302734375,
      "learning_rate": 6.742002063983488e-05,
      "loss": 13.2252,
      "step": 6914
    },
    {
      "epoch": 6.92,
      "grad_norm": 939.8351440429688,
      "learning_rate": 6.741486068111456e-05,
      "loss": 14.2641,
      "step": 6915
    },
    {
      "epoch": 6.92,
      "grad_norm": 8070.6455078125,
      "learning_rate": 6.740970072239422e-05,
      "loss": 14.3572,
      "step": 6916
    },
    {
      "epoch": 6.92,
      "grad_norm": 4923.26025390625,
      "learning_rate": 6.740454076367389e-05,
      "loss": 16.4321,
      "step": 6917
    },
    {
      "epoch": 6.92,
      "grad_norm": 6042.943359375,
      "learning_rate": 6.739938080495357e-05,
      "loss": 15.1723,
      "step": 6918
    },
    {
      "epoch": 6.93,
      "grad_norm": 1218.61083984375,
      "learning_rate": 6.739422084623324e-05,
      "loss": 11.9237,
      "step": 6919
    },
    {
      "epoch": 6.93,
      "grad_norm": 4349.56591796875,
      "learning_rate": 6.73890608875129e-05,
      "loss": 14.0888,
      "step": 6920
    },
    {
      "epoch": 6.93,
      "grad_norm": 8218.8447265625,
      "learning_rate": 6.738390092879258e-05,
      "loss": 16.7236,
      "step": 6921
    },
    {
      "epoch": 6.93,
      "grad_norm": 7547.74462890625,
      "learning_rate": 6.737874097007224e-05,
      "loss": 14.9652,
      "step": 6922
    },
    {
      "epoch": 6.93,
      "grad_norm": 2263.70654296875,
      "learning_rate": 6.737358101135191e-05,
      "loss": 11.7918,
      "step": 6923
    },
    {
      "epoch": 6.93,
      "grad_norm": 3983.8544921875,
      "learning_rate": 6.736842105263159e-05,
      "loss": 13.4736,
      "step": 6924
    },
    {
      "epoch": 6.93,
      "grad_norm": 2138.873046875,
      "learning_rate": 6.736326109391126e-05,
      "loss": 12.6743,
      "step": 6925
    },
    {
      "epoch": 6.93,
      "grad_norm": 7225.78564453125,
      "learning_rate": 6.735810113519092e-05,
      "loss": 15.1086,
      "step": 6926
    },
    {
      "epoch": 6.93,
      "grad_norm": 4069.796875,
      "learning_rate": 6.73529411764706e-05,
      "loss": 17.4433,
      "step": 6927
    },
    {
      "epoch": 6.93,
      "grad_norm": 3551.870361328125,
      "learning_rate": 6.734778121775026e-05,
      "loss": 20.0088,
      "step": 6928
    },
    {
      "epoch": 6.94,
      "grad_norm": 2422.142578125,
      "learning_rate": 6.734262125902993e-05,
      "loss": 17.6313,
      "step": 6929
    },
    {
      "epoch": 6.94,
      "grad_norm": 12143.9931640625,
      "learning_rate": 6.73374613003096e-05,
      "loss": 14.8897,
      "step": 6930
    },
    {
      "epoch": 6.94,
      "grad_norm": 1648.205322265625,
      "learning_rate": 6.733230134158927e-05,
      "loss": 18.5683,
      "step": 6931
    },
    {
      "epoch": 6.94,
      "grad_norm": 11651.91796875,
      "learning_rate": 6.732714138286894e-05,
      "loss": 14.5205,
      "step": 6932
    },
    {
      "epoch": 6.94,
      "grad_norm": 13814.2421875,
      "learning_rate": 6.73219814241486e-05,
      "loss": 12.3347,
      "step": 6933
    },
    {
      "epoch": 6.94,
      "grad_norm": 2193.587890625,
      "learning_rate": 6.731682146542828e-05,
      "loss": 12.3496,
      "step": 6934
    },
    {
      "epoch": 6.94,
      "grad_norm": 4399.05810546875,
      "learning_rate": 6.731166150670795e-05,
      "loss": 12.3696,
      "step": 6935
    },
    {
      "epoch": 6.94,
      "grad_norm": 2349.22607421875,
      "learning_rate": 6.730650154798763e-05,
      "loss": 13.1446,
      "step": 6936
    },
    {
      "epoch": 6.94,
      "grad_norm": 5082.291015625,
      "learning_rate": 6.730134158926729e-05,
      "loss": 10.5075,
      "step": 6937
    },
    {
      "epoch": 6.94,
      "grad_norm": 8511.2548828125,
      "learning_rate": 6.729618163054696e-05,
      "loss": 14.5696,
      "step": 6938
    },
    {
      "epoch": 6.95,
      "grad_norm": 10785.31640625,
      "learning_rate": 6.729102167182662e-05,
      "loss": 13.7337,
      "step": 6939
    },
    {
      "epoch": 6.95,
      "grad_norm": 1996.5833740234375,
      "learning_rate": 6.72858617131063e-05,
      "loss": 11.6231,
      "step": 6940
    },
    {
      "epoch": 6.95,
      "grad_norm": 66467.328125,
      "learning_rate": 6.728070175438597e-05,
      "loss": 25.1448,
      "step": 6941
    },
    {
      "epoch": 6.95,
      "grad_norm": 7319.5615234375,
      "learning_rate": 6.727554179566565e-05,
      "loss": 15.439,
      "step": 6942
    },
    {
      "epoch": 6.95,
      "grad_norm": 547.3598022460938,
      "learning_rate": 6.727038183694531e-05,
      "loss": 18.0665,
      "step": 6943
    },
    {
      "epoch": 6.95,
      "grad_norm": 494.5804748535156,
      "learning_rate": 6.726522187822498e-05,
      "loss": 15.0234,
      "step": 6944
    },
    {
      "epoch": 6.95,
      "grad_norm": 1548.9591064453125,
      "learning_rate": 6.726006191950464e-05,
      "loss": 11.3796,
      "step": 6945
    },
    {
      "epoch": 6.95,
      "grad_norm": 16277.029296875,
      "learning_rate": 6.725490196078432e-05,
      "loss": 15.8712,
      "step": 6946
    },
    {
      "epoch": 6.95,
      "grad_norm": 1492.846435546875,
      "learning_rate": 6.724974200206399e-05,
      "loss": 13.2516,
      "step": 6947
    },
    {
      "epoch": 6.95,
      "grad_norm": 14711.1162109375,
      "learning_rate": 6.724458204334365e-05,
      "loss": 15.7901,
      "step": 6948
    },
    {
      "epoch": 6.96,
      "grad_norm": 18848.41015625,
      "learning_rate": 6.723942208462333e-05,
      "loss": 15.1787,
      "step": 6949
    },
    {
      "epoch": 6.96,
      "grad_norm": 18610.16796875,
      "learning_rate": 6.723426212590299e-05,
      "loss": 22.9376,
      "step": 6950
    },
    {
      "epoch": 6.96,
      "grad_norm": 1699.5299072265625,
      "learning_rate": 6.722910216718266e-05,
      "loss": 14.7013,
      "step": 6951
    },
    {
      "epoch": 6.96,
      "grad_norm": 11444.3857421875,
      "learning_rate": 6.722394220846234e-05,
      "loss": 21.3319,
      "step": 6952
    },
    {
      "epoch": 6.96,
      "grad_norm": 1209.62841796875,
      "learning_rate": 6.721878224974201e-05,
      "loss": 12.4724,
      "step": 6953
    },
    {
      "epoch": 6.96,
      "grad_norm": 315.0680236816406,
      "learning_rate": 6.721362229102167e-05,
      "loss": 10.8003,
      "step": 6954
    },
    {
      "epoch": 6.96,
      "grad_norm": 16039.150390625,
      "learning_rate": 6.720846233230135e-05,
      "loss": 16.0503,
      "step": 6955
    },
    {
      "epoch": 6.96,
      "grad_norm": 5078.00341796875,
      "learning_rate": 6.720330237358101e-05,
      "loss": 13.3043,
      "step": 6956
    },
    {
      "epoch": 6.96,
      "grad_norm": 8628.8671875,
      "learning_rate": 6.719814241486068e-05,
      "loss": 16.4962,
      "step": 6957
    },
    {
      "epoch": 6.96,
      "grad_norm": 7365.7509765625,
      "learning_rate": 6.719298245614036e-05,
      "loss": 14.3143,
      "step": 6958
    },
    {
      "epoch": 6.97,
      "grad_norm": 14492.2294921875,
      "learning_rate": 6.718782249742003e-05,
      "loss": 13.9911,
      "step": 6959
    },
    {
      "epoch": 6.97,
      "grad_norm": 16005.3154296875,
      "learning_rate": 6.718266253869969e-05,
      "loss": 14.2845,
      "step": 6960
    },
    {
      "epoch": 6.97,
      "grad_norm": 1278.21728515625,
      "learning_rate": 6.717750257997937e-05,
      "loss": 12.4964,
      "step": 6961
    },
    {
      "epoch": 6.97,
      "grad_norm": 1449.5504150390625,
      "learning_rate": 6.717234262125903e-05,
      "loss": 12.0554,
      "step": 6962
    },
    {
      "epoch": 6.97,
      "grad_norm": 2348.079345703125,
      "learning_rate": 6.71671826625387e-05,
      "loss": 14.0187,
      "step": 6963
    },
    {
      "epoch": 6.97,
      "grad_norm": 2641.194580078125,
      "learning_rate": 6.716202270381838e-05,
      "loss": 14.6273,
      "step": 6964
    },
    {
      "epoch": 6.97,
      "grad_norm": 958.3237915039062,
      "learning_rate": 6.715686274509804e-05,
      "loss": 12.5891,
      "step": 6965
    },
    {
      "epoch": 6.97,
      "grad_norm": 1127.8148193359375,
      "learning_rate": 6.715170278637771e-05,
      "loss": 12.7,
      "step": 6966
    },
    {
      "epoch": 6.97,
      "grad_norm": 16878.478515625,
      "learning_rate": 6.714654282765737e-05,
      "loss": 13.1716,
      "step": 6967
    },
    {
      "epoch": 6.97,
      "grad_norm": 16036.5400390625,
      "learning_rate": 6.714138286893705e-05,
      "loss": 19.7853,
      "step": 6968
    },
    {
      "epoch": 6.98,
      "grad_norm": 13019.2685546875,
      "learning_rate": 6.713622291021672e-05,
      "loss": 22.6011,
      "step": 6969
    },
    {
      "epoch": 6.98,
      "grad_norm": 5115.63818359375,
      "learning_rate": 6.71310629514964e-05,
      "loss": 12.0152,
      "step": 6970
    },
    {
      "epoch": 6.98,
      "grad_norm": 4270.9345703125,
      "learning_rate": 6.712590299277606e-05,
      "loss": 18.7995,
      "step": 6971
    },
    {
      "epoch": 6.98,
      "grad_norm": 673.2689208984375,
      "learning_rate": 6.712074303405573e-05,
      "loss": 14.5674,
      "step": 6972
    },
    {
      "epoch": 6.98,
      "grad_norm": 12288.2802734375,
      "learning_rate": 6.71155830753354e-05,
      "loss": 16.2661,
      "step": 6973
    },
    {
      "epoch": 6.98,
      "grad_norm": 1407.375244140625,
      "learning_rate": 6.711042311661507e-05,
      "loss": 10.9426,
      "step": 6974
    },
    {
      "epoch": 6.98,
      "grad_norm": 576.0426635742188,
      "learning_rate": 6.710526315789474e-05,
      "loss": 9.599,
      "step": 6975
    },
    {
      "epoch": 6.98,
      "grad_norm": 1171.3865966796875,
      "learning_rate": 6.710010319917442e-05,
      "loss": 14.2424,
      "step": 6976
    },
    {
      "epoch": 6.98,
      "grad_norm": 22209.158203125,
      "learning_rate": 6.709494324045408e-05,
      "loss": 30.4258,
      "step": 6977
    },
    {
      "epoch": 6.98,
      "grad_norm": 849.4557495117188,
      "learning_rate": 6.708978328173375e-05,
      "loss": 12.5984,
      "step": 6978
    },
    {
      "epoch": 6.99,
      "grad_norm": 7110.57666015625,
      "learning_rate": 6.708462332301341e-05,
      "loss": 25.7652,
      "step": 6979
    },
    {
      "epoch": 6.99,
      "grad_norm": 5166.50927734375,
      "learning_rate": 6.707946336429309e-05,
      "loss": 12.0326,
      "step": 6980
    },
    {
      "epoch": 6.99,
      "grad_norm": 311437.1875,
      "learning_rate": 6.707430340557276e-05,
      "loss": 18.1091,
      "step": 6981
    },
    {
      "epoch": 6.99,
      "grad_norm": 12207.8544921875,
      "learning_rate": 6.706914344685244e-05,
      "loss": 15.7486,
      "step": 6982
    },
    {
      "epoch": 6.99,
      "grad_norm": 14347.779296875,
      "learning_rate": 6.70639834881321e-05,
      "loss": 13.853,
      "step": 6983
    },
    {
      "epoch": 6.99,
      "grad_norm": 5580.93310546875,
      "learning_rate": 6.705882352941176e-05,
      "loss": 14.0738,
      "step": 6984
    },
    {
      "epoch": 6.99,
      "grad_norm": 2329.08447265625,
      "learning_rate": 6.705366357069143e-05,
      "loss": 16.9134,
      "step": 6985
    },
    {
      "epoch": 6.99,
      "grad_norm": 1657.5115966796875,
      "learning_rate": 6.704850361197111e-05,
      "loss": 15.0701,
      "step": 6986
    },
    {
      "epoch": 6.99,
      "grad_norm": 17969.1796875,
      "learning_rate": 6.704334365325078e-05,
      "loss": 12.8858,
      "step": 6987
    },
    {
      "epoch": 6.99,
      "grad_norm": 18301.111328125,
      "learning_rate": 6.703818369453044e-05,
      "loss": 12.7444,
      "step": 6988
    },
    {
      "epoch": 7.0,
      "grad_norm": 16073.3212890625,
      "learning_rate": 6.703302373581012e-05,
      "loss": 15.1752,
      "step": 6989
    },
    {
      "epoch": 7.0,
      "grad_norm": 947.1160278320312,
      "learning_rate": 6.702786377708978e-05,
      "loss": 12.2011,
      "step": 6990
    },
    {
      "epoch": 7.0,
      "grad_norm": 9346.9853515625,
      "learning_rate": 6.702270381836945e-05,
      "loss": 12.2804,
      "step": 6991
    },
    {
      "epoch": 7.0,
      "grad_norm": 5397.65625,
      "learning_rate": 6.701754385964913e-05,
      "loss": 20.3517,
      "step": 6992
    },
    {
      "epoch": 7.0,
      "grad_norm": 64547.46875,
      "learning_rate": 6.70123839009288e-05,
      "loss": 11.7459,
      "step": 6993
    },
    {
      "epoch": 7.0,
      "grad_norm": 7768.53857421875,
      "learning_rate": 6.700722394220846e-05,
      "loss": 15.7705,
      "step": 6994
    },
    {
      "epoch": 7.0,
      "grad_norm": 8907.220703125,
      "learning_rate": 6.700206398348814e-05,
      "loss": 19.9964,
      "step": 6995
    },
    {
      "epoch": 7.0,
      "grad_norm": 11141.5693359375,
      "learning_rate": 6.69969040247678e-05,
      "loss": 15.1229,
      "step": 6996
    },
    {
      "epoch": 7.0,
      "grad_norm": 1627.6802978515625,
      "learning_rate": 6.699174406604747e-05,
      "loss": 10.2847,
      "step": 6997
    },
    {
      "epoch": 7.01,
      "grad_norm": 4087.929931640625,
      "learning_rate": 6.698658410732715e-05,
      "loss": 19.355,
      "step": 6998
    },
    {
      "epoch": 7.01,
      "grad_norm": 9641.1962890625,
      "learning_rate": 6.698142414860682e-05,
      "loss": 14.5315,
      "step": 6999
    },
    {
      "epoch": 7.01,
      "grad_norm": 9055.9482421875,
      "learning_rate": 6.697626418988648e-05,
      "loss": 12.1061,
      "step": 7000
    },
    {
      "epoch": 7.01,
      "grad_norm": 13197.4150390625,
      "learning_rate": 6.697110423116614e-05,
      "loss": 20.9656,
      "step": 7001
    },
    {
      "epoch": 7.01,
      "grad_norm": 15778.326171875,
      "learning_rate": 6.696594427244582e-05,
      "loss": 33.983,
      "step": 7002
    },
    {
      "epoch": 7.01,
      "grad_norm": 5347.0625,
      "learning_rate": 6.69607843137255e-05,
      "loss": 11.8799,
      "step": 7003
    },
    {
      "epoch": 7.01,
      "grad_norm": 877.1384887695312,
      "learning_rate": 6.695562435500517e-05,
      "loss": 13.0267,
      "step": 7004
    },
    {
      "epoch": 7.01,
      "grad_norm": 2833.161376953125,
      "learning_rate": 6.695046439628483e-05,
      "loss": 11.6908,
      "step": 7005
    },
    {
      "epoch": 7.01,
      "grad_norm": 18014.25390625,
      "learning_rate": 6.69453044375645e-05,
      "loss": 14.8825,
      "step": 7006
    },
    {
      "epoch": 7.01,
      "grad_norm": 3918.73974609375,
      "learning_rate": 6.694014447884416e-05,
      "loss": 19.7584,
      "step": 7007
    },
    {
      "epoch": 7.02,
      "grad_norm": 1963.5926513671875,
      "learning_rate": 6.693498452012384e-05,
      "loss": 11.7408,
      "step": 7008
    },
    {
      "epoch": 7.02,
      "grad_norm": 117761.3046875,
      "learning_rate": 6.692982456140351e-05,
      "loss": 15.4771,
      "step": 7009
    },
    {
      "epoch": 7.02,
      "grad_norm": 12765.87109375,
      "learning_rate": 6.692466460268319e-05,
      "loss": 14.6454,
      "step": 7010
    },
    {
      "epoch": 7.02,
      "grad_norm": 1769.0804443359375,
      "learning_rate": 6.691950464396285e-05,
      "loss": 11.3036,
      "step": 7011
    },
    {
      "epoch": 7.02,
      "grad_norm": 4132.5595703125,
      "learning_rate": 6.691434468524252e-05,
      "loss": 15.165,
      "step": 7012
    },
    {
      "epoch": 7.02,
      "grad_norm": 8359.759765625,
      "learning_rate": 6.690918472652218e-05,
      "loss": 17.5188,
      "step": 7013
    },
    {
      "epoch": 7.02,
      "grad_norm": 48267.04296875,
      "learning_rate": 6.690402476780186e-05,
      "loss": 21.3387,
      "step": 7014
    },
    {
      "epoch": 7.02,
      "grad_norm": 2168.84716796875,
      "learning_rate": 6.689886480908153e-05,
      "loss": 21.1105,
      "step": 7015
    },
    {
      "epoch": 7.02,
      "grad_norm": 459.2823486328125,
      "learning_rate": 6.689370485036121e-05,
      "loss": 10.8752,
      "step": 7016
    },
    {
      "epoch": 7.02,
      "grad_norm": 2177.17578125,
      "learning_rate": 6.688854489164087e-05,
      "loss": 12.2503,
      "step": 7017
    },
    {
      "epoch": 7.03,
      "grad_norm": 5066.31640625,
      "learning_rate": 6.688338493292054e-05,
      "loss": 18.6221,
      "step": 7018
    },
    {
      "epoch": 7.03,
      "grad_norm": 185489.53125,
      "learning_rate": 6.68782249742002e-05,
      "loss": 20.7663,
      "step": 7019
    },
    {
      "epoch": 7.03,
      "grad_norm": 9386.51953125,
      "learning_rate": 6.687306501547988e-05,
      "loss": 16.5436,
      "step": 7020
    },
    {
      "epoch": 7.03,
      "grad_norm": 20882.5625,
      "learning_rate": 6.686790505675955e-05,
      "loss": 19.3283,
      "step": 7021
    },
    {
      "epoch": 7.03,
      "grad_norm": 1993.7032470703125,
      "learning_rate": 6.686274509803921e-05,
      "loss": 12.837,
      "step": 7022
    },
    {
      "epoch": 7.03,
      "grad_norm": 5280.0400390625,
      "learning_rate": 6.685758513931889e-05,
      "loss": 14.8933,
      "step": 7023
    },
    {
      "epoch": 7.03,
      "grad_norm": 1486.826416015625,
      "learning_rate": 6.685242518059855e-05,
      "loss": 12.356,
      "step": 7024
    },
    {
      "epoch": 7.03,
      "grad_norm": 711.7438354492188,
      "learning_rate": 6.684726522187822e-05,
      "loss": 13.7358,
      "step": 7025
    },
    {
      "epoch": 7.03,
      "grad_norm": 4623.80712890625,
      "learning_rate": 6.68421052631579e-05,
      "loss": 20.5769,
      "step": 7026
    },
    {
      "epoch": 7.03,
      "grad_norm": 6940.0458984375,
      "learning_rate": 6.683694530443757e-05,
      "loss": 18.9342,
      "step": 7027
    },
    {
      "epoch": 7.04,
      "grad_norm": 3699.739990234375,
      "learning_rate": 6.683178534571723e-05,
      "loss": 11.8413,
      "step": 7028
    },
    {
      "epoch": 7.04,
      "grad_norm": 11091.724609375,
      "learning_rate": 6.682662538699691e-05,
      "loss": 21.1086,
      "step": 7029
    },
    {
      "epoch": 7.04,
      "grad_norm": 3821.487548828125,
      "learning_rate": 6.682146542827657e-05,
      "loss": 11.1015,
      "step": 7030
    },
    {
      "epoch": 7.04,
      "grad_norm": 7965.81689453125,
      "learning_rate": 6.681630546955624e-05,
      "loss": 11.361,
      "step": 7031
    },
    {
      "epoch": 7.04,
      "grad_norm": 2996.336669921875,
      "learning_rate": 6.681114551083592e-05,
      "loss": 11.3336,
      "step": 7032
    },
    {
      "epoch": 7.04,
      "grad_norm": 736.15087890625,
      "learning_rate": 6.68059855521156e-05,
      "loss": 14.0373,
      "step": 7033
    },
    {
      "epoch": 7.04,
      "grad_norm": 13041.087890625,
      "learning_rate": 6.680082559339525e-05,
      "loss": 11.4463,
      "step": 7034
    },
    {
      "epoch": 7.04,
      "grad_norm": 1087.1492919921875,
      "learning_rate": 6.679566563467493e-05,
      "loss": 15.1598,
      "step": 7035
    },
    {
      "epoch": 7.04,
      "grad_norm": 4861.029296875,
      "learning_rate": 6.679050567595459e-05,
      "loss": 17.5086,
      "step": 7036
    },
    {
      "epoch": 7.04,
      "grad_norm": 6151.67041015625,
      "learning_rate": 6.678534571723426e-05,
      "loss": 16.2248,
      "step": 7037
    },
    {
      "epoch": 7.05,
      "grad_norm": 22422.814453125,
      "learning_rate": 6.678018575851394e-05,
      "loss": 11.8942,
      "step": 7038
    },
    {
      "epoch": 7.05,
      "grad_norm": 17977.7265625,
      "learning_rate": 6.67750257997936e-05,
      "loss": 24.6712,
      "step": 7039
    },
    {
      "epoch": 7.05,
      "grad_norm": 5674.1533203125,
      "learning_rate": 6.676986584107327e-05,
      "loss": 15.4626,
      "step": 7040
    },
    {
      "epoch": 7.05,
      "grad_norm": 17616.453125,
      "learning_rate": 6.676470588235294e-05,
      "loss": 26.0331,
      "step": 7041
    },
    {
      "epoch": 7.05,
      "grad_norm": 7070.59765625,
      "learning_rate": 6.675954592363261e-05,
      "loss": 18.5151,
      "step": 7042
    },
    {
      "epoch": 7.05,
      "grad_norm": 27616.822265625,
      "learning_rate": 6.675438596491228e-05,
      "loss": 16.1267,
      "step": 7043
    },
    {
      "epoch": 7.05,
      "grad_norm": 3990.9306640625,
      "learning_rate": 6.674922600619196e-05,
      "loss": 17.1955,
      "step": 7044
    },
    {
      "epoch": 7.05,
      "grad_norm": 3027.496826171875,
      "learning_rate": 6.674406604747162e-05,
      "loss": 14.0653,
      "step": 7045
    },
    {
      "epoch": 7.05,
      "grad_norm": 5156.0556640625,
      "learning_rate": 6.67389060887513e-05,
      "loss": 15.1372,
      "step": 7046
    },
    {
      "epoch": 7.05,
      "grad_norm": 1353.923583984375,
      "learning_rate": 6.673374613003096e-05,
      "loss": 14.2398,
      "step": 7047
    },
    {
      "epoch": 7.06,
      "grad_norm": 1601.48779296875,
      "learning_rate": 6.672858617131063e-05,
      "loss": 17.4118,
      "step": 7048
    },
    {
      "epoch": 7.06,
      "grad_norm": 33361.73046875,
      "learning_rate": 6.67234262125903e-05,
      "loss": 14.6136,
      "step": 7049
    },
    {
      "epoch": 7.06,
      "grad_norm": 327.47027587890625,
      "learning_rate": 6.671826625386998e-05,
      "loss": 18.7728,
      "step": 7050
    },
    {
      "epoch": 7.06,
      "grad_norm": 2880.09521484375,
      "learning_rate": 6.671310629514964e-05,
      "loss": 12.7218,
      "step": 7051
    },
    {
      "epoch": 7.06,
      "grad_norm": 3816.96533203125,
      "learning_rate": 6.670794633642931e-05,
      "loss": 12.8328,
      "step": 7052
    },
    {
      "epoch": 7.06,
      "grad_norm": 23403.58203125,
      "learning_rate": 6.670278637770898e-05,
      "loss": 20.5002,
      "step": 7053
    },
    {
      "epoch": 7.06,
      "grad_norm": 4284.9072265625,
      "learning_rate": 6.669762641898866e-05,
      "loss": 15.2061,
      "step": 7054
    },
    {
      "epoch": 7.06,
      "grad_norm": 5035.24658203125,
      "learning_rate": 6.669246646026832e-05,
      "loss": 14.3657,
      "step": 7055
    },
    {
      "epoch": 7.06,
      "grad_norm": 3091.41748046875,
      "learning_rate": 6.668730650154799e-05,
      "loss": 12.277,
      "step": 7056
    },
    {
      "epoch": 7.06,
      "grad_norm": 8261.8486328125,
      "learning_rate": 6.668214654282766e-05,
      "loss": 12.0898,
      "step": 7057
    },
    {
      "epoch": 7.07,
      "grad_norm": 906.7722778320312,
      "learning_rate": 6.667698658410732e-05,
      "loss": 11.4136,
      "step": 7058
    },
    {
      "epoch": 7.07,
      "grad_norm": 12831.583984375,
      "learning_rate": 6.6671826625387e-05,
      "loss": 18.7888,
      "step": 7059
    },
    {
      "epoch": 7.07,
      "grad_norm": 1371.543212890625,
      "learning_rate": 6.666666666666667e-05,
      "loss": 11.3221,
      "step": 7060
    },
    {
      "epoch": 7.07,
      "grad_norm": 2388.54345703125,
      "learning_rate": 6.666150670794634e-05,
      "loss": 14.2541,
      "step": 7061
    },
    {
      "epoch": 7.07,
      "grad_norm": 6222.38671875,
      "learning_rate": 6.6656346749226e-05,
      "loss": 13.9786,
      "step": 7062
    },
    {
      "epoch": 7.07,
      "grad_norm": 2777.173828125,
      "learning_rate": 6.665118679050568e-05,
      "loss": 17.5057,
      "step": 7063
    },
    {
      "epoch": 7.07,
      "grad_norm": 1863.91357421875,
      "learning_rate": 6.664602683178534e-05,
      "loss": 12.2664,
      "step": 7064
    },
    {
      "epoch": 7.07,
      "grad_norm": 8004.22900390625,
      "learning_rate": 6.664086687306502e-05,
      "loss": 14.446,
      "step": 7065
    },
    {
      "epoch": 7.07,
      "grad_norm": 3507.275390625,
      "learning_rate": 6.663570691434469e-05,
      "loss": 12.3029,
      "step": 7066
    },
    {
      "epoch": 7.07,
      "grad_norm": 998.5217895507812,
      "learning_rate": 6.663054695562436e-05,
      "loss": 14.6116,
      "step": 7067
    },
    {
      "epoch": 7.08,
      "grad_norm": 4354.1044921875,
      "learning_rate": 6.662538699690403e-05,
      "loss": 12.159,
      "step": 7068
    },
    {
      "epoch": 7.08,
      "grad_norm": 11820.0458984375,
      "learning_rate": 6.66202270381837e-05,
      "loss": 11.2159,
      "step": 7069
    },
    {
      "epoch": 7.08,
      "grad_norm": 18643.892578125,
      "learning_rate": 6.661506707946336e-05,
      "loss": 12.8351,
      "step": 7070
    },
    {
      "epoch": 7.08,
      "grad_norm": 2759.830322265625,
      "learning_rate": 6.660990712074305e-05,
      "loss": 13.4603,
      "step": 7071
    },
    {
      "epoch": 7.08,
      "grad_norm": 1038.87255859375,
      "learning_rate": 6.660474716202271e-05,
      "loss": 16.7298,
      "step": 7072
    },
    {
      "epoch": 7.08,
      "grad_norm": 1947.04443359375,
      "learning_rate": 6.659958720330237e-05,
      "loss": 11.6589,
      "step": 7073
    },
    {
      "epoch": 7.08,
      "grad_norm": 373.71533203125,
      "learning_rate": 6.659442724458205e-05,
      "loss": 11.5362,
      "step": 7074
    },
    {
      "epoch": 7.08,
      "grad_norm": 2133.15185546875,
      "learning_rate": 6.65892672858617e-05,
      "loss": 12.5504,
      "step": 7075
    },
    {
      "epoch": 7.08,
      "grad_norm": 8380.7685546875,
      "learning_rate": 6.658410732714138e-05,
      "loss": 18.2502,
      "step": 7076
    },
    {
      "epoch": 7.08,
      "grad_norm": 529.0999145507812,
      "learning_rate": 6.657894736842106e-05,
      "loss": 11.8073,
      "step": 7077
    },
    {
      "epoch": 7.09,
      "grad_norm": 15766.7451171875,
      "learning_rate": 6.657378740970073e-05,
      "loss": 11.7023,
      "step": 7078
    },
    {
      "epoch": 7.09,
      "grad_norm": 8680.884765625,
      "learning_rate": 6.656862745098039e-05,
      "loss": 14.9464,
      "step": 7079
    },
    {
      "epoch": 7.09,
      "grad_norm": 26325.412109375,
      "learning_rate": 6.656346749226007e-05,
      "loss": 24.9229,
      "step": 7080
    },
    {
      "epoch": 7.09,
      "grad_norm": 8681.0263671875,
      "learning_rate": 6.655830753353973e-05,
      "loss": 17.0528,
      "step": 7081
    },
    {
      "epoch": 7.09,
      "grad_norm": 2434.07470703125,
      "learning_rate": 6.655314757481941e-05,
      "loss": 20.6329,
      "step": 7082
    },
    {
      "epoch": 7.09,
      "grad_norm": 3205.603759765625,
      "learning_rate": 6.654798761609908e-05,
      "loss": 16.0083,
      "step": 7083
    },
    {
      "epoch": 7.09,
      "grad_norm": 1253.894287109375,
      "learning_rate": 6.654282765737875e-05,
      "loss": 13.6227,
      "step": 7084
    },
    {
      "epoch": 7.09,
      "grad_norm": 26226.806640625,
      "learning_rate": 6.653766769865841e-05,
      "loss": 13.5574,
      "step": 7085
    },
    {
      "epoch": 7.09,
      "grad_norm": 3073.63427734375,
      "learning_rate": 6.653250773993809e-05,
      "loss": 14.5819,
      "step": 7086
    },
    {
      "epoch": 7.09,
      "grad_norm": 15741.025390625,
      "learning_rate": 6.652734778121775e-05,
      "loss": 11.6498,
      "step": 7087
    },
    {
      "epoch": 7.1,
      "grad_norm": 2671.505126953125,
      "learning_rate": 6.652218782249743e-05,
      "loss": 14.8417,
      "step": 7088
    },
    {
      "epoch": 7.1,
      "grad_norm": 5900.5439453125,
      "learning_rate": 6.65170278637771e-05,
      "loss": 13.7302,
      "step": 7089
    },
    {
      "epoch": 7.1,
      "grad_norm": 4874.17822265625,
      "learning_rate": 6.651186790505676e-05,
      "loss": 19.3834,
      "step": 7090
    },
    {
      "epoch": 7.1,
      "grad_norm": 179880.59375,
      "learning_rate": 6.650670794633643e-05,
      "loss": 16.4117,
      "step": 7091
    },
    {
      "epoch": 7.1,
      "grad_norm": 1823.4107666015625,
      "learning_rate": 6.650154798761609e-05,
      "loss": 12.5081,
      "step": 7092
    },
    {
      "epoch": 7.1,
      "grad_norm": 2993.087890625,
      "learning_rate": 6.649638802889577e-05,
      "loss": 12.2763,
      "step": 7093
    },
    {
      "epoch": 7.1,
      "grad_norm": 4918.287109375,
      "learning_rate": 6.649122807017544e-05,
      "loss": 14.8636,
      "step": 7094
    },
    {
      "epoch": 7.1,
      "grad_norm": 1512.9276123046875,
      "learning_rate": 6.648606811145512e-05,
      "loss": 11.5202,
      "step": 7095
    },
    {
      "epoch": 7.1,
      "grad_norm": 17977.892578125,
      "learning_rate": 6.648090815273478e-05,
      "loss": 18.3628,
      "step": 7096
    },
    {
      "epoch": 7.1,
      "grad_norm": 1042.162109375,
      "learning_rate": 6.647574819401445e-05,
      "loss": 12.3456,
      "step": 7097
    },
    {
      "epoch": 7.11,
      "grad_norm": 5195.97900390625,
      "learning_rate": 6.647058823529411e-05,
      "loss": 15.9047,
      "step": 7098
    },
    {
      "epoch": 7.11,
      "grad_norm": 795.0225830078125,
      "learning_rate": 6.64654282765738e-05,
      "loss": 13.9086,
      "step": 7099
    },
    {
      "epoch": 7.11,
      "grad_norm": 2537.513671875,
      "learning_rate": 6.646026831785346e-05,
      "loss": 11.4787,
      "step": 7100
    },
    {
      "epoch": 7.11,
      "grad_norm": 6390.15234375,
      "learning_rate": 6.645510835913314e-05,
      "loss": 10.5682,
      "step": 7101
    },
    {
      "epoch": 7.11,
      "grad_norm": 5058.69140625,
      "learning_rate": 6.64499484004128e-05,
      "loss": 12.588,
      "step": 7102
    },
    {
      "epoch": 7.11,
      "grad_norm": 20135.90234375,
      "learning_rate": 6.644478844169247e-05,
      "loss": 17.9208,
      "step": 7103
    },
    {
      "epoch": 7.11,
      "grad_norm": 1991.46923828125,
      "learning_rate": 6.643962848297213e-05,
      "loss": 13.1366,
      "step": 7104
    },
    {
      "epoch": 7.11,
      "grad_norm": 3866.05224609375,
      "learning_rate": 6.643446852425182e-05,
      "loss": 13.9135,
      "step": 7105
    },
    {
      "epoch": 7.11,
      "grad_norm": 1392.721435546875,
      "learning_rate": 6.642930856553148e-05,
      "loss": 11.2512,
      "step": 7106
    },
    {
      "epoch": 7.11,
      "grad_norm": 12483.962890625,
      "learning_rate": 6.642414860681116e-05,
      "loss": 12.145,
      "step": 7107
    },
    {
      "epoch": 7.12,
      "grad_norm": 2310.201171875,
      "learning_rate": 6.641898864809082e-05,
      "loss": 12.5468,
      "step": 7108
    },
    {
      "epoch": 7.12,
      "grad_norm": 11836.7412109375,
      "learning_rate": 6.641382868937048e-05,
      "loss": 15.531,
      "step": 7109
    },
    {
      "epoch": 7.12,
      "grad_norm": 2851.136962890625,
      "learning_rate": 6.640866873065017e-05,
      "loss": 13.6293,
      "step": 7110
    },
    {
      "epoch": 7.12,
      "grad_norm": 1385.613037109375,
      "learning_rate": 6.640350877192983e-05,
      "loss": 11.6496,
      "step": 7111
    },
    {
      "epoch": 7.12,
      "grad_norm": 4269.72998046875,
      "learning_rate": 6.63983488132095e-05,
      "loss": 17.1715,
      "step": 7112
    },
    {
      "epoch": 7.12,
      "grad_norm": 53906.40625,
      "learning_rate": 6.639318885448916e-05,
      "loss": 13.6004,
      "step": 7113
    },
    {
      "epoch": 7.12,
      "grad_norm": 16516.783203125,
      "learning_rate": 6.638802889576884e-05,
      "loss": 12.004,
      "step": 7114
    },
    {
      "epoch": 7.12,
      "grad_norm": 17053.427734375,
      "learning_rate": 6.63828689370485e-05,
      "loss": 13.5015,
      "step": 7115
    },
    {
      "epoch": 7.12,
      "grad_norm": 4981.06396484375,
      "learning_rate": 6.637770897832819e-05,
      "loss": 13.5786,
      "step": 7116
    },
    {
      "epoch": 7.12,
      "grad_norm": 172106.21875,
      "learning_rate": 6.637254901960785e-05,
      "loss": 28.9841,
      "step": 7117
    },
    {
      "epoch": 7.13,
      "grad_norm": 724.4569091796875,
      "learning_rate": 6.636738906088752e-05,
      "loss": 11.6153,
      "step": 7118
    },
    {
      "epoch": 7.13,
      "grad_norm": 35704.953125,
      "learning_rate": 6.636222910216718e-05,
      "loss": 18.4779,
      "step": 7119
    },
    {
      "epoch": 7.13,
      "grad_norm": 11656.849609375,
      "learning_rate": 6.635706914344686e-05,
      "loss": 16.7833,
      "step": 7120
    },
    {
      "epoch": 7.13,
      "grad_norm": 5521.1845703125,
      "learning_rate": 6.635190918472652e-05,
      "loss": 12.22,
      "step": 7121
    },
    {
      "epoch": 7.13,
      "grad_norm": 14406.017578125,
      "learning_rate": 6.63467492260062e-05,
      "loss": 25.0025,
      "step": 7122
    },
    {
      "epoch": 7.13,
      "grad_norm": 5497.42236328125,
      "learning_rate": 6.634158926728587e-05,
      "loss": 10.796,
      "step": 7123
    },
    {
      "epoch": 7.13,
      "grad_norm": 3848.685791015625,
      "learning_rate": 6.633642930856554e-05,
      "loss": 17.9639,
      "step": 7124
    },
    {
      "epoch": 7.13,
      "grad_norm": 869.3811645507812,
      "learning_rate": 6.63312693498452e-05,
      "loss": 13.0434,
      "step": 7125
    },
    {
      "epoch": 7.13,
      "grad_norm": 1687.1905517578125,
      "learning_rate": 6.632610939112486e-05,
      "loss": 14.0927,
      "step": 7126
    },
    {
      "epoch": 7.13,
      "grad_norm": 732.0737915039062,
      "learning_rate": 6.632094943240455e-05,
      "loss": 12.1943,
      "step": 7127
    },
    {
      "epoch": 7.14,
      "grad_norm": 581.3088989257812,
      "learning_rate": 6.631578947368421e-05,
      "loss": 10.4636,
      "step": 7128
    },
    {
      "epoch": 7.14,
      "grad_norm": 29929.3828125,
      "learning_rate": 6.631062951496389e-05,
      "loss": 18.5829,
      "step": 7129
    },
    {
      "epoch": 7.14,
      "grad_norm": 604.8776245117188,
      "learning_rate": 6.630546955624355e-05,
      "loss": 14.1797,
      "step": 7130
    },
    {
      "epoch": 7.14,
      "grad_norm": 1406.10693359375,
      "learning_rate": 6.630030959752322e-05,
      "loss": 13.7454,
      "step": 7131
    },
    {
      "epoch": 7.14,
      "grad_norm": 1577.7279052734375,
      "learning_rate": 6.629514963880288e-05,
      "loss": 12.2092,
      "step": 7132
    },
    {
      "epoch": 7.14,
      "grad_norm": 980.66796875,
      "learning_rate": 6.628998968008257e-05,
      "loss": 13.0055,
      "step": 7133
    },
    {
      "epoch": 7.14,
      "grad_norm": 15827.353515625,
      "learning_rate": 6.628482972136223e-05,
      "loss": 13.9352,
      "step": 7134
    },
    {
      "epoch": 7.14,
      "grad_norm": 21654.919921875,
      "learning_rate": 6.62796697626419e-05,
      "loss": 15.1133,
      "step": 7135
    },
    {
      "epoch": 7.14,
      "grad_norm": 1544.3468017578125,
      "learning_rate": 6.627450980392157e-05,
      "loss": 18.6196,
      "step": 7136
    },
    {
      "epoch": 7.14,
      "grad_norm": 1909.484130859375,
      "learning_rate": 6.626934984520124e-05,
      "loss": 14.2153,
      "step": 7137
    },
    {
      "epoch": 7.15,
      "grad_norm": 3912.101318359375,
      "learning_rate": 6.626418988648092e-05,
      "loss": 15.7116,
      "step": 7138
    },
    {
      "epoch": 7.15,
      "grad_norm": 13187.0576171875,
      "learning_rate": 6.625902992776059e-05,
      "loss": 13.1998,
      "step": 7139
    },
    {
      "epoch": 7.15,
      "grad_norm": 10223.162109375,
      "learning_rate": 6.625386996904025e-05,
      "loss": 15.1402,
      "step": 7140
    },
    {
      "epoch": 7.15,
      "grad_norm": 12554.5048828125,
      "learning_rate": 6.624871001031993e-05,
      "loss": 11.1713,
      "step": 7141
    },
    {
      "epoch": 7.15,
      "grad_norm": 6640.4775390625,
      "learning_rate": 6.624355005159959e-05,
      "loss": 13.6019,
      "step": 7142
    },
    {
      "epoch": 7.15,
      "grad_norm": 1411.697509765625,
      "learning_rate": 6.623839009287926e-05,
      "loss": 13.8267,
      "step": 7143
    },
    {
      "epoch": 7.15,
      "grad_norm": 8710.8486328125,
      "learning_rate": 6.623323013415894e-05,
      "loss": 15.699,
      "step": 7144
    },
    {
      "epoch": 7.15,
      "grad_norm": 1114.8333740234375,
      "learning_rate": 6.62280701754386e-05,
      "loss": 11.7306,
      "step": 7145
    },
    {
      "epoch": 7.15,
      "grad_norm": 3124.780029296875,
      "learning_rate": 6.622291021671827e-05,
      "loss": 15.1719,
      "step": 7146
    },
    {
      "epoch": 7.15,
      "grad_norm": 815.9351196289062,
      "learning_rate": 6.621775025799793e-05,
      "loss": 12.5414,
      "step": 7147
    },
    {
      "epoch": 7.16,
      "grad_norm": 3200.81201171875,
      "learning_rate": 6.621259029927761e-05,
      "loss": 12.957,
      "step": 7148
    },
    {
      "epoch": 7.16,
      "grad_norm": 10312.7548828125,
      "learning_rate": 6.620743034055727e-05,
      "loss": 13.586,
      "step": 7149
    },
    {
      "epoch": 7.16,
      "grad_norm": 4798.17236328125,
      "learning_rate": 6.620227038183696e-05,
      "loss": 13.7225,
      "step": 7150
    },
    {
      "epoch": 7.16,
      "grad_norm": 616.1024780273438,
      "learning_rate": 6.619711042311662e-05,
      "loss": 13.0707,
      "step": 7151
    },
    {
      "epoch": 7.16,
      "grad_norm": 3164.487060546875,
      "learning_rate": 6.619195046439629e-05,
      "loss": 14.0146,
      "step": 7152
    },
    {
      "epoch": 7.16,
      "grad_norm": 6080.59130859375,
      "learning_rate": 6.618679050567595e-05,
      "loss": 11.82,
      "step": 7153
    },
    {
      "epoch": 7.16,
      "grad_norm": 6774.9248046875,
      "learning_rate": 6.618163054695563e-05,
      "loss": 13.3134,
      "step": 7154
    },
    {
      "epoch": 7.16,
      "grad_norm": 10195.087890625,
      "learning_rate": 6.61764705882353e-05,
      "loss": 12.906,
      "step": 7155
    },
    {
      "epoch": 7.16,
      "grad_norm": 3706.37353515625,
      "learning_rate": 6.617131062951498e-05,
      "loss": 11.4826,
      "step": 7156
    },
    {
      "epoch": 7.16,
      "grad_norm": 1551.25830078125,
      "learning_rate": 6.616615067079464e-05,
      "loss": 12.581,
      "step": 7157
    },
    {
      "epoch": 7.17,
      "grad_norm": 2582.924072265625,
      "learning_rate": 6.616099071207431e-05,
      "loss": 12.2969,
      "step": 7158
    },
    {
      "epoch": 7.17,
      "grad_norm": 1384.466552734375,
      "learning_rate": 6.615583075335397e-05,
      "loss": 13.1947,
      "step": 7159
    },
    {
      "epoch": 7.17,
      "grad_norm": 1510.6378173828125,
      "learning_rate": 6.615067079463365e-05,
      "loss": 18.0582,
      "step": 7160
    },
    {
      "epoch": 7.17,
      "grad_norm": 107399.203125,
      "learning_rate": 6.614551083591332e-05,
      "loss": 23.1699,
      "step": 7161
    },
    {
      "epoch": 7.17,
      "grad_norm": 6763.81982421875,
      "learning_rate": 6.614035087719298e-05,
      "loss": 15.8123,
      "step": 7162
    },
    {
      "epoch": 7.17,
      "grad_norm": 7101.07373046875,
      "learning_rate": 6.613519091847266e-05,
      "loss": 12.3397,
      "step": 7163
    },
    {
      "epoch": 7.17,
      "grad_norm": 6368.5126953125,
      "learning_rate": 6.613003095975232e-05,
      "loss": 17.0574,
      "step": 7164
    },
    {
      "epoch": 7.17,
      "grad_norm": 97598.078125,
      "learning_rate": 6.612487100103199e-05,
      "loss": 15.8476,
      "step": 7165
    },
    {
      "epoch": 7.17,
      "grad_norm": 6395.28271484375,
      "learning_rate": 6.611971104231165e-05,
      "loss": 16.2166,
      "step": 7166
    },
    {
      "epoch": 7.17,
      "grad_norm": 3318.64453125,
      "learning_rate": 6.611455108359134e-05,
      "loss": 13.3899,
      "step": 7167
    },
    {
      "epoch": 7.18,
      "grad_norm": 13223.7734375,
      "learning_rate": 6.6109391124871e-05,
      "loss": 14.3727,
      "step": 7168
    },
    {
      "epoch": 7.18,
      "grad_norm": 2158.2138671875,
      "learning_rate": 6.610423116615068e-05,
      "loss": 11.3177,
      "step": 7169
    },
    {
      "epoch": 7.18,
      "grad_norm": 140.18589782714844,
      "learning_rate": 6.609907120743034e-05,
      "loss": 12.9974,
      "step": 7170
    },
    {
      "epoch": 7.18,
      "grad_norm": 8236.578125,
      "learning_rate": 6.609391124871001e-05,
      "loss": 13.3069,
      "step": 7171
    },
    {
      "epoch": 7.18,
      "grad_norm": 1299.6102294921875,
      "learning_rate": 6.608875128998969e-05,
      "loss": 12.8299,
      "step": 7172
    },
    {
      "epoch": 7.18,
      "grad_norm": 2761.215576171875,
      "learning_rate": 6.608359133126936e-05,
      "loss": 12.0131,
      "step": 7173
    },
    {
      "epoch": 7.18,
      "grad_norm": 2147.52294921875,
      "learning_rate": 6.607843137254902e-05,
      "loss": 13.4715,
      "step": 7174
    },
    {
      "epoch": 7.18,
      "grad_norm": 8070.4775390625,
      "learning_rate": 6.60732714138287e-05,
      "loss": 13.5332,
      "step": 7175
    },
    {
      "epoch": 7.18,
      "grad_norm": 5565.96875,
      "learning_rate": 6.606811145510836e-05,
      "loss": 18.5934,
      "step": 7176
    },
    {
      "epoch": 7.18,
      "grad_norm": 9381.0,
      "learning_rate": 6.606295149638803e-05,
      "loss": 19.0026,
      "step": 7177
    },
    {
      "epoch": 7.19,
      "grad_norm": 881.5364990234375,
      "learning_rate": 6.605779153766771e-05,
      "loss": 11.0041,
      "step": 7178
    },
    {
      "epoch": 7.19,
      "grad_norm": 1585.0987548828125,
      "learning_rate": 6.605263157894738e-05,
      "loss": 14.0055,
      "step": 7179
    },
    {
      "epoch": 7.19,
      "grad_norm": 5017.263671875,
      "learning_rate": 6.604747162022704e-05,
      "loss": 12.6025,
      "step": 7180
    },
    {
      "epoch": 7.19,
      "grad_norm": 8730.748046875,
      "learning_rate": 6.60423116615067e-05,
      "loss": 18.535,
      "step": 7181
    },
    {
      "epoch": 7.19,
      "grad_norm": 1145.6641845703125,
      "learning_rate": 6.603715170278638e-05,
      "loss": 14.4472,
      "step": 7182
    },
    {
      "epoch": 7.19,
      "grad_norm": 1258.8785400390625,
      "learning_rate": 6.603199174406605e-05,
      "loss": 12.7472,
      "step": 7183
    },
    {
      "epoch": 7.19,
      "grad_norm": 2643.005859375,
      "learning_rate": 6.602683178534573e-05,
      "loss": 12.7977,
      "step": 7184
    },
    {
      "epoch": 7.19,
      "grad_norm": 8939.35546875,
      "learning_rate": 6.602167182662539e-05,
      "loss": 12.6679,
      "step": 7185
    },
    {
      "epoch": 7.19,
      "grad_norm": 9606.8828125,
      "learning_rate": 6.601651186790506e-05,
      "loss": 12.7744,
      "step": 7186
    },
    {
      "epoch": 7.19,
      "grad_norm": 7739.783203125,
      "learning_rate": 6.601135190918472e-05,
      "loss": 13.0956,
      "step": 7187
    },
    {
      "epoch": 7.2,
      "grad_norm": 3473.492431640625,
      "learning_rate": 6.60061919504644e-05,
      "loss": 13.7332,
      "step": 7188
    },
    {
      "epoch": 7.2,
      "grad_norm": 2288.236083984375,
      "learning_rate": 6.600103199174407e-05,
      "loss": 11.9387,
      "step": 7189
    },
    {
      "epoch": 7.2,
      "grad_norm": 11163.2041015625,
      "learning_rate": 6.599587203302375e-05,
      "loss": 21.5283,
      "step": 7190
    },
    {
      "epoch": 7.2,
      "grad_norm": 1938.62109375,
      "learning_rate": 6.599071207430341e-05,
      "loss": 11.2315,
      "step": 7191
    },
    {
      "epoch": 7.2,
      "grad_norm": 847.3612670898438,
      "learning_rate": 6.598555211558308e-05,
      "loss": 15.3889,
      "step": 7192
    },
    {
      "epoch": 7.2,
      "grad_norm": 8035.59375,
      "learning_rate": 6.598039215686274e-05,
      "loss": 13.9621,
      "step": 7193
    },
    {
      "epoch": 7.2,
      "grad_norm": 9443.5791015625,
      "learning_rate": 6.597523219814242e-05,
      "loss": 13.7824,
      "step": 7194
    },
    {
      "epoch": 7.2,
      "grad_norm": 4893.2314453125,
      "learning_rate": 6.597007223942209e-05,
      "loss": 15.583,
      "step": 7195
    },
    {
      "epoch": 7.2,
      "grad_norm": 2947.79345703125,
      "learning_rate": 6.596491228070177e-05,
      "loss": 11.862,
      "step": 7196
    },
    {
      "epoch": 7.2,
      "grad_norm": 2288.662353515625,
      "learning_rate": 6.595975232198143e-05,
      "loss": 14.2121,
      "step": 7197
    },
    {
      "epoch": 7.21,
      "grad_norm": 4664.15283203125,
      "learning_rate": 6.595459236326109e-05,
      "loss": 12.2223,
      "step": 7198
    },
    {
      "epoch": 7.21,
      "grad_norm": 1452.772216796875,
      "learning_rate": 6.594943240454076e-05,
      "loss": 11.2322,
      "step": 7199
    },
    {
      "epoch": 7.21,
      "grad_norm": 2490.314453125,
      "learning_rate": 6.594427244582044e-05,
      "loss": 13.7689,
      "step": 7200
    },
    {
      "epoch": 7.21,
      "grad_norm": 7489.734375,
      "learning_rate": 6.593911248710011e-05,
      "loss": 17.1458,
      "step": 7201
    },
    {
      "epoch": 7.21,
      "grad_norm": 6599.81201171875,
      "learning_rate": 6.593395252837977e-05,
      "loss": 11.0625,
      "step": 7202
    },
    {
      "epoch": 7.21,
      "grad_norm": 1031.0401611328125,
      "learning_rate": 6.592879256965945e-05,
      "loss": 12.9585,
      "step": 7203
    },
    {
      "epoch": 7.21,
      "grad_norm": 17305.6796875,
      "learning_rate": 6.592363261093911e-05,
      "loss": 14.337,
      "step": 7204
    },
    {
      "epoch": 7.21,
      "grad_norm": 2078.207275390625,
      "learning_rate": 6.591847265221878e-05,
      "loss": 11.9248,
      "step": 7205
    },
    {
      "epoch": 7.21,
      "grad_norm": 14599.3857421875,
      "learning_rate": 6.591331269349846e-05,
      "loss": 11.0518,
      "step": 7206
    },
    {
      "epoch": 7.21,
      "grad_norm": 6104.1142578125,
      "learning_rate": 6.590815273477813e-05,
      "loss": 19.7956,
      "step": 7207
    },
    {
      "epoch": 7.22,
      "grad_norm": 9554.6416015625,
      "learning_rate": 6.59029927760578e-05,
      "loss": 32.3083,
      "step": 7208
    },
    {
      "epoch": 7.22,
      "grad_norm": 6823.66357421875,
      "learning_rate": 6.589783281733747e-05,
      "loss": 10.9762,
      "step": 7209
    },
    {
      "epoch": 7.22,
      "grad_norm": 2832.17724609375,
      "learning_rate": 6.589267285861713e-05,
      "loss": 12.2631,
      "step": 7210
    },
    {
      "epoch": 7.22,
      "grad_norm": 5503.625,
      "learning_rate": 6.58875128998968e-05,
      "loss": 13.7069,
      "step": 7211
    },
    {
      "epoch": 7.22,
      "grad_norm": 17316.12109375,
      "learning_rate": 6.588235294117648e-05,
      "loss": 11.0245,
      "step": 7212
    },
    {
      "epoch": 7.22,
      "grad_norm": 2688.745361328125,
      "learning_rate": 6.587719298245615e-05,
      "loss": 16.8441,
      "step": 7213
    },
    {
      "epoch": 7.22,
      "grad_norm": 1463.7860107421875,
      "learning_rate": 6.587203302373581e-05,
      "loss": 13.5049,
      "step": 7214
    },
    {
      "epoch": 7.22,
      "grad_norm": 11905.0830078125,
      "learning_rate": 6.586687306501547e-05,
      "loss": 12.9167,
      "step": 7215
    },
    {
      "epoch": 7.22,
      "grad_norm": 5324.94287109375,
      "learning_rate": 6.586171310629515e-05,
      "loss": 18.5249,
      "step": 7216
    },
    {
      "epoch": 7.22,
      "grad_norm": 5538.8271484375,
      "learning_rate": 6.585655314757482e-05,
      "loss": 13.1392,
      "step": 7217
    },
    {
      "epoch": 7.23,
      "grad_norm": 1319.36865234375,
      "learning_rate": 6.58513931888545e-05,
      "loss": 14.9163,
      "step": 7218
    },
    {
      "epoch": 7.23,
      "grad_norm": 11744.7392578125,
      "learning_rate": 6.584623323013416e-05,
      "loss": 16.2167,
      "step": 7219
    },
    {
      "epoch": 7.23,
      "grad_norm": 19607.974609375,
      "learning_rate": 6.584107327141383e-05,
      "loss": 13.1687,
      "step": 7220
    },
    {
      "epoch": 7.23,
      "grad_norm": 14183.7607421875,
      "learning_rate": 6.58359133126935e-05,
      "loss": 14.7425,
      "step": 7221
    },
    {
      "epoch": 7.23,
      "grad_norm": 4863.92041015625,
      "learning_rate": 6.583075335397317e-05,
      "loss": 13.763,
      "step": 7222
    },
    {
      "epoch": 7.23,
      "grad_norm": 2590.9775390625,
      "learning_rate": 6.582559339525284e-05,
      "loss": 12.5045,
      "step": 7223
    },
    {
      "epoch": 7.23,
      "grad_norm": 6297.24560546875,
      "learning_rate": 6.582043343653252e-05,
      "loss": 11.039,
      "step": 7224
    },
    {
      "epoch": 7.23,
      "grad_norm": 6063.06591796875,
      "learning_rate": 6.581527347781218e-05,
      "loss": 13.305,
      "step": 7225
    },
    {
      "epoch": 7.23,
      "grad_norm": 24211.17578125,
      "learning_rate": 6.581011351909185e-05,
      "loss": 14.6274,
      "step": 7226
    },
    {
      "epoch": 7.23,
      "grad_norm": 2535.0537109375,
      "learning_rate": 6.580495356037151e-05,
      "loss": 15.1283,
      "step": 7227
    },
    {
      "epoch": 7.24,
      "grad_norm": 12163.78125,
      "learning_rate": 6.579979360165119e-05,
      "loss": 12.5199,
      "step": 7228
    },
    {
      "epoch": 7.24,
      "grad_norm": 5625.62255859375,
      "learning_rate": 6.579463364293086e-05,
      "loss": 16.277,
      "step": 7229
    },
    {
      "epoch": 7.24,
      "grad_norm": 15357.935546875,
      "learning_rate": 6.578947368421054e-05,
      "loss": 11.7,
      "step": 7230
    },
    {
      "epoch": 7.24,
      "grad_norm": 12609.66796875,
      "learning_rate": 6.57843137254902e-05,
      "loss": 15.3706,
      "step": 7231
    },
    {
      "epoch": 7.24,
      "grad_norm": 5273.359375,
      "learning_rate": 6.577915376676987e-05,
      "loss": 12.8188,
      "step": 7232
    },
    {
      "epoch": 7.24,
      "grad_norm": 8599.0654296875,
      "learning_rate": 6.577399380804953e-05,
      "loss": 14.0607,
      "step": 7233
    },
    {
      "epoch": 7.24,
      "grad_norm": 12455.9111328125,
      "learning_rate": 6.576883384932921e-05,
      "loss": 22.148,
      "step": 7234
    },
    {
      "epoch": 7.24,
      "grad_norm": 3946.513427734375,
      "learning_rate": 6.576367389060888e-05,
      "loss": 12.9955,
      "step": 7235
    },
    {
      "epoch": 7.24,
      "grad_norm": 2723.725830078125,
      "learning_rate": 6.575851393188854e-05,
      "loss": 18.0983,
      "step": 7236
    },
    {
      "epoch": 7.24,
      "grad_norm": 2817.14892578125,
      "learning_rate": 6.575335397316822e-05,
      "loss": 12.7196,
      "step": 7237
    },
    {
      "epoch": 7.25,
      "grad_norm": 3039.125732421875,
      "learning_rate": 6.574819401444788e-05,
      "loss": 12.4846,
      "step": 7238
    },
    {
      "epoch": 7.25,
      "grad_norm": 7554.45458984375,
      "learning_rate": 6.574303405572755e-05,
      "loss": 13.9099,
      "step": 7239
    },
    {
      "epoch": 7.25,
      "grad_norm": 5996.7666015625,
      "learning_rate": 6.573787409700723e-05,
      "loss": 13.1834,
      "step": 7240
    },
    {
      "epoch": 7.25,
      "grad_norm": 5069.2890625,
      "learning_rate": 6.57327141382869e-05,
      "loss": 12.9649,
      "step": 7241
    },
    {
      "epoch": 7.25,
      "grad_norm": 1068.3387451171875,
      "learning_rate": 6.572755417956656e-05,
      "loss": 12.2555,
      "step": 7242
    },
    {
      "epoch": 7.25,
      "grad_norm": 5534.30419921875,
      "learning_rate": 6.572239422084624e-05,
      "loss": 13.487,
      "step": 7243
    },
    {
      "epoch": 7.25,
      "grad_norm": 5056.0576171875,
      "learning_rate": 6.57172342621259e-05,
      "loss": 11.6667,
      "step": 7244
    },
    {
      "epoch": 7.25,
      "grad_norm": 13519.833984375,
      "learning_rate": 6.571207430340557e-05,
      "loss": 12.3764,
      "step": 7245
    },
    {
      "epoch": 7.25,
      "grad_norm": 5863.564453125,
      "learning_rate": 6.570691434468525e-05,
      "loss": 14.9544,
      "step": 7246
    },
    {
      "epoch": 7.25,
      "grad_norm": 562.4871826171875,
      "learning_rate": 6.570175438596492e-05,
      "loss": 14.3445,
      "step": 7247
    },
    {
      "epoch": 7.26,
      "grad_norm": 2310.646240234375,
      "learning_rate": 6.569659442724458e-05,
      "loss": 12.6926,
      "step": 7248
    },
    {
      "epoch": 7.26,
      "grad_norm": 6372.48583984375,
      "learning_rate": 6.569143446852426e-05,
      "loss": 14.7138,
      "step": 7249
    },
    {
      "epoch": 7.26,
      "grad_norm": 1454.93017578125,
      "learning_rate": 6.568627450980392e-05,
      "loss": 11.6873,
      "step": 7250
    },
    {
      "epoch": 7.26,
      "grad_norm": 20848.57421875,
      "learning_rate": 6.56811145510836e-05,
      "loss": 12.0463,
      "step": 7251
    },
    {
      "epoch": 7.26,
      "grad_norm": 13867.966796875,
      "learning_rate": 6.567595459236327e-05,
      "loss": 21.1532,
      "step": 7252
    },
    {
      "epoch": 7.26,
      "grad_norm": 7381.45361328125,
      "learning_rate": 6.567079463364293e-05,
      "loss": 10.7027,
      "step": 7253
    },
    {
      "epoch": 7.26,
      "grad_norm": 11671.0166015625,
      "learning_rate": 6.56656346749226e-05,
      "loss": 29.1698,
      "step": 7254
    },
    {
      "epoch": 7.26,
      "grad_norm": 603.5245971679688,
      "learning_rate": 6.566047471620227e-05,
      "loss": 11.9883,
      "step": 7255
    },
    {
      "epoch": 7.26,
      "grad_norm": 7533.85302734375,
      "learning_rate": 6.565531475748194e-05,
      "loss": 15.0888,
      "step": 7256
    },
    {
      "epoch": 7.26,
      "grad_norm": 11222.97265625,
      "learning_rate": 6.565015479876161e-05,
      "loss": 21.9844,
      "step": 7257
    },
    {
      "epoch": 7.27,
      "grad_norm": 1914.021484375,
      "learning_rate": 6.564499484004129e-05,
      "loss": 18.0318,
      "step": 7258
    },
    {
      "epoch": 7.27,
      "grad_norm": 13688.86328125,
      "learning_rate": 6.563983488132095e-05,
      "loss": 17.8561,
      "step": 7259
    },
    {
      "epoch": 7.27,
      "grad_norm": 17304.203125,
      "learning_rate": 6.563467492260062e-05,
      "loss": 21.6429,
      "step": 7260
    },
    {
      "epoch": 7.27,
      "grad_norm": 1717.8314208984375,
      "learning_rate": 6.562951496388029e-05,
      "loss": 15.1836,
      "step": 7261
    },
    {
      "epoch": 7.27,
      "grad_norm": 4258.611328125,
      "learning_rate": 6.562435500515996e-05,
      "loss": 14.6621,
      "step": 7262
    },
    {
      "epoch": 7.27,
      "grad_norm": 5275.6416015625,
      "learning_rate": 6.561919504643963e-05,
      "loss": 12.0147,
      "step": 7263
    },
    {
      "epoch": 7.27,
      "grad_norm": 1443.359619140625,
      "learning_rate": 6.561403508771931e-05,
      "loss": 16.8124,
      "step": 7264
    },
    {
      "epoch": 7.27,
      "grad_norm": 12627.908203125,
      "learning_rate": 6.560887512899897e-05,
      "loss": 12.7995,
      "step": 7265
    },
    {
      "epoch": 7.27,
      "grad_norm": 1835.502197265625,
      "learning_rate": 6.560371517027864e-05,
      "loss": 15.6606,
      "step": 7266
    },
    {
      "epoch": 7.27,
      "grad_norm": 19796.69921875,
      "learning_rate": 6.55985552115583e-05,
      "loss": 13.5256,
      "step": 7267
    },
    {
      "epoch": 7.28,
      "grad_norm": 429284.75,
      "learning_rate": 6.559339525283798e-05,
      "loss": 14.2011,
      "step": 7268
    },
    {
      "epoch": 7.28,
      "grad_norm": 3474.485595703125,
      "learning_rate": 6.558823529411765e-05,
      "loss": 12.3969,
      "step": 7269
    },
    {
      "epoch": 7.28,
      "grad_norm": 3722.3486328125,
      "learning_rate": 6.558307533539732e-05,
      "loss": 17.1366,
      "step": 7270
    },
    {
      "epoch": 7.28,
      "grad_norm": 12793.4296875,
      "learning_rate": 6.557791537667699e-05,
      "loss": 11.2836,
      "step": 7271
    },
    {
      "epoch": 7.28,
      "grad_norm": 13150.7451171875,
      "learning_rate": 6.557275541795665e-05,
      "loss": 19.9896,
      "step": 7272
    },
    {
      "epoch": 7.28,
      "grad_norm": 4300.349609375,
      "learning_rate": 6.556759545923633e-05,
      "loss": 10.1046,
      "step": 7273
    },
    {
      "epoch": 7.28,
      "grad_norm": 10995.6953125,
      "learning_rate": 6.5562435500516e-05,
      "loss": 13.3352,
      "step": 7274
    },
    {
      "epoch": 7.28,
      "grad_norm": 11089.478515625,
      "learning_rate": 6.555727554179567e-05,
      "loss": 21.6005,
      "step": 7275
    },
    {
      "epoch": 7.28,
      "grad_norm": 1113.7060546875,
      "learning_rate": 6.555211558307534e-05,
      "loss": 11.1799,
      "step": 7276
    },
    {
      "epoch": 7.28,
      "grad_norm": 2727.292724609375,
      "learning_rate": 6.554695562435501e-05,
      "loss": 12.4349,
      "step": 7277
    },
    {
      "epoch": 7.29,
      "grad_norm": 6411.81640625,
      "learning_rate": 6.554179566563467e-05,
      "loss": 23.2247,
      "step": 7278
    },
    {
      "epoch": 7.29,
      "grad_norm": 4346.51123046875,
      "learning_rate": 6.553663570691435e-05,
      "loss": 13.3727,
      "step": 7279
    },
    {
      "epoch": 7.29,
      "grad_norm": 1169.1690673828125,
      "learning_rate": 6.553147574819402e-05,
      "loss": 14.3633,
      "step": 7280
    },
    {
      "epoch": 7.29,
      "grad_norm": 9111.380859375,
      "learning_rate": 6.55263157894737e-05,
      "loss": 11.8091,
      "step": 7281
    },
    {
      "epoch": 7.29,
      "grad_norm": 2027.6409912109375,
      "learning_rate": 6.552115583075336e-05,
      "loss": 13.6162,
      "step": 7282
    },
    {
      "epoch": 7.29,
      "grad_norm": 14660.2685546875,
      "learning_rate": 6.551599587203303e-05,
      "loss": 19.1705,
      "step": 7283
    },
    {
      "epoch": 7.29,
      "grad_norm": 7182.85205078125,
      "learning_rate": 6.551083591331269e-05,
      "loss": 26.645,
      "step": 7284
    },
    {
      "epoch": 7.29,
      "grad_norm": 5340.5166015625,
      "learning_rate": 6.550567595459237e-05,
      "loss": 15.4753,
      "step": 7285
    },
    {
      "epoch": 7.29,
      "grad_norm": 4656.17138671875,
      "learning_rate": 6.550051599587204e-05,
      "loss": 14.1556,
      "step": 7286
    },
    {
      "epoch": 7.29,
      "grad_norm": 11424.724609375,
      "learning_rate": 6.54953560371517e-05,
      "loss": 20.2414,
      "step": 7287
    },
    {
      "epoch": 7.3,
      "grad_norm": 3778.057861328125,
      "learning_rate": 6.549019607843138e-05,
      "loss": 13.772,
      "step": 7288
    },
    {
      "epoch": 7.3,
      "grad_norm": 1723.1151123046875,
      "learning_rate": 6.548503611971104e-05,
      "loss": 13.8254,
      "step": 7289
    },
    {
      "epoch": 7.3,
      "grad_norm": 5951.85302734375,
      "learning_rate": 6.547987616099071e-05,
      "loss": 16.4685,
      "step": 7290
    },
    {
      "epoch": 7.3,
      "grad_norm": 19824.205078125,
      "learning_rate": 6.547471620227039e-05,
      "loss": 16.9547,
      "step": 7291
    },
    {
      "epoch": 7.3,
      "grad_norm": 34063.48828125,
      "learning_rate": 6.546955624355006e-05,
      "loss": 11.7386,
      "step": 7292
    },
    {
      "epoch": 7.3,
      "grad_norm": 5665.7451171875,
      "learning_rate": 6.546439628482972e-05,
      "loss": 12.7485,
      "step": 7293
    },
    {
      "epoch": 7.3,
      "grad_norm": 2701.28369140625,
      "learning_rate": 6.54592363261094e-05,
      "loss": 14.4036,
      "step": 7294
    },
    {
      "epoch": 7.3,
      "grad_norm": 5706.7685546875,
      "learning_rate": 6.545407636738906e-05,
      "loss": 18.1455,
      "step": 7295
    },
    {
      "epoch": 7.3,
      "grad_norm": 4322.263671875,
      "learning_rate": 6.544891640866873e-05,
      "loss": 12.8636,
      "step": 7296
    },
    {
      "epoch": 7.3,
      "grad_norm": 1254.686767578125,
      "learning_rate": 6.54437564499484e-05,
      "loss": 22.0551,
      "step": 7297
    },
    {
      "epoch": 7.31,
      "grad_norm": 800.7202758789062,
      "learning_rate": 6.543859649122808e-05,
      "loss": 13.1392,
      "step": 7298
    },
    {
      "epoch": 7.31,
      "grad_norm": 6862.5380859375,
      "learning_rate": 6.543343653250774e-05,
      "loss": 19.323,
      "step": 7299
    },
    {
      "epoch": 7.31,
      "grad_norm": 2867.527587890625,
      "learning_rate": 6.542827657378742e-05,
      "loss": 21.5957,
      "step": 7300
    },
    {
      "epoch": 7.31,
      "grad_norm": 7610.44091796875,
      "learning_rate": 6.542311661506708e-05,
      "loss": 11.3336,
      "step": 7301
    },
    {
      "epoch": 7.31,
      "grad_norm": 1860.070556640625,
      "learning_rate": 6.541795665634675e-05,
      "loss": 16.8703,
      "step": 7302
    },
    {
      "epoch": 7.31,
      "grad_norm": 34398.28125,
      "learning_rate": 6.541279669762643e-05,
      "loss": 19.5797,
      "step": 7303
    },
    {
      "epoch": 7.31,
      "grad_norm": 11088.6142578125,
      "learning_rate": 6.54076367389061e-05,
      "loss": 16.8231,
      "step": 7304
    },
    {
      "epoch": 7.31,
      "grad_norm": 9625.52734375,
      "learning_rate": 6.540247678018576e-05,
      "loss": 12.6129,
      "step": 7305
    },
    {
      "epoch": 7.31,
      "grad_norm": 2465.59912109375,
      "learning_rate": 6.539731682146542e-05,
      "loss": 11.2177,
      "step": 7306
    },
    {
      "epoch": 7.31,
      "grad_norm": 20672.1953125,
      "learning_rate": 6.53921568627451e-05,
      "loss": 17.4222,
      "step": 7307
    },
    {
      "epoch": 7.32,
      "grad_norm": 13133.77734375,
      "learning_rate": 6.538699690402477e-05,
      "loss": 11.1464,
      "step": 7308
    },
    {
      "epoch": 7.32,
      "grad_norm": 1341.6627197265625,
      "learning_rate": 6.538183694530445e-05,
      "loss": 15.5514,
      "step": 7309
    },
    {
      "epoch": 7.32,
      "grad_norm": 20510.814453125,
      "learning_rate": 6.537667698658411e-05,
      "loss": 16.3,
      "step": 7310
    },
    {
      "epoch": 7.32,
      "grad_norm": 9849.8583984375,
      "learning_rate": 6.537151702786378e-05,
      "loss": 19.0671,
      "step": 7311
    },
    {
      "epoch": 7.32,
      "grad_norm": 4920.14208984375,
      "learning_rate": 6.536635706914344e-05,
      "loss": 13.6964,
      "step": 7312
    },
    {
      "epoch": 7.32,
      "grad_norm": 3463.08203125,
      "learning_rate": 6.536119711042312e-05,
      "loss": 12.4431,
      "step": 7313
    },
    {
      "epoch": 7.32,
      "grad_norm": 1123.84521484375,
      "learning_rate": 6.535603715170279e-05,
      "loss": 15.8447,
      "step": 7314
    },
    {
      "epoch": 7.32,
      "grad_norm": 1359.9903564453125,
      "learning_rate": 6.535087719298247e-05,
      "loss": 17.1087,
      "step": 7315
    },
    {
      "epoch": 7.32,
      "grad_norm": 1388.3463134765625,
      "learning_rate": 6.534571723426213e-05,
      "loss": 13.3732,
      "step": 7316
    },
    {
      "epoch": 7.32,
      "grad_norm": 1187.7181396484375,
      "learning_rate": 6.53405572755418e-05,
      "loss": 14.478,
      "step": 7317
    },
    {
      "epoch": 7.33,
      "grad_norm": 2807.774658203125,
      "learning_rate": 6.533539731682146e-05,
      "loss": 15.9167,
      "step": 7318
    },
    {
      "epoch": 7.33,
      "grad_norm": 2094.233642578125,
      "learning_rate": 6.533023735810114e-05,
      "loss": 17.9995,
      "step": 7319
    },
    {
      "epoch": 7.33,
      "grad_norm": 74917.7890625,
      "learning_rate": 6.532507739938081e-05,
      "loss": 20.6388,
      "step": 7320
    },
    {
      "epoch": 7.33,
      "grad_norm": 1500.904541015625,
      "learning_rate": 6.531991744066049e-05,
      "loss": 11.0406,
      "step": 7321
    },
    {
      "epoch": 7.33,
      "grad_norm": 21578.18359375,
      "learning_rate": 6.531475748194015e-05,
      "loss": 15.0393,
      "step": 7322
    },
    {
      "epoch": 7.33,
      "grad_norm": 5928.47412109375,
      "learning_rate": 6.530959752321981e-05,
      "loss": 16.1397,
      "step": 7323
    },
    {
      "epoch": 7.33,
      "grad_norm": 1765.82958984375,
      "learning_rate": 6.530443756449948e-05,
      "loss": 12.5074,
      "step": 7324
    },
    {
      "epoch": 7.33,
      "grad_norm": 1534.8992919921875,
      "learning_rate": 6.529927760577916e-05,
      "loss": 12.9404,
      "step": 7325
    },
    {
      "epoch": 7.33,
      "grad_norm": 2805.1904296875,
      "learning_rate": 6.529411764705883e-05,
      "loss": 13.5531,
      "step": 7326
    },
    {
      "epoch": 7.33,
      "grad_norm": 17259.970703125,
      "learning_rate": 6.528895768833849e-05,
      "loss": 15.4842,
      "step": 7327
    },
    {
      "epoch": 7.34,
      "grad_norm": 7201.38134765625,
      "learning_rate": 6.528379772961817e-05,
      "loss": 12.2559,
      "step": 7328
    },
    {
      "epoch": 7.34,
      "grad_norm": 27295.466796875,
      "learning_rate": 6.527863777089783e-05,
      "loss": 12.7324,
      "step": 7329
    },
    {
      "epoch": 7.34,
      "grad_norm": 32377.693359375,
      "learning_rate": 6.52734778121775e-05,
      "loss": 13.7133,
      "step": 7330
    },
    {
      "epoch": 7.34,
      "grad_norm": 922.46435546875,
      "learning_rate": 6.526831785345718e-05,
      "loss": 11.873,
      "step": 7331
    },
    {
      "epoch": 7.34,
      "grad_norm": 3659.246826171875,
      "learning_rate": 6.526315789473685e-05,
      "loss": 16.3336,
      "step": 7332
    },
    {
      "epoch": 7.34,
      "grad_norm": 5232.615234375,
      "learning_rate": 6.525799793601651e-05,
      "loss": 11.1418,
      "step": 7333
    },
    {
      "epoch": 7.34,
      "grad_norm": 4349.353515625,
      "learning_rate": 6.525283797729619e-05,
      "loss": 12.7558,
      "step": 7334
    },
    {
      "epoch": 7.34,
      "grad_norm": 10902.25390625,
      "learning_rate": 6.524767801857585e-05,
      "loss": 18.3337,
      "step": 7335
    },
    {
      "epoch": 7.34,
      "grad_norm": 5820.65771484375,
      "learning_rate": 6.524251805985552e-05,
      "loss": 14.0069,
      "step": 7336
    },
    {
      "epoch": 7.34,
      "grad_norm": 1282.118896484375,
      "learning_rate": 6.52373581011352e-05,
      "loss": 11.7668,
      "step": 7337
    },
    {
      "epoch": 7.35,
      "grad_norm": 6555.53515625,
      "learning_rate": 6.523219814241487e-05,
      "loss": 12.1742,
      "step": 7338
    },
    {
      "epoch": 7.35,
      "grad_norm": 3292.617919921875,
      "learning_rate": 6.522703818369453e-05,
      "loss": 13.1433,
      "step": 7339
    },
    {
      "epoch": 7.35,
      "grad_norm": 5228.30029296875,
      "learning_rate": 6.522187822497421e-05,
      "loss": 14.1855,
      "step": 7340
    },
    {
      "epoch": 7.35,
      "grad_norm": 1339.4229736328125,
      "learning_rate": 6.521671826625387e-05,
      "loss": 14.1178,
      "step": 7341
    },
    {
      "epoch": 7.35,
      "grad_norm": 2560.969970703125,
      "learning_rate": 6.521155830753354e-05,
      "loss": 13.1656,
      "step": 7342
    },
    {
      "epoch": 7.35,
      "grad_norm": 1891.5584716796875,
      "learning_rate": 6.520639834881322e-05,
      "loss": 11.5877,
      "step": 7343
    },
    {
      "epoch": 7.35,
      "grad_norm": 14146.123046875,
      "learning_rate": 6.520123839009288e-05,
      "loss": 13.7871,
      "step": 7344
    },
    {
      "epoch": 7.35,
      "grad_norm": 21040.869140625,
      "learning_rate": 6.519607843137255e-05,
      "loss": 18.6929,
      "step": 7345
    },
    {
      "epoch": 7.35,
      "grad_norm": 22302.720703125,
      "learning_rate": 6.519091847265221e-05,
      "loss": 13.204,
      "step": 7346
    },
    {
      "epoch": 7.35,
      "grad_norm": 51339.16015625,
      "learning_rate": 6.518575851393189e-05,
      "loss": 14.67,
      "step": 7347
    },
    {
      "epoch": 7.36,
      "grad_norm": 13607.240234375,
      "learning_rate": 6.518059855521156e-05,
      "loss": 20.7975,
      "step": 7348
    },
    {
      "epoch": 7.36,
      "grad_norm": 60632.53515625,
      "learning_rate": 6.517543859649124e-05,
      "loss": 16.0134,
      "step": 7349
    },
    {
      "epoch": 7.36,
      "grad_norm": 4212.482421875,
      "learning_rate": 6.51702786377709e-05,
      "loss": 18.2977,
      "step": 7350
    },
    {
      "epoch": 7.36,
      "grad_norm": 7666.7548828125,
      "learning_rate": 6.516511867905057e-05,
      "loss": 15.2789,
      "step": 7351
    },
    {
      "epoch": 7.36,
      "grad_norm": 2799.83740234375,
      "learning_rate": 6.515995872033023e-05,
      "loss": 10.7247,
      "step": 7352
    },
    {
      "epoch": 7.36,
      "grad_norm": 1855.63623046875,
      "learning_rate": 6.515479876160991e-05,
      "loss": 10.0782,
      "step": 7353
    },
    {
      "epoch": 7.36,
      "grad_norm": 13949.8173828125,
      "learning_rate": 6.514963880288958e-05,
      "loss": 15.0901,
      "step": 7354
    },
    {
      "epoch": 7.36,
      "grad_norm": 1307.4552001953125,
      "learning_rate": 6.514447884416926e-05,
      "loss": 12.3714,
      "step": 7355
    },
    {
      "epoch": 7.36,
      "grad_norm": 1415.4083251953125,
      "learning_rate": 6.513931888544892e-05,
      "loss": 13.0062,
      "step": 7356
    },
    {
      "epoch": 7.36,
      "grad_norm": 8239.2197265625,
      "learning_rate": 6.513415892672859e-05,
      "loss": 15.569,
      "step": 7357
    },
    {
      "epoch": 7.37,
      "grad_norm": 8641.5498046875,
      "learning_rate": 6.512899896800825e-05,
      "loss": 16.7991,
      "step": 7358
    },
    {
      "epoch": 7.37,
      "grad_norm": 27834.736328125,
      "learning_rate": 6.512383900928793e-05,
      "loss": 16.0893,
      "step": 7359
    },
    {
      "epoch": 7.37,
      "grad_norm": 33145.1640625,
      "learning_rate": 6.51186790505676e-05,
      "loss": 14.2572,
      "step": 7360
    },
    {
      "epoch": 7.37,
      "grad_norm": 6543.736328125,
      "learning_rate": 6.511351909184726e-05,
      "loss": 15.0126,
      "step": 7361
    },
    {
      "epoch": 7.37,
      "grad_norm": 6403.3955078125,
      "learning_rate": 6.510835913312694e-05,
      "loss": 15.9467,
      "step": 7362
    },
    {
      "epoch": 7.37,
      "grad_norm": 14049.5439453125,
      "learning_rate": 6.51031991744066e-05,
      "loss": 18.4547,
      "step": 7363
    },
    {
      "epoch": 7.37,
      "grad_norm": 11527.150390625,
      "learning_rate": 6.509803921568627e-05,
      "loss": 15.0679,
      "step": 7364
    },
    {
      "epoch": 7.37,
      "grad_norm": 11943.72265625,
      "learning_rate": 6.509287925696595e-05,
      "loss": 12.216,
      "step": 7365
    },
    {
      "epoch": 7.37,
      "grad_norm": 5323.0693359375,
      "learning_rate": 6.508771929824562e-05,
      "loss": 21.852,
      "step": 7366
    },
    {
      "epoch": 7.37,
      "grad_norm": 119769.0,
      "learning_rate": 6.508255933952528e-05,
      "loss": 13.7321,
      "step": 7367
    },
    {
      "epoch": 7.38,
      "grad_norm": 1408.490234375,
      "learning_rate": 6.507739938080496e-05,
      "loss": 14.3177,
      "step": 7368
    },
    {
      "epoch": 7.38,
      "grad_norm": 2767.802734375,
      "learning_rate": 6.507223942208462e-05,
      "loss": 14.1696,
      "step": 7369
    },
    {
      "epoch": 7.38,
      "grad_norm": 4730.54638671875,
      "learning_rate": 6.50670794633643e-05,
      "loss": 22.5396,
      "step": 7370
    },
    {
      "epoch": 7.38,
      "grad_norm": 5836.06640625,
      "learning_rate": 6.506191950464397e-05,
      "loss": 12.9862,
      "step": 7371
    },
    {
      "epoch": 7.38,
      "grad_norm": 37304.0703125,
      "learning_rate": 6.505675954592364e-05,
      "loss": 17.1409,
      "step": 7372
    },
    {
      "epoch": 7.38,
      "grad_norm": 2449.466064453125,
      "learning_rate": 6.50515995872033e-05,
      "loss": 12.9342,
      "step": 7373
    },
    {
      "epoch": 7.38,
      "grad_norm": 7584.52392578125,
      "learning_rate": 6.504643962848298e-05,
      "loss": 19.5136,
      "step": 7374
    },
    {
      "epoch": 7.38,
      "grad_norm": 7037.61083984375,
      "learning_rate": 6.504127966976264e-05,
      "loss": 12.6089,
      "step": 7375
    },
    {
      "epoch": 7.38,
      "grad_norm": 4143.47802734375,
      "learning_rate": 6.503611971104231e-05,
      "loss": 13.1409,
      "step": 7376
    },
    {
      "epoch": 7.38,
      "grad_norm": 17483.2109375,
      "learning_rate": 6.503095975232199e-05,
      "loss": 13.9583,
      "step": 7377
    },
    {
      "epoch": 7.39,
      "grad_norm": 3998.57470703125,
      "learning_rate": 6.502579979360165e-05,
      "loss": 15.3189,
      "step": 7378
    },
    {
      "epoch": 7.39,
      "grad_norm": 6291.47216796875,
      "learning_rate": 6.502063983488132e-05,
      "loss": 11.5284,
      "step": 7379
    },
    {
      "epoch": 7.39,
      "grad_norm": 878.2954711914062,
      "learning_rate": 6.501547987616098e-05,
      "loss": 12.1567,
      "step": 7380
    },
    {
      "epoch": 7.39,
      "grad_norm": 3453.335693359375,
      "learning_rate": 6.501031991744066e-05,
      "loss": 29.3582,
      "step": 7381
    },
    {
      "epoch": 7.39,
      "grad_norm": 6043.0810546875,
      "learning_rate": 6.500515995872033e-05,
      "loss": 14.1118,
      "step": 7382
    },
    {
      "epoch": 7.39,
      "grad_norm": 8065.939453125,
      "learning_rate": 6.500000000000001e-05,
      "loss": 16.2868,
      "step": 7383
    },
    {
      "epoch": 7.39,
      "grad_norm": 535.281982421875,
      "learning_rate": 6.499484004127967e-05,
      "loss": 12.2454,
      "step": 7384
    },
    {
      "epoch": 7.39,
      "grad_norm": 821.4850463867188,
      "learning_rate": 6.498968008255934e-05,
      "loss": 11.2488,
      "step": 7385
    },
    {
      "epoch": 7.39,
      "grad_norm": 2032.3291015625,
      "learning_rate": 6.4984520123839e-05,
      "loss": 18.1913,
      "step": 7386
    },
    {
      "epoch": 7.39,
      "grad_norm": 87294.0,
      "learning_rate": 6.497936016511869e-05,
      "loss": 14.6712,
      "step": 7387
    },
    {
      "epoch": 7.4,
      "grad_norm": 5346.4326171875,
      "learning_rate": 6.497420020639835e-05,
      "loss": 16.1547,
      "step": 7388
    },
    {
      "epoch": 7.4,
      "grad_norm": 14238.60546875,
      "learning_rate": 6.496904024767803e-05,
      "loss": 14.8286,
      "step": 7389
    },
    {
      "epoch": 7.4,
      "grad_norm": 2127.589599609375,
      "learning_rate": 6.496388028895769e-05,
      "loss": 13.2247,
      "step": 7390
    },
    {
      "epoch": 7.4,
      "grad_norm": 2636.666015625,
      "learning_rate": 6.495872033023736e-05,
      "loss": 24.5953,
      "step": 7391
    },
    {
      "epoch": 7.4,
      "grad_norm": 2465.55224609375,
      "learning_rate": 6.495356037151702e-05,
      "loss": 12.2731,
      "step": 7392
    },
    {
      "epoch": 7.4,
      "grad_norm": 1072.5303955078125,
      "learning_rate": 6.494840041279671e-05,
      "loss": 12.3024,
      "step": 7393
    },
    {
      "epoch": 7.4,
      "grad_norm": 5982.6259765625,
      "learning_rate": 6.494324045407637e-05,
      "loss": 14.0938,
      "step": 7394
    },
    {
      "epoch": 7.4,
      "grad_norm": 11834.4921875,
      "learning_rate": 6.493808049535603e-05,
      "loss": 14.8312,
      "step": 7395
    },
    {
      "epoch": 7.4,
      "grad_norm": 383.98773193359375,
      "learning_rate": 6.493292053663571e-05,
      "loss": 18.1815,
      "step": 7396
    },
    {
      "epoch": 7.4,
      "grad_norm": 902.4490356445312,
      "learning_rate": 6.492776057791537e-05,
      "loss": 14.2728,
      "step": 7397
    },
    {
      "epoch": 7.41,
      "grad_norm": 4012.755126953125,
      "learning_rate": 6.492260061919504e-05,
      "loss": 14.0226,
      "step": 7398
    },
    {
      "epoch": 7.41,
      "grad_norm": 15078.0859375,
      "learning_rate": 6.491744066047472e-05,
      "loss": 21.8831,
      "step": 7399
    },
    {
      "epoch": 7.41,
      "grad_norm": 796.194091796875,
      "learning_rate": 6.49122807017544e-05,
      "loss": 11.9699,
      "step": 7400
    },
    {
      "epoch": 7.41,
      "grad_norm": 3473.894287109375,
      "learning_rate": 6.490712074303405e-05,
      "loss": 15.0135,
      "step": 7401
    },
    {
      "epoch": 7.41,
      "grad_norm": 999.7501831054688,
      "learning_rate": 6.490196078431373e-05,
      "loss": 11.5189,
      "step": 7402
    },
    {
      "epoch": 7.41,
      "grad_norm": 1484.546142578125,
      "learning_rate": 6.489680082559339e-05,
      "loss": 21.626,
      "step": 7403
    },
    {
      "epoch": 7.41,
      "grad_norm": 7047.96533203125,
      "learning_rate": 6.489164086687308e-05,
      "loss": 15.2669,
      "step": 7404
    },
    {
      "epoch": 7.41,
      "grad_norm": 3691.21142578125,
      "learning_rate": 6.488648090815274e-05,
      "loss": 15.4285,
      "step": 7405
    },
    {
      "epoch": 7.41,
      "grad_norm": 11083.9755859375,
      "learning_rate": 6.488132094943241e-05,
      "loss": 17.2719,
      "step": 7406
    },
    {
      "epoch": 7.41,
      "grad_norm": 1275.1685791015625,
      "learning_rate": 6.487616099071207e-05,
      "loss": 10.4924,
      "step": 7407
    },
    {
      "epoch": 7.42,
      "grad_norm": 32078.3203125,
      "learning_rate": 6.487100103199175e-05,
      "loss": 23.3866,
      "step": 7408
    },
    {
      "epoch": 7.42,
      "grad_norm": 411.7781066894531,
      "learning_rate": 6.486584107327141e-05,
      "loss": 11.2661,
      "step": 7409
    },
    {
      "epoch": 7.42,
      "grad_norm": 1394.1234130859375,
      "learning_rate": 6.48606811145511e-05,
      "loss": 12.1224,
      "step": 7410
    },
    {
      "epoch": 7.42,
      "grad_norm": 5268.5078125,
      "learning_rate": 6.485552115583076e-05,
      "loss": 16.2787,
      "step": 7411
    },
    {
      "epoch": 7.42,
      "grad_norm": 7710.15625,
      "learning_rate": 6.485036119711042e-05,
      "loss": 14.5156,
      "step": 7412
    },
    {
      "epoch": 7.42,
      "grad_norm": 2852.908203125,
      "learning_rate": 6.48452012383901e-05,
      "loss": 18.3473,
      "step": 7413
    },
    {
      "epoch": 7.42,
      "grad_norm": 1177.85205078125,
      "learning_rate": 6.484004127966976e-05,
      "loss": 11.538,
      "step": 7414
    },
    {
      "epoch": 7.42,
      "grad_norm": 14600.44921875,
      "learning_rate": 6.483488132094943e-05,
      "loss": 11.4787,
      "step": 7415
    },
    {
      "epoch": 7.42,
      "grad_norm": 3323.671630859375,
      "learning_rate": 6.48297213622291e-05,
      "loss": 14.5823,
      "step": 7416
    },
    {
      "epoch": 7.42,
      "grad_norm": 6838.560546875,
      "learning_rate": 6.482456140350878e-05,
      "loss": 12.4031,
      "step": 7417
    },
    {
      "epoch": 7.43,
      "grad_norm": 2598.579833984375,
      "learning_rate": 6.481940144478844e-05,
      "loss": 15.4901,
      "step": 7418
    },
    {
      "epoch": 7.43,
      "grad_norm": 2890.1875,
      "learning_rate": 6.481424148606811e-05,
      "loss": 12.6445,
      "step": 7419
    },
    {
      "epoch": 7.43,
      "grad_norm": 4920.3349609375,
      "learning_rate": 6.480908152734778e-05,
      "loss": 11.9119,
      "step": 7420
    },
    {
      "epoch": 7.43,
      "grad_norm": 3767.44970703125,
      "learning_rate": 6.480392156862746e-05,
      "loss": 15.9841,
      "step": 7421
    },
    {
      "epoch": 7.43,
      "grad_norm": 11585.3515625,
      "learning_rate": 6.479876160990712e-05,
      "loss": 16.5816,
      "step": 7422
    },
    {
      "epoch": 7.43,
      "grad_norm": 11032.6083984375,
      "learning_rate": 6.47936016511868e-05,
      "loss": 23.2158,
      "step": 7423
    },
    {
      "epoch": 7.43,
      "grad_norm": 2338.8984375,
      "learning_rate": 6.478844169246646e-05,
      "loss": 18.7184,
      "step": 7424
    },
    {
      "epoch": 7.43,
      "grad_norm": 3237.53662109375,
      "learning_rate": 6.478328173374613e-05,
      "loss": 18.076,
      "step": 7425
    },
    {
      "epoch": 7.43,
      "grad_norm": 27472.03125,
      "learning_rate": 6.47781217750258e-05,
      "loss": 17.8698,
      "step": 7426
    },
    {
      "epoch": 7.43,
      "grad_norm": 26332.033203125,
      "learning_rate": 6.477296181630548e-05,
      "loss": 16.6114,
      "step": 7427
    },
    {
      "epoch": 7.44,
      "grad_norm": 344.9670104980469,
      "learning_rate": 6.476780185758514e-05,
      "loss": 10.8256,
      "step": 7428
    },
    {
      "epoch": 7.44,
      "grad_norm": 509.4158020019531,
      "learning_rate": 6.476264189886482e-05,
      "loss": 11.8818,
      "step": 7429
    },
    {
      "epoch": 7.44,
      "grad_norm": 6356.26220703125,
      "learning_rate": 6.475748194014448e-05,
      "loss": 22.4129,
      "step": 7430
    },
    {
      "epoch": 7.44,
      "grad_norm": 1224.2366943359375,
      "learning_rate": 6.475232198142414e-05,
      "loss": 19.1595,
      "step": 7431
    },
    {
      "epoch": 7.44,
      "grad_norm": 1451.6932373046875,
      "learning_rate": 6.474716202270383e-05,
      "loss": 18.1466,
      "step": 7432
    },
    {
      "epoch": 7.44,
      "grad_norm": 4421.0068359375,
      "learning_rate": 6.474200206398349e-05,
      "loss": 18.3767,
      "step": 7433
    },
    {
      "epoch": 7.44,
      "grad_norm": 4543.576171875,
      "learning_rate": 6.473684210526316e-05,
      "loss": 14.8426,
      "step": 7434
    },
    {
      "epoch": 7.44,
      "grad_norm": 11417.671875,
      "learning_rate": 6.473168214654283e-05,
      "loss": 12.7381,
      "step": 7435
    },
    {
      "epoch": 7.44,
      "grad_norm": 359556.1875,
      "learning_rate": 6.47265221878225e-05,
      "loss": 33.7547,
      "step": 7436
    },
    {
      "epoch": 7.44,
      "grad_norm": 12273.224609375,
      "learning_rate": 6.472136222910216e-05,
      "loss": 12.9096,
      "step": 7437
    },
    {
      "epoch": 7.45,
      "grad_norm": 2061.182861328125,
      "learning_rate": 6.471620227038185e-05,
      "loss": 16.0601,
      "step": 7438
    },
    {
      "epoch": 7.45,
      "grad_norm": 47624.0546875,
      "learning_rate": 6.471104231166151e-05,
      "loss": 14.2606,
      "step": 7439
    },
    {
      "epoch": 7.45,
      "grad_norm": 9501.6435546875,
      "learning_rate": 6.470588235294118e-05,
      "loss": 16.4743,
      "step": 7440
    },
    {
      "epoch": 7.45,
      "grad_norm": 12238.443359375,
      "learning_rate": 6.470072239422085e-05,
      "loss": 14.3193,
      "step": 7441
    },
    {
      "epoch": 7.45,
      "grad_norm": 2610.58154296875,
      "learning_rate": 6.469556243550052e-05,
      "loss": 12.9606,
      "step": 7442
    },
    {
      "epoch": 7.45,
      "grad_norm": 1123.2071533203125,
      "learning_rate": 6.469040247678018e-05,
      "loss": 13.0335,
      "step": 7443
    },
    {
      "epoch": 7.45,
      "grad_norm": 2579.804443359375,
      "learning_rate": 6.468524251805987e-05,
      "loss": 15.9676,
      "step": 7444
    },
    {
      "epoch": 7.45,
      "grad_norm": 3136.132080078125,
      "learning_rate": 6.468008255933953e-05,
      "loss": 15.8883,
      "step": 7445
    },
    {
      "epoch": 7.45,
      "grad_norm": 2173.917724609375,
      "learning_rate": 6.46749226006192e-05,
      "loss": 11.7304,
      "step": 7446
    },
    {
      "epoch": 7.45,
      "grad_norm": 3082.83642578125,
      "learning_rate": 6.466976264189887e-05,
      "loss": 12.1922,
      "step": 7447
    },
    {
      "epoch": 7.46,
      "grad_norm": 8547.7705078125,
      "learning_rate": 6.466460268317853e-05,
      "loss": 12.0546,
      "step": 7448
    },
    {
      "epoch": 7.46,
      "grad_norm": 18203.71875,
      "learning_rate": 6.465944272445821e-05,
      "loss": 12.9143,
      "step": 7449
    },
    {
      "epoch": 7.46,
      "grad_norm": 9807.9033203125,
      "learning_rate": 6.465428276573788e-05,
      "loss": 15.9437,
      "step": 7450
    },
    {
      "epoch": 7.46,
      "grad_norm": 4946.44482421875,
      "learning_rate": 6.464912280701755e-05,
      "loss": 12.9694,
      "step": 7451
    },
    {
      "epoch": 7.46,
      "grad_norm": 30475.7109375,
      "learning_rate": 6.464396284829721e-05,
      "loss": 22.2584,
      "step": 7452
    },
    {
      "epoch": 7.46,
      "grad_norm": 1664.738525390625,
      "learning_rate": 6.463880288957689e-05,
      "loss": 14.2002,
      "step": 7453
    },
    {
      "epoch": 7.46,
      "grad_norm": 3869.887939453125,
      "learning_rate": 6.463364293085655e-05,
      "loss": 16.7713,
      "step": 7454
    },
    {
      "epoch": 7.46,
      "grad_norm": 13767.8173828125,
      "learning_rate": 6.462848297213623e-05,
      "loss": 11.4525,
      "step": 7455
    },
    {
      "epoch": 7.46,
      "grad_norm": 7286.26611328125,
      "learning_rate": 6.46233230134159e-05,
      "loss": 13.0346,
      "step": 7456
    },
    {
      "epoch": 7.46,
      "grad_norm": 3341.052001953125,
      "learning_rate": 6.461816305469557e-05,
      "loss": 15.4794,
      "step": 7457
    },
    {
      "epoch": 7.47,
      "grad_norm": 1975.3228759765625,
      "learning_rate": 6.461300309597523e-05,
      "loss": 12.3847,
      "step": 7458
    },
    {
      "epoch": 7.47,
      "grad_norm": 6354.009765625,
      "learning_rate": 6.46078431372549e-05,
      "loss": 18.2325,
      "step": 7459
    },
    {
      "epoch": 7.47,
      "grad_norm": 8296.57421875,
      "learning_rate": 6.460268317853458e-05,
      "loss": 15.4994,
      "step": 7460
    },
    {
      "epoch": 7.47,
      "grad_norm": 827.359619140625,
      "learning_rate": 6.459752321981425e-05,
      "loss": 13.5699,
      "step": 7461
    },
    {
      "epoch": 7.47,
      "grad_norm": 2947.25830078125,
      "learning_rate": 6.459236326109392e-05,
      "loss": 10.2116,
      "step": 7462
    },
    {
      "epoch": 7.47,
      "grad_norm": 18969.66796875,
      "learning_rate": 6.458720330237359e-05,
      "loss": 12.173,
      "step": 7463
    },
    {
      "epoch": 7.47,
      "grad_norm": 7326.90234375,
      "learning_rate": 6.458204334365325e-05,
      "loss": 12.8213,
      "step": 7464
    },
    {
      "epoch": 7.47,
      "grad_norm": 2715.13525390625,
      "learning_rate": 6.457688338493293e-05,
      "loss": 15.3477,
      "step": 7465
    },
    {
      "epoch": 7.47,
      "grad_norm": 5926.52490234375,
      "learning_rate": 6.45717234262126e-05,
      "loss": 12.0295,
      "step": 7466
    },
    {
      "epoch": 7.47,
      "grad_norm": 4720.98828125,
      "learning_rate": 6.456656346749226e-05,
      "loss": 13.6458,
      "step": 7467
    },
    {
      "epoch": 7.48,
      "grad_norm": 3073.108154296875,
      "learning_rate": 6.456140350877194e-05,
      "loss": 10.9263,
      "step": 7468
    },
    {
      "epoch": 7.48,
      "grad_norm": 4497.1845703125,
      "learning_rate": 6.45562435500516e-05,
      "loss": 12.3224,
      "step": 7469
    },
    {
      "epoch": 7.48,
      "grad_norm": 15394.0517578125,
      "learning_rate": 6.455108359133127e-05,
      "loss": 17.9244,
      "step": 7470
    },
    {
      "epoch": 7.48,
      "grad_norm": 4236.51611328125,
      "learning_rate": 6.454592363261093e-05,
      "loss": 15.0414,
      "step": 7471
    },
    {
      "epoch": 7.48,
      "grad_norm": 8820.0634765625,
      "learning_rate": 6.454076367389062e-05,
      "loss": 14.1495,
      "step": 7472
    },
    {
      "epoch": 7.48,
      "grad_norm": 4773.42822265625,
      "learning_rate": 6.453560371517028e-05,
      "loss": 19.9758,
      "step": 7473
    },
    {
      "epoch": 7.48,
      "grad_norm": 6389.1484375,
      "learning_rate": 6.453044375644996e-05,
      "loss": 13.5527,
      "step": 7474
    },
    {
      "epoch": 7.48,
      "grad_norm": 4776.26904296875,
      "learning_rate": 6.452528379772962e-05,
      "loss": 17.9227,
      "step": 7475
    },
    {
      "epoch": 7.48,
      "grad_norm": 3429.818115234375,
      "learning_rate": 6.452012383900929e-05,
      "loss": 14.1246,
      "step": 7476
    },
    {
      "epoch": 7.48,
      "grad_norm": 7377.25634765625,
      "learning_rate": 6.451496388028897e-05,
      "loss": 17.7955,
      "step": 7477
    },
    {
      "epoch": 7.49,
      "grad_norm": 737.3397827148438,
      "learning_rate": 6.450980392156864e-05,
      "loss": 17.6842,
      "step": 7478
    },
    {
      "epoch": 7.49,
      "grad_norm": 7427.47802734375,
      "learning_rate": 6.45046439628483e-05,
      "loss": 25.0722,
      "step": 7479
    },
    {
      "epoch": 7.49,
      "grad_norm": 14866.1953125,
      "learning_rate": 6.449948400412798e-05,
      "loss": 12.3359,
      "step": 7480
    },
    {
      "epoch": 7.49,
      "grad_norm": 7977.8486328125,
      "learning_rate": 6.449432404540764e-05,
      "loss": 14.0982,
      "step": 7481
    },
    {
      "epoch": 7.49,
      "grad_norm": 10725.4072265625,
      "learning_rate": 6.448916408668731e-05,
      "loss": 14.2638,
      "step": 7482
    },
    {
      "epoch": 7.49,
      "grad_norm": 8857.45703125,
      "learning_rate": 6.448400412796699e-05,
      "loss": 21.0667,
      "step": 7483
    },
    {
      "epoch": 7.49,
      "grad_norm": 7810.08837890625,
      "learning_rate": 6.447884416924665e-05,
      "loss": 15.8027,
      "step": 7484
    },
    {
      "epoch": 7.49,
      "grad_norm": 2759.834716796875,
      "learning_rate": 6.447368421052632e-05,
      "loss": 12.3937,
      "step": 7485
    },
    {
      "epoch": 7.49,
      "grad_norm": 3928.458740234375,
      "learning_rate": 6.446852425180598e-05,
      "loss": 12.4618,
      "step": 7486
    },
    {
      "epoch": 7.49,
      "grad_norm": 7500.783203125,
      "learning_rate": 6.446336429308566e-05,
      "loss": 14.0959,
      "step": 7487
    },
    {
      "epoch": 7.5,
      "grad_norm": 3279.962158203125,
      "learning_rate": 6.445820433436533e-05,
      "loss": 14.9508,
      "step": 7488
    },
    {
      "epoch": 7.5,
      "grad_norm": 4204.80810546875,
      "learning_rate": 6.4453044375645e-05,
      "loss": 13.0979,
      "step": 7489
    },
    {
      "epoch": 7.5,
      "grad_norm": 2065.70458984375,
      "learning_rate": 6.444788441692467e-05,
      "loss": 20.1637,
      "step": 7490
    },
    {
      "epoch": 7.5,
      "grad_norm": 46037.1875,
      "learning_rate": 6.444272445820434e-05,
      "loss": 19.5784,
      "step": 7491
    },
    {
      "epoch": 7.5,
      "grad_norm": 819.4845581054688,
      "learning_rate": 6.4437564499484e-05,
      "loss": 13.5987,
      "step": 7492
    },
    {
      "epoch": 7.5,
      "grad_norm": 518.3704833984375,
      "learning_rate": 6.443240454076368e-05,
      "loss": 17.9547,
      "step": 7493
    },
    {
      "epoch": 7.5,
      "grad_norm": 23811.1484375,
      "learning_rate": 6.442724458204335e-05,
      "loss": 19.6168,
      "step": 7494
    },
    {
      "epoch": 7.5,
      "grad_norm": 1366.25830078125,
      "learning_rate": 6.442208462332303e-05,
      "loss": 13.7748,
      "step": 7495
    },
    {
      "epoch": 7.5,
      "grad_norm": 4859.28662109375,
      "learning_rate": 6.441692466460269e-05,
      "loss": 12.1479,
      "step": 7496
    },
    {
      "epoch": 7.5,
      "grad_norm": 8151.5361328125,
      "learning_rate": 6.441176470588236e-05,
      "loss": 14.1644,
      "step": 7497
    },
    {
      "epoch": 7.51,
      "grad_norm": 1110.9296875,
      "learning_rate": 6.440660474716202e-05,
      "loss": 10.6677,
      "step": 7498
    },
    {
      "epoch": 7.51,
      "grad_norm": 26293.958984375,
      "learning_rate": 6.44014447884417e-05,
      "loss": 13.5125,
      "step": 7499
    },
    {
      "epoch": 7.51,
      "grad_norm": 3449.92138671875,
      "learning_rate": 6.439628482972137e-05,
      "loss": 17.458,
      "step": 7500
    },
    {
      "epoch": 7.51,
      "grad_norm": 9217.7734375,
      "learning_rate": 6.439112487100103e-05,
      "loss": 15.7412,
      "step": 7501
    },
    {
      "epoch": 7.51,
      "grad_norm": 13906.35546875,
      "learning_rate": 6.43859649122807e-05,
      "loss": 14.7673,
      "step": 7502
    },
    {
      "epoch": 7.51,
      "grad_norm": 7584.79833984375,
      "learning_rate": 6.438080495356037e-05,
      "loss": 17.5161,
      "step": 7503
    },
    {
      "epoch": 7.51,
      "grad_norm": 511.0133972167969,
      "learning_rate": 6.437564499484004e-05,
      "loss": 12.9746,
      "step": 7504
    },
    {
      "epoch": 7.51,
      "grad_norm": 5749.35107421875,
      "learning_rate": 6.437048503611972e-05,
      "loss": 15.2499,
      "step": 7505
    },
    {
      "epoch": 7.51,
      "grad_norm": 2520.98876953125,
      "learning_rate": 6.436532507739939e-05,
      "loss": 11.9434,
      "step": 7506
    },
    {
      "epoch": 7.51,
      "grad_norm": 13081.216796875,
      "learning_rate": 6.436016511867905e-05,
      "loss": 13.3765,
      "step": 7507
    },
    {
      "epoch": 7.52,
      "grad_norm": 3338.64501953125,
      "learning_rate": 6.435500515995873e-05,
      "loss": 11.4193,
      "step": 7508
    },
    {
      "epoch": 7.52,
      "grad_norm": 9222.2099609375,
      "learning_rate": 6.434984520123839e-05,
      "loss": 11.9978,
      "step": 7509
    },
    {
      "epoch": 7.52,
      "grad_norm": 10223.1123046875,
      "learning_rate": 6.434468524251806e-05,
      "loss": 23.9513,
      "step": 7510
    },
    {
      "epoch": 7.52,
      "grad_norm": 681.6715087890625,
      "learning_rate": 6.433952528379774e-05,
      "loss": 11.3124,
      "step": 7511
    },
    {
      "epoch": 7.52,
      "grad_norm": 1822.9359130859375,
      "learning_rate": 6.433436532507741e-05,
      "loss": 11.5175,
      "step": 7512
    },
    {
      "epoch": 7.52,
      "grad_norm": 832.6510009765625,
      "learning_rate": 6.432920536635707e-05,
      "loss": 14.148,
      "step": 7513
    },
    {
      "epoch": 7.52,
      "grad_norm": 915.0066528320312,
      "learning_rate": 6.432404540763675e-05,
      "loss": 12.3837,
      "step": 7514
    },
    {
      "epoch": 7.52,
      "grad_norm": 3204.59912109375,
      "learning_rate": 6.431888544891641e-05,
      "loss": 12.7136,
      "step": 7515
    },
    {
      "epoch": 7.52,
      "grad_norm": 5797.02392578125,
      "learning_rate": 6.431372549019608e-05,
      "loss": 18.7973,
      "step": 7516
    },
    {
      "epoch": 7.52,
      "grad_norm": 4022.542724609375,
      "learning_rate": 6.430856553147576e-05,
      "loss": 13.6891,
      "step": 7517
    },
    {
      "epoch": 7.53,
      "grad_norm": 1656.574462890625,
      "learning_rate": 6.430340557275543e-05,
      "loss": 10.9178,
      "step": 7518
    },
    {
      "epoch": 7.53,
      "grad_norm": 3795.623779296875,
      "learning_rate": 6.429824561403509e-05,
      "loss": 13.2472,
      "step": 7519
    },
    {
      "epoch": 7.53,
      "grad_norm": 7996.41357421875,
      "learning_rate": 6.429308565531475e-05,
      "loss": 18.1978,
      "step": 7520
    },
    {
      "epoch": 7.53,
      "grad_norm": 3475.5498046875,
      "learning_rate": 6.428792569659443e-05,
      "loss": 10.6694,
      "step": 7521
    },
    {
      "epoch": 7.53,
      "grad_norm": 1784.9404296875,
      "learning_rate": 6.42827657378741e-05,
      "loss": 11.0497,
      "step": 7522
    },
    {
      "epoch": 7.53,
      "grad_norm": 3912.19873046875,
      "learning_rate": 6.427760577915378e-05,
      "loss": 14.2971,
      "step": 7523
    },
    {
      "epoch": 7.53,
      "grad_norm": 2808.801513671875,
      "learning_rate": 6.427244582043344e-05,
      "loss": 9.7338,
      "step": 7524
    },
    {
      "epoch": 7.53,
      "grad_norm": 1432.7783203125,
      "learning_rate": 6.426728586171311e-05,
      "loss": 14.3237,
      "step": 7525
    },
    {
      "epoch": 7.53,
      "grad_norm": 4309.5615234375,
      "learning_rate": 6.426212590299277e-05,
      "loss": 14.9113,
      "step": 7526
    },
    {
      "epoch": 7.53,
      "grad_norm": 5805.359375,
      "learning_rate": 6.425696594427245e-05,
      "loss": 15.4224,
      "step": 7527
    },
    {
      "epoch": 7.54,
      "grad_norm": 26366.189453125,
      "learning_rate": 6.425180598555212e-05,
      "loss": 16.0035,
      "step": 7528
    },
    {
      "epoch": 7.54,
      "grad_norm": 1066.6619873046875,
      "learning_rate": 6.42466460268318e-05,
      "loss": 15.0029,
      "step": 7529
    },
    {
      "epoch": 7.54,
      "grad_norm": 2566.81982421875,
      "learning_rate": 6.424148606811146e-05,
      "loss": 13.9455,
      "step": 7530
    },
    {
      "epoch": 7.54,
      "grad_norm": 34072.63671875,
      "learning_rate": 6.423632610939113e-05,
      "loss": 27.444,
      "step": 7531
    },
    {
      "epoch": 7.54,
      "grad_norm": 26001.82421875,
      "learning_rate": 6.423116615067079e-05,
      "loss": 16.3343,
      "step": 7532
    },
    {
      "epoch": 7.54,
      "grad_norm": 2620.3935546875,
      "learning_rate": 6.422600619195047e-05,
      "loss": 13.3736,
      "step": 7533
    },
    {
      "epoch": 7.54,
      "grad_norm": 2308.12353515625,
      "learning_rate": 6.422084623323014e-05,
      "loss": 13.2437,
      "step": 7534
    },
    {
      "epoch": 7.54,
      "grad_norm": 10641.19921875,
      "learning_rate": 6.421568627450982e-05,
      "loss": 12.2543,
      "step": 7535
    },
    {
      "epoch": 7.54,
      "grad_norm": 23305.208984375,
      "learning_rate": 6.421052631578948e-05,
      "loss": 16.8349,
      "step": 7536
    },
    {
      "epoch": 7.54,
      "grad_norm": 819.5599975585938,
      "learning_rate": 6.420536635706914e-05,
      "loss": 14.0812,
      "step": 7537
    },
    {
      "epoch": 7.55,
      "grad_norm": 1442.217529296875,
      "learning_rate": 6.420020639834881e-05,
      "loss": 12.7992,
      "step": 7538
    },
    {
      "epoch": 7.55,
      "grad_norm": 6779.21630859375,
      "learning_rate": 6.419504643962849e-05,
      "loss": 12.4735,
      "step": 7539
    },
    {
      "epoch": 7.55,
      "grad_norm": 4496.7978515625,
      "learning_rate": 6.418988648090816e-05,
      "loss": 12.0896,
      "step": 7540
    },
    {
      "epoch": 7.55,
      "grad_norm": 11006.8212890625,
      "learning_rate": 6.418472652218782e-05,
      "loss": 11.1919,
      "step": 7541
    },
    {
      "epoch": 7.55,
      "grad_norm": 25942.037109375,
      "learning_rate": 6.41795665634675e-05,
      "loss": 13.0433,
      "step": 7542
    },
    {
      "epoch": 7.55,
      "grad_norm": 3894.81396484375,
      "learning_rate": 6.417440660474716e-05,
      "loss": 16.0182,
      "step": 7543
    },
    {
      "epoch": 7.55,
      "grad_norm": 1001.20556640625,
      "learning_rate": 6.416924664602683e-05,
      "loss": 11.8139,
      "step": 7544
    },
    {
      "epoch": 7.55,
      "grad_norm": 2425.32568359375,
      "learning_rate": 6.416408668730651e-05,
      "loss": 16.253,
      "step": 7545
    },
    {
      "epoch": 7.55,
      "grad_norm": 3046.333740234375,
      "learning_rate": 6.415892672858618e-05,
      "loss": 14.5536,
      "step": 7546
    },
    {
      "epoch": 7.55,
      "grad_norm": 2927.106689453125,
      "learning_rate": 6.415376676986584e-05,
      "loss": 12.9144,
      "step": 7547
    },
    {
      "epoch": 7.56,
      "grad_norm": 9461.8564453125,
      "learning_rate": 6.414860681114552e-05,
      "loss": 15.3496,
      "step": 7548
    },
    {
      "epoch": 7.56,
      "grad_norm": 5237.3701171875,
      "learning_rate": 6.414344685242518e-05,
      "loss": 13.514,
      "step": 7549
    },
    {
      "epoch": 7.56,
      "grad_norm": 9416.716796875,
      "learning_rate": 6.413828689370485e-05,
      "loss": 13.5211,
      "step": 7550
    },
    {
      "epoch": 7.56,
      "grad_norm": 15795.876953125,
      "learning_rate": 6.413312693498453e-05,
      "loss": 14.4903,
      "step": 7551
    },
    {
      "epoch": 7.56,
      "grad_norm": 5693.68310546875,
      "learning_rate": 6.41279669762642e-05,
      "loss": 11.8749,
      "step": 7552
    },
    {
      "epoch": 7.56,
      "grad_norm": 22893.10546875,
      "learning_rate": 6.412280701754386e-05,
      "loss": 26.9485,
      "step": 7553
    },
    {
      "epoch": 7.56,
      "grad_norm": 5716.603515625,
      "learning_rate": 6.411764705882354e-05,
      "loss": 11.4149,
      "step": 7554
    },
    {
      "epoch": 7.56,
      "grad_norm": 5417.48291015625,
      "learning_rate": 6.41124871001032e-05,
      "loss": 15.4031,
      "step": 7555
    },
    {
      "epoch": 7.56,
      "grad_norm": 3464.55419921875,
      "learning_rate": 6.410732714138287e-05,
      "loss": 15.3383,
      "step": 7556
    },
    {
      "epoch": 7.56,
      "grad_norm": 21411.517578125,
      "learning_rate": 6.410216718266255e-05,
      "loss": 11.1048,
      "step": 7557
    },
    {
      "epoch": 7.57,
      "grad_norm": 15543.9404296875,
      "learning_rate": 6.409700722394221e-05,
      "loss": 12.4914,
      "step": 7558
    },
    {
      "epoch": 7.57,
      "grad_norm": 70151.9609375,
      "learning_rate": 6.409184726522188e-05,
      "loss": 23.84,
      "step": 7559
    },
    {
      "epoch": 7.57,
      "grad_norm": 25729.46484375,
      "learning_rate": 6.408668730650154e-05,
      "loss": 20.27,
      "step": 7560
    },
    {
      "epoch": 7.57,
      "grad_norm": 31017.53125,
      "learning_rate": 6.408152734778122e-05,
      "loss": 18.1558,
      "step": 7561
    },
    {
      "epoch": 7.57,
      "grad_norm": 2945.00830078125,
      "learning_rate": 6.407636738906089e-05,
      "loss": 12.8794,
      "step": 7562
    },
    {
      "epoch": 7.57,
      "grad_norm": 19722.837890625,
      "learning_rate": 6.407120743034057e-05,
      "loss": 20.5445,
      "step": 7563
    },
    {
      "epoch": 7.57,
      "grad_norm": 11262.2431640625,
      "learning_rate": 6.406604747162023e-05,
      "loss": 19.9305,
      "step": 7564
    },
    {
      "epoch": 7.57,
      "grad_norm": 1740.0364990234375,
      "learning_rate": 6.40608875128999e-05,
      "loss": 11.6597,
      "step": 7565
    },
    {
      "epoch": 7.57,
      "grad_norm": 6882.02587890625,
      "learning_rate": 6.405572755417956e-05,
      "loss": 15.04,
      "step": 7566
    },
    {
      "epoch": 7.57,
      "grad_norm": 2232.5107421875,
      "learning_rate": 6.405056759545924e-05,
      "loss": 18.1215,
      "step": 7567
    },
    {
      "epoch": 7.58,
      "grad_norm": 11220.353515625,
      "learning_rate": 6.404540763673891e-05,
      "loss": 15.2532,
      "step": 7568
    },
    {
      "epoch": 7.58,
      "grad_norm": 859.52734375,
      "learning_rate": 6.404024767801859e-05,
      "loss": 10.472,
      "step": 7569
    },
    {
      "epoch": 7.58,
      "grad_norm": 3153.820068359375,
      "learning_rate": 6.403508771929825e-05,
      "loss": 14.5337,
      "step": 7570
    },
    {
      "epoch": 7.58,
      "grad_norm": 4710.28271484375,
      "learning_rate": 6.402992776057792e-05,
      "loss": 17.5398,
      "step": 7571
    },
    {
      "epoch": 7.58,
      "grad_norm": 3431.820068359375,
      "learning_rate": 6.402476780185758e-05,
      "loss": 14.8032,
      "step": 7572
    },
    {
      "epoch": 7.58,
      "grad_norm": 2407.16845703125,
      "learning_rate": 6.401960784313726e-05,
      "loss": 12.8237,
      "step": 7573
    },
    {
      "epoch": 7.58,
      "grad_norm": 3469.8662109375,
      "learning_rate": 6.401444788441693e-05,
      "loss": 11.78,
      "step": 7574
    },
    {
      "epoch": 7.58,
      "grad_norm": 16033.5927734375,
      "learning_rate": 6.40092879256966e-05,
      "loss": 17.204,
      "step": 7575
    },
    {
      "epoch": 7.58,
      "grad_norm": 2631.931640625,
      "learning_rate": 6.400412796697627e-05,
      "loss": 12.1477,
      "step": 7576
    },
    {
      "epoch": 7.58,
      "grad_norm": 14686.888671875,
      "learning_rate": 6.399896800825593e-05,
      "loss": 18.2348,
      "step": 7577
    },
    {
      "epoch": 7.59,
      "grad_norm": 523.2498779296875,
      "learning_rate": 6.39938080495356e-05,
      "loss": 12.4734,
      "step": 7578
    },
    {
      "epoch": 7.59,
      "grad_norm": 18654.4296875,
      "learning_rate": 6.398864809081528e-05,
      "loss": 19.6518,
      "step": 7579
    },
    {
      "epoch": 7.59,
      "grad_norm": 4412.673828125,
      "learning_rate": 6.398348813209495e-05,
      "loss": 14.9713,
      "step": 7580
    },
    {
      "epoch": 7.59,
      "grad_norm": 24376.0703125,
      "learning_rate": 6.397832817337461e-05,
      "loss": 19.3178,
      "step": 7581
    },
    {
      "epoch": 7.59,
      "grad_norm": 41505.5234375,
      "learning_rate": 6.397316821465429e-05,
      "loss": 24.255,
      "step": 7582
    },
    {
      "epoch": 7.59,
      "grad_norm": 13060.3466796875,
      "learning_rate": 6.396800825593395e-05,
      "loss": 12.7982,
      "step": 7583
    },
    {
      "epoch": 7.59,
      "grad_norm": 2362.99072265625,
      "learning_rate": 6.396284829721362e-05,
      "loss": 16.0724,
      "step": 7584
    },
    {
      "epoch": 7.59,
      "grad_norm": 8736.787109375,
      "learning_rate": 6.39576883384933e-05,
      "loss": 14.2339,
      "step": 7585
    },
    {
      "epoch": 7.59,
      "grad_norm": 9475.8994140625,
      "learning_rate": 6.395252837977297e-05,
      "loss": 17.6335,
      "step": 7586
    },
    {
      "epoch": 7.59,
      "grad_norm": 18098.041015625,
      "learning_rate": 6.394736842105263e-05,
      "loss": 13.8825,
      "step": 7587
    },
    {
      "epoch": 7.6,
      "grad_norm": 5165.9521484375,
      "learning_rate": 6.394220846233231e-05,
      "loss": 11.1655,
      "step": 7588
    },
    {
      "epoch": 7.6,
      "grad_norm": 23248.45703125,
      "learning_rate": 6.393704850361197e-05,
      "loss": 22.0187,
      "step": 7589
    },
    {
      "epoch": 7.6,
      "grad_norm": 2251.596923828125,
      "learning_rate": 6.393188854489164e-05,
      "loss": 17.4126,
      "step": 7590
    },
    {
      "epoch": 7.6,
      "grad_norm": 47285.74609375,
      "learning_rate": 6.392672858617132e-05,
      "loss": 15.2288,
      "step": 7591
    },
    {
      "epoch": 7.6,
      "grad_norm": 10842.224609375,
      "learning_rate": 6.392156862745098e-05,
      "loss": 17.5212,
      "step": 7592
    },
    {
      "epoch": 7.6,
      "grad_norm": 12392.947265625,
      "learning_rate": 6.391640866873065e-05,
      "loss": 15.7167,
      "step": 7593
    },
    {
      "epoch": 7.6,
      "grad_norm": 71707.7734375,
      "learning_rate": 6.391124871001031e-05,
      "loss": 14.2506,
      "step": 7594
    },
    {
      "epoch": 7.6,
      "grad_norm": 1718.0877685546875,
      "learning_rate": 6.390608875128999e-05,
      "loss": 14.2916,
      "step": 7595
    },
    {
      "epoch": 7.6,
      "grad_norm": 682.5016479492188,
      "learning_rate": 6.390092879256966e-05,
      "loss": 10.3528,
      "step": 7596
    },
    {
      "epoch": 7.6,
      "grad_norm": 918.3692626953125,
      "learning_rate": 6.389576883384934e-05,
      "loss": 12.9693,
      "step": 7597
    },
    {
      "epoch": 7.61,
      "grad_norm": 1015.6743774414062,
      "learning_rate": 6.3890608875129e-05,
      "loss": 16.2983,
      "step": 7598
    },
    {
      "epoch": 7.61,
      "grad_norm": 985.1824340820312,
      "learning_rate": 6.388544891640867e-05,
      "loss": 21.5985,
      "step": 7599
    },
    {
      "epoch": 7.61,
      "grad_norm": 5363.0107421875,
      "learning_rate": 6.388028895768833e-05,
      "loss": 13.9318,
      "step": 7600
    },
    {
      "epoch": 7.61,
      "grad_norm": 8900.8642578125,
      "learning_rate": 6.387512899896801e-05,
      "loss": 12.8554,
      "step": 7601
    },
    {
      "epoch": 7.61,
      "grad_norm": 3798.97705078125,
      "learning_rate": 6.386996904024768e-05,
      "loss": 13.2223,
      "step": 7602
    },
    {
      "epoch": 7.61,
      "grad_norm": 2113.339111328125,
      "learning_rate": 6.386480908152736e-05,
      "loss": 11.9047,
      "step": 7603
    },
    {
      "epoch": 7.61,
      "grad_norm": 8563.314453125,
      "learning_rate": 6.385964912280702e-05,
      "loss": 12.7959,
      "step": 7604
    },
    {
      "epoch": 7.61,
      "grad_norm": 1547.365966796875,
      "learning_rate": 6.38544891640867e-05,
      "loss": 11.5877,
      "step": 7605
    },
    {
      "epoch": 7.61,
      "grad_norm": 3295.527099609375,
      "learning_rate": 6.384932920536635e-05,
      "loss": 13.6378,
      "step": 7606
    },
    {
      "epoch": 7.61,
      "grad_norm": 1987.6151123046875,
      "learning_rate": 6.384416924664603e-05,
      "loss": 14.9787,
      "step": 7607
    },
    {
      "epoch": 7.62,
      "grad_norm": 805.1311645507812,
      "learning_rate": 6.38390092879257e-05,
      "loss": 11.3288,
      "step": 7608
    },
    {
      "epoch": 7.62,
      "grad_norm": 2948.75048828125,
      "learning_rate": 6.383384932920536e-05,
      "loss": 12.3612,
      "step": 7609
    },
    {
      "epoch": 7.62,
      "grad_norm": 6549.849609375,
      "learning_rate": 6.382868937048504e-05,
      "loss": 14.4479,
      "step": 7610
    },
    {
      "epoch": 7.62,
      "grad_norm": 7656.4091796875,
      "learning_rate": 6.38235294117647e-05,
      "loss": 25.8787,
      "step": 7611
    },
    {
      "epoch": 7.62,
      "grad_norm": 40686.21484375,
      "learning_rate": 6.381836945304437e-05,
      "loss": 12.7209,
      "step": 7612
    },
    {
      "epoch": 7.62,
      "grad_norm": 20722.44921875,
      "learning_rate": 6.381320949432405e-05,
      "loss": 16.5961,
      "step": 7613
    },
    {
      "epoch": 7.62,
      "grad_norm": 1553.7200927734375,
      "learning_rate": 6.380804953560372e-05,
      "loss": 11.5852,
      "step": 7614
    },
    {
      "epoch": 7.62,
      "grad_norm": 3656.55126953125,
      "learning_rate": 6.380288957688338e-05,
      "loss": 13.2284,
      "step": 7615
    },
    {
      "epoch": 7.62,
      "grad_norm": 11227.8193359375,
      "learning_rate": 6.379772961816306e-05,
      "loss": 21.8899,
      "step": 7616
    },
    {
      "epoch": 7.62,
      "grad_norm": 6976.650390625,
      "learning_rate": 6.379256965944272e-05,
      "loss": 15.0115,
      "step": 7617
    },
    {
      "epoch": 7.63,
      "grad_norm": 29040.068359375,
      "learning_rate": 6.37874097007224e-05,
      "loss": 17.6229,
      "step": 7618
    },
    {
      "epoch": 7.63,
      "grad_norm": 3133.011474609375,
      "learning_rate": 6.378224974200207e-05,
      "loss": 11.5148,
      "step": 7619
    },
    {
      "epoch": 7.63,
      "grad_norm": 2674.095703125,
      "learning_rate": 6.377708978328174e-05,
      "loss": 17.4649,
      "step": 7620
    },
    {
      "epoch": 7.63,
      "grad_norm": 681.7415161132812,
      "learning_rate": 6.37719298245614e-05,
      "loss": 13.7932,
      "step": 7621
    },
    {
      "epoch": 7.63,
      "grad_norm": 1727.7979736328125,
      "learning_rate": 6.376676986584108e-05,
      "loss": 10.1682,
      "step": 7622
    },
    {
      "epoch": 7.63,
      "grad_norm": 2646.347900390625,
      "learning_rate": 6.376160990712074e-05,
      "loss": 19.3137,
      "step": 7623
    },
    {
      "epoch": 7.63,
      "grad_norm": 3298.693115234375,
      "learning_rate": 6.375644994840041e-05,
      "loss": 11.8415,
      "step": 7624
    },
    {
      "epoch": 7.63,
      "grad_norm": 2443.7587890625,
      "learning_rate": 6.375128998968009e-05,
      "loss": 17.2333,
      "step": 7625
    },
    {
      "epoch": 7.63,
      "grad_norm": 1235.770263671875,
      "learning_rate": 6.374613003095976e-05,
      "loss": 16.0675,
      "step": 7626
    },
    {
      "epoch": 7.63,
      "grad_norm": 1438.713134765625,
      "learning_rate": 6.374097007223942e-05,
      "loss": 15.7736,
      "step": 7627
    },
    {
      "epoch": 7.64,
      "grad_norm": 869.0884399414062,
      "learning_rate": 6.373581011351909e-05,
      "loss": 12.3138,
      "step": 7628
    },
    {
      "epoch": 7.64,
      "grad_norm": 1780.8240966796875,
      "learning_rate": 6.373065015479876e-05,
      "loss": 12.6637,
      "step": 7629
    },
    {
      "epoch": 7.64,
      "grad_norm": 15290.62890625,
      "learning_rate": 6.372549019607843e-05,
      "loss": 15.4607,
      "step": 7630
    },
    {
      "epoch": 7.64,
      "grad_norm": 1723.5672607421875,
      "learning_rate": 6.372033023735811e-05,
      "loss": 13.3375,
      "step": 7631
    },
    {
      "epoch": 7.64,
      "grad_norm": 5361.16015625,
      "learning_rate": 6.371517027863777e-05,
      "loss": 13.0628,
      "step": 7632
    },
    {
      "epoch": 7.64,
      "grad_norm": 640.5717163085938,
      "learning_rate": 6.371001031991744e-05,
      "loss": 13.651,
      "step": 7633
    },
    {
      "epoch": 7.64,
      "grad_norm": 1014.3085327148438,
      "learning_rate": 6.37048503611971e-05,
      "loss": 12.7538,
      "step": 7634
    },
    {
      "epoch": 7.64,
      "grad_norm": 9768.533203125,
      "learning_rate": 6.369969040247678e-05,
      "loss": 13.2719,
      "step": 7635
    },
    {
      "epoch": 7.64,
      "grad_norm": 956.2741088867188,
      "learning_rate": 6.369453044375645e-05,
      "loss": 11.3394,
      "step": 7636
    },
    {
      "epoch": 7.64,
      "grad_norm": 3060.4716796875,
      "learning_rate": 6.368937048503613e-05,
      "loss": 13.8681,
      "step": 7637
    },
    {
      "epoch": 7.65,
      "grad_norm": 1895.0296630859375,
      "learning_rate": 6.368421052631579e-05,
      "loss": 16.734,
      "step": 7638
    },
    {
      "epoch": 7.65,
      "grad_norm": 22374.138671875,
      "learning_rate": 6.367905056759546e-05,
      "loss": 22.9131,
      "step": 7639
    },
    {
      "epoch": 7.65,
      "grad_norm": 16777.3125,
      "learning_rate": 6.367389060887513e-05,
      "loss": 16.213,
      "step": 7640
    },
    {
      "epoch": 7.65,
      "grad_norm": 2996.31884765625,
      "learning_rate": 6.36687306501548e-05,
      "loss": 17.328,
      "step": 7641
    },
    {
      "epoch": 7.65,
      "grad_norm": 5518.49169921875,
      "learning_rate": 6.366357069143447e-05,
      "loss": 19.4325,
      "step": 7642
    },
    {
      "epoch": 7.65,
      "grad_norm": 2073.440185546875,
      "learning_rate": 6.365841073271415e-05,
      "loss": 12.0543,
      "step": 7643
    },
    {
      "epoch": 7.65,
      "grad_norm": 9482.091796875,
      "learning_rate": 6.365325077399381e-05,
      "loss": 11.2393,
      "step": 7644
    },
    {
      "epoch": 7.65,
      "grad_norm": 2649.057373046875,
      "learning_rate": 6.364809081527347e-05,
      "loss": 16.0279,
      "step": 7645
    },
    {
      "epoch": 7.65,
      "grad_norm": 12510.6689453125,
      "learning_rate": 6.364293085655315e-05,
      "loss": 11.9449,
      "step": 7646
    },
    {
      "epoch": 7.65,
      "grad_norm": 6437.3505859375,
      "learning_rate": 6.363777089783282e-05,
      "loss": 15.6499,
      "step": 7647
    },
    {
      "epoch": 7.66,
      "grad_norm": 7992.22119140625,
      "learning_rate": 6.36326109391125e-05,
      "loss": 19.4639,
      "step": 7648
    },
    {
      "epoch": 7.66,
      "grad_norm": 17016.08984375,
      "learning_rate": 6.362745098039216e-05,
      "loss": 17.2566,
      "step": 7649
    },
    {
      "epoch": 7.66,
      "grad_norm": 3084.0556640625,
      "learning_rate": 6.362229102167183e-05,
      "loss": 13.27,
      "step": 7650
    },
    {
      "epoch": 7.66,
      "grad_norm": 7854.47705078125,
      "learning_rate": 6.361713106295149e-05,
      "loss": 16.226,
      "step": 7651
    },
    {
      "epoch": 7.66,
      "grad_norm": 3455.793701171875,
      "learning_rate": 6.361197110423117e-05,
      "loss": 15.4699,
      "step": 7652
    },
    {
      "epoch": 7.66,
      "grad_norm": 1359.8853759765625,
      "learning_rate": 6.360681114551084e-05,
      "loss": 11.2511,
      "step": 7653
    },
    {
      "epoch": 7.66,
      "grad_norm": 6442.21875,
      "learning_rate": 6.360165118679051e-05,
      "loss": 24.8857,
      "step": 7654
    },
    {
      "epoch": 7.66,
      "grad_norm": 1744.038330078125,
      "learning_rate": 6.359649122807018e-05,
      "loss": 15.292,
      "step": 7655
    },
    {
      "epoch": 7.66,
      "grad_norm": 1007.36767578125,
      "learning_rate": 6.359133126934985e-05,
      "loss": 10.5021,
      "step": 7656
    },
    {
      "epoch": 7.66,
      "grad_norm": 6147.38427734375,
      "learning_rate": 6.358617131062951e-05,
      "loss": 13.8928,
      "step": 7657
    },
    {
      "epoch": 7.67,
      "grad_norm": 55509.734375,
      "learning_rate": 6.358101135190919e-05,
      "loss": 12.8666,
      "step": 7658
    },
    {
      "epoch": 7.67,
      "grad_norm": 6342.78857421875,
      "learning_rate": 6.357585139318886e-05,
      "loss": 20.5218,
      "step": 7659
    },
    {
      "epoch": 7.67,
      "grad_norm": 3725.1953125,
      "learning_rate": 6.357069143446853e-05,
      "loss": 16.1539,
      "step": 7660
    },
    {
      "epoch": 7.67,
      "grad_norm": 2408.851318359375,
      "learning_rate": 6.35655314757482e-05,
      "loss": 12.8482,
      "step": 7661
    },
    {
      "epoch": 7.67,
      "grad_norm": 10174.609375,
      "learning_rate": 6.356037151702786e-05,
      "loss": 11.8123,
      "step": 7662
    },
    {
      "epoch": 7.67,
      "grad_norm": 12312.0625,
      "learning_rate": 6.355521155830753e-05,
      "loss": 14.4398,
      "step": 7663
    },
    {
      "epoch": 7.67,
      "grad_norm": 13362.2587890625,
      "learning_rate": 6.35500515995872e-05,
      "loss": 13.6931,
      "step": 7664
    },
    {
      "epoch": 7.67,
      "grad_norm": 1167.9105224609375,
      "learning_rate": 6.354489164086688e-05,
      "loss": 13.7104,
      "step": 7665
    },
    {
      "epoch": 7.67,
      "grad_norm": 5735.13037109375,
      "learning_rate": 6.353973168214654e-05,
      "loss": 16.9482,
      "step": 7666
    },
    {
      "epoch": 7.67,
      "grad_norm": 1455.810546875,
      "learning_rate": 6.353457172342622e-05,
      "loss": 13.2953,
      "step": 7667
    },
    {
      "epoch": 7.68,
      "grad_norm": 2295.01123046875,
      "learning_rate": 6.352941176470588e-05,
      "loss": 10.5409,
      "step": 7668
    },
    {
      "epoch": 7.68,
      "grad_norm": 12693.947265625,
      "learning_rate": 6.352425180598555e-05,
      "loss": 17.6998,
      "step": 7669
    },
    {
      "epoch": 7.68,
      "grad_norm": 1195.439453125,
      "learning_rate": 6.351909184726523e-05,
      "loss": 15.5269,
      "step": 7670
    },
    {
      "epoch": 7.68,
      "grad_norm": 5961.11083984375,
      "learning_rate": 6.35139318885449e-05,
      "loss": 13.9495,
      "step": 7671
    },
    {
      "epoch": 7.68,
      "grad_norm": 2771.78759765625,
      "learning_rate": 6.350877192982456e-05,
      "loss": 14.0488,
      "step": 7672
    },
    {
      "epoch": 7.68,
      "grad_norm": 3139.704345703125,
      "learning_rate": 6.350361197110424e-05,
      "loss": 14.8938,
      "step": 7673
    },
    {
      "epoch": 7.68,
      "grad_norm": 1345.55029296875,
      "learning_rate": 6.34984520123839e-05,
      "loss": 12.7181,
      "step": 7674
    },
    {
      "epoch": 7.68,
      "grad_norm": 14931.3076171875,
      "learning_rate": 6.349329205366357e-05,
      "loss": 18.3145,
      "step": 7675
    },
    {
      "epoch": 7.68,
      "grad_norm": 86787.9765625,
      "learning_rate": 6.348813209494325e-05,
      "loss": 16.105,
      "step": 7676
    },
    {
      "epoch": 7.68,
      "grad_norm": 3964.559814453125,
      "learning_rate": 6.348297213622292e-05,
      "loss": 14.412,
      "step": 7677
    },
    {
      "epoch": 7.69,
      "grad_norm": 876.694091796875,
      "learning_rate": 6.347781217750258e-05,
      "loss": 13.1881,
      "step": 7678
    },
    {
      "epoch": 7.69,
      "grad_norm": 3061.54296875,
      "learning_rate": 6.347265221878226e-05,
      "loss": 11.3745,
      "step": 7679
    },
    {
      "epoch": 7.69,
      "grad_norm": 1607.029541015625,
      "learning_rate": 6.346749226006192e-05,
      "loss": 11.9997,
      "step": 7680
    },
    {
      "epoch": 7.69,
      "grad_norm": 8481.0810546875,
      "learning_rate": 6.346233230134159e-05,
      "loss": 22.9723,
      "step": 7681
    },
    {
      "epoch": 7.69,
      "grad_norm": 50564.96484375,
      "learning_rate": 6.345717234262127e-05,
      "loss": 30.9904,
      "step": 7682
    },
    {
      "epoch": 7.69,
      "grad_norm": 7595.517578125,
      "learning_rate": 6.345201238390093e-05,
      "loss": 15.6885,
      "step": 7683
    },
    {
      "epoch": 7.69,
      "grad_norm": 5071.10107421875,
      "learning_rate": 6.34468524251806e-05,
      "loss": 14.1614,
      "step": 7684
    },
    {
      "epoch": 7.69,
      "grad_norm": 3858.288818359375,
      "learning_rate": 6.344169246646026e-05,
      "loss": 16.9144,
      "step": 7685
    },
    {
      "epoch": 7.69,
      "grad_norm": 4586.8896484375,
      "learning_rate": 6.343653250773994e-05,
      "loss": 12.0998,
      "step": 7686
    },
    {
      "epoch": 7.69,
      "grad_norm": 15164.5126953125,
      "learning_rate": 6.343137254901961e-05,
      "loss": 15.8139,
      "step": 7687
    },
    {
      "epoch": 7.7,
      "grad_norm": 5691.55810546875,
      "learning_rate": 6.342621259029929e-05,
      "loss": 19.7583,
      "step": 7688
    },
    {
      "epoch": 7.7,
      "grad_norm": 65447.76953125,
      "learning_rate": 6.342105263157895e-05,
      "loss": 13.5611,
      "step": 7689
    },
    {
      "epoch": 7.7,
      "grad_norm": 1531.322021484375,
      "learning_rate": 6.341589267285862e-05,
      "loss": 13.0151,
      "step": 7690
    },
    {
      "epoch": 7.7,
      "grad_norm": 9354.2763671875,
      "learning_rate": 6.341073271413828e-05,
      "loss": 15.7064,
      "step": 7691
    },
    {
      "epoch": 7.7,
      "grad_norm": 2695.334716796875,
      "learning_rate": 6.340557275541796e-05,
      "loss": 14.247,
      "step": 7692
    },
    {
      "epoch": 7.7,
      "grad_norm": 2083.22998046875,
      "learning_rate": 6.340041279669763e-05,
      "loss": 13.283,
      "step": 7693
    },
    {
      "epoch": 7.7,
      "grad_norm": 4792.48876953125,
      "learning_rate": 6.33952528379773e-05,
      "loss": 12.0152,
      "step": 7694
    },
    {
      "epoch": 7.7,
      "grad_norm": 11401.0732421875,
      "learning_rate": 6.339009287925697e-05,
      "loss": 17.4191,
      "step": 7695
    },
    {
      "epoch": 7.7,
      "grad_norm": 10852.4560546875,
      "learning_rate": 6.338493292053664e-05,
      "loss": 13.6988,
      "step": 7696
    },
    {
      "epoch": 7.7,
      "grad_norm": 2840.704833984375,
      "learning_rate": 6.33797729618163e-05,
      "loss": 22.493,
      "step": 7697
    },
    {
      "epoch": 7.71,
      "grad_norm": 13975.244140625,
      "learning_rate": 6.337461300309598e-05,
      "loss": 13.4038,
      "step": 7698
    },
    {
      "epoch": 7.71,
      "grad_norm": 18272.91796875,
      "learning_rate": 6.336945304437565e-05,
      "loss": 18.2132,
      "step": 7699
    },
    {
      "epoch": 7.71,
      "grad_norm": 2038.1759033203125,
      "learning_rate": 6.336429308565531e-05,
      "loss": 12.0027,
      "step": 7700
    },
    {
      "epoch": 7.71,
      "grad_norm": 927.6050415039062,
      "learning_rate": 6.335913312693499e-05,
      "loss": 11.8856,
      "step": 7701
    },
    {
      "epoch": 7.71,
      "grad_norm": 3694.54736328125,
      "learning_rate": 6.335397316821465e-05,
      "loss": 11.1829,
      "step": 7702
    },
    {
      "epoch": 7.71,
      "grad_norm": 4357.47412109375,
      "learning_rate": 6.334881320949432e-05,
      "loss": 13.4579,
      "step": 7703
    },
    {
      "epoch": 7.71,
      "grad_norm": 45467.37109375,
      "learning_rate": 6.3343653250774e-05,
      "loss": 19.5924,
      "step": 7704
    },
    {
      "epoch": 7.71,
      "grad_norm": 1270.0106201171875,
      "learning_rate": 6.333849329205367e-05,
      "loss": 12.741,
      "step": 7705
    },
    {
      "epoch": 7.71,
      "grad_norm": 4790.2998046875,
      "learning_rate": 6.333333333333333e-05,
      "loss": 15.4352,
      "step": 7706
    },
    {
      "epoch": 7.71,
      "grad_norm": 1221.8177490234375,
      "learning_rate": 6.332817337461301e-05,
      "loss": 13.945,
      "step": 7707
    },
    {
      "epoch": 7.72,
      "grad_norm": 6154.92138671875,
      "learning_rate": 6.332301341589267e-05,
      "loss": 10.7215,
      "step": 7708
    },
    {
      "epoch": 7.72,
      "grad_norm": 9643.5146484375,
      "learning_rate": 6.331785345717236e-05,
      "loss": 19.9848,
      "step": 7709
    },
    {
      "epoch": 7.72,
      "grad_norm": 1323.34130859375,
      "learning_rate": 6.331269349845202e-05,
      "loss": 12.6694,
      "step": 7710
    },
    {
      "epoch": 7.72,
      "grad_norm": 4783.6845703125,
      "learning_rate": 6.330753353973169e-05,
      "loss": 14.5713,
      "step": 7711
    },
    {
      "epoch": 7.72,
      "grad_norm": 4346.97314453125,
      "learning_rate": 6.330237358101135e-05,
      "loss": 12.0626,
      "step": 7712
    },
    {
      "epoch": 7.72,
      "grad_norm": 1871.5198974609375,
      "learning_rate": 6.329721362229103e-05,
      "loss": 17.8995,
      "step": 7713
    },
    {
      "epoch": 7.72,
      "grad_norm": 18397.6015625,
      "learning_rate": 6.329205366357069e-05,
      "loss": 21.2373,
      "step": 7714
    },
    {
      "epoch": 7.72,
      "grad_norm": 3897.395263671875,
      "learning_rate": 6.328689370485038e-05,
      "loss": 14.6516,
      "step": 7715
    },
    {
      "epoch": 7.72,
      "grad_norm": 3275.710205078125,
      "learning_rate": 6.328173374613004e-05,
      "loss": 13.6624,
      "step": 7716
    },
    {
      "epoch": 7.72,
      "grad_norm": 3870.888671875,
      "learning_rate": 6.32765737874097e-05,
      "loss": 12.0149,
      "step": 7717
    },
    {
      "epoch": 7.73,
      "grad_norm": 1443.6282958984375,
      "learning_rate": 6.327141382868937e-05,
      "loss": 13.5893,
      "step": 7718
    },
    {
      "epoch": 7.73,
      "grad_norm": 1285.00732421875,
      "learning_rate": 6.326625386996903e-05,
      "loss": 12.2853,
      "step": 7719
    },
    {
      "epoch": 7.73,
      "grad_norm": 644.2945556640625,
      "learning_rate": 6.326109391124871e-05,
      "loss": 11.8606,
      "step": 7720
    },
    {
      "epoch": 7.73,
      "grad_norm": 629.1117553710938,
      "learning_rate": 6.325593395252838e-05,
      "loss": 12.3258,
      "step": 7721
    },
    {
      "epoch": 7.73,
      "grad_norm": 1151.49462890625,
      "learning_rate": 6.325077399380806e-05,
      "loss": 11.6391,
      "step": 7722
    },
    {
      "epoch": 7.73,
      "grad_norm": 8142.7890625,
      "learning_rate": 6.324561403508772e-05,
      "loss": 24.034,
      "step": 7723
    },
    {
      "epoch": 7.73,
      "grad_norm": 7020.7158203125,
      "learning_rate": 6.324045407636739e-05,
      "loss": 16.635,
      "step": 7724
    },
    {
      "epoch": 7.73,
      "grad_norm": 1628.94677734375,
      "learning_rate": 6.323529411764705e-05,
      "loss": 12.7801,
      "step": 7725
    },
    {
      "epoch": 7.73,
      "grad_norm": 7598.47021484375,
      "learning_rate": 6.323013415892674e-05,
      "loss": 12.8848,
      "step": 7726
    },
    {
      "epoch": 7.73,
      "grad_norm": 695.8335571289062,
      "learning_rate": 6.32249742002064e-05,
      "loss": 11.4359,
      "step": 7727
    },
    {
      "epoch": 7.74,
      "grad_norm": 621.4734497070312,
      "learning_rate": 6.321981424148608e-05,
      "loss": 11.6546,
      "step": 7728
    },
    {
      "epoch": 7.74,
      "grad_norm": 1570.5137939453125,
      "learning_rate": 6.321465428276574e-05,
      "loss": 12.3398,
      "step": 7729
    },
    {
      "epoch": 7.74,
      "grad_norm": 2295.72314453125,
      "learning_rate": 6.320949432404541e-05,
      "loss": 14.7905,
      "step": 7730
    },
    {
      "epoch": 7.74,
      "grad_norm": 29120.177734375,
      "learning_rate": 6.320433436532507e-05,
      "loss": 14.0865,
      "step": 7731
    },
    {
      "epoch": 7.74,
      "grad_norm": 633.3959350585938,
      "learning_rate": 6.319917440660476e-05,
      "loss": 11.2147,
      "step": 7732
    },
    {
      "epoch": 7.74,
      "grad_norm": 22412.267578125,
      "learning_rate": 6.319401444788442e-05,
      "loss": 16.8467,
      "step": 7733
    },
    {
      "epoch": 7.74,
      "grad_norm": 3812.34912109375,
      "learning_rate": 6.318885448916408e-05,
      "loss": 14.4887,
      "step": 7734
    },
    {
      "epoch": 7.74,
      "grad_norm": 8492.646484375,
      "learning_rate": 6.318369453044376e-05,
      "loss": 15.2216,
      "step": 7735
    },
    {
      "epoch": 7.74,
      "grad_norm": 54720.48828125,
      "learning_rate": 6.317853457172342e-05,
      "loss": 15.6191,
      "step": 7736
    },
    {
      "epoch": 7.74,
      "grad_norm": 41758.1640625,
      "learning_rate": 6.317337461300311e-05,
      "loss": 15.4983,
      "step": 7737
    },
    {
      "epoch": 7.75,
      "grad_norm": 2994.717041015625,
      "learning_rate": 6.316821465428277e-05,
      "loss": 15.5609,
      "step": 7738
    },
    {
      "epoch": 7.75,
      "grad_norm": 9915.0673828125,
      "learning_rate": 6.316305469556244e-05,
      "loss": 20.6695,
      "step": 7739
    },
    {
      "epoch": 7.75,
      "grad_norm": 2664.158447265625,
      "learning_rate": 6.31578947368421e-05,
      "loss": 12.1344,
      "step": 7740
    },
    {
      "epoch": 7.75,
      "grad_norm": 1170.6602783203125,
      "learning_rate": 6.315273477812178e-05,
      "loss": 14.1767,
      "step": 7741
    },
    {
      "epoch": 7.75,
      "grad_norm": 6869.34814453125,
      "learning_rate": 6.314757481940144e-05,
      "loss": 23.0954,
      "step": 7742
    },
    {
      "epoch": 7.75,
      "grad_norm": 4097.1396484375,
      "learning_rate": 6.314241486068113e-05,
      "loss": 20.3934,
      "step": 7743
    },
    {
      "epoch": 7.75,
      "grad_norm": 1494.71728515625,
      "learning_rate": 6.313725490196079e-05,
      "loss": 13.9931,
      "step": 7744
    },
    {
      "epoch": 7.75,
      "grad_norm": 12874.5068359375,
      "learning_rate": 6.313209494324046e-05,
      "loss": 19.6537,
      "step": 7745
    },
    {
      "epoch": 7.75,
      "grad_norm": 2093.0380859375,
      "learning_rate": 6.312693498452012e-05,
      "loss": 11.5646,
      "step": 7746
    },
    {
      "epoch": 7.75,
      "grad_norm": 1154.68701171875,
      "learning_rate": 6.31217750257998e-05,
      "loss": 10.9958,
      "step": 7747
    },
    {
      "epoch": 7.76,
      "grad_norm": 433.02667236328125,
      "learning_rate": 6.311661506707946e-05,
      "loss": 10.3731,
      "step": 7748
    },
    {
      "epoch": 7.76,
      "grad_norm": 8016.67431640625,
      "learning_rate": 6.311145510835915e-05,
      "loss": 14.7791,
      "step": 7749
    },
    {
      "epoch": 7.76,
      "grad_norm": 2768.15087890625,
      "learning_rate": 6.310629514963881e-05,
      "loss": 13.7766,
      "step": 7750
    },
    {
      "epoch": 7.76,
      "grad_norm": 28664.515625,
      "learning_rate": 6.310113519091848e-05,
      "loss": 13.7984,
      "step": 7751
    },
    {
      "epoch": 7.76,
      "grad_norm": 32324.806640625,
      "learning_rate": 6.309597523219814e-05,
      "loss": 19.6511,
      "step": 7752
    },
    {
      "epoch": 7.76,
      "grad_norm": 2331.650390625,
      "learning_rate": 6.30908152734778e-05,
      "loss": 13.6022,
      "step": 7753
    },
    {
      "epoch": 7.76,
      "grad_norm": 17614.40234375,
      "learning_rate": 6.308565531475749e-05,
      "loss": 13.2068,
      "step": 7754
    },
    {
      "epoch": 7.76,
      "grad_norm": 2486.048583984375,
      "learning_rate": 6.308049535603715e-05,
      "loss": 12.994,
      "step": 7755
    },
    {
      "epoch": 7.76,
      "grad_norm": 2615.722412109375,
      "learning_rate": 6.307533539731683e-05,
      "loss": 13.4536,
      "step": 7756
    },
    {
      "epoch": 7.76,
      "grad_norm": 3078.047119140625,
      "learning_rate": 6.307017543859649e-05,
      "loss": 13.0311,
      "step": 7757
    },
    {
      "epoch": 7.77,
      "grad_norm": 756.1614379882812,
      "learning_rate": 6.306501547987616e-05,
      "loss": 12.0414,
      "step": 7758
    },
    {
      "epoch": 7.77,
      "grad_norm": 24727.08203125,
      "learning_rate": 6.305985552115582e-05,
      "loss": 19.2717,
      "step": 7759
    },
    {
      "epoch": 7.77,
      "grad_norm": 2063.898681640625,
      "learning_rate": 6.305469556243551e-05,
      "loss": 11.9091,
      "step": 7760
    },
    {
      "epoch": 7.77,
      "grad_norm": 2786.113037109375,
      "learning_rate": 6.304953560371517e-05,
      "loss": 11.6581,
      "step": 7761
    },
    {
      "epoch": 7.77,
      "grad_norm": 6259.123046875,
      "learning_rate": 6.304437564499485e-05,
      "loss": 19.293,
      "step": 7762
    },
    {
      "epoch": 7.77,
      "grad_norm": 15040.2626953125,
      "learning_rate": 6.303921568627451e-05,
      "loss": 12.4115,
      "step": 7763
    },
    {
      "epoch": 7.77,
      "grad_norm": 19558.728515625,
      "learning_rate": 6.303405572755418e-05,
      "loss": 13.8408,
      "step": 7764
    },
    {
      "epoch": 7.77,
      "grad_norm": 24047.0078125,
      "learning_rate": 6.302889576883384e-05,
      "loss": 14.263,
      "step": 7765
    },
    {
      "epoch": 7.77,
      "grad_norm": 248475.15625,
      "learning_rate": 6.302373581011353e-05,
      "loss": 19.6781,
      "step": 7766
    },
    {
      "epoch": 7.77,
      "grad_norm": 6182.705078125,
      "learning_rate": 6.30185758513932e-05,
      "loss": 13.366,
      "step": 7767
    },
    {
      "epoch": 7.78,
      "grad_norm": 15368.150390625,
      "learning_rate": 6.301341589267287e-05,
      "loss": 16.3426,
      "step": 7768
    },
    {
      "epoch": 7.78,
      "grad_norm": 3976.525146484375,
      "learning_rate": 6.300825593395253e-05,
      "loss": 18.4243,
      "step": 7769
    },
    {
      "epoch": 7.78,
      "grad_norm": 1619.9647216796875,
      "learning_rate": 6.300309597523219e-05,
      "loss": 13.3224,
      "step": 7770
    },
    {
      "epoch": 7.78,
      "grad_norm": 4481.89892578125,
      "learning_rate": 6.299793601651188e-05,
      "loss": 10.6946,
      "step": 7771
    },
    {
      "epoch": 7.78,
      "grad_norm": 6399.41455078125,
      "learning_rate": 6.299277605779154e-05,
      "loss": 14.2619,
      "step": 7772
    },
    {
      "epoch": 7.78,
      "grad_norm": 17764.583984375,
      "learning_rate": 6.298761609907121e-05,
      "loss": 13.9572,
      "step": 7773
    },
    {
      "epoch": 7.78,
      "grad_norm": 2462.0888671875,
      "learning_rate": 6.298245614035087e-05,
      "loss": 15.6257,
      "step": 7774
    },
    {
      "epoch": 7.78,
      "grad_norm": 9130.25,
      "learning_rate": 6.297729618163055e-05,
      "loss": 10.3211,
      "step": 7775
    },
    {
      "epoch": 7.78,
      "grad_norm": 3326.641357421875,
      "learning_rate": 6.297213622291021e-05,
      "loss": 11.7444,
      "step": 7776
    },
    {
      "epoch": 7.78,
      "grad_norm": 11176.708984375,
      "learning_rate": 6.29669762641899e-05,
      "loss": 12.0999,
      "step": 7777
    },
    {
      "epoch": 7.79,
      "grad_norm": 9590.99609375,
      "learning_rate": 6.296181630546956e-05,
      "loss": 15.5388,
      "step": 7778
    },
    {
      "epoch": 7.79,
      "grad_norm": 11089.4365234375,
      "learning_rate": 6.295665634674923e-05,
      "loss": 10.9638,
      "step": 7779
    },
    {
      "epoch": 7.79,
      "grad_norm": 7319.96142578125,
      "learning_rate": 6.29514963880289e-05,
      "loss": 10.5039,
      "step": 7780
    },
    {
      "epoch": 7.79,
      "grad_norm": 10857.099609375,
      "learning_rate": 6.294633642930857e-05,
      "loss": 15.4563,
      "step": 7781
    },
    {
      "epoch": 7.79,
      "grad_norm": 26216.427734375,
      "learning_rate": 6.294117647058824e-05,
      "loss": 16.7194,
      "step": 7782
    },
    {
      "epoch": 7.79,
      "grad_norm": 30716.73046875,
      "learning_rate": 6.293601651186792e-05,
      "loss": 21.0232,
      "step": 7783
    },
    {
      "epoch": 7.79,
      "grad_norm": 663.9575805664062,
      "learning_rate": 6.293085655314758e-05,
      "loss": 13.9744,
      "step": 7784
    },
    {
      "epoch": 7.79,
      "grad_norm": 3358.3203125,
      "learning_rate": 6.292569659442725e-05,
      "loss": 13.3217,
      "step": 7785
    },
    {
      "epoch": 7.79,
      "grad_norm": 27245.84375,
      "learning_rate": 6.292053663570691e-05,
      "loss": 11.6603,
      "step": 7786
    },
    {
      "epoch": 7.79,
      "grad_norm": 7696.041015625,
      "learning_rate": 6.291537667698659e-05,
      "loss": 14.7455,
      "step": 7787
    },
    {
      "epoch": 7.8,
      "grad_norm": 1664.348876953125,
      "learning_rate": 6.291021671826626e-05,
      "loss": 11.3797,
      "step": 7788
    },
    {
      "epoch": 7.8,
      "grad_norm": 8673.103515625,
      "learning_rate": 6.290505675954592e-05,
      "loss": 17.0401,
      "step": 7789
    },
    {
      "epoch": 7.8,
      "grad_norm": 16103.51171875,
      "learning_rate": 6.28998968008256e-05,
      "loss": 13.7716,
      "step": 7790
    },
    {
      "epoch": 7.8,
      "grad_norm": 891.033203125,
      "learning_rate": 6.289473684210526e-05,
      "loss": 10.8197,
      "step": 7791
    },
    {
      "epoch": 7.8,
      "grad_norm": 4988.93896484375,
      "learning_rate": 6.288957688338493e-05,
      "loss": 11.8513,
      "step": 7792
    },
    {
      "epoch": 7.8,
      "grad_norm": 6947.8525390625,
      "learning_rate": 6.28844169246646e-05,
      "loss": 15.746,
      "step": 7793
    },
    {
      "epoch": 7.8,
      "grad_norm": 5493.48779296875,
      "learning_rate": 6.287925696594428e-05,
      "loss": 12.3332,
      "step": 7794
    },
    {
      "epoch": 7.8,
      "grad_norm": 964.6858520507812,
      "learning_rate": 6.287409700722394e-05,
      "loss": 12.3793,
      "step": 7795
    },
    {
      "epoch": 7.8,
      "grad_norm": 5421.45556640625,
      "learning_rate": 6.286893704850362e-05,
      "loss": 14.571,
      "step": 7796
    },
    {
      "epoch": 7.8,
      "grad_norm": 8134.40673828125,
      "learning_rate": 6.286377708978328e-05,
      "loss": 18.8055,
      "step": 7797
    },
    {
      "epoch": 7.81,
      "grad_norm": 9974.8759765625,
      "learning_rate": 6.285861713106295e-05,
      "loss": 15.081,
      "step": 7798
    },
    {
      "epoch": 7.81,
      "grad_norm": 4222.294921875,
      "learning_rate": 6.285345717234263e-05,
      "loss": 12.9211,
      "step": 7799
    },
    {
      "epoch": 7.81,
      "grad_norm": 691.0697021484375,
      "learning_rate": 6.28482972136223e-05,
      "loss": 10.9144,
      "step": 7800
    },
    {
      "epoch": 7.81,
      "grad_norm": 8458.291015625,
      "learning_rate": 6.284313725490196e-05,
      "loss": 15.0774,
      "step": 7801
    },
    {
      "epoch": 7.81,
      "grad_norm": 11289.88671875,
      "learning_rate": 6.283797729618164e-05,
      "loss": 11.9713,
      "step": 7802
    },
    {
      "epoch": 7.81,
      "grad_norm": 1575.6300048828125,
      "learning_rate": 6.28328173374613e-05,
      "loss": 10.6369,
      "step": 7803
    },
    {
      "epoch": 7.81,
      "grad_norm": 10102.1904296875,
      "learning_rate": 6.282765737874097e-05,
      "loss": 14.8362,
      "step": 7804
    },
    {
      "epoch": 7.81,
      "grad_norm": 2672.5048828125,
      "learning_rate": 6.282249742002065e-05,
      "loss": 12.9026,
      "step": 7805
    },
    {
      "epoch": 7.81,
      "grad_norm": 7021.53271484375,
      "learning_rate": 6.281733746130031e-05,
      "loss": 12.3139,
      "step": 7806
    },
    {
      "epoch": 7.81,
      "grad_norm": 3627.93798828125,
      "learning_rate": 6.281217750257998e-05,
      "loss": 15.9842,
      "step": 7807
    },
    {
      "epoch": 7.82,
      "grad_norm": 3524.879150390625,
      "learning_rate": 6.280701754385965e-05,
      "loss": 15.4992,
      "step": 7808
    },
    {
      "epoch": 7.82,
      "grad_norm": 5046.52880859375,
      "learning_rate": 6.280185758513932e-05,
      "loss": 16.357,
      "step": 7809
    },
    {
      "epoch": 7.82,
      "grad_norm": 17013.556640625,
      "learning_rate": 6.2796697626419e-05,
      "loss": 13.2779,
      "step": 7810
    },
    {
      "epoch": 7.82,
      "grad_norm": 1711.2276611328125,
      "learning_rate": 6.279153766769867e-05,
      "loss": 13.3998,
      "step": 7811
    },
    {
      "epoch": 7.82,
      "grad_norm": 8958.291015625,
      "learning_rate": 6.278637770897833e-05,
      "loss": 12.1085,
      "step": 7812
    },
    {
      "epoch": 7.82,
      "grad_norm": 2099.591796875,
      "learning_rate": 6.2781217750258e-05,
      "loss": 13.042,
      "step": 7813
    },
    {
      "epoch": 7.82,
      "grad_norm": 11776.603515625,
      "learning_rate": 6.277605779153767e-05,
      "loss": 13.1607,
      "step": 7814
    },
    {
      "epoch": 7.82,
      "grad_norm": 984.9031982421875,
      "learning_rate": 6.277089783281734e-05,
      "loss": 20.2617,
      "step": 7815
    },
    {
      "epoch": 7.82,
      "grad_norm": 2984.51513671875,
      "learning_rate": 6.276573787409701e-05,
      "loss": 11.4978,
      "step": 7816
    },
    {
      "epoch": 7.82,
      "grad_norm": 11578.0107421875,
      "learning_rate": 6.276057791537669e-05,
      "loss": 20.595,
      "step": 7817
    },
    {
      "epoch": 7.83,
      "grad_norm": 2483.458984375,
      "learning_rate": 6.275541795665635e-05,
      "loss": 10.9522,
      "step": 7818
    },
    {
      "epoch": 7.83,
      "grad_norm": 2799.3271484375,
      "learning_rate": 6.275025799793602e-05,
      "loss": 11.5409,
      "step": 7819
    },
    {
      "epoch": 7.83,
      "grad_norm": 2477.062255859375,
      "learning_rate": 6.274509803921569e-05,
      "loss": 15.0529,
      "step": 7820
    },
    {
      "epoch": 7.83,
      "grad_norm": 6283.5458984375,
      "learning_rate": 6.273993808049536e-05,
      "loss": 12.8205,
      "step": 7821
    },
    {
      "epoch": 7.83,
      "grad_norm": 2891.61083984375,
      "learning_rate": 6.273477812177503e-05,
      "loss": 11.7735,
      "step": 7822
    },
    {
      "epoch": 7.83,
      "grad_norm": 8033.15380859375,
      "learning_rate": 6.27296181630547e-05,
      "loss": 15.7866,
      "step": 7823
    },
    {
      "epoch": 7.83,
      "grad_norm": 27348.5,
      "learning_rate": 6.272445820433437e-05,
      "loss": 19.6398,
      "step": 7824
    },
    {
      "epoch": 7.83,
      "grad_norm": 8945.0556640625,
      "learning_rate": 6.271929824561403e-05,
      "loss": 17.9812,
      "step": 7825
    },
    {
      "epoch": 7.83,
      "grad_norm": 3001.1865234375,
      "learning_rate": 6.27141382868937e-05,
      "loss": 12.2796,
      "step": 7826
    },
    {
      "epoch": 7.83,
      "grad_norm": 3671.580078125,
      "learning_rate": 6.270897832817338e-05,
      "loss": 14.7,
      "step": 7827
    },
    {
      "epoch": 7.84,
      "grad_norm": 3702.952880859375,
      "learning_rate": 6.270381836945305e-05,
      "loss": 13.3084,
      "step": 7828
    },
    {
      "epoch": 7.84,
      "grad_norm": 24503.98046875,
      "learning_rate": 6.269865841073272e-05,
      "loss": 19.2977,
      "step": 7829
    },
    {
      "epoch": 7.84,
      "grad_norm": 494.5415344238281,
      "learning_rate": 6.269349845201239e-05,
      "loss": 16.6122,
      "step": 7830
    },
    {
      "epoch": 7.84,
      "grad_norm": 39834.51953125,
      "learning_rate": 6.268833849329205e-05,
      "loss": 22.5046,
      "step": 7831
    },
    {
      "epoch": 7.84,
      "grad_norm": 6104.080078125,
      "learning_rate": 6.268317853457173e-05,
      "loss": 17.2985,
      "step": 7832
    },
    {
      "epoch": 7.84,
      "grad_norm": 1716.04150390625,
      "learning_rate": 6.26780185758514e-05,
      "loss": 12.5074,
      "step": 7833
    },
    {
      "epoch": 7.84,
      "grad_norm": 3994.338623046875,
      "learning_rate": 6.267285861713107e-05,
      "loss": 12.161,
      "step": 7834
    },
    {
      "epoch": 7.84,
      "grad_norm": 7056.541015625,
      "learning_rate": 6.266769865841074e-05,
      "loss": 15.9008,
      "step": 7835
    },
    {
      "epoch": 7.84,
      "grad_norm": 2133.3125,
      "learning_rate": 6.266253869969041e-05,
      "loss": 11.2377,
      "step": 7836
    },
    {
      "epoch": 7.84,
      "grad_norm": 3020.932861328125,
      "learning_rate": 6.265737874097007e-05,
      "loss": 13.6891,
      "step": 7837
    },
    {
      "epoch": 7.85,
      "grad_norm": 1214.73095703125,
      "learning_rate": 6.265221878224975e-05,
      "loss": 12.1054,
      "step": 7838
    },
    {
      "epoch": 7.85,
      "grad_norm": 6074.73828125,
      "learning_rate": 6.264705882352942e-05,
      "loss": 10.9039,
      "step": 7839
    },
    {
      "epoch": 7.85,
      "grad_norm": 18176.5,
      "learning_rate": 6.26418988648091e-05,
      "loss": 13.8466,
      "step": 7840
    },
    {
      "epoch": 7.85,
      "grad_norm": 6527.11767578125,
      "learning_rate": 6.263673890608876e-05,
      "loss": 17.6289,
      "step": 7841
    },
    {
      "epoch": 7.85,
      "grad_norm": 1001.2224731445312,
      "learning_rate": 6.263157894736842e-05,
      "loss": 14.3733,
      "step": 7842
    },
    {
      "epoch": 7.85,
      "grad_norm": 4189.20361328125,
      "learning_rate": 6.262641898864809e-05,
      "loss": 11.1879,
      "step": 7843
    },
    {
      "epoch": 7.85,
      "grad_norm": 2694.775146484375,
      "learning_rate": 6.262125902992777e-05,
      "loss": 12.9518,
      "step": 7844
    },
    {
      "epoch": 7.85,
      "grad_norm": 6399.0595703125,
      "learning_rate": 6.261609907120744e-05,
      "loss": 13.1596,
      "step": 7845
    },
    {
      "epoch": 7.85,
      "grad_norm": 1044.7630615234375,
      "learning_rate": 6.26109391124871e-05,
      "loss": 12.5464,
      "step": 7846
    },
    {
      "epoch": 7.85,
      "grad_norm": 1979.2271728515625,
      "learning_rate": 6.260577915376678e-05,
      "loss": 17.5397,
      "step": 7847
    },
    {
      "epoch": 7.86,
      "grad_norm": 3147.11767578125,
      "learning_rate": 6.260061919504644e-05,
      "loss": 11.3179,
      "step": 7848
    },
    {
      "epoch": 7.86,
      "grad_norm": 1631.712158203125,
      "learning_rate": 6.259545923632611e-05,
      "loss": 15.3647,
      "step": 7849
    },
    {
      "epoch": 7.86,
      "grad_norm": 8872.720703125,
      "learning_rate": 6.259029927760579e-05,
      "loss": 17.4565,
      "step": 7850
    },
    {
      "epoch": 7.86,
      "grad_norm": 1383.9210205078125,
      "learning_rate": 6.258513931888546e-05,
      "loss": 15.906,
      "step": 7851
    },
    {
      "epoch": 7.86,
      "grad_norm": 56514.2890625,
      "learning_rate": 6.257997936016512e-05,
      "loss": 11.9792,
      "step": 7852
    },
    {
      "epoch": 7.86,
      "grad_norm": 12237.1259765625,
      "learning_rate": 6.25748194014448e-05,
      "loss": 14.0502,
      "step": 7853
    },
    {
      "epoch": 7.86,
      "grad_norm": 784.94189453125,
      "learning_rate": 6.256965944272446e-05,
      "loss": 18.1929,
      "step": 7854
    },
    {
      "epoch": 7.86,
      "grad_norm": 6339.205078125,
      "learning_rate": 6.256449948400413e-05,
      "loss": 13.6035,
      "step": 7855
    },
    {
      "epoch": 7.86,
      "grad_norm": 2570.22705078125,
      "learning_rate": 6.25593395252838e-05,
      "loss": 12.9595,
      "step": 7856
    },
    {
      "epoch": 7.86,
      "grad_norm": 797.4886474609375,
      "learning_rate": 6.255417956656348e-05,
      "loss": 13.9958,
      "step": 7857
    },
    {
      "epoch": 7.87,
      "grad_norm": 2370.75,
      "learning_rate": 6.254901960784314e-05,
      "loss": 13.9172,
      "step": 7858
    },
    {
      "epoch": 7.87,
      "grad_norm": 9165.994140625,
      "learning_rate": 6.25438596491228e-05,
      "loss": 19.2596,
      "step": 7859
    },
    {
      "epoch": 7.87,
      "grad_norm": 1218.4488525390625,
      "learning_rate": 6.253869969040248e-05,
      "loss": 16.5069,
      "step": 7860
    },
    {
      "epoch": 7.87,
      "grad_norm": 31697.23828125,
      "learning_rate": 6.253353973168215e-05,
      "loss": 22.3165,
      "step": 7861
    },
    {
      "epoch": 7.87,
      "grad_norm": 1677.375,
      "learning_rate": 6.252837977296183e-05,
      "loss": 15.5528,
      "step": 7862
    },
    {
      "epoch": 7.87,
      "grad_norm": 14015.220703125,
      "learning_rate": 6.252321981424149e-05,
      "loss": 17.0812,
      "step": 7863
    },
    {
      "epoch": 7.87,
      "grad_norm": 2801.53759765625,
      "learning_rate": 6.251805985552116e-05,
      "loss": 13.7497,
      "step": 7864
    },
    {
      "epoch": 7.87,
      "grad_norm": 90839.4453125,
      "learning_rate": 6.251289989680082e-05,
      "loss": 24.3805,
      "step": 7865
    },
    {
      "epoch": 7.87,
      "grad_norm": 928.02197265625,
      "learning_rate": 6.25077399380805e-05,
      "loss": 12.5121,
      "step": 7866
    },
    {
      "epoch": 7.87,
      "grad_norm": 601.1787719726562,
      "learning_rate": 6.250257997936017e-05,
      "loss": 13.1992,
      "step": 7867
    },
    {
      "epoch": 7.88,
      "grad_norm": 10199.5341796875,
      "learning_rate": 6.249742002063985e-05,
      "loss": 14.2848,
      "step": 7868
    },
    {
      "epoch": 7.88,
      "grad_norm": 3848.063720703125,
      "learning_rate": 6.24922600619195e-05,
      "loss": 15.0527,
      "step": 7869
    },
    {
      "epoch": 7.88,
      "grad_norm": 11918.8095703125,
      "learning_rate": 6.248710010319918e-05,
      "loss": 13.6654,
      "step": 7870
    },
    {
      "epoch": 7.88,
      "grad_norm": 2161.17822265625,
      "learning_rate": 6.248194014447884e-05,
      "loss": 12.7385,
      "step": 7871
    },
    {
      "epoch": 7.88,
      "grad_norm": 783.8223876953125,
      "learning_rate": 6.247678018575852e-05,
      "loss": 14.6482,
      "step": 7872
    },
    {
      "epoch": 7.88,
      "grad_norm": 3511.125,
      "learning_rate": 6.247162022703819e-05,
      "loss": 12.7856,
      "step": 7873
    },
    {
      "epoch": 7.88,
      "grad_norm": 157967.65625,
      "learning_rate": 6.246646026831787e-05,
      "loss": 18.5679,
      "step": 7874
    },
    {
      "epoch": 7.88,
      "grad_norm": 9125.18359375,
      "learning_rate": 6.246130030959753e-05,
      "loss": 13.503,
      "step": 7875
    },
    {
      "epoch": 7.88,
      "grad_norm": 5607.40185546875,
      "learning_rate": 6.24561403508772e-05,
      "loss": 15.0299,
      "step": 7876
    },
    {
      "epoch": 7.88,
      "grad_norm": 2942.379150390625,
      "learning_rate": 6.245098039215686e-05,
      "loss": 13.4889,
      "step": 7877
    },
    {
      "epoch": 7.89,
      "grad_norm": 3783.05078125,
      "learning_rate": 6.244582043343654e-05,
      "loss": 13.3267,
      "step": 7878
    },
    {
      "epoch": 7.89,
      "grad_norm": 34596.1953125,
      "learning_rate": 6.244066047471621e-05,
      "loss": 13.9794,
      "step": 7879
    },
    {
      "epoch": 7.89,
      "grad_norm": 3815.203125,
      "learning_rate": 6.243550051599587e-05,
      "loss": 16.5615,
      "step": 7880
    },
    {
      "epoch": 7.89,
      "grad_norm": 7168.88671875,
      "learning_rate": 6.243034055727555e-05,
      "loss": 19.9487,
      "step": 7881
    },
    {
      "epoch": 7.89,
      "grad_norm": 1071.35107421875,
      "learning_rate": 6.242518059855521e-05,
      "loss": 16.4871,
      "step": 7882
    },
    {
      "epoch": 7.89,
      "grad_norm": 2456.28564453125,
      "learning_rate": 6.242002063983488e-05,
      "loss": 14.8763,
      "step": 7883
    },
    {
      "epoch": 7.89,
      "grad_norm": 5821.25,
      "learning_rate": 6.241486068111456e-05,
      "loss": 20.8429,
      "step": 7884
    },
    {
      "epoch": 7.89,
      "grad_norm": 14708.009765625,
      "learning_rate": 6.240970072239423e-05,
      "loss": 16.4845,
      "step": 7885
    },
    {
      "epoch": 7.89,
      "grad_norm": 3136.579833984375,
      "learning_rate": 6.240454076367389e-05,
      "loss": 12.5589,
      "step": 7886
    },
    {
      "epoch": 7.89,
      "grad_norm": 6033.365234375,
      "learning_rate": 6.239938080495357e-05,
      "loss": 12.4866,
      "step": 7887
    },
    {
      "epoch": 7.9,
      "grad_norm": 5091.45947265625,
      "learning_rate": 6.239422084623323e-05,
      "loss": 27.2264,
      "step": 7888
    },
    {
      "epoch": 7.9,
      "grad_norm": 2342.1875,
      "learning_rate": 6.23890608875129e-05,
      "loss": 12.7034,
      "step": 7889
    },
    {
      "epoch": 7.9,
      "grad_norm": 4500.984375,
      "learning_rate": 6.238390092879258e-05,
      "loss": 11.7114,
      "step": 7890
    },
    {
      "epoch": 7.9,
      "grad_norm": 2998.681884765625,
      "learning_rate": 6.237874097007225e-05,
      "loss": 13.0483,
      "step": 7891
    },
    {
      "epoch": 7.9,
      "grad_norm": 1898.57275390625,
      "learning_rate": 6.237358101135191e-05,
      "loss": 14.2287,
      "step": 7892
    },
    {
      "epoch": 7.9,
      "grad_norm": 2908.0185546875,
      "learning_rate": 6.236842105263159e-05,
      "loss": 22.7686,
      "step": 7893
    },
    {
      "epoch": 7.9,
      "grad_norm": 8897.890625,
      "learning_rate": 6.236326109391125e-05,
      "loss": 16.8382,
      "step": 7894
    },
    {
      "epoch": 7.9,
      "grad_norm": 3853.272705078125,
      "learning_rate": 6.235810113519092e-05,
      "loss": 13.0106,
      "step": 7895
    },
    {
      "epoch": 7.9,
      "grad_norm": 724.918212890625,
      "learning_rate": 6.23529411764706e-05,
      "loss": 12.9574,
      "step": 7896
    },
    {
      "epoch": 7.9,
      "grad_norm": 68837.1484375,
      "learning_rate": 6.234778121775026e-05,
      "loss": 22.9177,
      "step": 7897
    },
    {
      "epoch": 7.91,
      "grad_norm": 3514.66748046875,
      "learning_rate": 6.234262125902993e-05,
      "loss": 15.1078,
      "step": 7898
    },
    {
      "epoch": 7.91,
      "grad_norm": 8892.5205078125,
      "learning_rate": 6.233746130030959e-05,
      "loss": 15.4631,
      "step": 7899
    },
    {
      "epoch": 7.91,
      "grad_norm": 20892.509765625,
      "learning_rate": 6.233230134158927e-05,
      "loss": 19.2727,
      "step": 7900
    },
    {
      "epoch": 7.91,
      "grad_norm": 4898.859375,
      "learning_rate": 6.232714138286894e-05,
      "loss": 17.2531,
      "step": 7901
    },
    {
      "epoch": 7.91,
      "grad_norm": 10135.7451171875,
      "learning_rate": 6.232198142414862e-05,
      "loss": 11.8522,
      "step": 7902
    },
    {
      "epoch": 7.91,
      "grad_norm": 1876.7479248046875,
      "learning_rate": 6.231682146542828e-05,
      "loss": 17.3113,
      "step": 7903
    },
    {
      "epoch": 7.91,
      "grad_norm": 5730.04833984375,
      "learning_rate": 6.231166150670795e-05,
      "loss": 11.1397,
      "step": 7904
    },
    {
      "epoch": 7.91,
      "grad_norm": 47370.0390625,
      "learning_rate": 6.230650154798761e-05,
      "loss": 12.2259,
      "step": 7905
    },
    {
      "epoch": 7.91,
      "grad_norm": 1242.291259765625,
      "learning_rate": 6.230134158926729e-05,
      "loss": 10.5347,
      "step": 7906
    },
    {
      "epoch": 7.91,
      "grad_norm": 1725.5311279296875,
      "learning_rate": 6.229618163054696e-05,
      "loss": 14.948,
      "step": 7907
    },
    {
      "epoch": 7.92,
      "grad_norm": 1517.5179443359375,
      "learning_rate": 6.229102167182664e-05,
      "loss": 12.6813,
      "step": 7908
    },
    {
      "epoch": 7.92,
      "grad_norm": 9582.587890625,
      "learning_rate": 6.22858617131063e-05,
      "loss": 18.7814,
      "step": 7909
    },
    {
      "epoch": 7.92,
      "grad_norm": 3664.1005859375,
      "learning_rate": 6.228070175438597e-05,
      "loss": 12.5578,
      "step": 7910
    },
    {
      "epoch": 7.92,
      "grad_norm": 15650.5380859375,
      "learning_rate": 6.227554179566563e-05,
      "loss": 12.6722,
      "step": 7911
    },
    {
      "epoch": 7.92,
      "grad_norm": 697.8411865234375,
      "learning_rate": 6.227038183694531e-05,
      "loss": 12.3827,
      "step": 7912
    },
    {
      "epoch": 7.92,
      "grad_norm": 18311.580078125,
      "learning_rate": 6.226522187822498e-05,
      "loss": 13.4871,
      "step": 7913
    },
    {
      "epoch": 7.92,
      "grad_norm": 4099.76513671875,
      "learning_rate": 6.226006191950464e-05,
      "loss": 12.0019,
      "step": 7914
    },
    {
      "epoch": 7.92,
      "grad_norm": 970.634033203125,
      "learning_rate": 6.225490196078432e-05,
      "loss": 11.2022,
      "step": 7915
    },
    {
      "epoch": 7.92,
      "grad_norm": 17751.46875,
      "learning_rate": 6.224974200206398e-05,
      "loss": 15.7701,
      "step": 7916
    },
    {
      "epoch": 7.92,
      "grad_norm": 6914.74365234375,
      "learning_rate": 6.224458204334365e-05,
      "loss": 13.5623,
      "step": 7917
    },
    {
      "epoch": 7.93,
      "grad_norm": 9079.658203125,
      "learning_rate": 6.223942208462333e-05,
      "loss": 13.5751,
      "step": 7918
    },
    {
      "epoch": 7.93,
      "grad_norm": 1907.173583984375,
      "learning_rate": 6.2234262125903e-05,
      "loss": 12.1748,
      "step": 7919
    },
    {
      "epoch": 7.93,
      "grad_norm": 13523.7392578125,
      "learning_rate": 6.222910216718266e-05,
      "loss": 13.431,
      "step": 7920
    },
    {
      "epoch": 7.93,
      "grad_norm": 3630.10791015625,
      "learning_rate": 6.222394220846234e-05,
      "loss": 12.5672,
      "step": 7921
    },
    {
      "epoch": 7.93,
      "grad_norm": 1323.362060546875,
      "learning_rate": 6.2218782249742e-05,
      "loss": 12.0519,
      "step": 7922
    },
    {
      "epoch": 7.93,
      "grad_norm": 80136.5546875,
      "learning_rate": 6.221362229102167e-05,
      "loss": 18.6067,
      "step": 7923
    },
    {
      "epoch": 7.93,
      "grad_norm": 4738.29248046875,
      "learning_rate": 6.220846233230135e-05,
      "loss": 14.3422,
      "step": 7924
    },
    {
      "epoch": 7.93,
      "grad_norm": 4830.01904296875,
      "learning_rate": 6.220330237358102e-05,
      "loss": 13.1405,
      "step": 7925
    },
    {
      "epoch": 7.93,
      "grad_norm": 8278.310546875,
      "learning_rate": 6.219814241486068e-05,
      "loss": 13.692,
      "step": 7926
    },
    {
      "epoch": 7.93,
      "grad_norm": 46269.50390625,
      "learning_rate": 6.219298245614036e-05,
      "loss": 16.7682,
      "step": 7927
    },
    {
      "epoch": 7.94,
      "grad_norm": 2467.210205078125,
      "learning_rate": 6.218782249742002e-05,
      "loss": 10.7943,
      "step": 7928
    },
    {
      "epoch": 7.94,
      "grad_norm": 3890.6845703125,
      "learning_rate": 6.218266253869969e-05,
      "loss": 13.6362,
      "step": 7929
    },
    {
      "epoch": 7.94,
      "grad_norm": 9656.0107421875,
      "learning_rate": 6.217750257997937e-05,
      "loss": 15.8763,
      "step": 7930
    },
    {
      "epoch": 7.94,
      "grad_norm": 2727.737548828125,
      "learning_rate": 6.217234262125903e-05,
      "loss": 13.3665,
      "step": 7931
    },
    {
      "epoch": 7.94,
      "grad_norm": 2266.76025390625,
      "learning_rate": 6.21671826625387e-05,
      "loss": 13.7197,
      "step": 7932
    },
    {
      "epoch": 7.94,
      "grad_norm": 37563.82421875,
      "learning_rate": 6.216202270381836e-05,
      "loss": 16.1701,
      "step": 7933
    },
    {
      "epoch": 7.94,
      "grad_norm": 694.4247436523438,
      "learning_rate": 6.215686274509804e-05,
      "loss": 13.3861,
      "step": 7934
    },
    {
      "epoch": 7.94,
      "grad_norm": 71665.3984375,
      "learning_rate": 6.215170278637771e-05,
      "loss": 16.511,
      "step": 7935
    },
    {
      "epoch": 7.94,
      "grad_norm": 352.7084655761719,
      "learning_rate": 6.214654282765739e-05,
      "loss": 12.1003,
      "step": 7936
    },
    {
      "epoch": 7.94,
      "grad_norm": 1708.1705322265625,
      "learning_rate": 6.214138286893705e-05,
      "loss": 21.6348,
      "step": 7937
    },
    {
      "epoch": 7.95,
      "grad_norm": 1832.818603515625,
      "learning_rate": 6.213622291021672e-05,
      "loss": 12.1167,
      "step": 7938
    },
    {
      "epoch": 7.95,
      "grad_norm": 7495.40966796875,
      "learning_rate": 6.213106295149638e-05,
      "loss": 13.5754,
      "step": 7939
    },
    {
      "epoch": 7.95,
      "grad_norm": 12734.8779296875,
      "learning_rate": 6.212590299277606e-05,
      "loss": 17.2084,
      "step": 7940
    },
    {
      "epoch": 7.95,
      "grad_norm": 42270.0390625,
      "learning_rate": 6.212074303405573e-05,
      "loss": 14.5266,
      "step": 7941
    },
    {
      "epoch": 7.95,
      "grad_norm": 30846.46484375,
      "learning_rate": 6.211558307533541e-05,
      "loss": 16.5754,
      "step": 7942
    },
    {
      "epoch": 7.95,
      "grad_norm": 8441.771484375,
      "learning_rate": 6.211042311661507e-05,
      "loss": 27.8596,
      "step": 7943
    },
    {
      "epoch": 7.95,
      "grad_norm": 3741.433837890625,
      "learning_rate": 6.210526315789474e-05,
      "loss": 13.9759,
      "step": 7944
    },
    {
      "epoch": 7.95,
      "grad_norm": 7830.24609375,
      "learning_rate": 6.21001031991744e-05,
      "loss": 29.0756,
      "step": 7945
    },
    {
      "epoch": 7.95,
      "grad_norm": 1920.6812744140625,
      "learning_rate": 6.209494324045408e-05,
      "loss": 12.015,
      "step": 7946
    },
    {
      "epoch": 7.95,
      "grad_norm": 5189.3271484375,
      "learning_rate": 6.208978328173375e-05,
      "loss": 18.7559,
      "step": 7947
    },
    {
      "epoch": 7.96,
      "grad_norm": 4405.314453125,
      "learning_rate": 6.208462332301341e-05,
      "loss": 18.8721,
      "step": 7948
    },
    {
      "epoch": 7.96,
      "grad_norm": 8355.12109375,
      "learning_rate": 6.207946336429309e-05,
      "loss": 16.1246,
      "step": 7949
    },
    {
      "epoch": 7.96,
      "grad_norm": 8953.716796875,
      "learning_rate": 6.207430340557275e-05,
      "loss": 14.1775,
      "step": 7950
    },
    {
      "epoch": 7.96,
      "grad_norm": 6564.6748046875,
      "learning_rate": 6.206914344685242e-05,
      "loss": 19.3764,
      "step": 7951
    },
    {
      "epoch": 7.96,
      "grad_norm": 33234.93359375,
      "learning_rate": 6.20639834881321e-05,
      "loss": 14.9419,
      "step": 7952
    },
    {
      "epoch": 7.96,
      "grad_norm": 1778.6729736328125,
      "learning_rate": 6.205882352941177e-05,
      "loss": 11.3381,
      "step": 7953
    },
    {
      "epoch": 7.96,
      "grad_norm": 3806.818115234375,
      "learning_rate": 6.205366357069143e-05,
      "loss": 11.9128,
      "step": 7954
    },
    {
      "epoch": 7.96,
      "grad_norm": 7519.23681640625,
      "learning_rate": 6.204850361197111e-05,
      "loss": 12.5194,
      "step": 7955
    },
    {
      "epoch": 7.96,
      "grad_norm": 5923.06396484375,
      "learning_rate": 6.204334365325077e-05,
      "loss": 13.4837,
      "step": 7956
    },
    {
      "epoch": 7.96,
      "grad_norm": 1641.59521484375,
      "learning_rate": 6.203818369453044e-05,
      "loss": 13.6435,
      "step": 7957
    },
    {
      "epoch": 7.97,
      "grad_norm": 1127.291259765625,
      "learning_rate": 6.203302373581012e-05,
      "loss": 13.6492,
      "step": 7958
    },
    {
      "epoch": 7.97,
      "grad_norm": 20635.580078125,
      "learning_rate": 6.202786377708979e-05,
      "loss": 13.0397,
      "step": 7959
    },
    {
      "epoch": 7.97,
      "grad_norm": 1890.239013671875,
      "learning_rate": 6.202270381836945e-05,
      "loss": 12.4004,
      "step": 7960
    },
    {
      "epoch": 7.97,
      "grad_norm": 3961.193603515625,
      "learning_rate": 6.201754385964913e-05,
      "loss": 13.3269,
      "step": 7961
    },
    {
      "epoch": 7.97,
      "grad_norm": 6357.51806640625,
      "learning_rate": 6.201238390092879e-05,
      "loss": 16.3663,
      "step": 7962
    },
    {
      "epoch": 7.97,
      "grad_norm": 5234.865234375,
      "learning_rate": 6.200722394220846e-05,
      "loss": 13.0968,
      "step": 7963
    },
    {
      "epoch": 7.97,
      "grad_norm": 4695.87939453125,
      "learning_rate": 6.200206398348814e-05,
      "loss": 17.7422,
      "step": 7964
    },
    {
      "epoch": 7.97,
      "grad_norm": 3459.671630859375,
      "learning_rate": 6.199690402476781e-05,
      "loss": 11.5655,
      "step": 7965
    },
    {
      "epoch": 7.97,
      "grad_norm": 5239.0791015625,
      "learning_rate": 6.199174406604747e-05,
      "loss": 23.2974,
      "step": 7966
    },
    {
      "epoch": 7.97,
      "grad_norm": 27864.40625,
      "learning_rate": 6.198658410732713e-05,
      "loss": 18.4128,
      "step": 7967
    },
    {
      "epoch": 7.98,
      "grad_norm": 12192.84375,
      "learning_rate": 6.198142414860681e-05,
      "loss": 14.8578,
      "step": 7968
    },
    {
      "epoch": 7.98,
      "grad_norm": 6603.6630859375,
      "learning_rate": 6.197626418988648e-05,
      "loss": 12.1729,
      "step": 7969
    },
    {
      "epoch": 7.98,
      "grad_norm": 3099.707275390625,
      "learning_rate": 6.197110423116616e-05,
      "loss": 14.1638,
      "step": 7970
    },
    {
      "epoch": 7.98,
      "grad_norm": 725.3978881835938,
      "learning_rate": 6.196594427244582e-05,
      "loss": 10.9627,
      "step": 7971
    },
    {
      "epoch": 7.98,
      "grad_norm": 9704.21484375,
      "learning_rate": 6.19607843137255e-05,
      "loss": 17.8697,
      "step": 7972
    },
    {
      "epoch": 7.98,
      "grad_norm": 5373.64501953125,
      "learning_rate": 6.195562435500515e-05,
      "loss": 17.7266,
      "step": 7973
    },
    {
      "epoch": 7.98,
      "grad_norm": 5237.14794921875,
      "learning_rate": 6.195046439628483e-05,
      "loss": 20.5036,
      "step": 7974
    },
    {
      "epoch": 7.98,
      "grad_norm": 985.9452514648438,
      "learning_rate": 6.19453044375645e-05,
      "loss": 12.985,
      "step": 7975
    },
    {
      "epoch": 7.98,
      "grad_norm": 11423.9033203125,
      "learning_rate": 6.194014447884418e-05,
      "loss": 14.5493,
      "step": 7976
    },
    {
      "epoch": 7.98,
      "grad_norm": 9489.0830078125,
      "learning_rate": 6.193498452012384e-05,
      "loss": 17.7685,
      "step": 7977
    },
    {
      "epoch": 7.99,
      "grad_norm": 1620.35888671875,
      "learning_rate": 6.192982456140351e-05,
      "loss": 12.4565,
      "step": 7978
    },
    {
      "epoch": 7.99,
      "grad_norm": 1871.0361328125,
      "learning_rate": 6.192466460268317e-05,
      "loss": 14.9925,
      "step": 7979
    },
    {
      "epoch": 7.99,
      "grad_norm": 990.8800048828125,
      "learning_rate": 6.191950464396285e-05,
      "loss": 12.1772,
      "step": 7980
    },
    {
      "epoch": 7.99,
      "grad_norm": 7355.13037109375,
      "learning_rate": 6.191434468524252e-05,
      "loss": 14.6832,
      "step": 7981
    },
    {
      "epoch": 7.99,
      "grad_norm": 5740.564453125,
      "learning_rate": 6.19091847265222e-05,
      "loss": 19.0108,
      "step": 7982
    },
    {
      "epoch": 7.99,
      "grad_norm": 736.4356079101562,
      "learning_rate": 6.190402476780186e-05,
      "loss": 15.5314,
      "step": 7983
    },
    {
      "epoch": 7.99,
      "grad_norm": 81488.0390625,
      "learning_rate": 6.189886480908152e-05,
      "loss": 14.5598,
      "step": 7984
    },
    {
      "epoch": 7.99,
      "grad_norm": 16311.9619140625,
      "learning_rate": 6.18937048503612e-05,
      "loss": 27.8806,
      "step": 7985
    },
    {
      "epoch": 7.99,
      "grad_norm": 4646.81005859375,
      "learning_rate": 6.188854489164087e-05,
      "loss": 10.7746,
      "step": 7986
    },
    {
      "epoch": 7.99,
      "grad_norm": 1325.9476318359375,
      "learning_rate": 6.188338493292054e-05,
      "loss": 14.4001,
      "step": 7987
    },
    {
      "epoch": 8.0,
      "grad_norm": 161881.953125,
      "learning_rate": 6.18782249742002e-05,
      "loss": 12.3889,
      "step": 7988
    },
    {
      "epoch": 8.0,
      "grad_norm": 7927.060546875,
      "learning_rate": 6.187306501547988e-05,
      "loss": 14.5105,
      "step": 7989
    },
    {
      "epoch": 8.0,
      "grad_norm": 906.418212890625,
      "learning_rate": 6.186790505675954e-05,
      "loss": 12.0544,
      "step": 7990
    },
    {
      "epoch": 8.0,
      "grad_norm": 887.2362060546875,
      "learning_rate": 6.186274509803921e-05,
      "loss": 11.7988,
      "step": 7991
    },
    {
      "epoch": 8.0,
      "grad_norm": 22715.08203125,
      "learning_rate": 6.185758513931889e-05,
      "loss": 12.8353,
      "step": 7992
    },
    {
      "epoch": 8.0,
      "grad_norm": 15734.9599609375,
      "learning_rate": 6.185242518059856e-05,
      "loss": 13.3987,
      "step": 7993
    },
    {
      "epoch": 8.0,
      "grad_norm": 1536.305419921875,
      "learning_rate": 6.184726522187822e-05,
      "loss": 13.2071,
      "step": 7994
    },
    {
      "epoch": 8.0,
      "grad_norm": 1764.093505859375,
      "learning_rate": 6.18421052631579e-05,
      "loss": 13.1197,
      "step": 7995
    },
    {
      "epoch": 8.0,
      "grad_norm": 19296.248046875,
      "learning_rate": 6.183694530443756e-05,
      "loss": 15.3197,
      "step": 7996
    },
    {
      "epoch": 8.01,
      "grad_norm": 567.8334350585938,
      "learning_rate": 6.183178534571723e-05,
      "loss": 12.3599,
      "step": 7997
    },
    {
      "epoch": 8.01,
      "grad_norm": 6517.92431640625,
      "learning_rate": 6.182662538699691e-05,
      "loss": 13.5248,
      "step": 7998
    },
    {
      "epoch": 8.01,
      "grad_norm": 3857.498046875,
      "learning_rate": 6.182146542827658e-05,
      "loss": 21.8025,
      "step": 7999
    },
    {
      "epoch": 8.01,
      "grad_norm": 15706.6455078125,
      "learning_rate": 6.181630546955624e-05,
      "loss": 21.942,
      "step": 8000
    },
    {
      "epoch": 8.01,
      "grad_norm": 10243.1669921875,
      "learning_rate": 6.181114551083592e-05,
      "loss": 17.0685,
      "step": 8001
    },
    {
      "epoch": 8.01,
      "grad_norm": 23589.830078125,
      "learning_rate": 6.180598555211558e-05,
      "loss": 11.1946,
      "step": 8002
    },
    {
      "epoch": 8.01,
      "grad_norm": 2421.68408203125,
      "learning_rate": 6.180082559339525e-05,
      "loss": 12.2776,
      "step": 8003
    },
    {
      "epoch": 8.01,
      "grad_norm": 3426.00732421875,
      "learning_rate": 6.179566563467493e-05,
      "loss": 16.0772,
      "step": 8004
    },
    {
      "epoch": 8.01,
      "grad_norm": 5818.1474609375,
      "learning_rate": 6.179050567595459e-05,
      "loss": 20.2296,
      "step": 8005
    },
    {
      "epoch": 8.01,
      "grad_norm": 5151.166015625,
      "learning_rate": 6.178534571723426e-05,
      "loss": 16.3609,
      "step": 8006
    },
    {
      "epoch": 8.02,
      "grad_norm": 5322.7861328125,
      "learning_rate": 6.178018575851393e-05,
      "loss": 17.5193,
      "step": 8007
    },
    {
      "epoch": 8.02,
      "grad_norm": 13585.7880859375,
      "learning_rate": 6.17750257997936e-05,
      "loss": 13.226,
      "step": 8008
    },
    {
      "epoch": 8.02,
      "grad_norm": 1920.793212890625,
      "learning_rate": 6.176986584107327e-05,
      "loss": 14.5077,
      "step": 8009
    },
    {
      "epoch": 8.02,
      "grad_norm": 4643.13232421875,
      "learning_rate": 6.176470588235295e-05,
      "loss": 20.1926,
      "step": 8010
    },
    {
      "epoch": 8.02,
      "grad_norm": 693.702392578125,
      "learning_rate": 6.175954592363261e-05,
      "loss": 12.0735,
      "step": 8011
    },
    {
      "epoch": 8.02,
      "grad_norm": 555.5538330078125,
      "learning_rate": 6.175438596491228e-05,
      "loss": 13.8066,
      "step": 8012
    },
    {
      "epoch": 8.02,
      "grad_norm": 2073.2021484375,
      "learning_rate": 6.174922600619195e-05,
      "loss": 10.5043,
      "step": 8013
    },
    {
      "epoch": 8.02,
      "grad_norm": 16117.43359375,
      "learning_rate": 6.174406604747162e-05,
      "loss": 19.5897,
      "step": 8014
    },
    {
      "epoch": 8.02,
      "grad_norm": 17173.896484375,
      "learning_rate": 6.17389060887513e-05,
      "loss": 14.093,
      "step": 8015
    },
    {
      "epoch": 8.02,
      "grad_norm": 3378.34375,
      "learning_rate": 6.173374613003097e-05,
      "loss": 12.7285,
      "step": 8016
    },
    {
      "epoch": 8.03,
      "grad_norm": 11362.78125,
      "learning_rate": 6.172858617131063e-05,
      "loss": 18.3907,
      "step": 8017
    },
    {
      "epoch": 8.03,
      "grad_norm": 77500.1171875,
      "learning_rate": 6.17234262125903e-05,
      "loss": 16.7781,
      "step": 8018
    },
    {
      "epoch": 8.03,
      "grad_norm": 1717.7222900390625,
      "learning_rate": 6.171826625386997e-05,
      "loss": 13.0619,
      "step": 8019
    },
    {
      "epoch": 8.03,
      "grad_norm": 899.0440063476562,
      "learning_rate": 6.171310629514964e-05,
      "loss": 12.4861,
      "step": 8020
    },
    {
      "epoch": 8.03,
      "grad_norm": 10381.43359375,
      "learning_rate": 6.170794633642931e-05,
      "loss": 14.7644,
      "step": 8021
    },
    {
      "epoch": 8.03,
      "grad_norm": 17213.72265625,
      "learning_rate": 6.170278637770898e-05,
      "loss": 16.3333,
      "step": 8022
    },
    {
      "epoch": 8.03,
      "grad_norm": 3657.753173828125,
      "learning_rate": 6.169762641898865e-05,
      "loss": 21.4402,
      "step": 8023
    },
    {
      "epoch": 8.03,
      "grad_norm": 1798.733154296875,
      "learning_rate": 6.169246646026831e-05,
      "loss": 12.9129,
      "step": 8024
    },
    {
      "epoch": 8.03,
      "grad_norm": 70579.9765625,
      "learning_rate": 6.168730650154799e-05,
      "loss": 20.3668,
      "step": 8025
    },
    {
      "epoch": 8.03,
      "grad_norm": 12868.720703125,
      "learning_rate": 6.168214654282766e-05,
      "loss": 19.2193,
      "step": 8026
    },
    {
      "epoch": 8.04,
      "grad_norm": 4666.2939453125,
      "learning_rate": 6.167698658410733e-05,
      "loss": 14.1715,
      "step": 8027
    },
    {
      "epoch": 8.04,
      "grad_norm": 6086.66796875,
      "learning_rate": 6.1671826625387e-05,
      "loss": 13.5389,
      "step": 8028
    },
    {
      "epoch": 8.04,
      "grad_norm": 4383.69775390625,
      "learning_rate": 6.166666666666667e-05,
      "loss": 15.2234,
      "step": 8029
    },
    {
      "epoch": 8.04,
      "grad_norm": 7217.9501953125,
      "learning_rate": 6.166150670794633e-05,
      "loss": 13.7251,
      "step": 8030
    },
    {
      "epoch": 8.04,
      "grad_norm": 495.5471496582031,
      "learning_rate": 6.165634674922602e-05,
      "loss": 11.5812,
      "step": 8031
    },
    {
      "epoch": 8.04,
      "grad_norm": 952.5323486328125,
      "learning_rate": 6.165118679050568e-05,
      "loss": 12.7804,
      "step": 8032
    },
    {
      "epoch": 8.04,
      "grad_norm": 1047.070556640625,
      "learning_rate": 6.164602683178535e-05,
      "loss": 11.8098,
      "step": 8033
    },
    {
      "epoch": 8.04,
      "grad_norm": 1456.68212890625,
      "learning_rate": 6.164086687306502e-05,
      "loss": 11.9953,
      "step": 8034
    },
    {
      "epoch": 8.04,
      "grad_norm": 1326.346435546875,
      "learning_rate": 6.163570691434469e-05,
      "loss": 15.7219,
      "step": 8035
    },
    {
      "epoch": 8.04,
      "grad_norm": 4711.22265625,
      "learning_rate": 6.163054695562435e-05,
      "loss": 22.5993,
      "step": 8036
    },
    {
      "epoch": 8.05,
      "grad_norm": 1634.8724365234375,
      "learning_rate": 6.162538699690404e-05,
      "loss": 16.0455,
      "step": 8037
    },
    {
      "epoch": 8.05,
      "grad_norm": 3099.3837890625,
      "learning_rate": 6.16202270381837e-05,
      "loss": 12.9364,
      "step": 8038
    },
    {
      "epoch": 8.05,
      "grad_norm": 6526.25,
      "learning_rate": 6.161506707946336e-05,
      "loss": 21.6527,
      "step": 8039
    },
    {
      "epoch": 8.05,
      "grad_norm": 15076.939453125,
      "learning_rate": 6.160990712074304e-05,
      "loss": 18.0504,
      "step": 8040
    },
    {
      "epoch": 8.05,
      "grad_norm": 97909.03125,
      "learning_rate": 6.16047471620227e-05,
      "loss": 13.3956,
      "step": 8041
    },
    {
      "epoch": 8.05,
      "grad_norm": 54337.75390625,
      "learning_rate": 6.159958720330237e-05,
      "loss": 14.4896,
      "step": 8042
    },
    {
      "epoch": 8.05,
      "grad_norm": 20315.556640625,
      "learning_rate": 6.159442724458205e-05,
      "loss": 14.8074,
      "step": 8043
    },
    {
      "epoch": 8.05,
      "grad_norm": 8140.65185546875,
      "learning_rate": 6.158926728586172e-05,
      "loss": 12.0057,
      "step": 8044
    },
    {
      "epoch": 8.05,
      "grad_norm": 1571.0428466796875,
      "learning_rate": 6.158410732714138e-05,
      "loss": 16.4118,
      "step": 8045
    },
    {
      "epoch": 8.05,
      "grad_norm": 21516.884765625,
      "learning_rate": 6.157894736842106e-05,
      "loss": 14.2008,
      "step": 8046
    },
    {
      "epoch": 8.06,
      "grad_norm": 1221.577880859375,
      "learning_rate": 6.157378740970072e-05,
      "loss": 14.254,
      "step": 8047
    },
    {
      "epoch": 8.06,
      "grad_norm": 2120.054443359375,
      "learning_rate": 6.15686274509804e-05,
      "loss": 13.8528,
      "step": 8048
    },
    {
      "epoch": 8.06,
      "grad_norm": 10938.056640625,
      "learning_rate": 6.156346749226007e-05,
      "loss": 15.5007,
      "step": 8049
    },
    {
      "epoch": 8.06,
      "grad_norm": 1415.4727783203125,
      "learning_rate": 6.155830753353974e-05,
      "loss": 14.8198,
      "step": 8050
    },
    {
      "epoch": 8.06,
      "grad_norm": 13369.412109375,
      "learning_rate": 6.15531475748194e-05,
      "loss": 16.8195,
      "step": 8051
    },
    {
      "epoch": 8.06,
      "grad_norm": 21158.384765625,
      "learning_rate": 6.154798761609908e-05,
      "loss": 18.3193,
      "step": 8052
    },
    {
      "epoch": 8.06,
      "grad_norm": 17577.875,
      "learning_rate": 6.154282765737874e-05,
      "loss": 10.4835,
      "step": 8053
    },
    {
      "epoch": 8.06,
      "grad_norm": 1858.4599609375,
      "learning_rate": 6.153766769865842e-05,
      "loss": 14.6901,
      "step": 8054
    },
    {
      "epoch": 8.06,
      "grad_norm": 4413.01611328125,
      "learning_rate": 6.153250773993809e-05,
      "loss": 14.3673,
      "step": 8055
    },
    {
      "epoch": 8.06,
      "grad_norm": 532.51025390625,
      "learning_rate": 6.152734778121775e-05,
      "loss": 10.8752,
      "step": 8056
    },
    {
      "epoch": 8.07,
      "grad_norm": 8316.5439453125,
      "learning_rate": 6.152218782249742e-05,
      "loss": 20.016,
      "step": 8057
    },
    {
      "epoch": 8.07,
      "grad_norm": 4716.69775390625,
      "learning_rate": 6.151702786377708e-05,
      "loss": 17.709,
      "step": 8058
    },
    {
      "epoch": 8.07,
      "grad_norm": 7143.57177734375,
      "learning_rate": 6.151186790505677e-05,
      "loss": 16.3704,
      "step": 8059
    },
    {
      "epoch": 8.07,
      "grad_norm": 1586.841064453125,
      "learning_rate": 6.150670794633643e-05,
      "loss": 10.0599,
      "step": 8060
    },
    {
      "epoch": 8.07,
      "grad_norm": 9592.1513671875,
      "learning_rate": 6.15015479876161e-05,
      "loss": 12.9317,
      "step": 8061
    },
    {
      "epoch": 8.07,
      "grad_norm": 156473.734375,
      "learning_rate": 6.149638802889577e-05,
      "loss": 16.6641,
      "step": 8062
    },
    {
      "epoch": 8.07,
      "grad_norm": 3689.845458984375,
      "learning_rate": 6.149122807017544e-05,
      "loss": 12.9985,
      "step": 8063
    },
    {
      "epoch": 8.07,
      "grad_norm": 4508.623046875,
      "learning_rate": 6.14860681114551e-05,
      "loss": 15.2379,
      "step": 8064
    },
    {
      "epoch": 8.07,
      "grad_norm": 942.35205078125,
      "learning_rate": 6.148090815273479e-05,
      "loss": 12.4211,
      "step": 8065
    },
    {
      "epoch": 8.07,
      "grad_norm": 26876.37890625,
      "learning_rate": 6.147574819401445e-05,
      "loss": 13.8223,
      "step": 8066
    },
    {
      "epoch": 8.08,
      "grad_norm": 26848.05078125,
      "learning_rate": 6.147058823529413e-05,
      "loss": 14.1556,
      "step": 8067
    },
    {
      "epoch": 8.08,
      "grad_norm": 13686.876953125,
      "learning_rate": 6.146542827657379e-05,
      "loss": 14.2326,
      "step": 8068
    },
    {
      "epoch": 8.08,
      "grad_norm": 3978.468505859375,
      "learning_rate": 6.146026831785346e-05,
      "loss": 13.0636,
      "step": 8069
    },
    {
      "epoch": 8.08,
      "grad_norm": 7380.98095703125,
      "learning_rate": 6.145510835913312e-05,
      "loss": 13.3018,
      "step": 8070
    },
    {
      "epoch": 8.08,
      "grad_norm": 1852.93359375,
      "learning_rate": 6.144994840041281e-05,
      "loss": 15.0528,
      "step": 8071
    },
    {
      "epoch": 8.08,
      "grad_norm": 626332.3125,
      "learning_rate": 6.144478844169247e-05,
      "loss": 18.8444,
      "step": 8072
    },
    {
      "epoch": 8.08,
      "grad_norm": 5804.54638671875,
      "learning_rate": 6.143962848297215e-05,
      "loss": 11.3655,
      "step": 8073
    },
    {
      "epoch": 8.08,
      "grad_norm": 7747.498046875,
      "learning_rate": 6.143446852425181e-05,
      "loss": 14.2621,
      "step": 8074
    },
    {
      "epoch": 8.08,
      "grad_norm": 7001.68701171875,
      "learning_rate": 6.142930856553147e-05,
      "loss": 13.9855,
      "step": 8075
    },
    {
      "epoch": 8.08,
      "grad_norm": 8374.6767578125,
      "learning_rate": 6.142414860681116e-05,
      "loss": 14.5149,
      "step": 8076
    },
    {
      "epoch": 8.09,
      "grad_norm": 1176.9676513671875,
      "learning_rate": 6.141898864809082e-05,
      "loss": 13.0561,
      "step": 8077
    },
    {
      "epoch": 8.09,
      "grad_norm": 4482.494140625,
      "learning_rate": 6.141382868937049e-05,
      "loss": 11.5133,
      "step": 8078
    },
    {
      "epoch": 8.09,
      "grad_norm": 4685.15087890625,
      "learning_rate": 6.140866873065015e-05,
      "loss": 21.2362,
      "step": 8079
    },
    {
      "epoch": 8.09,
      "grad_norm": 5434.736328125,
      "learning_rate": 6.140350877192983e-05,
      "loss": 11.8745,
      "step": 8080
    },
    {
      "epoch": 8.09,
      "grad_norm": 5918.02197265625,
      "learning_rate": 6.139834881320949e-05,
      "loss": 12.8359,
      "step": 8081
    },
    {
      "epoch": 8.09,
      "grad_norm": 14893.001953125,
      "learning_rate": 6.139318885448918e-05,
      "loss": 21.5226,
      "step": 8082
    },
    {
      "epoch": 8.09,
      "grad_norm": 2637.137451171875,
      "learning_rate": 6.138802889576884e-05,
      "loss": 12.026,
      "step": 8083
    },
    {
      "epoch": 8.09,
      "grad_norm": 7695.45849609375,
      "learning_rate": 6.138286893704851e-05,
      "loss": 13.1002,
      "step": 8084
    },
    {
      "epoch": 8.09,
      "grad_norm": 4293.6181640625,
      "learning_rate": 6.137770897832817e-05,
      "loss": 22.3779,
      "step": 8085
    },
    {
      "epoch": 8.09,
      "grad_norm": 15513.91015625,
      "learning_rate": 6.137254901960785e-05,
      "loss": 10.8849,
      "step": 8086
    },
    {
      "epoch": 8.1,
      "grad_norm": 3896.9150390625,
      "learning_rate": 6.136738906088752e-05,
      "loss": 13.6132,
      "step": 8087
    },
    {
      "epoch": 8.1,
      "grad_norm": 26023.513671875,
      "learning_rate": 6.13622291021672e-05,
      "loss": 16.4085,
      "step": 8088
    },
    {
      "epoch": 8.1,
      "grad_norm": 12258.2314453125,
      "learning_rate": 6.135706914344686e-05,
      "loss": 17.9656,
      "step": 8089
    },
    {
      "epoch": 8.1,
      "grad_norm": 4239.2041015625,
      "learning_rate": 6.135190918472653e-05,
      "loss": 14.4163,
      "step": 8090
    },
    {
      "epoch": 8.1,
      "grad_norm": 13293.4658203125,
      "learning_rate": 6.134674922600619e-05,
      "loss": 13.1179,
      "step": 8091
    },
    {
      "epoch": 8.1,
      "grad_norm": 21007.201171875,
      "learning_rate": 6.134158926728585e-05,
      "loss": 17.3307,
      "step": 8092
    },
    {
      "epoch": 8.1,
      "grad_norm": 4185.0244140625,
      "learning_rate": 6.133642930856554e-05,
      "loss": 13.0956,
      "step": 8093
    },
    {
      "epoch": 8.1,
      "grad_norm": 1994.393310546875,
      "learning_rate": 6.13312693498452e-05,
      "loss": 9.7106,
      "step": 8094
    },
    {
      "epoch": 8.1,
      "grad_norm": 6402.22216796875,
      "learning_rate": 6.132610939112488e-05,
      "loss": 19.8966,
      "step": 8095
    },
    {
      "epoch": 8.1,
      "grad_norm": 1108.46435546875,
      "learning_rate": 6.132094943240454e-05,
      "loss": 13.0063,
      "step": 8096
    },
    {
      "epoch": 8.11,
      "grad_norm": 3453.22509765625,
      "learning_rate": 6.131578947368421e-05,
      "loss": 10.8042,
      "step": 8097
    },
    {
      "epoch": 8.11,
      "grad_norm": 761.3862915039062,
      "learning_rate": 6.131062951496387e-05,
      "loss": 12.4531,
      "step": 8098
    },
    {
      "epoch": 8.11,
      "grad_norm": 7822.193359375,
      "learning_rate": 6.130546955624356e-05,
      "loss": 13.3589,
      "step": 8099
    },
    {
      "epoch": 8.11,
      "grad_norm": 1388.038818359375,
      "learning_rate": 6.130030959752322e-05,
      "loss": 12.1832,
      "step": 8100
    },
    {
      "epoch": 8.11,
      "grad_norm": 6551.4248046875,
      "learning_rate": 6.12951496388029e-05,
      "loss": 11.9186,
      "step": 8101
    },
    {
      "epoch": 8.11,
      "grad_norm": 3423.603759765625,
      "learning_rate": 6.128998968008256e-05,
      "loss": 12.825,
      "step": 8102
    },
    {
      "epoch": 8.11,
      "grad_norm": 5675.1328125,
      "learning_rate": 6.128482972136223e-05,
      "loss": 12.5648,
      "step": 8103
    },
    {
      "epoch": 8.11,
      "grad_norm": 15294.1875,
      "learning_rate": 6.127966976264191e-05,
      "loss": 13.9675,
      "step": 8104
    },
    {
      "epoch": 8.11,
      "grad_norm": 9257.3369140625,
      "learning_rate": 6.127450980392158e-05,
      "loss": 12.3599,
      "step": 8105
    },
    {
      "epoch": 8.11,
      "grad_norm": 3000.377197265625,
      "learning_rate": 6.126934984520124e-05,
      "loss": 13.1048,
      "step": 8106
    },
    {
      "epoch": 8.12,
      "grad_norm": 7817.7998046875,
      "learning_rate": 6.126418988648092e-05,
      "loss": 16.6257,
      "step": 8107
    },
    {
      "epoch": 8.12,
      "grad_norm": 5308.1923828125,
      "learning_rate": 6.125902992776058e-05,
      "loss": 16.4723,
      "step": 8108
    },
    {
      "epoch": 8.12,
      "grad_norm": 6277.82080078125,
      "learning_rate": 6.125386996904024e-05,
      "loss": 13.8724,
      "step": 8109
    },
    {
      "epoch": 8.12,
      "grad_norm": 2256.017822265625,
      "learning_rate": 6.124871001031993e-05,
      "loss": 12.5744,
      "step": 8110
    },
    {
      "epoch": 8.12,
      "grad_norm": 5604.6640625,
      "learning_rate": 6.124355005159959e-05,
      "loss": 16.1285,
      "step": 8111
    },
    {
      "epoch": 8.12,
      "grad_norm": 535.3345336914062,
      "learning_rate": 6.123839009287926e-05,
      "loss": 12.0925,
      "step": 8112
    },
    {
      "epoch": 8.12,
      "grad_norm": 20822.572265625,
      "learning_rate": 6.123323013415892e-05,
      "loss": 21.2496,
      "step": 8113
    },
    {
      "epoch": 8.12,
      "grad_norm": 2407.913818359375,
      "learning_rate": 6.12280701754386e-05,
      "loss": 15.097,
      "step": 8114
    },
    {
      "epoch": 8.12,
      "grad_norm": 1768.9244384765625,
      "learning_rate": 6.122291021671827e-05,
      "loss": 11.348,
      "step": 8115
    },
    {
      "epoch": 8.12,
      "grad_norm": 3499.061767578125,
      "learning_rate": 6.121775025799795e-05,
      "loss": 11.542,
      "step": 8116
    },
    {
      "epoch": 8.13,
      "grad_norm": 918.676025390625,
      "learning_rate": 6.121259029927761e-05,
      "loss": 12.3354,
      "step": 8117
    },
    {
      "epoch": 8.13,
      "grad_norm": 2211.148681640625,
      "learning_rate": 6.120743034055728e-05,
      "loss": 12.8194,
      "step": 8118
    },
    {
      "epoch": 8.13,
      "grad_norm": 18360.064453125,
      "learning_rate": 6.120227038183694e-05,
      "loss": 22.1618,
      "step": 8119
    },
    {
      "epoch": 8.13,
      "grad_norm": 9715.1806640625,
      "learning_rate": 6.119711042311662e-05,
      "loss": 22.3491,
      "step": 8120
    },
    {
      "epoch": 8.13,
      "grad_norm": 824.5978393554688,
      "learning_rate": 6.119195046439629e-05,
      "loss": 13.3428,
      "step": 8121
    },
    {
      "epoch": 8.13,
      "grad_norm": 1298.216064453125,
      "learning_rate": 6.118679050567597e-05,
      "loss": 13.3291,
      "step": 8122
    },
    {
      "epoch": 8.13,
      "grad_norm": 11911.5087890625,
      "learning_rate": 6.118163054695563e-05,
      "loss": 12.9237,
      "step": 8123
    },
    {
      "epoch": 8.13,
      "grad_norm": 11903.8154296875,
      "learning_rate": 6.11764705882353e-05,
      "loss": 17.0583,
      "step": 8124
    },
    {
      "epoch": 8.13,
      "grad_norm": 52154.87890625,
      "learning_rate": 6.117131062951496e-05,
      "loss": 14.1355,
      "step": 8125
    },
    {
      "epoch": 8.13,
      "grad_norm": 29988.9296875,
      "learning_rate": 6.116615067079464e-05,
      "loss": 18.3821,
      "step": 8126
    },
    {
      "epoch": 8.14,
      "grad_norm": 9090.4208984375,
      "learning_rate": 6.116099071207431e-05,
      "loss": 12.1162,
      "step": 8127
    },
    {
      "epoch": 8.14,
      "grad_norm": 1934.4871826171875,
      "learning_rate": 6.115583075335397e-05,
      "loss": 14.1109,
      "step": 8128
    },
    {
      "epoch": 8.14,
      "grad_norm": 33425.82421875,
      "learning_rate": 6.115067079463365e-05,
      "loss": 13.5005,
      "step": 8129
    },
    {
      "epoch": 8.14,
      "grad_norm": 5506.60693359375,
      "learning_rate": 6.114551083591331e-05,
      "loss": 12.7317,
      "step": 8130
    },
    {
      "epoch": 8.14,
      "grad_norm": 1169.7198486328125,
      "learning_rate": 6.114035087719298e-05,
      "loss": 19.5585,
      "step": 8131
    },
    {
      "epoch": 8.14,
      "grad_norm": 20835.779296875,
      "learning_rate": 6.113519091847266e-05,
      "loss": 15.3851,
      "step": 8132
    },
    {
      "epoch": 8.14,
      "grad_norm": 3285.4189453125,
      "learning_rate": 6.113003095975233e-05,
      "loss": 13.6943,
      "step": 8133
    },
    {
      "epoch": 8.14,
      "grad_norm": 1414.4525146484375,
      "learning_rate": 6.1124871001032e-05,
      "loss": 13.1625,
      "step": 8134
    },
    {
      "epoch": 8.14,
      "grad_norm": 24384.744140625,
      "learning_rate": 6.111971104231167e-05,
      "loss": 20.9401,
      "step": 8135
    },
    {
      "epoch": 8.14,
      "grad_norm": 3909.67919921875,
      "learning_rate": 6.111455108359133e-05,
      "loss": 12.5196,
      "step": 8136
    },
    {
      "epoch": 8.15,
      "grad_norm": 5123.73095703125,
      "learning_rate": 6.1109391124871e-05,
      "loss": 13.8793,
      "step": 8137
    },
    {
      "epoch": 8.15,
      "grad_norm": 7806.55517578125,
      "learning_rate": 6.110423116615068e-05,
      "loss": 12.6738,
      "step": 8138
    },
    {
      "epoch": 8.15,
      "grad_norm": 792.6883544921875,
      "learning_rate": 6.109907120743035e-05,
      "loss": 12.5295,
      "step": 8139
    },
    {
      "epoch": 8.15,
      "grad_norm": 23907.125,
      "learning_rate": 6.109391124871001e-05,
      "loss": 11.7105,
      "step": 8140
    },
    {
      "epoch": 8.15,
      "grad_norm": 4856.10302734375,
      "learning_rate": 6.108875128998969e-05,
      "loss": 17.0227,
      "step": 8141
    },
    {
      "epoch": 8.15,
      "grad_norm": 1380.8131103515625,
      "learning_rate": 6.108359133126935e-05,
      "loss": 15.5458,
      "step": 8142
    },
    {
      "epoch": 8.15,
      "grad_norm": 4977.4365234375,
      "learning_rate": 6.107843137254902e-05,
      "loss": 13.798,
      "step": 8143
    },
    {
      "epoch": 8.15,
      "grad_norm": 8544.4873046875,
      "learning_rate": 6.10732714138287e-05,
      "loss": 15.9307,
      "step": 8144
    },
    {
      "epoch": 8.15,
      "grad_norm": 17393.8359375,
      "learning_rate": 6.106811145510836e-05,
      "loss": 14.6161,
      "step": 8145
    },
    {
      "epoch": 8.15,
      "grad_norm": 1950.5562744140625,
      "learning_rate": 6.106295149638803e-05,
      "loss": 13.4909,
      "step": 8146
    },
    {
      "epoch": 8.16,
      "grad_norm": 3410.45751953125,
      "learning_rate": 6.10577915376677e-05,
      "loss": 12.8641,
      "step": 8147
    },
    {
      "epoch": 8.16,
      "grad_norm": 7892.568359375,
      "learning_rate": 6.105263157894737e-05,
      "loss": 22.1474,
      "step": 8148
    },
    {
      "epoch": 8.16,
      "grad_norm": 7347.52685546875,
      "learning_rate": 6.104747162022704e-05,
      "loss": 17.2425,
      "step": 8149
    },
    {
      "epoch": 8.16,
      "grad_norm": 2392.855712890625,
      "learning_rate": 6.104231166150672e-05,
      "loss": 17.8199,
      "step": 8150
    },
    {
      "epoch": 8.16,
      "grad_norm": 3277.645751953125,
      "learning_rate": 6.103715170278638e-05,
      "loss": 17.7917,
      "step": 8151
    },
    {
      "epoch": 8.16,
      "grad_norm": 592.9613647460938,
      "learning_rate": 6.103199174406605e-05,
      "loss": 14.8374,
      "step": 8152
    },
    {
      "epoch": 8.16,
      "grad_norm": 1548.9879150390625,
      "learning_rate": 6.1026831785345714e-05,
      "loss": 18.4034,
      "step": 8153
    },
    {
      "epoch": 8.16,
      "grad_norm": 1442.3055419921875,
      "learning_rate": 6.1021671826625396e-05,
      "loss": 13.1753,
      "step": 8154
    },
    {
      "epoch": 8.16,
      "grad_norm": 22155.58203125,
      "learning_rate": 6.1016511867905057e-05,
      "loss": 13.3537,
      "step": 8155
    },
    {
      "epoch": 8.16,
      "grad_norm": 65461.69921875,
      "learning_rate": 6.101135190918473e-05,
      "loss": 16.0316,
      "step": 8156
    },
    {
      "epoch": 8.17,
      "grad_norm": 1231.8779296875,
      "learning_rate": 6.10061919504644e-05,
      "loss": 12.9376,
      "step": 8157
    },
    {
      "epoch": 8.17,
      "grad_norm": 46497.11328125,
      "learning_rate": 6.100103199174407e-05,
      "loss": 13.6421,
      "step": 8158
    },
    {
      "epoch": 8.17,
      "grad_norm": 1741.4141845703125,
      "learning_rate": 6.0995872033023734e-05,
      "loss": 13.3993,
      "step": 8159
    },
    {
      "epoch": 8.17,
      "grad_norm": 2077.733154296875,
      "learning_rate": 6.0990712074303416e-05,
      "loss": 16.7029,
      "step": 8160
    },
    {
      "epoch": 8.17,
      "grad_norm": 15330.9345703125,
      "learning_rate": 6.0985552115583077e-05,
      "loss": 19.1217,
      "step": 8161
    },
    {
      "epoch": 8.17,
      "grad_norm": 17936.361328125,
      "learning_rate": 6.098039215686275e-05,
      "loss": 20.1491,
      "step": 8162
    },
    {
      "epoch": 8.17,
      "grad_norm": 23268.68359375,
      "learning_rate": 6.097523219814242e-05,
      "loss": 13.5789,
      "step": 8163
    },
    {
      "epoch": 8.17,
      "grad_norm": 17446.109375,
      "learning_rate": 6.097007223942208e-05,
      "loss": 14.6322,
      "step": 8164
    },
    {
      "epoch": 8.17,
      "grad_norm": 13475.12890625,
      "learning_rate": 6.096491228070176e-05,
      "loss": 14.0743,
      "step": 8165
    },
    {
      "epoch": 8.17,
      "grad_norm": 296.6795654296875,
      "learning_rate": 6.095975232198142e-05,
      "loss": 13.2611,
      "step": 8166
    },
    {
      "epoch": 8.18,
      "grad_norm": 5164.81298828125,
      "learning_rate": 6.0954592363261097e-05,
      "loss": 14.0506,
      "step": 8167
    },
    {
      "epoch": 8.18,
      "grad_norm": 2046.3763427734375,
      "learning_rate": 6.0949432404540764e-05,
      "loss": 12.6107,
      "step": 8168
    },
    {
      "epoch": 8.18,
      "grad_norm": 6665.94091796875,
      "learning_rate": 6.094427244582044e-05,
      "loss": 12.8504,
      "step": 8169
    },
    {
      "epoch": 8.18,
      "grad_norm": 108330.5234375,
      "learning_rate": 6.09391124871001e-05,
      "loss": 14.347,
      "step": 8170
    },
    {
      "epoch": 8.18,
      "grad_norm": 9912.7939453125,
      "learning_rate": 6.093395252837978e-05,
      "loss": 20.2964,
      "step": 8171
    },
    {
      "epoch": 8.18,
      "grad_norm": 12058.22265625,
      "learning_rate": 6.092879256965944e-05,
      "loss": 15.3559,
      "step": 8172
    },
    {
      "epoch": 8.18,
      "grad_norm": 297771.625,
      "learning_rate": 6.0923632610939117e-05,
      "loss": 12.4364,
      "step": 8173
    },
    {
      "epoch": 8.18,
      "grad_norm": 4987.7177734375,
      "learning_rate": 6.0918472652218784e-05,
      "loss": 12.9736,
      "step": 8174
    },
    {
      "epoch": 8.18,
      "grad_norm": 13135.25390625,
      "learning_rate": 6.091331269349846e-05,
      "loss": 13.4398,
      "step": 8175
    },
    {
      "epoch": 8.18,
      "grad_norm": 1899.0968017578125,
      "learning_rate": 6.0908152734778127e-05,
      "loss": 12.6698,
      "step": 8176
    },
    {
      "epoch": 8.19,
      "grad_norm": 7106.31640625,
      "learning_rate": 6.09029927760578e-05,
      "loss": 12.995,
      "step": 8177
    },
    {
      "epoch": 8.19,
      "grad_norm": 538.1331787109375,
      "learning_rate": 6.089783281733746e-05,
      "loss": 13.0114,
      "step": 8178
    },
    {
      "epoch": 8.19,
      "grad_norm": 5862.8916015625,
      "learning_rate": 6.0892672858617137e-05,
      "loss": 20.0448,
      "step": 8179
    },
    {
      "epoch": 8.19,
      "grad_norm": 28913.837890625,
      "learning_rate": 6.0887512899896804e-05,
      "loss": 13.5757,
      "step": 8180
    },
    {
      "epoch": 8.19,
      "grad_norm": 1684.08544921875,
      "learning_rate": 6.0882352941176465e-05,
      "loss": 14.6481,
      "step": 8181
    },
    {
      "epoch": 8.19,
      "grad_norm": 3252.789794921875,
      "learning_rate": 6.0877192982456147e-05,
      "loss": 13.255,
      "step": 8182
    },
    {
      "epoch": 8.19,
      "grad_norm": 5330.3056640625,
      "learning_rate": 6.087203302373581e-05,
      "loss": 11.3818,
      "step": 8183
    },
    {
      "epoch": 8.19,
      "grad_norm": 2124.35009765625,
      "learning_rate": 6.086687306501548e-05,
      "loss": 12.4116,
      "step": 8184
    },
    {
      "epoch": 8.19,
      "grad_norm": 11491.6767578125,
      "learning_rate": 6.086171310629515e-05,
      "loss": 13.9327,
      "step": 8185
    },
    {
      "epoch": 8.19,
      "grad_norm": 5937.0537109375,
      "learning_rate": 6.0856553147574824e-05,
      "loss": 15.9523,
      "step": 8186
    },
    {
      "epoch": 8.2,
      "grad_norm": 8973.3369140625,
      "learning_rate": 6.0851393188854485e-05,
      "loss": 27.1291,
      "step": 8187
    },
    {
      "epoch": 8.2,
      "grad_norm": 7503.1806640625,
      "learning_rate": 6.0846233230134167e-05,
      "loss": 12.6508,
      "step": 8188
    },
    {
      "epoch": 8.2,
      "grad_norm": 608.6671752929688,
      "learning_rate": 6.084107327141383e-05,
      "loss": 12.5834,
      "step": 8189
    },
    {
      "epoch": 8.2,
      "grad_norm": 765.4189453125,
      "learning_rate": 6.08359133126935e-05,
      "loss": 15.4451,
      "step": 8190
    },
    {
      "epoch": 8.2,
      "grad_norm": 4499.33740234375,
      "learning_rate": 6.083075335397317e-05,
      "loss": 14.3505,
      "step": 8191
    },
    {
      "epoch": 8.2,
      "grad_norm": 5357.8193359375,
      "learning_rate": 6.0825593395252844e-05,
      "loss": 13.6425,
      "step": 8192
    },
    {
      "epoch": 8.2,
      "grad_norm": 462.0823669433594,
      "learning_rate": 6.082043343653251e-05,
      "loss": 11.423,
      "step": 8193
    },
    {
      "epoch": 8.2,
      "grad_norm": 9731.13671875,
      "learning_rate": 6.0815273477812187e-05,
      "loss": 16.6074,
      "step": 8194
    },
    {
      "epoch": 8.2,
      "grad_norm": 45133.98828125,
      "learning_rate": 6.081011351909185e-05,
      "loss": 21.076,
      "step": 8195
    },
    {
      "epoch": 8.2,
      "grad_norm": 11511.9296875,
      "learning_rate": 6.080495356037152e-05,
      "loss": 11.8384,
      "step": 8196
    },
    {
      "epoch": 8.21,
      "grad_norm": 1082.1512451171875,
      "learning_rate": 6.079979360165119e-05,
      "loss": 12.6465,
      "step": 8197
    },
    {
      "epoch": 8.21,
      "grad_norm": 1576.608642578125,
      "learning_rate": 6.0794633642930864e-05,
      "loss": 13.5723,
      "step": 8198
    },
    {
      "epoch": 8.21,
      "grad_norm": 9685.947265625,
      "learning_rate": 6.078947368421053e-05,
      "loss": 13.1237,
      "step": 8199
    },
    {
      "epoch": 8.21,
      "grad_norm": 8199.126953125,
      "learning_rate": 6.078431372549019e-05,
      "loss": 14.2228,
      "step": 8200
    },
    {
      "epoch": 8.21,
      "grad_norm": 25926.525390625,
      "learning_rate": 6.077915376676987e-05,
      "loss": 19.2616,
      "step": 8201
    },
    {
      "epoch": 8.21,
      "grad_norm": 2489.605712890625,
      "learning_rate": 6.0773993808049535e-05,
      "loss": 12.7207,
      "step": 8202
    },
    {
      "epoch": 8.21,
      "grad_norm": 1326.2637939453125,
      "learning_rate": 6.076883384932921e-05,
      "loss": 11.2795,
      "step": 8203
    },
    {
      "epoch": 8.21,
      "grad_norm": 2214.826171875,
      "learning_rate": 6.076367389060888e-05,
      "loss": 13.5588,
      "step": 8204
    },
    {
      "epoch": 8.21,
      "grad_norm": 3243.304443359375,
      "learning_rate": 6.075851393188855e-05,
      "loss": 10.3821,
      "step": 8205
    },
    {
      "epoch": 8.21,
      "grad_norm": 6242.43115234375,
      "learning_rate": 6.075335397316821e-05,
      "loss": 15.1656,
      "step": 8206
    },
    {
      "epoch": 8.22,
      "grad_norm": 8729.505859375,
      "learning_rate": 6.074819401444789e-05,
      "loss": 15.6192,
      "step": 8207
    },
    {
      "epoch": 8.22,
      "grad_norm": 20831.18359375,
      "learning_rate": 6.0743034055727555e-05,
      "loss": 14.8539,
      "step": 8208
    },
    {
      "epoch": 8.22,
      "grad_norm": 3990.58154296875,
      "learning_rate": 6.073787409700723e-05,
      "loss": 12.6726,
      "step": 8209
    },
    {
      "epoch": 8.22,
      "grad_norm": 1424.3055419921875,
      "learning_rate": 6.07327141382869e-05,
      "loss": 14.6721,
      "step": 8210
    },
    {
      "epoch": 8.22,
      "grad_norm": 3221.14697265625,
      "learning_rate": 6.072755417956657e-05,
      "loss": 12.2333,
      "step": 8211
    },
    {
      "epoch": 8.22,
      "grad_norm": 4444.5830078125,
      "learning_rate": 6.072239422084623e-05,
      "loss": 12.6957,
      "step": 8212
    },
    {
      "epoch": 8.22,
      "grad_norm": 17535.212890625,
      "learning_rate": 6.071723426212591e-05,
      "loss": 14.2239,
      "step": 8213
    },
    {
      "epoch": 8.22,
      "grad_norm": 1178.1849365234375,
      "learning_rate": 6.0712074303405575e-05,
      "loss": 11.7251,
      "step": 8214
    },
    {
      "epoch": 8.22,
      "grad_norm": 1069.4071044921875,
      "learning_rate": 6.070691434468525e-05,
      "loss": 11.1054,
      "step": 8215
    },
    {
      "epoch": 8.22,
      "grad_norm": 10787.048828125,
      "learning_rate": 6.070175438596492e-05,
      "loss": 12.9707,
      "step": 8216
    },
    {
      "epoch": 8.23,
      "grad_norm": 4408.31591796875,
      "learning_rate": 6.069659442724458e-05,
      "loss": 15.2884,
      "step": 8217
    },
    {
      "epoch": 8.23,
      "grad_norm": 110731.859375,
      "learning_rate": 6.069143446852425e-05,
      "loss": 16.0206,
      "step": 8218
    },
    {
      "epoch": 8.23,
      "grad_norm": 6202.23974609375,
      "learning_rate": 6.068627450980392e-05,
      "loss": 14.309,
      "step": 8219
    },
    {
      "epoch": 8.23,
      "grad_norm": 11252.4228515625,
      "learning_rate": 6.0681114551083595e-05,
      "loss": 15.0293,
      "step": 8220
    },
    {
      "epoch": 8.23,
      "grad_norm": 24494.162109375,
      "learning_rate": 6.067595459236326e-05,
      "loss": 17.47,
      "step": 8221
    },
    {
      "epoch": 8.23,
      "grad_norm": 6018.7099609375,
      "learning_rate": 6.067079463364294e-05,
      "loss": 13.1132,
      "step": 8222
    },
    {
      "epoch": 8.23,
      "grad_norm": 8530.943359375,
      "learning_rate": 6.06656346749226e-05,
      "loss": 14.4361,
      "step": 8223
    },
    {
      "epoch": 8.23,
      "grad_norm": 15342.4296875,
      "learning_rate": 6.066047471620227e-05,
      "loss": 15.6318,
      "step": 8224
    },
    {
      "epoch": 8.23,
      "grad_norm": 4267.08740234375,
      "learning_rate": 6.065531475748194e-05,
      "loss": 12.8085,
      "step": 8225
    },
    {
      "epoch": 8.23,
      "grad_norm": 14648.4775390625,
      "learning_rate": 6.0650154798761615e-05,
      "loss": 20.8313,
      "step": 8226
    },
    {
      "epoch": 8.24,
      "grad_norm": 2248.49609375,
      "learning_rate": 6.064499484004128e-05,
      "loss": 13.2035,
      "step": 8227
    },
    {
      "epoch": 8.24,
      "grad_norm": 2766.05517578125,
      "learning_rate": 6.063983488132096e-05,
      "loss": 11.7228,
      "step": 8228
    },
    {
      "epoch": 8.24,
      "grad_norm": 7470.2724609375,
      "learning_rate": 6.063467492260062e-05,
      "loss": 23.6594,
      "step": 8229
    },
    {
      "epoch": 8.24,
      "grad_norm": 141974.40625,
      "learning_rate": 6.062951496388029e-05,
      "loss": 14.4358,
      "step": 8230
    },
    {
      "epoch": 8.24,
      "grad_norm": 1916.142578125,
      "learning_rate": 6.062435500515996e-05,
      "loss": 16.4017,
      "step": 8231
    },
    {
      "epoch": 8.24,
      "grad_norm": 3169.354248046875,
      "learning_rate": 6.0619195046439635e-05,
      "loss": 16.1715,
      "step": 8232
    },
    {
      "epoch": 8.24,
      "grad_norm": 45153.34375,
      "learning_rate": 6.06140350877193e-05,
      "loss": 14.2742,
      "step": 8233
    },
    {
      "epoch": 8.24,
      "grad_norm": 1656.182373046875,
      "learning_rate": 6.060887512899898e-05,
      "loss": 11.0176,
      "step": 8234
    },
    {
      "epoch": 8.24,
      "grad_norm": 12470.1591796875,
      "learning_rate": 6.060371517027864e-05,
      "loss": 14.9692,
      "step": 8235
    },
    {
      "epoch": 8.24,
      "grad_norm": 1426.63623046875,
      "learning_rate": 6.0598555211558306e-05,
      "loss": 11.0718,
      "step": 8236
    },
    {
      "epoch": 8.25,
      "grad_norm": 1109.0712890625,
      "learning_rate": 6.059339525283798e-05,
      "loss": 13.7257,
      "step": 8237
    },
    {
      "epoch": 8.25,
      "grad_norm": 3911.453369140625,
      "learning_rate": 6.058823529411765e-05,
      "loss": 16.7182,
      "step": 8238
    },
    {
      "epoch": 8.25,
      "grad_norm": 827.667236328125,
      "learning_rate": 6.058307533539732e-05,
      "loss": 12.8186,
      "step": 8239
    },
    {
      "epoch": 8.25,
      "grad_norm": 8847.2978515625,
      "learning_rate": 6.0577915376676984e-05,
      "loss": 14.7953,
      "step": 8240
    },
    {
      "epoch": 8.25,
      "grad_norm": 1210.4637451171875,
      "learning_rate": 6.057275541795666e-05,
      "loss": 14.9757,
      "step": 8241
    },
    {
      "epoch": 8.25,
      "grad_norm": 30327.66796875,
      "learning_rate": 6.0567595459236326e-05,
      "loss": 12.4722,
      "step": 8242
    },
    {
      "epoch": 8.25,
      "grad_norm": 7502.24462890625,
      "learning_rate": 6.0562435500516e-05,
      "loss": 13.3975,
      "step": 8243
    },
    {
      "epoch": 8.25,
      "grad_norm": 4262.34326171875,
      "learning_rate": 6.055727554179567e-05,
      "loss": 16.208,
      "step": 8244
    },
    {
      "epoch": 8.25,
      "grad_norm": 986.0858764648438,
      "learning_rate": 6.055211558307534e-05,
      "loss": 12.5582,
      "step": 8245
    },
    {
      "epoch": 8.25,
      "grad_norm": 7365.3544921875,
      "learning_rate": 6.0546955624355004e-05,
      "loss": 16.6979,
      "step": 8246
    },
    {
      "epoch": 8.26,
      "grad_norm": 1917.88818359375,
      "learning_rate": 6.054179566563468e-05,
      "loss": 16.5579,
      "step": 8247
    },
    {
      "epoch": 8.26,
      "grad_norm": 1675.8355712890625,
      "learning_rate": 6.0536635706914346e-05,
      "loss": 16.7779,
      "step": 8248
    },
    {
      "epoch": 8.26,
      "grad_norm": 4365.50439453125,
      "learning_rate": 6.053147574819402e-05,
      "loss": 11.8109,
      "step": 8249
    },
    {
      "epoch": 8.26,
      "grad_norm": 1965.945068359375,
      "learning_rate": 6.052631578947369e-05,
      "loss": 17.2722,
      "step": 8250
    },
    {
      "epoch": 8.26,
      "grad_norm": 4427.96923828125,
      "learning_rate": 6.052115583075336e-05,
      "loss": 15.1323,
      "step": 8251
    },
    {
      "epoch": 8.26,
      "grad_norm": 16938.916015625,
      "learning_rate": 6.0515995872033024e-05,
      "loss": 17.58,
      "step": 8252
    },
    {
      "epoch": 8.26,
      "grad_norm": 15475.5830078125,
      "learning_rate": 6.051083591331269e-05,
      "loss": 12.663,
      "step": 8253
    },
    {
      "epoch": 8.26,
      "grad_norm": 11702.6474609375,
      "learning_rate": 6.0505675954592366e-05,
      "loss": 16.9267,
      "step": 8254
    },
    {
      "epoch": 8.26,
      "grad_norm": 1144.324951171875,
      "learning_rate": 6.0500515995872034e-05,
      "loss": 12.3957,
      "step": 8255
    },
    {
      "epoch": 8.26,
      "grad_norm": 2987.22216796875,
      "learning_rate": 6.049535603715171e-05,
      "loss": 12.4845,
      "step": 8256
    },
    {
      "epoch": 8.27,
      "grad_norm": 6201.7880859375,
      "learning_rate": 6.049019607843137e-05,
      "loss": 14.3587,
      "step": 8257
    },
    {
      "epoch": 8.27,
      "grad_norm": 31788.4609375,
      "learning_rate": 6.0485036119711044e-05,
      "loss": 11.9982,
      "step": 8258
    },
    {
      "epoch": 8.27,
      "grad_norm": 11274.5771484375,
      "learning_rate": 6.047987616099071e-05,
      "loss": 17.1066,
      "step": 8259
    },
    {
      "epoch": 8.27,
      "grad_norm": 896.214599609375,
      "learning_rate": 6.0474716202270386e-05,
      "loss": 15.8084,
      "step": 8260
    },
    {
      "epoch": 8.27,
      "grad_norm": 8957.7119140625,
      "learning_rate": 6.0469556243550054e-05,
      "loss": 15.1807,
      "step": 8261
    },
    {
      "epoch": 8.27,
      "grad_norm": 17706.142578125,
      "learning_rate": 6.046439628482973e-05,
      "loss": 15.4524,
      "step": 8262
    },
    {
      "epoch": 8.27,
      "grad_norm": 14441.49609375,
      "learning_rate": 6.045923632610939e-05,
      "loss": 20.8487,
      "step": 8263
    },
    {
      "epoch": 8.27,
      "grad_norm": 5584.2861328125,
      "learning_rate": 6.045407636738907e-05,
      "loss": 12.7749,
      "step": 8264
    },
    {
      "epoch": 8.27,
      "grad_norm": 94786.6484375,
      "learning_rate": 6.044891640866873e-05,
      "loss": 15.1115,
      "step": 8265
    },
    {
      "epoch": 8.27,
      "grad_norm": 1140.8486328125,
      "learning_rate": 6.0443756449948406e-05,
      "loss": 12.9281,
      "step": 8266
    },
    {
      "epoch": 8.28,
      "grad_norm": 12204.6416015625,
      "learning_rate": 6.0438596491228074e-05,
      "loss": 16.3476,
      "step": 8267
    },
    {
      "epoch": 8.28,
      "grad_norm": 15894.845703125,
      "learning_rate": 6.043343653250775e-05,
      "loss": 19.2171,
      "step": 8268
    },
    {
      "epoch": 8.28,
      "grad_norm": 8900.0498046875,
      "learning_rate": 6.042827657378741e-05,
      "loss": 15.1546,
      "step": 8269
    },
    {
      "epoch": 8.28,
      "grad_norm": 5129.75537109375,
      "learning_rate": 6.042311661506708e-05,
      "loss": 20.0792,
      "step": 8270
    },
    {
      "epoch": 8.28,
      "grad_norm": 46460.7265625,
      "learning_rate": 6.041795665634675e-05,
      "loss": 13.4434,
      "step": 8271
    },
    {
      "epoch": 8.28,
      "grad_norm": 2779.7255859375,
      "learning_rate": 6.041279669762642e-05,
      "loss": 13.6209,
      "step": 8272
    },
    {
      "epoch": 8.28,
      "grad_norm": 1581.551025390625,
      "learning_rate": 6.0407636738906094e-05,
      "loss": 14.8504,
      "step": 8273
    },
    {
      "epoch": 8.28,
      "grad_norm": 3247.265869140625,
      "learning_rate": 6.0402476780185755e-05,
      "loss": 11.6024,
      "step": 8274
    },
    {
      "epoch": 8.28,
      "grad_norm": 9801.1103515625,
      "learning_rate": 6.039731682146543e-05,
      "loss": 12.8844,
      "step": 8275
    },
    {
      "epoch": 8.28,
      "grad_norm": 4244.2197265625,
      "learning_rate": 6.03921568627451e-05,
      "loss": 11.7229,
      "step": 8276
    },
    {
      "epoch": 8.29,
      "grad_norm": 1126.0777587890625,
      "learning_rate": 6.038699690402477e-05,
      "loss": 15.7456,
      "step": 8277
    },
    {
      "epoch": 8.29,
      "grad_norm": 16690.408203125,
      "learning_rate": 6.038183694530444e-05,
      "loss": 27.9351,
      "step": 8278
    },
    {
      "epoch": 8.29,
      "grad_norm": 62592.46484375,
      "learning_rate": 6.0376676986584114e-05,
      "loss": 12.6121,
      "step": 8279
    },
    {
      "epoch": 8.29,
      "grad_norm": 2619.857666015625,
      "learning_rate": 6.0371517027863775e-05,
      "loss": 12.9232,
      "step": 8280
    },
    {
      "epoch": 8.29,
      "grad_norm": 3648.41015625,
      "learning_rate": 6.0366357069143456e-05,
      "loss": 13.3763,
      "step": 8281
    },
    {
      "epoch": 8.29,
      "grad_norm": 6003.458984375,
      "learning_rate": 6.036119711042312e-05,
      "loss": 13.6492,
      "step": 8282
    },
    {
      "epoch": 8.29,
      "grad_norm": 2275.538330078125,
      "learning_rate": 6.035603715170279e-05,
      "loss": 11.5967,
      "step": 8283
    },
    {
      "epoch": 8.29,
      "grad_norm": 1439.858154296875,
      "learning_rate": 6.035087719298246e-05,
      "loss": 13.596,
      "step": 8284
    },
    {
      "epoch": 8.29,
      "grad_norm": 1534.8525390625,
      "learning_rate": 6.0345717234262134e-05,
      "loss": 14.0862,
      "step": 8285
    },
    {
      "epoch": 8.29,
      "grad_norm": 3548.54150390625,
      "learning_rate": 6.0340557275541795e-05,
      "loss": 26.1204,
      "step": 8286
    },
    {
      "epoch": 8.3,
      "grad_norm": 6103.04248046875,
      "learning_rate": 6.0335397316821476e-05,
      "loss": 13.1834,
      "step": 8287
    },
    {
      "epoch": 8.3,
      "grad_norm": 3358.518310546875,
      "learning_rate": 6.033023735810114e-05,
      "loss": 12.7696,
      "step": 8288
    },
    {
      "epoch": 8.3,
      "grad_norm": 3343.538330078125,
      "learning_rate": 6.0325077399380805e-05,
      "loss": 14.1275,
      "step": 8289
    },
    {
      "epoch": 8.3,
      "grad_norm": 13574.78125,
      "learning_rate": 6.031991744066048e-05,
      "loss": 26.7652,
      "step": 8290
    },
    {
      "epoch": 8.3,
      "grad_norm": 11109.1708984375,
      "learning_rate": 6.031475748194014e-05,
      "loss": 12.9909,
      "step": 8291
    },
    {
      "epoch": 8.3,
      "grad_norm": 19039.34375,
      "learning_rate": 6.030959752321982e-05,
      "loss": 15.6792,
      "step": 8292
    },
    {
      "epoch": 8.3,
      "grad_norm": 1296.201416015625,
      "learning_rate": 6.030443756449948e-05,
      "loss": 10.571,
      "step": 8293
    },
    {
      "epoch": 8.3,
      "grad_norm": 6224.25439453125,
      "learning_rate": 6.029927760577916e-05,
      "loss": 12.7084,
      "step": 8294
    },
    {
      "epoch": 8.3,
      "grad_norm": 7577.47412109375,
      "learning_rate": 6.0294117647058825e-05,
      "loss": 11.8562,
      "step": 8295
    },
    {
      "epoch": 8.3,
      "grad_norm": 883.30712890625,
      "learning_rate": 6.02889576883385e-05,
      "loss": 12.4571,
      "step": 8296
    },
    {
      "epoch": 8.31,
      "grad_norm": 3792.9296875,
      "learning_rate": 6.028379772961816e-05,
      "loss": 16.4313,
      "step": 8297
    },
    {
      "epoch": 8.31,
      "grad_norm": 2526.0654296875,
      "learning_rate": 6.027863777089784e-05,
      "loss": 12.2262,
      "step": 8298
    },
    {
      "epoch": 8.31,
      "grad_norm": 146834.40625,
      "learning_rate": 6.02734778121775e-05,
      "loss": 23.2864,
      "step": 8299
    },
    {
      "epoch": 8.31,
      "grad_norm": 3859.9189453125,
      "learning_rate": 6.026831785345718e-05,
      "loss": 12.8514,
      "step": 8300
    },
    {
      "epoch": 8.31,
      "grad_norm": 1797.636962890625,
      "learning_rate": 6.0263157894736845e-05,
      "loss": 11.8254,
      "step": 8301
    },
    {
      "epoch": 8.31,
      "grad_norm": 19946.41796875,
      "learning_rate": 6.025799793601652e-05,
      "loss": 16.7451,
      "step": 8302
    },
    {
      "epoch": 8.31,
      "grad_norm": 12370.2109375,
      "learning_rate": 6.025283797729618e-05,
      "loss": 12.5687,
      "step": 8303
    },
    {
      "epoch": 8.31,
      "grad_norm": 9408.5546875,
      "learning_rate": 6.024767801857586e-05,
      "loss": 15.4944,
      "step": 8304
    },
    {
      "epoch": 8.31,
      "grad_norm": 1708.149658203125,
      "learning_rate": 6.024251805985552e-05,
      "loss": 11.5186,
      "step": 8305
    },
    {
      "epoch": 8.31,
      "grad_norm": 6456.89794921875,
      "learning_rate": 6.023735810113519e-05,
      "loss": 16.0035,
      "step": 8306
    },
    {
      "epoch": 8.32,
      "grad_norm": 32677.787109375,
      "learning_rate": 6.0232198142414865e-05,
      "loss": 24.4021,
      "step": 8307
    },
    {
      "epoch": 8.32,
      "grad_norm": 16634.353515625,
      "learning_rate": 6.0227038183694526e-05,
      "loss": 11.7125,
      "step": 8308
    },
    {
      "epoch": 8.32,
      "grad_norm": 24544.7421875,
      "learning_rate": 6.022187822497421e-05,
      "loss": 21.9075,
      "step": 8309
    },
    {
      "epoch": 8.32,
      "grad_norm": 10802.2724609375,
      "learning_rate": 6.021671826625387e-05,
      "loss": 12.4845,
      "step": 8310
    },
    {
      "epoch": 8.32,
      "grad_norm": 547.7667236328125,
      "learning_rate": 6.021155830753354e-05,
      "loss": 12.0169,
      "step": 8311
    },
    {
      "epoch": 8.32,
      "grad_norm": 11417.5244140625,
      "learning_rate": 6.020639834881321e-05,
      "loss": 16.3101,
      "step": 8312
    },
    {
      "epoch": 8.32,
      "grad_norm": 8918.66796875,
      "learning_rate": 6.0201238390092885e-05,
      "loss": 20.5583,
      "step": 8313
    },
    {
      "epoch": 8.32,
      "grad_norm": 3006.209228515625,
      "learning_rate": 6.0196078431372546e-05,
      "loss": 17.2502,
      "step": 8314
    },
    {
      "epoch": 8.32,
      "grad_norm": 10156.998046875,
      "learning_rate": 6.019091847265223e-05,
      "loss": 23.3762,
      "step": 8315
    },
    {
      "epoch": 8.32,
      "grad_norm": 4454.1806640625,
      "learning_rate": 6.018575851393189e-05,
      "loss": 16.4788,
      "step": 8316
    },
    {
      "epoch": 8.33,
      "grad_norm": 796.1248168945312,
      "learning_rate": 6.018059855521156e-05,
      "loss": 11.248,
      "step": 8317
    },
    {
      "epoch": 8.33,
      "grad_norm": 747.7064819335938,
      "learning_rate": 6.017543859649123e-05,
      "loss": 11.0511,
      "step": 8318
    },
    {
      "epoch": 8.33,
      "grad_norm": 7691.9345703125,
      "learning_rate": 6.0170278637770905e-05,
      "loss": 13.0966,
      "step": 8319
    },
    {
      "epoch": 8.33,
      "grad_norm": 4536.80419921875,
      "learning_rate": 6.016511867905057e-05,
      "loss": 13.8414,
      "step": 8320
    },
    {
      "epoch": 8.33,
      "grad_norm": 13455.7392578125,
      "learning_rate": 6.015995872033025e-05,
      "loss": 14.9534,
      "step": 8321
    },
    {
      "epoch": 8.33,
      "grad_norm": 9967.7001953125,
      "learning_rate": 6.015479876160991e-05,
      "loss": 16.4216,
      "step": 8322
    },
    {
      "epoch": 8.33,
      "grad_norm": 6138.388671875,
      "learning_rate": 6.014963880288958e-05,
      "loss": 17.6451,
      "step": 8323
    },
    {
      "epoch": 8.33,
      "grad_norm": 2839.5458984375,
      "learning_rate": 6.014447884416925e-05,
      "loss": 12.703,
      "step": 8324
    },
    {
      "epoch": 8.33,
      "grad_norm": 2339.691162109375,
      "learning_rate": 6.013931888544891e-05,
      "loss": 16.122,
      "step": 8325
    },
    {
      "epoch": 8.33,
      "grad_norm": 8024.90771484375,
      "learning_rate": 6.013415892672859e-05,
      "loss": 16.879,
      "step": 8326
    },
    {
      "epoch": 8.34,
      "grad_norm": 54766.8671875,
      "learning_rate": 6.0128998968008254e-05,
      "loss": 12.0058,
      "step": 8327
    },
    {
      "epoch": 8.34,
      "grad_norm": 1701.33544921875,
      "learning_rate": 6.012383900928793e-05,
      "loss": 15.4041,
      "step": 8328
    },
    {
      "epoch": 8.34,
      "grad_norm": 2048.519287109375,
      "learning_rate": 6.0118679050567596e-05,
      "loss": 12.7868,
      "step": 8329
    },
    {
      "epoch": 8.34,
      "grad_norm": 1159.4193115234375,
      "learning_rate": 6.011351909184727e-05,
      "loss": 12.5569,
      "step": 8330
    },
    {
      "epoch": 8.34,
      "grad_norm": 5273.517578125,
      "learning_rate": 6.010835913312693e-05,
      "loss": 16.5355,
      "step": 8331
    },
    {
      "epoch": 8.34,
      "grad_norm": 493.6612854003906,
      "learning_rate": 6.010319917440661e-05,
      "loss": 14.5735,
      "step": 8332
    },
    {
      "epoch": 8.34,
      "grad_norm": 680.5882568359375,
      "learning_rate": 6.0098039215686274e-05,
      "loss": 11.3286,
      "step": 8333
    },
    {
      "epoch": 8.34,
      "grad_norm": 17168.1875,
      "learning_rate": 6.009287925696595e-05,
      "loss": 16.6242,
      "step": 8334
    },
    {
      "epoch": 8.34,
      "grad_norm": 7311.62890625,
      "learning_rate": 6.0087719298245616e-05,
      "loss": 12.5039,
      "step": 8335
    },
    {
      "epoch": 8.34,
      "grad_norm": 339581.0625,
      "learning_rate": 6.008255933952529e-05,
      "loss": 16.1794,
      "step": 8336
    },
    {
      "epoch": 8.35,
      "grad_norm": 669.9773559570312,
      "learning_rate": 6.007739938080496e-05,
      "loss": 14.2847,
      "step": 8337
    },
    {
      "epoch": 8.35,
      "grad_norm": 167125.90625,
      "learning_rate": 6.007223942208463e-05,
      "loss": 13.1321,
      "step": 8338
    },
    {
      "epoch": 8.35,
      "grad_norm": 5387.54736328125,
      "learning_rate": 6.0067079463364294e-05,
      "loss": 12.1292,
      "step": 8339
    },
    {
      "epoch": 8.35,
      "grad_norm": 1544.1142578125,
      "learning_rate": 6.006191950464397e-05,
      "loss": 11.3924,
      "step": 8340
    },
    {
      "epoch": 8.35,
      "grad_norm": 3338.5,
      "learning_rate": 6.0056759545923636e-05,
      "loss": 14.887,
      "step": 8341
    },
    {
      "epoch": 8.35,
      "grad_norm": 11637.66015625,
      "learning_rate": 6.00515995872033e-05,
      "loss": 15.8399,
      "step": 8342
    },
    {
      "epoch": 8.35,
      "grad_norm": 1452.137939453125,
      "learning_rate": 6.004643962848298e-05,
      "loss": 13.013,
      "step": 8343
    },
    {
      "epoch": 8.35,
      "grad_norm": 1653.0537109375,
      "learning_rate": 6.004127966976264e-05,
      "loss": 14.8319,
      "step": 8344
    },
    {
      "epoch": 8.35,
      "grad_norm": 2462.76171875,
      "learning_rate": 6.0036119711042314e-05,
      "loss": 13.1868,
      "step": 8345
    },
    {
      "epoch": 8.35,
      "grad_norm": 7212.98486328125,
      "learning_rate": 6.003095975232198e-05,
      "loss": 12.316,
      "step": 8346
    },
    {
      "epoch": 8.36,
      "grad_norm": 15360.0380859375,
      "learning_rate": 6.0025799793601656e-05,
      "loss": 14.4169,
      "step": 8347
    },
    {
      "epoch": 8.36,
      "grad_norm": 15138.9560546875,
      "learning_rate": 6.002063983488132e-05,
      "loss": 18.3067,
      "step": 8348
    },
    {
      "epoch": 8.36,
      "grad_norm": 90310.109375,
      "learning_rate": 6.0015479876161e-05,
      "loss": 20.331,
      "step": 8349
    },
    {
      "epoch": 8.36,
      "grad_norm": 4871.611328125,
      "learning_rate": 6.001031991744066e-05,
      "loss": 16.1171,
      "step": 8350
    },
    {
      "epoch": 8.36,
      "grad_norm": 22337.201171875,
      "learning_rate": 6.0005159958720334e-05,
      "loss": 13.4875,
      "step": 8351
    },
    {
      "epoch": 8.36,
      "grad_norm": 6679.4482421875,
      "learning_rate": 6e-05,
      "loss": 12.9085,
      "step": 8352
    },
    {
      "epoch": 8.36,
      "grad_norm": 523.1471557617188,
      "learning_rate": 5.9994840041279676e-05,
      "loss": 10.8509,
      "step": 8353
    },
    {
      "epoch": 8.36,
      "grad_norm": 2106.515869140625,
      "learning_rate": 5.9989680082559344e-05,
      "loss": 13.6005,
      "step": 8354
    },
    {
      "epoch": 8.36,
      "grad_norm": 10307.3603515625,
      "learning_rate": 5.998452012383902e-05,
      "loss": 12.381,
      "step": 8355
    },
    {
      "epoch": 8.36,
      "grad_norm": 3929.90185546875,
      "learning_rate": 5.997936016511868e-05,
      "loss": 13.3199,
      "step": 8356
    },
    {
      "epoch": 8.37,
      "grad_norm": 1395.2967529296875,
      "learning_rate": 5.9974200206398354e-05,
      "loss": 12.7903,
      "step": 8357
    },
    {
      "epoch": 8.37,
      "grad_norm": 1806.4677734375,
      "learning_rate": 5.996904024767802e-05,
      "loss": 11.9516,
      "step": 8358
    },
    {
      "epoch": 8.37,
      "grad_norm": 6241.6259765625,
      "learning_rate": 5.9963880288957696e-05,
      "loss": 16.0109,
      "step": 8359
    },
    {
      "epoch": 8.37,
      "grad_norm": 888.7664794921875,
      "learning_rate": 5.9958720330237364e-05,
      "loss": 16.2973,
      "step": 8360
    },
    {
      "epoch": 8.37,
      "grad_norm": 4777.80517578125,
      "learning_rate": 5.9953560371517025e-05,
      "loss": 15.1007,
      "step": 8361
    },
    {
      "epoch": 8.37,
      "grad_norm": 9223.41015625,
      "learning_rate": 5.99484004127967e-05,
      "loss": 11.9778,
      "step": 8362
    },
    {
      "epoch": 8.37,
      "grad_norm": 7396.28759765625,
      "learning_rate": 5.994324045407637e-05,
      "loss": 11.4575,
      "step": 8363
    },
    {
      "epoch": 8.37,
      "grad_norm": 1913.5849609375,
      "learning_rate": 5.993808049535604e-05,
      "loss": 12.9723,
      "step": 8364
    },
    {
      "epoch": 8.37,
      "grad_norm": 3085.40087890625,
      "learning_rate": 5.993292053663571e-05,
      "loss": 15.0864,
      "step": 8365
    },
    {
      "epoch": 8.37,
      "grad_norm": 36640.94140625,
      "learning_rate": 5.9927760577915384e-05,
      "loss": 13.4034,
      "step": 8366
    },
    {
      "epoch": 8.38,
      "grad_norm": 5141.0419921875,
      "learning_rate": 5.9922600619195045e-05,
      "loss": 12.3605,
      "step": 8367
    },
    {
      "epoch": 8.38,
      "grad_norm": 59672.421875,
      "learning_rate": 5.991744066047472e-05,
      "loss": 16.1862,
      "step": 8368
    },
    {
      "epoch": 8.38,
      "grad_norm": 3033.4150390625,
      "learning_rate": 5.991228070175439e-05,
      "loss": 13.0564,
      "step": 8369
    },
    {
      "epoch": 8.38,
      "grad_norm": 3149.48291015625,
      "learning_rate": 5.990712074303406e-05,
      "loss": 14.4187,
      "step": 8370
    },
    {
      "epoch": 8.38,
      "grad_norm": 1516.6646728515625,
      "learning_rate": 5.990196078431373e-05,
      "loss": 12.6113,
      "step": 8371
    },
    {
      "epoch": 8.38,
      "grad_norm": 5561.77880859375,
      "learning_rate": 5.9896800825593404e-05,
      "loss": 13.3924,
      "step": 8372
    },
    {
      "epoch": 8.38,
      "grad_norm": 2879.157470703125,
      "learning_rate": 5.9891640866873065e-05,
      "loss": 18.59,
      "step": 8373
    },
    {
      "epoch": 8.38,
      "grad_norm": 2787.43115234375,
      "learning_rate": 5.988648090815274e-05,
      "loss": 17.3804,
      "step": 8374
    },
    {
      "epoch": 8.38,
      "grad_norm": 7096.296875,
      "learning_rate": 5.988132094943241e-05,
      "loss": 15.1193,
      "step": 8375
    },
    {
      "epoch": 8.38,
      "grad_norm": 7713.70263671875,
      "learning_rate": 5.987616099071208e-05,
      "loss": 14.6635,
      "step": 8376
    },
    {
      "epoch": 8.39,
      "grad_norm": 1355.7821044921875,
      "learning_rate": 5.987100103199175e-05,
      "loss": 12.5577,
      "step": 8377
    },
    {
      "epoch": 8.39,
      "grad_norm": 2390.458984375,
      "learning_rate": 5.986584107327141e-05,
      "loss": 16.1926,
      "step": 8378
    },
    {
      "epoch": 8.39,
      "grad_norm": 1994.485595703125,
      "learning_rate": 5.9860681114551085e-05,
      "loss": 13.4651,
      "step": 8379
    },
    {
      "epoch": 8.39,
      "grad_norm": 4045.50830078125,
      "learning_rate": 5.985552115583075e-05,
      "loss": 16.6462,
      "step": 8380
    },
    {
      "epoch": 8.39,
      "grad_norm": 5052.35595703125,
      "learning_rate": 5.985036119711043e-05,
      "loss": 12.1014,
      "step": 8381
    },
    {
      "epoch": 8.39,
      "grad_norm": 1587.8870849609375,
      "learning_rate": 5.9845201238390095e-05,
      "loss": 12.7461,
      "step": 8382
    },
    {
      "epoch": 8.39,
      "grad_norm": 1564.3717041015625,
      "learning_rate": 5.984004127966977e-05,
      "loss": 16.3112,
      "step": 8383
    },
    {
      "epoch": 8.39,
      "grad_norm": 9906.427734375,
      "learning_rate": 5.983488132094943e-05,
      "loss": 13.0773,
      "step": 8384
    },
    {
      "epoch": 8.39,
      "grad_norm": 1824.0933837890625,
      "learning_rate": 5.9829721362229105e-05,
      "loss": 13.6316,
      "step": 8385
    },
    {
      "epoch": 8.39,
      "grad_norm": 6199.861328125,
      "learning_rate": 5.982456140350877e-05,
      "loss": 13.3151,
      "step": 8386
    },
    {
      "epoch": 8.4,
      "grad_norm": 6451.86669921875,
      "learning_rate": 5.981940144478845e-05,
      "loss": 15.5142,
      "step": 8387
    },
    {
      "epoch": 8.4,
      "grad_norm": 5261.8466796875,
      "learning_rate": 5.9814241486068115e-05,
      "loss": 12.0174,
      "step": 8388
    },
    {
      "epoch": 8.4,
      "grad_norm": 4004.736328125,
      "learning_rate": 5.980908152734779e-05,
      "loss": 11.5204,
      "step": 8389
    },
    {
      "epoch": 8.4,
      "grad_norm": 1698.0712890625,
      "learning_rate": 5.980392156862745e-05,
      "loss": 12.2025,
      "step": 8390
    },
    {
      "epoch": 8.4,
      "grad_norm": 2201.457275390625,
      "learning_rate": 5.9798761609907125e-05,
      "loss": 17.0111,
      "step": 8391
    },
    {
      "epoch": 8.4,
      "grad_norm": 2294.182373046875,
      "learning_rate": 5.979360165118679e-05,
      "loss": 17.2231,
      "step": 8392
    },
    {
      "epoch": 8.4,
      "grad_norm": 745.62548828125,
      "learning_rate": 5.978844169246647e-05,
      "loss": 12.2538,
      "step": 8393
    },
    {
      "epoch": 8.4,
      "grad_norm": 1254.721923828125,
      "learning_rate": 5.9783281733746135e-05,
      "loss": 16.6,
      "step": 8394
    },
    {
      "epoch": 8.4,
      "grad_norm": 7757.4560546875,
      "learning_rate": 5.9778121775025796e-05,
      "loss": 13.2888,
      "step": 8395
    },
    {
      "epoch": 8.4,
      "grad_norm": 6741.34912109375,
      "learning_rate": 5.977296181630547e-05,
      "loss": 17.5831,
      "step": 8396
    },
    {
      "epoch": 8.41,
      "grad_norm": 6876.92236328125,
      "learning_rate": 5.976780185758514e-05,
      "loss": 15.8009,
      "step": 8397
    },
    {
      "epoch": 8.41,
      "grad_norm": 473.3315124511719,
      "learning_rate": 5.976264189886481e-05,
      "loss": 12.9497,
      "step": 8398
    },
    {
      "epoch": 8.41,
      "grad_norm": 4689.68359375,
      "learning_rate": 5.975748194014448e-05,
      "loss": 14.1378,
      "step": 8399
    },
    {
      "epoch": 8.41,
      "grad_norm": 824.002197265625,
      "learning_rate": 5.9752321981424155e-05,
      "loss": 11.9193,
      "step": 8400
    },
    {
      "epoch": 8.41,
      "grad_norm": 4873.1904296875,
      "learning_rate": 5.9747162022703816e-05,
      "loss": 14.7252,
      "step": 8401
    },
    {
      "epoch": 8.41,
      "grad_norm": 10127.8349609375,
      "learning_rate": 5.974200206398349e-05,
      "loss": 13.8427,
      "step": 8402
    },
    {
      "epoch": 8.41,
      "grad_norm": 1747.3414306640625,
      "learning_rate": 5.973684210526316e-05,
      "loss": 13.7931,
      "step": 8403
    },
    {
      "epoch": 8.41,
      "grad_norm": 34068.4375,
      "learning_rate": 5.973168214654283e-05,
      "loss": 17.8049,
      "step": 8404
    },
    {
      "epoch": 8.41,
      "grad_norm": 2112.197021484375,
      "learning_rate": 5.97265221878225e-05,
      "loss": 13.6049,
      "step": 8405
    },
    {
      "epoch": 8.41,
      "grad_norm": 17709.630859375,
      "learning_rate": 5.9721362229102175e-05,
      "loss": 11.4494,
      "step": 8406
    },
    {
      "epoch": 8.42,
      "grad_norm": 15150.2587890625,
      "learning_rate": 5.9716202270381836e-05,
      "loss": 11.8897,
      "step": 8407
    },
    {
      "epoch": 8.42,
      "grad_norm": 3809.11083984375,
      "learning_rate": 5.971104231166151e-05,
      "loss": 11.7497,
      "step": 8408
    },
    {
      "epoch": 8.42,
      "grad_norm": 20469.59375,
      "learning_rate": 5.970588235294118e-05,
      "loss": 11.8379,
      "step": 8409
    },
    {
      "epoch": 8.42,
      "grad_norm": 19257.3671875,
      "learning_rate": 5.970072239422085e-05,
      "loss": 12.2475,
      "step": 8410
    },
    {
      "epoch": 8.42,
      "grad_norm": 7061.68408203125,
      "learning_rate": 5.969556243550052e-05,
      "loss": 19.0029,
      "step": 8411
    },
    {
      "epoch": 8.42,
      "grad_norm": 26376.765625,
      "learning_rate": 5.9690402476780195e-05,
      "loss": 12.0909,
      "step": 8412
    },
    {
      "epoch": 8.42,
      "grad_norm": 2477.9814453125,
      "learning_rate": 5.9685242518059856e-05,
      "loss": 12.7302,
      "step": 8413
    },
    {
      "epoch": 8.42,
      "grad_norm": 10631.61328125,
      "learning_rate": 5.9680082559339524e-05,
      "loss": 13.9368,
      "step": 8414
    },
    {
      "epoch": 8.42,
      "grad_norm": 2269.7705078125,
      "learning_rate": 5.96749226006192e-05,
      "loss": 12.2009,
      "step": 8415
    },
    {
      "epoch": 8.42,
      "grad_norm": 649.4714965820312,
      "learning_rate": 5.9669762641898866e-05,
      "loss": 15.719,
      "step": 8416
    },
    {
      "epoch": 8.43,
      "grad_norm": 2323.30859375,
      "learning_rate": 5.966460268317854e-05,
      "loss": 11.378,
      "step": 8417
    },
    {
      "epoch": 8.43,
      "grad_norm": 7764.83154296875,
      "learning_rate": 5.96594427244582e-05,
      "loss": 20.6271,
      "step": 8418
    },
    {
      "epoch": 8.43,
      "grad_norm": 5888.48388671875,
      "learning_rate": 5.9654282765737876e-05,
      "loss": 21.7703,
      "step": 8419
    },
    {
      "epoch": 8.43,
      "grad_norm": 3568.7509765625,
      "learning_rate": 5.9649122807017544e-05,
      "loss": 14.2779,
      "step": 8420
    },
    {
      "epoch": 8.43,
      "grad_norm": 29428.724609375,
      "learning_rate": 5.964396284829722e-05,
      "loss": 16.8875,
      "step": 8421
    },
    {
      "epoch": 8.43,
      "grad_norm": 7456.3251953125,
      "learning_rate": 5.9638802889576886e-05,
      "loss": 20.5507,
      "step": 8422
    },
    {
      "epoch": 8.43,
      "grad_norm": 2868.063720703125,
      "learning_rate": 5.963364293085656e-05,
      "loss": 15.795,
      "step": 8423
    },
    {
      "epoch": 8.43,
      "grad_norm": 840.5671997070312,
      "learning_rate": 5.962848297213622e-05,
      "loss": 12.7724,
      "step": 8424
    },
    {
      "epoch": 8.43,
      "grad_norm": 3928.73291015625,
      "learning_rate": 5.96233230134159e-05,
      "loss": 13.3438,
      "step": 8425
    },
    {
      "epoch": 8.43,
      "grad_norm": 12456.1142578125,
      "learning_rate": 5.9618163054695564e-05,
      "loss": 17.3249,
      "step": 8426
    },
    {
      "epoch": 8.44,
      "grad_norm": 18964.52734375,
      "learning_rate": 5.961300309597524e-05,
      "loss": 14.9121,
      "step": 8427
    },
    {
      "epoch": 8.44,
      "grad_norm": 346.98077392578125,
      "learning_rate": 5.9607843137254906e-05,
      "loss": 12.1931,
      "step": 8428
    },
    {
      "epoch": 8.44,
      "grad_norm": 1828.346923828125,
      "learning_rate": 5.960268317853458e-05,
      "loss": 20.0723,
      "step": 8429
    },
    {
      "epoch": 8.44,
      "grad_norm": 14741.8837890625,
      "learning_rate": 5.959752321981424e-05,
      "loss": 13.4379,
      "step": 8430
    },
    {
      "epoch": 8.44,
      "grad_norm": 564.1802978515625,
      "learning_rate": 5.959236326109391e-05,
      "loss": 16.967,
      "step": 8431
    },
    {
      "epoch": 8.44,
      "grad_norm": 14197.509765625,
      "learning_rate": 5.9587203302373584e-05,
      "loss": 14.3277,
      "step": 8432
    },
    {
      "epoch": 8.44,
      "grad_norm": 22315.400390625,
      "learning_rate": 5.958204334365325e-05,
      "loss": 17.1321,
      "step": 8433
    },
    {
      "epoch": 8.44,
      "grad_norm": 6551.1357421875,
      "learning_rate": 5.9576883384932926e-05,
      "loss": 12.1773,
      "step": 8434
    },
    {
      "epoch": 8.44,
      "grad_norm": 13525.052734375,
      "learning_rate": 5.957172342621259e-05,
      "loss": 19.99,
      "step": 8435
    },
    {
      "epoch": 8.44,
      "grad_norm": 65393.33203125,
      "learning_rate": 5.956656346749226e-05,
      "loss": 13.1725,
      "step": 8436
    },
    {
      "epoch": 8.45,
      "grad_norm": 20656.349609375,
      "learning_rate": 5.956140350877193e-05,
      "loss": 18.3813,
      "step": 8437
    },
    {
      "epoch": 8.45,
      "grad_norm": 19925.724609375,
      "learning_rate": 5.9556243550051604e-05,
      "loss": 25.4755,
      "step": 8438
    },
    {
      "epoch": 8.45,
      "grad_norm": 2446.992431640625,
      "learning_rate": 5.955108359133127e-05,
      "loss": 15.0978,
      "step": 8439
    },
    {
      "epoch": 8.45,
      "grad_norm": 12812.05078125,
      "learning_rate": 5.9545923632610946e-05,
      "loss": 19.9784,
      "step": 8440
    },
    {
      "epoch": 8.45,
      "grad_norm": 44649.19921875,
      "learning_rate": 5.954076367389061e-05,
      "loss": 14.8801,
      "step": 8441
    },
    {
      "epoch": 8.45,
      "grad_norm": 5935.13720703125,
      "learning_rate": 5.953560371517029e-05,
      "loss": 16.3134,
      "step": 8442
    },
    {
      "epoch": 8.45,
      "grad_norm": 46877.88671875,
      "learning_rate": 5.953044375644995e-05,
      "loss": 15.5679,
      "step": 8443
    },
    {
      "epoch": 8.45,
      "grad_norm": 15176.2607421875,
      "learning_rate": 5.9525283797729624e-05,
      "loss": 15.2624,
      "step": 8444
    },
    {
      "epoch": 8.45,
      "grad_norm": 3694.444091796875,
      "learning_rate": 5.952012383900929e-05,
      "loss": 12.815,
      "step": 8445
    },
    {
      "epoch": 8.45,
      "grad_norm": 6104.47119140625,
      "learning_rate": 5.9514963880288966e-05,
      "loss": 13.5791,
      "step": 8446
    },
    {
      "epoch": 8.46,
      "grad_norm": 3530.7978515625,
      "learning_rate": 5.950980392156863e-05,
      "loss": 12.495,
      "step": 8447
    },
    {
      "epoch": 8.46,
      "grad_norm": 1155.884765625,
      "learning_rate": 5.950464396284831e-05,
      "loss": 13.9492,
      "step": 8448
    },
    {
      "epoch": 8.46,
      "grad_norm": 3617.26953125,
      "learning_rate": 5.949948400412797e-05,
      "loss": 16.7745,
      "step": 8449
    },
    {
      "epoch": 8.46,
      "grad_norm": 5157.7265625,
      "learning_rate": 5.949432404540764e-05,
      "loss": 12.147,
      "step": 8450
    },
    {
      "epoch": 8.46,
      "grad_norm": 3791.514404296875,
      "learning_rate": 5.948916408668731e-05,
      "loss": 12.597,
      "step": 8451
    },
    {
      "epoch": 8.46,
      "grad_norm": 3928.580078125,
      "learning_rate": 5.948400412796697e-05,
      "loss": 14.6434,
      "step": 8452
    },
    {
      "epoch": 8.46,
      "grad_norm": 39331.15234375,
      "learning_rate": 5.9478844169246654e-05,
      "loss": 15.9416,
      "step": 8453
    },
    {
      "epoch": 8.46,
      "grad_norm": 4125.75732421875,
      "learning_rate": 5.9473684210526315e-05,
      "loss": 20.4279,
      "step": 8454
    },
    {
      "epoch": 8.46,
      "grad_norm": 6112.89990234375,
      "learning_rate": 5.946852425180599e-05,
      "loss": 17.5758,
      "step": 8455
    },
    {
      "epoch": 8.46,
      "grad_norm": 81677.8828125,
      "learning_rate": 5.946336429308566e-05,
      "loss": 14.9372,
      "step": 8456
    },
    {
      "epoch": 8.47,
      "grad_norm": 18271.787109375,
      "learning_rate": 5.945820433436533e-05,
      "loss": 19.1181,
      "step": 8457
    },
    {
      "epoch": 8.47,
      "grad_norm": 4299.94140625,
      "learning_rate": 5.945304437564499e-05,
      "loss": 11.1748,
      "step": 8458
    },
    {
      "epoch": 8.47,
      "grad_norm": 4427.931640625,
      "learning_rate": 5.9447884416924674e-05,
      "loss": 16.015,
      "step": 8459
    },
    {
      "epoch": 8.47,
      "grad_norm": 1121.9947509765625,
      "learning_rate": 5.9442724458204335e-05,
      "loss": 16.262,
      "step": 8460
    },
    {
      "epoch": 8.47,
      "grad_norm": 500.186279296875,
      "learning_rate": 5.943756449948401e-05,
      "loss": 12.3323,
      "step": 8461
    },
    {
      "epoch": 8.47,
      "grad_norm": 1630.3607177734375,
      "learning_rate": 5.943240454076368e-05,
      "loss": 14.7152,
      "step": 8462
    },
    {
      "epoch": 8.47,
      "grad_norm": 2878.10498046875,
      "learning_rate": 5.942724458204335e-05,
      "loss": 16.9672,
      "step": 8463
    },
    {
      "epoch": 8.47,
      "grad_norm": 23147.50390625,
      "learning_rate": 5.942208462332301e-05,
      "loss": 16.5882,
      "step": 8464
    },
    {
      "epoch": 8.47,
      "grad_norm": 4297.73046875,
      "learning_rate": 5.9416924664602694e-05,
      "loss": 12.599,
      "step": 8465
    },
    {
      "epoch": 8.47,
      "grad_norm": 5309.74365234375,
      "learning_rate": 5.9411764705882355e-05,
      "loss": 12.342,
      "step": 8466
    },
    {
      "epoch": 8.48,
      "grad_norm": 2234.3876953125,
      "learning_rate": 5.940660474716202e-05,
      "loss": 12.0783,
      "step": 8467
    },
    {
      "epoch": 8.48,
      "grad_norm": 12314.42578125,
      "learning_rate": 5.94014447884417e-05,
      "loss": 17.0767,
      "step": 8468
    },
    {
      "epoch": 8.48,
      "grad_norm": 9585.111328125,
      "learning_rate": 5.939628482972136e-05,
      "loss": 13.9231,
      "step": 8469
    },
    {
      "epoch": 8.48,
      "grad_norm": 3489.748779296875,
      "learning_rate": 5.939112487100104e-05,
      "loss": 14.2263,
      "step": 8470
    },
    {
      "epoch": 8.48,
      "grad_norm": 32753.494140625,
      "learning_rate": 5.93859649122807e-05,
      "loss": 19.7396,
      "step": 8471
    },
    {
      "epoch": 8.48,
      "grad_norm": 1882.611572265625,
      "learning_rate": 5.9380804953560375e-05,
      "loss": 11.9665,
      "step": 8472
    },
    {
      "epoch": 8.48,
      "grad_norm": 7468.9375,
      "learning_rate": 5.937564499484004e-05,
      "loss": 14.7236,
      "step": 8473
    },
    {
      "epoch": 8.48,
      "grad_norm": 8020.8330078125,
      "learning_rate": 5.937048503611972e-05,
      "loss": 12.9575,
      "step": 8474
    },
    {
      "epoch": 8.48,
      "grad_norm": 8605.09375,
      "learning_rate": 5.936532507739938e-05,
      "loss": 13.0087,
      "step": 8475
    },
    {
      "epoch": 8.48,
      "grad_norm": 13413.662109375,
      "learning_rate": 5.936016511867906e-05,
      "loss": 13.2103,
      "step": 8476
    },
    {
      "epoch": 8.49,
      "grad_norm": 900.354248046875,
      "learning_rate": 5.935500515995872e-05,
      "loss": 12.1661,
      "step": 8477
    },
    {
      "epoch": 8.49,
      "grad_norm": 8694.515625,
      "learning_rate": 5.9349845201238395e-05,
      "loss": 16.027,
      "step": 8478
    },
    {
      "epoch": 8.49,
      "grad_norm": 11685.4189453125,
      "learning_rate": 5.934468524251806e-05,
      "loss": 14.3643,
      "step": 8479
    },
    {
      "epoch": 8.49,
      "grad_norm": 6953.236328125,
      "learning_rate": 5.933952528379774e-05,
      "loss": 13.2699,
      "step": 8480
    },
    {
      "epoch": 8.49,
      "grad_norm": 5404.36181640625,
      "learning_rate": 5.9334365325077405e-05,
      "loss": 16.5887,
      "step": 8481
    },
    {
      "epoch": 8.49,
      "grad_norm": 33618.32421875,
      "learning_rate": 5.932920536635708e-05,
      "loss": 13.8809,
      "step": 8482
    },
    {
      "epoch": 8.49,
      "grad_norm": 11767.5068359375,
      "learning_rate": 5.932404540763674e-05,
      "loss": 13.1326,
      "step": 8483
    },
    {
      "epoch": 8.49,
      "grad_norm": 14424.6142578125,
      "learning_rate": 5.9318885448916415e-05,
      "loss": 16.6418,
      "step": 8484
    },
    {
      "epoch": 8.49,
      "grad_norm": 3438.20263671875,
      "learning_rate": 5.931372549019608e-05,
      "loss": 18.0516,
      "step": 8485
    },
    {
      "epoch": 8.49,
      "grad_norm": 3103.933349609375,
      "learning_rate": 5.930856553147574e-05,
      "loss": 12.8152,
      "step": 8486
    },
    {
      "epoch": 8.5,
      "grad_norm": 1091.2568359375,
      "learning_rate": 5.9303405572755425e-05,
      "loss": 14.8876,
      "step": 8487
    },
    {
      "epoch": 8.5,
      "grad_norm": 5295.3173828125,
      "learning_rate": 5.9298245614035085e-05,
      "loss": 13.4219,
      "step": 8488
    },
    {
      "epoch": 8.5,
      "grad_norm": 28815.84765625,
      "learning_rate": 5.929308565531476e-05,
      "loss": 12.2788,
      "step": 8489
    },
    {
      "epoch": 8.5,
      "grad_norm": 1497.4058837890625,
      "learning_rate": 5.928792569659443e-05,
      "loss": 14.3442,
      "step": 8490
    },
    {
      "epoch": 8.5,
      "grad_norm": 1353.4267578125,
      "learning_rate": 5.92827657378741e-05,
      "loss": 15.3829,
      "step": 8491
    },
    {
      "epoch": 8.5,
      "grad_norm": 5472.76025390625,
      "learning_rate": 5.927760577915376e-05,
      "loss": 14.0415,
      "step": 8492
    },
    {
      "epoch": 8.5,
      "grad_norm": 1136.99560546875,
      "learning_rate": 5.9272445820433445e-05,
      "loss": 11.2321,
      "step": 8493
    },
    {
      "epoch": 8.5,
      "grad_norm": 3886.748291015625,
      "learning_rate": 5.9267285861713106e-05,
      "loss": 13.8692,
      "step": 8494
    },
    {
      "epoch": 8.5,
      "grad_norm": 6638.728515625,
      "learning_rate": 5.926212590299278e-05,
      "loss": 15.4568,
      "step": 8495
    },
    {
      "epoch": 8.5,
      "grad_norm": 6669.07275390625,
      "learning_rate": 5.925696594427245e-05,
      "loss": 12.0871,
      "step": 8496
    },
    {
      "epoch": 8.51,
      "grad_norm": 5790.7470703125,
      "learning_rate": 5.925180598555212e-05,
      "loss": 14.3008,
      "step": 8497
    },
    {
      "epoch": 8.51,
      "grad_norm": 1426.6644287109375,
      "learning_rate": 5.924664602683179e-05,
      "loss": 13.9429,
      "step": 8498
    },
    {
      "epoch": 8.51,
      "grad_norm": 4748.24853515625,
      "learning_rate": 5.9241486068111465e-05,
      "loss": 12.7714,
      "step": 8499
    },
    {
      "epoch": 8.51,
      "grad_norm": 1355.030517578125,
      "learning_rate": 5.9236326109391126e-05,
      "loss": 10.704,
      "step": 8500
    },
    {
      "epoch": 8.51,
      "grad_norm": 8313.1171875,
      "learning_rate": 5.92311661506708e-05,
      "loss": 15.3255,
      "step": 8501
    },
    {
      "epoch": 8.51,
      "grad_norm": 13673.1435546875,
      "learning_rate": 5.922600619195047e-05,
      "loss": 16.4238,
      "step": 8502
    },
    {
      "epoch": 8.51,
      "grad_norm": 26019.447265625,
      "learning_rate": 5.922084623323013e-05,
      "loss": 14.4126,
      "step": 8503
    },
    {
      "epoch": 8.51,
      "grad_norm": 9937.6474609375,
      "learning_rate": 5.921568627450981e-05,
      "loss": 18.3846,
      "step": 8504
    },
    {
      "epoch": 8.51,
      "grad_norm": 30321.5234375,
      "learning_rate": 5.921052631578947e-05,
      "loss": 14.7185,
      "step": 8505
    },
    {
      "epoch": 8.51,
      "grad_norm": 8656.861328125,
      "learning_rate": 5.9205366357069146e-05,
      "loss": 16.9126,
      "step": 8506
    },
    {
      "epoch": 8.52,
      "grad_norm": 1692.5748291015625,
      "learning_rate": 5.920020639834881e-05,
      "loss": 13.9951,
      "step": 8507
    },
    {
      "epoch": 8.52,
      "grad_norm": 1888.4420166015625,
      "learning_rate": 5.919504643962849e-05,
      "loss": 14.0921,
      "step": 8508
    },
    {
      "epoch": 8.52,
      "grad_norm": 722.7787475585938,
      "learning_rate": 5.9189886480908156e-05,
      "loss": 12.6445,
      "step": 8509
    },
    {
      "epoch": 8.52,
      "grad_norm": 6795.78466796875,
      "learning_rate": 5.918472652218783e-05,
      "loss": 13.6715,
      "step": 8510
    },
    {
      "epoch": 8.52,
      "grad_norm": 11884.9443359375,
      "learning_rate": 5.917956656346749e-05,
      "loss": 16.9563,
      "step": 8511
    },
    {
      "epoch": 8.52,
      "grad_norm": 1096.496826171875,
      "learning_rate": 5.9174406604747166e-05,
      "loss": 11.7423,
      "step": 8512
    },
    {
      "epoch": 8.52,
      "grad_norm": 9702.4736328125,
      "learning_rate": 5.916924664602683e-05,
      "loss": 12.4697,
      "step": 8513
    },
    {
      "epoch": 8.52,
      "grad_norm": 1460.590576171875,
      "learning_rate": 5.916408668730651e-05,
      "loss": 11.4573,
      "step": 8514
    },
    {
      "epoch": 8.52,
      "grad_norm": 28223.0390625,
      "learning_rate": 5.9158926728586176e-05,
      "loss": 20.287,
      "step": 8515
    },
    {
      "epoch": 8.52,
      "grad_norm": 16839.537109375,
      "learning_rate": 5.915376676986585e-05,
      "loss": 19.9502,
      "step": 8516
    },
    {
      "epoch": 8.53,
      "grad_norm": 3330.99853515625,
      "learning_rate": 5.914860681114551e-05,
      "loss": 13.9688,
      "step": 8517
    },
    {
      "epoch": 8.53,
      "grad_norm": 2979.669677734375,
      "learning_rate": 5.9143446852425186e-05,
      "loss": 12.7224,
      "step": 8518
    },
    {
      "epoch": 8.53,
      "grad_norm": 3283.329345703125,
      "learning_rate": 5.913828689370485e-05,
      "loss": 13.0021,
      "step": 8519
    },
    {
      "epoch": 8.53,
      "grad_norm": 2943.61962890625,
      "learning_rate": 5.913312693498453e-05,
      "loss": 16.2762,
      "step": 8520
    },
    {
      "epoch": 8.53,
      "grad_norm": 1905.293701171875,
      "learning_rate": 5.9127966976264196e-05,
      "loss": 12.6998,
      "step": 8521
    },
    {
      "epoch": 8.53,
      "grad_norm": 9856.830078125,
      "learning_rate": 5.9122807017543856e-05,
      "loss": 14.2466,
      "step": 8522
    },
    {
      "epoch": 8.53,
      "grad_norm": 2485.825927734375,
      "learning_rate": 5.911764705882353e-05,
      "loss": 13.9675,
      "step": 8523
    },
    {
      "epoch": 8.53,
      "grad_norm": 28730.103515625,
      "learning_rate": 5.91124871001032e-05,
      "loss": 18.4731,
      "step": 8524
    },
    {
      "epoch": 8.53,
      "grad_norm": 1200.7144775390625,
      "learning_rate": 5.910732714138287e-05,
      "loss": 13.7428,
      "step": 8525
    },
    {
      "epoch": 8.53,
      "grad_norm": 980.296630859375,
      "learning_rate": 5.910216718266254e-05,
      "loss": 13.625,
      "step": 8526
    },
    {
      "epoch": 8.54,
      "grad_norm": 18748.826171875,
      "learning_rate": 5.9097007223942216e-05,
      "loss": 10.2748,
      "step": 8527
    },
    {
      "epoch": 8.54,
      "grad_norm": 9546.38671875,
      "learning_rate": 5.9091847265221876e-05,
      "loss": 16.7937,
      "step": 8528
    },
    {
      "epoch": 8.54,
      "grad_norm": 15247.80078125,
      "learning_rate": 5.908668730650155e-05,
      "loss": 12.4607,
      "step": 8529
    },
    {
      "epoch": 8.54,
      "grad_norm": 2597.53466796875,
      "learning_rate": 5.908152734778122e-05,
      "loss": 11.8384,
      "step": 8530
    },
    {
      "epoch": 8.54,
      "grad_norm": 1909.1168212890625,
      "learning_rate": 5.907636738906089e-05,
      "loss": 15.8113,
      "step": 8531
    },
    {
      "epoch": 8.54,
      "grad_norm": 3682.5341796875,
      "learning_rate": 5.907120743034056e-05,
      "loss": 13.712,
      "step": 8532
    },
    {
      "epoch": 8.54,
      "grad_norm": 5579.68212890625,
      "learning_rate": 5.9066047471620236e-05,
      "loss": 14.9925,
      "step": 8533
    },
    {
      "epoch": 8.54,
      "grad_norm": 6012.103515625,
      "learning_rate": 5.9060887512899897e-05,
      "loss": 16.1236,
      "step": 8534
    },
    {
      "epoch": 8.54,
      "grad_norm": 4379.65478515625,
      "learning_rate": 5.905572755417957e-05,
      "loss": 22.4252,
      "step": 8535
    },
    {
      "epoch": 8.54,
      "grad_norm": 11282.5224609375,
      "learning_rate": 5.905056759545924e-05,
      "loss": 13.5067,
      "step": 8536
    },
    {
      "epoch": 8.55,
      "grad_norm": 4466.13525390625,
      "learning_rate": 5.904540763673891e-05,
      "loss": 14.7863,
      "step": 8537
    },
    {
      "epoch": 8.55,
      "grad_norm": 1541.3009033203125,
      "learning_rate": 5.904024767801858e-05,
      "loss": 16.1575,
      "step": 8538
    },
    {
      "epoch": 8.55,
      "grad_norm": 15676.62890625,
      "learning_rate": 5.903508771929824e-05,
      "loss": 19.8248,
      "step": 8539
    },
    {
      "epoch": 8.55,
      "grad_norm": 13214.283203125,
      "learning_rate": 5.9029927760577917e-05,
      "loss": 15.9831,
      "step": 8540
    },
    {
      "epoch": 8.55,
      "grad_norm": 664.8107299804688,
      "learning_rate": 5.9024767801857584e-05,
      "loss": 13.8137,
      "step": 8541
    },
    {
      "epoch": 8.55,
      "grad_norm": 12155.4580078125,
      "learning_rate": 5.901960784313726e-05,
      "loss": 15.7147,
      "step": 8542
    },
    {
      "epoch": 8.55,
      "grad_norm": 736.7322998046875,
      "learning_rate": 5.9014447884416927e-05,
      "loss": 12.6019,
      "step": 8543
    },
    {
      "epoch": 8.55,
      "grad_norm": 5517.04443359375,
      "learning_rate": 5.90092879256966e-05,
      "loss": 13.8284,
      "step": 8544
    },
    {
      "epoch": 8.55,
      "grad_norm": 1848.51220703125,
      "learning_rate": 5.900412796697626e-05,
      "loss": 11.1975,
      "step": 8545
    },
    {
      "epoch": 8.55,
      "grad_norm": 2131.755859375,
      "learning_rate": 5.8998968008255937e-05,
      "loss": 14.4391,
      "step": 8546
    },
    {
      "epoch": 8.56,
      "grad_norm": 22011.673828125,
      "learning_rate": 5.8993808049535604e-05,
      "loss": 14.6417,
      "step": 8547
    },
    {
      "epoch": 8.56,
      "grad_norm": 7506.60400390625,
      "learning_rate": 5.898864809081528e-05,
      "loss": 16.4708,
      "step": 8548
    },
    {
      "epoch": 8.56,
      "grad_norm": 1827.0731201171875,
      "learning_rate": 5.8983488132094947e-05,
      "loss": 13.7714,
      "step": 8549
    },
    {
      "epoch": 8.56,
      "grad_norm": 52059.390625,
      "learning_rate": 5.897832817337462e-05,
      "loss": 20.7103,
      "step": 8550
    },
    {
      "epoch": 8.56,
      "grad_norm": 14281.330078125,
      "learning_rate": 5.897316821465428e-05,
      "loss": 13.171,
      "step": 8551
    },
    {
      "epoch": 8.56,
      "grad_norm": 1504.029052734375,
      "learning_rate": 5.8968008255933957e-05,
      "loss": 11.9564,
      "step": 8552
    },
    {
      "epoch": 8.56,
      "grad_norm": 4641.13720703125,
      "learning_rate": 5.8962848297213624e-05,
      "loss": 12.9876,
      "step": 8553
    },
    {
      "epoch": 8.56,
      "grad_norm": 68312.890625,
      "learning_rate": 5.89576883384933e-05,
      "loss": 15.9627,
      "step": 8554
    },
    {
      "epoch": 8.56,
      "grad_norm": 23215.35546875,
      "learning_rate": 5.8952528379772967e-05,
      "loss": 15.0141,
      "step": 8555
    },
    {
      "epoch": 8.56,
      "grad_norm": 3210.1318359375,
      "learning_rate": 5.894736842105263e-05,
      "loss": 13.0487,
      "step": 8556
    },
    {
      "epoch": 8.57,
      "grad_norm": 54573.43359375,
      "learning_rate": 5.89422084623323e-05,
      "loss": 15.6317,
      "step": 8557
    },
    {
      "epoch": 8.57,
      "grad_norm": 18236.72265625,
      "learning_rate": 5.893704850361197e-05,
      "loss": 14.6186,
      "step": 8558
    },
    {
      "epoch": 8.57,
      "grad_norm": 7746.9033203125,
      "learning_rate": 5.8931888544891644e-05,
      "loss": 23.7234,
      "step": 8559
    },
    {
      "epoch": 8.57,
      "grad_norm": 10021.990234375,
      "learning_rate": 5.892672858617131e-05,
      "loss": 18.7555,
      "step": 8560
    },
    {
      "epoch": 8.57,
      "grad_norm": 4609.365234375,
      "learning_rate": 5.8921568627450987e-05,
      "loss": 14.546,
      "step": 8561
    },
    {
      "epoch": 8.57,
      "grad_norm": 19419.615234375,
      "learning_rate": 5.891640866873065e-05,
      "loss": 15.7801,
      "step": 8562
    },
    {
      "epoch": 8.57,
      "grad_norm": 21996.69921875,
      "learning_rate": 5.891124871001032e-05,
      "loss": 25.5627,
      "step": 8563
    },
    {
      "epoch": 8.57,
      "grad_norm": 5165.16455078125,
      "learning_rate": 5.890608875128999e-05,
      "loss": 14.2517,
      "step": 8564
    },
    {
      "epoch": 8.57,
      "grad_norm": 3656.055908203125,
      "learning_rate": 5.8900928792569664e-05,
      "loss": 12.9591,
      "step": 8565
    },
    {
      "epoch": 8.57,
      "grad_norm": 30425.994140625,
      "learning_rate": 5.889576883384933e-05,
      "loss": 16.2633,
      "step": 8566
    },
    {
      "epoch": 8.58,
      "grad_norm": 5543.15185546875,
      "learning_rate": 5.8890608875129007e-05,
      "loss": 24.6199,
      "step": 8567
    },
    {
      "epoch": 8.58,
      "grad_norm": 23076.84765625,
      "learning_rate": 5.888544891640867e-05,
      "loss": 14.228,
      "step": 8568
    },
    {
      "epoch": 8.58,
      "grad_norm": 8707.8427734375,
      "learning_rate": 5.888028895768835e-05,
      "loss": 12.5728,
      "step": 8569
    },
    {
      "epoch": 8.58,
      "grad_norm": 11956.7294921875,
      "learning_rate": 5.887512899896801e-05,
      "loss": 22.789,
      "step": 8570
    },
    {
      "epoch": 8.58,
      "grad_norm": 3283.308349609375,
      "learning_rate": 5.8869969040247684e-05,
      "loss": 14.4696,
      "step": 8571
    },
    {
      "epoch": 8.58,
      "grad_norm": 19347.91796875,
      "learning_rate": 5.886480908152735e-05,
      "loss": 13.9573,
      "step": 8572
    },
    {
      "epoch": 8.58,
      "grad_norm": 863.6389770507812,
      "learning_rate": 5.8859649122807027e-05,
      "loss": 16.8356,
      "step": 8573
    },
    {
      "epoch": 8.58,
      "grad_norm": 6715.72265625,
      "learning_rate": 5.885448916408669e-05,
      "loss": 11.8554,
      "step": 8574
    },
    {
      "epoch": 8.58,
      "grad_norm": 2397.32421875,
      "learning_rate": 5.8849329205366355e-05,
      "loss": 13.902,
      "step": 8575
    },
    {
      "epoch": 8.58,
      "grad_norm": 4011.36474609375,
      "learning_rate": 5.884416924664603e-05,
      "loss": 13.2446,
      "step": 8576
    },
    {
      "epoch": 8.59,
      "grad_norm": 10072.1748046875,
      "learning_rate": 5.88390092879257e-05,
      "loss": 18.1347,
      "step": 8577
    },
    {
      "epoch": 8.59,
      "grad_norm": 5120.00634765625,
      "learning_rate": 5.883384932920537e-05,
      "loss": 14.1646,
      "step": 8578
    },
    {
      "epoch": 8.59,
      "grad_norm": 1655.8760986328125,
      "learning_rate": 5.882868937048503e-05,
      "loss": 12.1295,
      "step": 8579
    },
    {
      "epoch": 8.59,
      "grad_norm": 6489.06396484375,
      "learning_rate": 5.882352941176471e-05,
      "loss": 11.963,
      "step": 8580
    },
    {
      "epoch": 8.59,
      "grad_norm": 10774.03515625,
      "learning_rate": 5.8818369453044375e-05,
      "loss": 19.9334,
      "step": 8581
    },
    {
      "epoch": 8.59,
      "grad_norm": 7842.564453125,
      "learning_rate": 5.881320949432405e-05,
      "loss": 12.0129,
      "step": 8582
    },
    {
      "epoch": 8.59,
      "grad_norm": 1982.1942138671875,
      "learning_rate": 5.880804953560372e-05,
      "loss": 18.5393,
      "step": 8583
    },
    {
      "epoch": 8.59,
      "grad_norm": 9406.6044921875,
      "learning_rate": 5.880288957688339e-05,
      "loss": 10.6384,
      "step": 8584
    },
    {
      "epoch": 8.59,
      "grad_norm": 3531.533203125,
      "learning_rate": 5.879772961816305e-05,
      "loss": 12.541,
      "step": 8585
    },
    {
      "epoch": 8.59,
      "grad_norm": 2224.60498046875,
      "learning_rate": 5.8792569659442734e-05,
      "loss": 13.3226,
      "step": 8586
    },
    {
      "epoch": 8.6,
      "grad_norm": 27353.927734375,
      "learning_rate": 5.8787409700722395e-05,
      "loss": 17.2082,
      "step": 8587
    },
    {
      "epoch": 8.6,
      "grad_norm": 3426.282470703125,
      "learning_rate": 5.878224974200207e-05,
      "loss": 14.8772,
      "step": 8588
    },
    {
      "epoch": 8.6,
      "grad_norm": 53642.53515625,
      "learning_rate": 5.877708978328174e-05,
      "loss": 19.5245,
      "step": 8589
    },
    {
      "epoch": 8.6,
      "grad_norm": 6804.080078125,
      "learning_rate": 5.877192982456141e-05,
      "loss": 14.3533,
      "step": 8590
    },
    {
      "epoch": 8.6,
      "grad_norm": 9147.501953125,
      "learning_rate": 5.876676986584107e-05,
      "loss": 14.4589,
      "step": 8591
    },
    {
      "epoch": 8.6,
      "grad_norm": 1497.451904296875,
      "learning_rate": 5.876160990712074e-05,
      "loss": 13.9069,
      "step": 8592
    },
    {
      "epoch": 8.6,
      "grad_norm": 1651.717041015625,
      "learning_rate": 5.8756449948400415e-05,
      "loss": 11.4889,
      "step": 8593
    },
    {
      "epoch": 8.6,
      "grad_norm": 3683.168212890625,
      "learning_rate": 5.875128998968008e-05,
      "loss": 12.0506,
      "step": 8594
    },
    {
      "epoch": 8.6,
      "grad_norm": 5737.865234375,
      "learning_rate": 5.874613003095976e-05,
      "loss": 13.1934,
      "step": 8595
    },
    {
      "epoch": 8.6,
      "grad_norm": 1559.322509765625,
      "learning_rate": 5.874097007223942e-05,
      "loss": 14.1896,
      "step": 8596
    },
    {
      "epoch": 8.61,
      "grad_norm": 6469.787109375,
      "learning_rate": 5.873581011351909e-05,
      "loss": 11.5561,
      "step": 8597
    },
    {
      "epoch": 8.61,
      "grad_norm": 142410.890625,
      "learning_rate": 5.873065015479876e-05,
      "loss": 19.0217,
      "step": 8598
    },
    {
      "epoch": 8.61,
      "grad_norm": 15008.7509765625,
      "learning_rate": 5.8725490196078435e-05,
      "loss": 15.8434,
      "step": 8599
    },
    {
      "epoch": 8.61,
      "grad_norm": 4375.84765625,
      "learning_rate": 5.87203302373581e-05,
      "loss": 13.09,
      "step": 8600
    },
    {
      "epoch": 8.61,
      "grad_norm": 8396.021484375,
      "learning_rate": 5.871517027863778e-05,
      "loss": 17.5288,
      "step": 8601
    },
    {
      "epoch": 8.61,
      "grad_norm": 13851.3232421875,
      "learning_rate": 5.871001031991744e-05,
      "loss": 14.5379,
      "step": 8602
    },
    {
      "epoch": 8.61,
      "grad_norm": 15908.2392578125,
      "learning_rate": 5.870485036119712e-05,
      "loss": 13.0254,
      "step": 8603
    },
    {
      "epoch": 8.61,
      "grad_norm": 1941.9805908203125,
      "learning_rate": 5.869969040247678e-05,
      "loss": 12.5998,
      "step": 8604
    },
    {
      "epoch": 8.61,
      "grad_norm": 1639.835693359375,
      "learning_rate": 5.8694530443756455e-05,
      "loss": 12.2617,
      "step": 8605
    },
    {
      "epoch": 8.61,
      "grad_norm": 21103.419921875,
      "learning_rate": 5.868937048503612e-05,
      "loss": 11.0338,
      "step": 8606
    },
    {
      "epoch": 8.62,
      "grad_norm": 28233.484375,
      "learning_rate": 5.86842105263158e-05,
      "loss": 14.4998,
      "step": 8607
    },
    {
      "epoch": 8.62,
      "grad_norm": 79975.1640625,
      "learning_rate": 5.867905056759546e-05,
      "loss": 17.3264,
      "step": 8608
    },
    {
      "epoch": 8.62,
      "grad_norm": 9300.115234375,
      "learning_rate": 5.867389060887514e-05,
      "loss": 16.0531,
      "step": 8609
    },
    {
      "epoch": 8.62,
      "grad_norm": 20491.619140625,
      "learning_rate": 5.86687306501548e-05,
      "loss": 13.4945,
      "step": 8610
    },
    {
      "epoch": 8.62,
      "grad_norm": 6239.24951171875,
      "learning_rate": 5.866357069143447e-05,
      "loss": 12.4131,
      "step": 8611
    },
    {
      "epoch": 8.62,
      "grad_norm": 5667.8828125,
      "learning_rate": 5.865841073271414e-05,
      "loss": 19.708,
      "step": 8612
    },
    {
      "epoch": 8.62,
      "grad_norm": 4491.86669921875,
      "learning_rate": 5.8653250773993804e-05,
      "loss": 17.1833,
      "step": 8613
    },
    {
      "epoch": 8.62,
      "grad_norm": 3788.33447265625,
      "learning_rate": 5.8648090815273485e-05,
      "loss": 12.0686,
      "step": 8614
    },
    {
      "epoch": 8.62,
      "grad_norm": 6608.1923828125,
      "learning_rate": 5.8642930856553146e-05,
      "loss": 16.2195,
      "step": 8615
    },
    {
      "epoch": 8.62,
      "grad_norm": 6460.388671875,
      "learning_rate": 5.863777089783282e-05,
      "loss": 20.7124,
      "step": 8616
    },
    {
      "epoch": 8.63,
      "grad_norm": 2809.241943359375,
      "learning_rate": 5.863261093911249e-05,
      "loss": 12.8895,
      "step": 8617
    },
    {
      "epoch": 8.63,
      "grad_norm": 11968.0869140625,
      "learning_rate": 5.862745098039216e-05,
      "loss": 12.1842,
      "step": 8618
    },
    {
      "epoch": 8.63,
      "grad_norm": 12436.333984375,
      "learning_rate": 5.8622291021671824e-05,
      "loss": 12.9741,
      "step": 8619
    },
    {
      "epoch": 8.63,
      "grad_norm": 5133.29736328125,
      "learning_rate": 5.8617131062951505e-05,
      "loss": 13.4786,
      "step": 8620
    },
    {
      "epoch": 8.63,
      "grad_norm": 1548.645263671875,
      "learning_rate": 5.8611971104231166e-05,
      "loss": 22.0682,
      "step": 8621
    },
    {
      "epoch": 8.63,
      "grad_norm": 5817.52978515625,
      "learning_rate": 5.860681114551084e-05,
      "loss": 17.4865,
      "step": 8622
    },
    {
      "epoch": 8.63,
      "grad_norm": 11285.7880859375,
      "learning_rate": 5.860165118679051e-05,
      "loss": 13.7358,
      "step": 8623
    },
    {
      "epoch": 8.63,
      "grad_norm": 14087.8896484375,
      "learning_rate": 5.859649122807018e-05,
      "loss": 15.6651,
      "step": 8624
    },
    {
      "epoch": 8.63,
      "grad_norm": 4314.69384765625,
      "learning_rate": 5.8591331269349844e-05,
      "loss": 19.7425,
      "step": 8625
    },
    {
      "epoch": 8.63,
      "grad_norm": 8435.6416015625,
      "learning_rate": 5.8586171310629525e-05,
      "loss": 12.4545,
      "step": 8626
    },
    {
      "epoch": 8.64,
      "grad_norm": 10752.89453125,
      "learning_rate": 5.8581011351909186e-05,
      "loss": 17.4461,
      "step": 8627
    },
    {
      "epoch": 8.64,
      "grad_norm": 5651.69775390625,
      "learning_rate": 5.8575851393188854e-05,
      "loss": 12.944,
      "step": 8628
    },
    {
      "epoch": 8.64,
      "grad_norm": 12896.2275390625,
      "learning_rate": 5.857069143446853e-05,
      "loss": 20.1613,
      "step": 8629
    },
    {
      "epoch": 8.64,
      "grad_norm": 6479.4794921875,
      "learning_rate": 5.856553147574819e-05,
      "loss": 14.4774,
      "step": 8630
    },
    {
      "epoch": 8.64,
      "grad_norm": 3653.5400390625,
      "learning_rate": 5.856037151702787e-05,
      "loss": 12.7791,
      "step": 8631
    },
    {
      "epoch": 8.64,
      "grad_norm": 1896.5484619140625,
      "learning_rate": 5.855521155830753e-05,
      "loss": 14.9156,
      "step": 8632
    },
    {
      "epoch": 8.64,
      "grad_norm": 2589.86572265625,
      "learning_rate": 5.8550051599587206e-05,
      "loss": 14.965,
      "step": 8633
    },
    {
      "epoch": 8.64,
      "grad_norm": 9330.392578125,
      "learning_rate": 5.8544891640866874e-05,
      "loss": 17.0843,
      "step": 8634
    },
    {
      "epoch": 8.64,
      "grad_norm": 997.5716552734375,
      "learning_rate": 5.853973168214655e-05,
      "loss": 13.1715,
      "step": 8635
    },
    {
      "epoch": 8.64,
      "grad_norm": 13557.486328125,
      "learning_rate": 5.853457172342621e-05,
      "loss": 17.1001,
      "step": 8636
    },
    {
      "epoch": 8.65,
      "grad_norm": 8148.61279296875,
      "learning_rate": 5.852941176470589e-05,
      "loss": 12.4255,
      "step": 8637
    },
    {
      "epoch": 8.65,
      "grad_norm": 6141.90087890625,
      "learning_rate": 5.852425180598555e-05,
      "loss": 16.2997,
      "step": 8638
    },
    {
      "epoch": 8.65,
      "grad_norm": 1400.354736328125,
      "learning_rate": 5.8519091847265226e-05,
      "loss": 15.6511,
      "step": 8639
    },
    {
      "epoch": 8.65,
      "grad_norm": 1492.3341064453125,
      "learning_rate": 5.8513931888544894e-05,
      "loss": 15.5916,
      "step": 8640
    },
    {
      "epoch": 8.65,
      "grad_norm": 9280.2939453125,
      "learning_rate": 5.850877192982457e-05,
      "loss": 16.5856,
      "step": 8641
    },
    {
      "epoch": 8.65,
      "grad_norm": 3662.4072265625,
      "learning_rate": 5.8503611971104236e-05,
      "loss": 20.7818,
      "step": 8642
    },
    {
      "epoch": 8.65,
      "grad_norm": 1799.723876953125,
      "learning_rate": 5.849845201238391e-05,
      "loss": 14.3216,
      "step": 8643
    },
    {
      "epoch": 8.65,
      "grad_norm": 8917.9638671875,
      "learning_rate": 5.849329205366357e-05,
      "loss": 17.2856,
      "step": 8644
    },
    {
      "epoch": 8.65,
      "grad_norm": 3228.48779296875,
      "learning_rate": 5.8488132094943246e-05,
      "loss": 14.0908,
      "step": 8645
    },
    {
      "epoch": 8.65,
      "grad_norm": 3457.482421875,
      "learning_rate": 5.8482972136222914e-05,
      "loss": 13.7157,
      "step": 8646
    },
    {
      "epoch": 8.66,
      "grad_norm": 7632.48486328125,
      "learning_rate": 5.8477812177502575e-05,
      "loss": 15.3171,
      "step": 8647
    },
    {
      "epoch": 8.66,
      "grad_norm": 3358.456787109375,
      "learning_rate": 5.8472652218782256e-05,
      "loss": 17.7579,
      "step": 8648
    },
    {
      "epoch": 8.66,
      "grad_norm": 2754.03662109375,
      "learning_rate": 5.846749226006192e-05,
      "loss": 13.2545,
      "step": 8649
    },
    {
      "epoch": 8.66,
      "grad_norm": 11126.7314453125,
      "learning_rate": 5.846233230134159e-05,
      "loss": 14.1078,
      "step": 8650
    },
    {
      "epoch": 8.66,
      "grad_norm": 2268.0810546875,
      "learning_rate": 5.845717234262126e-05,
      "loss": 14.3429,
      "step": 8651
    },
    {
      "epoch": 8.66,
      "grad_norm": 19357.88671875,
      "learning_rate": 5.8452012383900934e-05,
      "loss": 17.1056,
      "step": 8652
    },
    {
      "epoch": 8.66,
      "grad_norm": 4171.09375,
      "learning_rate": 5.8446852425180595e-05,
      "loss": 12.7839,
      "step": 8653
    },
    {
      "epoch": 8.66,
      "grad_norm": 3340.43701171875,
      "learning_rate": 5.8441692466460276e-05,
      "loss": 12.9738,
      "step": 8654
    },
    {
      "epoch": 8.66,
      "grad_norm": 6838.7626953125,
      "learning_rate": 5.843653250773994e-05,
      "loss": 13.7161,
      "step": 8655
    },
    {
      "epoch": 8.66,
      "grad_norm": 8111.0615234375,
      "learning_rate": 5.843137254901961e-05,
      "loss": 16.0665,
      "step": 8656
    },
    {
      "epoch": 8.67,
      "grad_norm": 5788.88525390625,
      "learning_rate": 5.842621259029928e-05,
      "loss": 16.7484,
      "step": 8657
    },
    {
      "epoch": 8.67,
      "grad_norm": 26786.064453125,
      "learning_rate": 5.8421052631578954e-05,
      "loss": 12.5876,
      "step": 8658
    },
    {
      "epoch": 8.67,
      "grad_norm": 528.7360229492188,
      "learning_rate": 5.841589267285862e-05,
      "loss": 12.1079,
      "step": 8659
    },
    {
      "epoch": 8.67,
      "grad_norm": 5006.4814453125,
      "learning_rate": 5.8410732714138296e-05,
      "loss": 12.8779,
      "step": 8660
    },
    {
      "epoch": 8.67,
      "grad_norm": 74544.09375,
      "learning_rate": 5.840557275541796e-05,
      "loss": 18.2175,
      "step": 8661
    },
    {
      "epoch": 8.67,
      "grad_norm": 5728.8369140625,
      "learning_rate": 5.840041279669763e-05,
      "loss": 16.5103,
      "step": 8662
    },
    {
      "epoch": 8.67,
      "grad_norm": 24203.234375,
      "learning_rate": 5.83952528379773e-05,
      "loss": 13.5855,
      "step": 8663
    },
    {
      "epoch": 8.67,
      "grad_norm": 4998.69140625,
      "learning_rate": 5.839009287925696e-05,
      "loss": 15.1031,
      "step": 8664
    },
    {
      "epoch": 8.67,
      "grad_norm": 5356.9638671875,
      "learning_rate": 5.838493292053664e-05,
      "loss": 15.9333,
      "step": 8665
    },
    {
      "epoch": 8.67,
      "grad_norm": 3714.543701171875,
      "learning_rate": 5.83797729618163e-05,
      "loss": 18.2602,
      "step": 8666
    },
    {
      "epoch": 8.68,
      "grad_norm": 3188.185302734375,
      "learning_rate": 5.837461300309598e-05,
      "loss": 18.9563,
      "step": 8667
    },
    {
      "epoch": 8.68,
      "grad_norm": 2433.909912109375,
      "learning_rate": 5.8369453044375645e-05,
      "loss": 12.9483,
      "step": 8668
    },
    {
      "epoch": 8.68,
      "grad_norm": 11123.2255859375,
      "learning_rate": 5.836429308565532e-05,
      "loss": 14.7737,
      "step": 8669
    },
    {
      "epoch": 8.68,
      "grad_norm": 2311.278564453125,
      "learning_rate": 5.835913312693499e-05,
      "loss": 18.6366,
      "step": 8670
    },
    {
      "epoch": 8.68,
      "grad_norm": 4897.7490234375,
      "learning_rate": 5.835397316821466e-05,
      "loss": 15.7781,
      "step": 8671
    },
    {
      "epoch": 8.68,
      "grad_norm": 4156.0458984375,
      "learning_rate": 5.834881320949432e-05,
      "loss": 12.874,
      "step": 8672
    },
    {
      "epoch": 8.68,
      "grad_norm": 5642.30712890625,
      "learning_rate": 5.8343653250774e-05,
      "loss": 16.9147,
      "step": 8673
    },
    {
      "epoch": 8.68,
      "grad_norm": 948.921630859375,
      "learning_rate": 5.8338493292053665e-05,
      "loss": 15.4947,
      "step": 8674
    },
    {
      "epoch": 8.68,
      "grad_norm": 3640.594482421875,
      "learning_rate": 5.833333333333334e-05,
      "loss": 18.0601,
      "step": 8675
    },
    {
      "epoch": 8.68,
      "grad_norm": 6777.74169921875,
      "learning_rate": 5.832817337461301e-05,
      "loss": 11.2713,
      "step": 8676
    },
    {
      "epoch": 8.69,
      "grad_norm": 5868.005859375,
      "learning_rate": 5.832301341589268e-05,
      "loss": 13.8416,
      "step": 8677
    },
    {
      "epoch": 8.69,
      "grad_norm": 9224.32421875,
      "learning_rate": 5.831785345717234e-05,
      "loss": 14.7093,
      "step": 8678
    },
    {
      "epoch": 8.69,
      "grad_norm": 7166.4189453125,
      "learning_rate": 5.831269349845202e-05,
      "loss": 15.6321,
      "step": 8679
    },
    {
      "epoch": 8.69,
      "grad_norm": 12417.625,
      "learning_rate": 5.8307533539731685e-05,
      "loss": 16.0718,
      "step": 8680
    },
    {
      "epoch": 8.69,
      "grad_norm": 4189.865234375,
      "learning_rate": 5.8302373581011346e-05,
      "loss": 14.599,
      "step": 8681
    },
    {
      "epoch": 8.69,
      "grad_norm": 3505.62158203125,
      "learning_rate": 5.829721362229103e-05,
      "loss": 13.095,
      "step": 8682
    },
    {
      "epoch": 8.69,
      "grad_norm": 6337.14208984375,
      "learning_rate": 5.829205366357069e-05,
      "loss": 16.26,
      "step": 8683
    },
    {
      "epoch": 8.69,
      "grad_norm": 3041.12646484375,
      "learning_rate": 5.828689370485036e-05,
      "loss": 17.303,
      "step": 8684
    },
    {
      "epoch": 8.69,
      "grad_norm": 4020.4677734375,
      "learning_rate": 5.828173374613003e-05,
      "loss": 13.4559,
      "step": 8685
    },
    {
      "epoch": 8.69,
      "grad_norm": 80981.15625,
      "learning_rate": 5.8276573787409705e-05,
      "loss": 12.8832,
      "step": 8686
    },
    {
      "epoch": 8.7,
      "grad_norm": 20591.74609375,
      "learning_rate": 5.827141382868937e-05,
      "loss": 13.3077,
      "step": 8687
    },
    {
      "epoch": 8.7,
      "grad_norm": 4386.435546875,
      "learning_rate": 5.826625386996905e-05,
      "loss": 14.3596,
      "step": 8688
    },
    {
      "epoch": 8.7,
      "grad_norm": 5363.20703125,
      "learning_rate": 5.826109391124871e-05,
      "loss": 14.5137,
      "step": 8689
    },
    {
      "epoch": 8.7,
      "grad_norm": 4206.5322265625,
      "learning_rate": 5.825593395252838e-05,
      "loss": 13.7627,
      "step": 8690
    },
    {
      "epoch": 8.7,
      "grad_norm": 33267.1953125,
      "learning_rate": 5.825077399380805e-05,
      "loss": 20.1403,
      "step": 8691
    },
    {
      "epoch": 8.7,
      "grad_norm": 4039.0185546875,
      "learning_rate": 5.8245614035087725e-05,
      "loss": 13.9174,
      "step": 8692
    },
    {
      "epoch": 8.7,
      "grad_norm": 11019.4580078125,
      "learning_rate": 5.824045407636739e-05,
      "loss": 15.4045,
      "step": 8693
    },
    {
      "epoch": 8.7,
      "grad_norm": 39977.0625,
      "learning_rate": 5.823529411764707e-05,
      "loss": 14.3767,
      "step": 8694
    },
    {
      "epoch": 8.7,
      "grad_norm": 6307.80224609375,
      "learning_rate": 5.823013415892673e-05,
      "loss": 16.9439,
      "step": 8695
    },
    {
      "epoch": 8.7,
      "grad_norm": 2751.6162109375,
      "learning_rate": 5.82249742002064e-05,
      "loss": 12.6994,
      "step": 8696
    },
    {
      "epoch": 8.71,
      "grad_norm": 235561.671875,
      "learning_rate": 5.821981424148607e-05,
      "loss": 18.883,
      "step": 8697
    },
    {
      "epoch": 8.71,
      "grad_norm": 20363.095703125,
      "learning_rate": 5.8214654282765745e-05,
      "loss": 17.0032,
      "step": 8698
    },
    {
      "epoch": 8.71,
      "grad_norm": 6253.13134765625,
      "learning_rate": 5.820949432404541e-05,
      "loss": 15.4647,
      "step": 8699
    },
    {
      "epoch": 8.71,
      "grad_norm": 34427.0703125,
      "learning_rate": 5.8204334365325074e-05,
      "loss": 12.3775,
      "step": 8700
    },
    {
      "epoch": 8.71,
      "grad_norm": 11004.2216796875,
      "learning_rate": 5.819917440660475e-05,
      "loss": 14.2845,
      "step": 8701
    },
    {
      "epoch": 8.71,
      "grad_norm": 4295.6455078125,
      "learning_rate": 5.8194014447884416e-05,
      "loss": 17.0572,
      "step": 8702
    },
    {
      "epoch": 8.71,
      "grad_norm": 4823.96435546875,
      "learning_rate": 5.818885448916409e-05,
      "loss": 16.3583,
      "step": 8703
    },
    {
      "epoch": 8.71,
      "grad_norm": 3355.69287109375,
      "learning_rate": 5.818369453044376e-05,
      "loss": 15.8154,
      "step": 8704
    },
    {
      "epoch": 8.71,
      "grad_norm": 1227.631103515625,
      "learning_rate": 5.817853457172343e-05,
      "loss": 13.7078,
      "step": 8705
    },
    {
      "epoch": 8.71,
      "grad_norm": 4402.30859375,
      "learning_rate": 5.8173374613003094e-05,
      "loss": 13.266,
      "step": 8706
    },
    {
      "epoch": 8.72,
      "grad_norm": 4718.14453125,
      "learning_rate": 5.816821465428277e-05,
      "loss": 14.0279,
      "step": 8707
    },
    {
      "epoch": 8.72,
      "grad_norm": 15850.7373046875,
      "learning_rate": 5.8163054695562436e-05,
      "loss": 10.5866,
      "step": 8708
    },
    {
      "epoch": 8.72,
      "grad_norm": 1690.2308349609375,
      "learning_rate": 5.815789473684211e-05,
      "loss": 16.6444,
      "step": 8709
    },
    {
      "epoch": 8.72,
      "grad_norm": 7506.6435546875,
      "learning_rate": 5.815273477812178e-05,
      "loss": 13.9748,
      "step": 8710
    },
    {
      "epoch": 8.72,
      "grad_norm": 11114.0166015625,
      "learning_rate": 5.814757481940145e-05,
      "loss": 18.2591,
      "step": 8711
    },
    {
      "epoch": 8.72,
      "grad_norm": 998.585205078125,
      "learning_rate": 5.8142414860681114e-05,
      "loss": 13.1215,
      "step": 8712
    },
    {
      "epoch": 8.72,
      "grad_norm": 5481.26123046875,
      "learning_rate": 5.813725490196079e-05,
      "loss": 15.8029,
      "step": 8713
    },
    {
      "epoch": 8.72,
      "grad_norm": 27899.09375,
      "learning_rate": 5.8132094943240456e-05,
      "loss": 13.9832,
      "step": 8714
    },
    {
      "epoch": 8.72,
      "grad_norm": 5359.59130859375,
      "learning_rate": 5.812693498452013e-05,
      "loss": 14.8856,
      "step": 8715
    },
    {
      "epoch": 8.72,
      "grad_norm": 31888.052734375,
      "learning_rate": 5.81217750257998e-05,
      "loss": 18.0513,
      "step": 8716
    },
    {
      "epoch": 8.73,
      "grad_norm": 32822.6796875,
      "learning_rate": 5.811661506707946e-05,
      "loss": 16.7705,
      "step": 8717
    },
    {
      "epoch": 8.73,
      "grad_norm": 6768.14404296875,
      "learning_rate": 5.8111455108359134e-05,
      "loss": 17.6241,
      "step": 8718
    },
    {
      "epoch": 8.73,
      "grad_norm": 3460.364013671875,
      "learning_rate": 5.81062951496388e-05,
      "loss": 14.0401,
      "step": 8719
    },
    {
      "epoch": 8.73,
      "grad_norm": 5040.80712890625,
      "learning_rate": 5.8101135190918476e-05,
      "loss": 14.7576,
      "step": 8720
    },
    {
      "epoch": 8.73,
      "grad_norm": 2589.287841796875,
      "learning_rate": 5.8095975232198144e-05,
      "loss": 10.877,
      "step": 8721
    },
    {
      "epoch": 8.73,
      "grad_norm": 772.84814453125,
      "learning_rate": 5.809081527347782e-05,
      "loss": 11.2648,
      "step": 8722
    },
    {
      "epoch": 8.73,
      "grad_norm": 2518.650146484375,
      "learning_rate": 5.808565531475748e-05,
      "loss": 16.2761,
      "step": 8723
    },
    {
      "epoch": 8.73,
      "grad_norm": 3558.774658203125,
      "learning_rate": 5.8080495356037154e-05,
      "loss": 14.8084,
      "step": 8724
    },
    {
      "epoch": 8.73,
      "grad_norm": 2417.342041015625,
      "learning_rate": 5.807533539731682e-05,
      "loss": 13.9173,
      "step": 8725
    },
    {
      "epoch": 8.73,
      "grad_norm": 4468.1318359375,
      "learning_rate": 5.8070175438596496e-05,
      "loss": 16.2593,
      "step": 8726
    },
    {
      "epoch": 8.74,
      "grad_norm": 34696.640625,
      "learning_rate": 5.8065015479876164e-05,
      "loss": 15.3996,
      "step": 8727
    },
    {
      "epoch": 8.74,
      "grad_norm": 3376.36279296875,
      "learning_rate": 5.805985552115584e-05,
      "loss": 16.937,
      "step": 8728
    },
    {
      "epoch": 8.74,
      "grad_norm": 4176.9873046875,
      "learning_rate": 5.80546955624355e-05,
      "loss": 15.798,
      "step": 8729
    },
    {
      "epoch": 8.74,
      "grad_norm": 4162.65478515625,
      "learning_rate": 5.804953560371518e-05,
      "loss": 22.0562,
      "step": 8730
    },
    {
      "epoch": 8.74,
      "grad_norm": 5904.61669921875,
      "learning_rate": 5.804437564499484e-05,
      "loss": 12.5534,
      "step": 8731
    },
    {
      "epoch": 8.74,
      "grad_norm": 5642.017578125,
      "learning_rate": 5.8039215686274516e-05,
      "loss": 17.3098,
      "step": 8732
    },
    {
      "epoch": 8.74,
      "grad_norm": 1549.20703125,
      "learning_rate": 5.8034055727554184e-05,
      "loss": 13.5048,
      "step": 8733
    },
    {
      "epoch": 8.74,
      "grad_norm": 907.5039672851562,
      "learning_rate": 5.802889576883386e-05,
      "loss": 11.5785,
      "step": 8734
    },
    {
      "epoch": 8.74,
      "grad_norm": 3165.40966796875,
      "learning_rate": 5.802373581011352e-05,
      "loss": 13.4687,
      "step": 8735
    },
    {
      "epoch": 8.74,
      "grad_norm": 2763.57177734375,
      "learning_rate": 5.801857585139319e-05,
      "loss": 15.4552,
      "step": 8736
    },
    {
      "epoch": 8.75,
      "grad_norm": 10976.1025390625,
      "learning_rate": 5.801341589267286e-05,
      "loss": 15.9561,
      "step": 8737
    },
    {
      "epoch": 8.75,
      "grad_norm": 10786.1689453125,
      "learning_rate": 5.800825593395253e-05,
      "loss": 13.8694,
      "step": 8738
    },
    {
      "epoch": 8.75,
      "grad_norm": 2989.2373046875,
      "learning_rate": 5.8003095975232204e-05,
      "loss": 13.5212,
      "step": 8739
    },
    {
      "epoch": 8.75,
      "grad_norm": 2110.997314453125,
      "learning_rate": 5.7997936016511865e-05,
      "loss": 14.4097,
      "step": 8740
    },
    {
      "epoch": 8.75,
      "grad_norm": 1923.8309326171875,
      "learning_rate": 5.799277605779154e-05,
      "loss": 13.8042,
      "step": 8741
    },
    {
      "epoch": 8.75,
      "grad_norm": 15703.21875,
      "learning_rate": 5.798761609907121e-05,
      "loss": 14.7327,
      "step": 8742
    },
    {
      "epoch": 8.75,
      "grad_norm": 3307.55419921875,
      "learning_rate": 5.798245614035088e-05,
      "loss": 14.4268,
      "step": 8743
    },
    {
      "epoch": 8.75,
      "grad_norm": 2159.75537109375,
      "learning_rate": 5.797729618163055e-05,
      "loss": 18.54,
      "step": 8744
    },
    {
      "epoch": 8.75,
      "grad_norm": 5140.5009765625,
      "learning_rate": 5.7972136222910224e-05,
      "loss": 19.7397,
      "step": 8745
    },
    {
      "epoch": 8.75,
      "grad_norm": 1588.7200927734375,
      "learning_rate": 5.7966976264189885e-05,
      "loss": 13.9906,
      "step": 8746
    },
    {
      "epoch": 8.76,
      "grad_norm": 17346.046875,
      "learning_rate": 5.7961816305469566e-05,
      "loss": 22.3466,
      "step": 8747
    },
    {
      "epoch": 8.76,
      "grad_norm": 3024.765380859375,
      "learning_rate": 5.795665634674923e-05,
      "loss": 16.6541,
      "step": 8748
    },
    {
      "epoch": 8.76,
      "grad_norm": 1918.7510986328125,
      "learning_rate": 5.79514963880289e-05,
      "loss": 12.3878,
      "step": 8749
    },
    {
      "epoch": 8.76,
      "grad_norm": 1218.471923828125,
      "learning_rate": 5.794633642930857e-05,
      "loss": 18.3321,
      "step": 8750
    },
    {
      "epoch": 8.76,
      "grad_norm": 7981.97607421875,
      "learning_rate": 5.7941176470588244e-05,
      "loss": 17.9844,
      "step": 8751
    },
    {
      "epoch": 8.76,
      "grad_norm": 2622.351806640625,
      "learning_rate": 5.7936016511867905e-05,
      "loss": 20.3022,
      "step": 8752
    },
    {
      "epoch": 8.76,
      "grad_norm": 2249.837158203125,
      "learning_rate": 5.793085655314757e-05,
      "loss": 14.9117,
      "step": 8753
    },
    {
      "epoch": 8.76,
      "grad_norm": 27359.552734375,
      "learning_rate": 5.792569659442725e-05,
      "loss": 12.6963,
      "step": 8754
    },
    {
      "epoch": 8.76,
      "grad_norm": 13374.4130859375,
      "learning_rate": 5.7920536635706915e-05,
      "loss": 22.6366,
      "step": 8755
    },
    {
      "epoch": 8.76,
      "grad_norm": 10320.7197265625,
      "learning_rate": 5.791537667698659e-05,
      "loss": 13.1993,
      "step": 8756
    },
    {
      "epoch": 8.77,
      "grad_norm": 24310.775390625,
      "learning_rate": 5.791021671826625e-05,
      "loss": 17.3237,
      "step": 8757
    },
    {
      "epoch": 8.77,
      "grad_norm": 1482.82958984375,
      "learning_rate": 5.7905056759545925e-05,
      "loss": 16.5256,
      "step": 8758
    },
    {
      "epoch": 8.77,
      "grad_norm": 3156.79150390625,
      "learning_rate": 5.789989680082559e-05,
      "loss": 16.433,
      "step": 8759
    },
    {
      "epoch": 8.77,
      "grad_norm": 40736.55078125,
      "learning_rate": 5.789473684210527e-05,
      "loss": 14.5125,
      "step": 8760
    },
    {
      "epoch": 8.77,
      "grad_norm": 486.8814392089844,
      "learning_rate": 5.7889576883384935e-05,
      "loss": 14.2439,
      "step": 8761
    },
    {
      "epoch": 8.77,
      "grad_norm": 40115.05078125,
      "learning_rate": 5.788441692466461e-05,
      "loss": 18.3464,
      "step": 8762
    },
    {
      "epoch": 8.77,
      "grad_norm": 1547.3541259765625,
      "learning_rate": 5.787925696594427e-05,
      "loss": 13.6615,
      "step": 8763
    },
    {
      "epoch": 8.77,
      "grad_norm": 19235.685546875,
      "learning_rate": 5.787409700722395e-05,
      "loss": 17.0952,
      "step": 8764
    },
    {
      "epoch": 8.77,
      "grad_norm": 2616.183837890625,
      "learning_rate": 5.786893704850361e-05,
      "loss": 15.4576,
      "step": 8765
    },
    {
      "epoch": 8.77,
      "grad_norm": 3498.734375,
      "learning_rate": 5.786377708978329e-05,
      "loss": 17.6985,
      "step": 8766
    },
    {
      "epoch": 8.78,
      "grad_norm": 1930.12548828125,
      "learning_rate": 5.7858617131062955e-05,
      "loss": 12.7897,
      "step": 8767
    },
    {
      "epoch": 8.78,
      "grad_norm": 9346.658203125,
      "learning_rate": 5.785345717234263e-05,
      "loss": 19.4405,
      "step": 8768
    },
    {
      "epoch": 8.78,
      "grad_norm": 15790.8125,
      "learning_rate": 5.784829721362229e-05,
      "loss": 11.8983,
      "step": 8769
    },
    {
      "epoch": 8.78,
      "grad_norm": 492.4185791015625,
      "learning_rate": 5.784313725490197e-05,
      "loss": 12.1629,
      "step": 8770
    },
    {
      "epoch": 8.78,
      "grad_norm": 1993.1748046875,
      "learning_rate": 5.783797729618163e-05,
      "loss": 12.1205,
      "step": 8771
    },
    {
      "epoch": 8.78,
      "grad_norm": 28395.28125,
      "learning_rate": 5.78328173374613e-05,
      "loss": 16.6694,
      "step": 8772
    },
    {
      "epoch": 8.78,
      "grad_norm": 2140.133544921875,
      "learning_rate": 5.7827657378740975e-05,
      "loss": 18.1442,
      "step": 8773
    },
    {
      "epoch": 8.78,
      "grad_norm": 10477.2900390625,
      "learning_rate": 5.7822497420020636e-05,
      "loss": 14.92,
      "step": 8774
    },
    {
      "epoch": 8.78,
      "grad_norm": 486.23956298828125,
      "learning_rate": 5.781733746130032e-05,
      "loss": 15.6475,
      "step": 8775
    },
    {
      "epoch": 8.78,
      "grad_norm": 2131.19482421875,
      "learning_rate": 5.781217750257998e-05,
      "loss": 13.9784,
      "step": 8776
    },
    {
      "epoch": 8.79,
      "grad_norm": 19100.90625,
      "learning_rate": 5.780701754385965e-05,
      "loss": 12.4689,
      "step": 8777
    },
    {
      "epoch": 8.79,
      "grad_norm": 1553.6650390625,
      "learning_rate": 5.780185758513932e-05,
      "loss": 11.9466,
      "step": 8778
    },
    {
      "epoch": 8.79,
      "grad_norm": 4737.974609375,
      "learning_rate": 5.7796697626418995e-05,
      "loss": 14.6016,
      "step": 8779
    },
    {
      "epoch": 8.79,
      "grad_norm": 308.1051025390625,
      "learning_rate": 5.7791537667698656e-05,
      "loss": 12.0522,
      "step": 8780
    },
    {
      "epoch": 8.79,
      "grad_norm": 1200.300537109375,
      "learning_rate": 5.778637770897834e-05,
      "loss": 13.4461,
      "step": 8781
    },
    {
      "epoch": 8.79,
      "grad_norm": 5733.8017578125,
      "learning_rate": 5.7781217750258e-05,
      "loss": 15.0885,
      "step": 8782
    },
    {
      "epoch": 8.79,
      "grad_norm": 5425.6142578125,
      "learning_rate": 5.777605779153767e-05,
      "loss": 12.0862,
      "step": 8783
    },
    {
      "epoch": 8.79,
      "grad_norm": 7234.517578125,
      "learning_rate": 5.777089783281734e-05,
      "loss": 13.9153,
      "step": 8784
    },
    {
      "epoch": 8.79,
      "grad_norm": 5031.60595703125,
      "learning_rate": 5.7765737874097015e-05,
      "loss": 17.0298,
      "step": 8785
    },
    {
      "epoch": 8.79,
      "grad_norm": 5207.11376953125,
      "learning_rate": 5.7760577915376676e-05,
      "loss": 18.4268,
      "step": 8786
    },
    {
      "epoch": 8.8,
      "grad_norm": 20985.091796875,
      "learning_rate": 5.775541795665636e-05,
      "loss": 13.839,
      "step": 8787
    },
    {
      "epoch": 8.8,
      "grad_norm": 4717.71923828125,
      "learning_rate": 5.775025799793602e-05,
      "loss": 12.2075,
      "step": 8788
    },
    {
      "epoch": 8.8,
      "grad_norm": 4603.4833984375,
      "learning_rate": 5.7745098039215686e-05,
      "loss": 13.3672,
      "step": 8789
    },
    {
      "epoch": 8.8,
      "grad_norm": 10078.6552734375,
      "learning_rate": 5.773993808049536e-05,
      "loss": 12.6763,
      "step": 8790
    },
    {
      "epoch": 8.8,
      "grad_norm": 4027.612060546875,
      "learning_rate": 5.773477812177502e-05,
      "loss": 16.198,
      "step": 8791
    },
    {
      "epoch": 8.8,
      "grad_norm": 5131.12109375,
      "learning_rate": 5.77296181630547e-05,
      "loss": 13.37,
      "step": 8792
    },
    {
      "epoch": 8.8,
      "grad_norm": 6658.15576171875,
      "learning_rate": 5.7724458204334363e-05,
      "loss": 14.3281,
      "step": 8793
    },
    {
      "epoch": 8.8,
      "grad_norm": 1808.1422119140625,
      "learning_rate": 5.771929824561404e-05,
      "loss": 13.625,
      "step": 8794
    },
    {
      "epoch": 8.8,
      "grad_norm": 20361.44921875,
      "learning_rate": 5.7714138286893706e-05,
      "loss": 12.381,
      "step": 8795
    },
    {
      "epoch": 8.8,
      "grad_norm": 5055.52587890625,
      "learning_rate": 5.770897832817338e-05,
      "loss": 13.8688,
      "step": 8796
    },
    {
      "epoch": 8.81,
      "grad_norm": 4717.41796875,
      "learning_rate": 5.770381836945304e-05,
      "loss": 13.541,
      "step": 8797
    },
    {
      "epoch": 8.81,
      "grad_norm": 5764.6474609375,
      "learning_rate": 5.769865841073272e-05,
      "loss": 15.0221,
      "step": 8798
    },
    {
      "epoch": 8.81,
      "grad_norm": 2215.866455078125,
      "learning_rate": 5.7693498452012383e-05,
      "loss": 16.0002,
      "step": 8799
    },
    {
      "epoch": 8.81,
      "grad_norm": 5574.02001953125,
      "learning_rate": 5.768833849329206e-05,
      "loss": 14.8718,
      "step": 8800
    },
    {
      "epoch": 8.81,
      "grad_norm": 7826.19580078125,
      "learning_rate": 5.7683178534571726e-05,
      "loss": 12.2513,
      "step": 8801
    },
    {
      "epoch": 8.81,
      "grad_norm": 4503.63134765625,
      "learning_rate": 5.76780185758514e-05,
      "loss": 17.5911,
      "step": 8802
    },
    {
      "epoch": 8.81,
      "grad_norm": 3865.056640625,
      "learning_rate": 5.767285861713107e-05,
      "loss": 12.8552,
      "step": 8803
    },
    {
      "epoch": 8.81,
      "grad_norm": 1025.688232421875,
      "learning_rate": 5.766769865841074e-05,
      "loss": 16.5988,
      "step": 8804
    },
    {
      "epoch": 8.81,
      "grad_norm": 11168.3388671875,
      "learning_rate": 5.7662538699690403e-05,
      "loss": 16.2001,
      "step": 8805
    },
    {
      "epoch": 8.81,
      "grad_norm": 12767.4765625,
      "learning_rate": 5.765737874097008e-05,
      "loss": 14.0975,
      "step": 8806
    },
    {
      "epoch": 8.82,
      "grad_norm": 4411.4296875,
      "learning_rate": 5.7652218782249746e-05,
      "loss": 12.9416,
      "step": 8807
    },
    {
      "epoch": 8.82,
      "grad_norm": 3305.248291015625,
      "learning_rate": 5.764705882352941e-05,
      "loss": 16.2413,
      "step": 8808
    },
    {
      "epoch": 8.82,
      "grad_norm": 11710.1826171875,
      "learning_rate": 5.764189886480909e-05,
      "loss": 23.1994,
      "step": 8809
    },
    {
      "epoch": 8.82,
      "grad_norm": 1046.2005615234375,
      "learning_rate": 5.763673890608875e-05,
      "loss": 14.2709,
      "step": 8810
    },
    {
      "epoch": 8.82,
      "grad_norm": 8731.59765625,
      "learning_rate": 5.7631578947368423e-05,
      "loss": 14.3837,
      "step": 8811
    },
    {
      "epoch": 8.82,
      "grad_norm": 2365.694091796875,
      "learning_rate": 5.762641898864809e-05,
      "loss": 14.1236,
      "step": 8812
    },
    {
      "epoch": 8.82,
      "grad_norm": 126038.5625,
      "learning_rate": 5.7621259029927766e-05,
      "loss": 32.1548,
      "step": 8813
    },
    {
      "epoch": 8.82,
      "grad_norm": 1203.52685546875,
      "learning_rate": 5.761609907120743e-05,
      "loss": 12.1037,
      "step": 8814
    },
    {
      "epoch": 8.82,
      "grad_norm": 2735.044189453125,
      "learning_rate": 5.761093911248711e-05,
      "loss": 11.7983,
      "step": 8815
    },
    {
      "epoch": 8.82,
      "grad_norm": 36171.67578125,
      "learning_rate": 5.760577915376677e-05,
      "loss": 19.0367,
      "step": 8816
    },
    {
      "epoch": 8.83,
      "grad_norm": 9266.7490234375,
      "learning_rate": 5.7600619195046443e-05,
      "loss": 21.7845,
      "step": 8817
    },
    {
      "epoch": 8.83,
      "grad_norm": 16719.505859375,
      "learning_rate": 5.759545923632611e-05,
      "loss": 14.2377,
      "step": 8818
    },
    {
      "epoch": 8.83,
      "grad_norm": 546.7705078125,
      "learning_rate": 5.7590299277605786e-05,
      "loss": 11.8809,
      "step": 8819
    },
    {
      "epoch": 8.83,
      "grad_norm": 13037.0869140625,
      "learning_rate": 5.7585139318885454e-05,
      "loss": 17.9405,
      "step": 8820
    },
    {
      "epoch": 8.83,
      "grad_norm": 6090.5380859375,
      "learning_rate": 5.757997936016513e-05,
      "loss": 16.28,
      "step": 8821
    },
    {
      "epoch": 8.83,
      "grad_norm": 3796.958984375,
      "learning_rate": 5.757481940144479e-05,
      "loss": 13.3011,
      "step": 8822
    },
    {
      "epoch": 8.83,
      "grad_norm": 6262.37451171875,
      "learning_rate": 5.7569659442724464e-05,
      "loss": 16.9412,
      "step": 8823
    },
    {
      "epoch": 8.83,
      "grad_norm": 2101.93701171875,
      "learning_rate": 5.756449948400413e-05,
      "loss": 12.8214,
      "step": 8824
    },
    {
      "epoch": 8.83,
      "grad_norm": 7643.65087890625,
      "learning_rate": 5.755933952528379e-05,
      "loss": 13.814,
      "step": 8825
    },
    {
      "epoch": 8.83,
      "grad_norm": 14364.5341796875,
      "learning_rate": 5.7554179566563474e-05,
      "loss": 10.4826,
      "step": 8826
    },
    {
      "epoch": 8.84,
      "grad_norm": 10718.3857421875,
      "learning_rate": 5.7549019607843134e-05,
      "loss": 11.5365,
      "step": 8827
    },
    {
      "epoch": 8.84,
      "grad_norm": 55351.5,
      "learning_rate": 5.754385964912281e-05,
      "loss": 12.7537,
      "step": 8828
    },
    {
      "epoch": 8.84,
      "grad_norm": 999.2688598632812,
      "learning_rate": 5.753869969040248e-05,
      "loss": 13.4628,
      "step": 8829
    },
    {
      "epoch": 8.84,
      "grad_norm": 10956.736328125,
      "learning_rate": 5.753353973168215e-05,
      "loss": 14.3526,
      "step": 8830
    },
    {
      "epoch": 8.84,
      "grad_norm": 2126.39453125,
      "learning_rate": 5.752837977296182e-05,
      "loss": 17.8958,
      "step": 8831
    },
    {
      "epoch": 8.84,
      "grad_norm": 1928.076904296875,
      "learning_rate": 5.7523219814241494e-05,
      "loss": 12.8436,
      "step": 8832
    },
    {
      "epoch": 8.84,
      "grad_norm": 3207.602783203125,
      "learning_rate": 5.7518059855521154e-05,
      "loss": 11.4889,
      "step": 8833
    },
    {
      "epoch": 8.84,
      "grad_norm": 7935.71533203125,
      "learning_rate": 5.751289989680083e-05,
      "loss": 16.7034,
      "step": 8834
    },
    {
      "epoch": 8.84,
      "grad_norm": 748.508544921875,
      "learning_rate": 5.75077399380805e-05,
      "loss": 13.8757,
      "step": 8835
    },
    {
      "epoch": 8.84,
      "grad_norm": 4344.537109375,
      "learning_rate": 5.750257997936017e-05,
      "loss": 19.1513,
      "step": 8836
    },
    {
      "epoch": 8.85,
      "grad_norm": 7234.00341796875,
      "learning_rate": 5.749742002063984e-05,
      "loss": 17.2433,
      "step": 8837
    },
    {
      "epoch": 8.85,
      "grad_norm": 20270.40234375,
      "learning_rate": 5.7492260061919514e-05,
      "loss": 19.0833,
      "step": 8838
    },
    {
      "epoch": 8.85,
      "grad_norm": 5214.265625,
      "learning_rate": 5.7487100103199174e-05,
      "loss": 14.5587,
      "step": 8839
    },
    {
      "epoch": 8.85,
      "grad_norm": 9544.4619140625,
      "learning_rate": 5.748194014447885e-05,
      "loss": 14.3862,
      "step": 8840
    },
    {
      "epoch": 8.85,
      "grad_norm": 69321.34375,
      "learning_rate": 5.747678018575852e-05,
      "loss": 20.2941,
      "step": 8841
    },
    {
      "epoch": 8.85,
      "grad_norm": 14767.279296875,
      "learning_rate": 5.747162022703818e-05,
      "loss": 18.719,
      "step": 8842
    },
    {
      "epoch": 8.85,
      "grad_norm": 1953.4239501953125,
      "learning_rate": 5.746646026831786e-05,
      "loss": 14.1805,
      "step": 8843
    },
    {
      "epoch": 8.85,
      "grad_norm": 9778.25390625,
      "learning_rate": 5.746130030959752e-05,
      "loss": 14.3667,
      "step": 8844
    },
    {
      "epoch": 8.85,
      "grad_norm": 32008.810546875,
      "learning_rate": 5.7456140350877194e-05,
      "loss": 14.2669,
      "step": 8845
    },
    {
      "epoch": 8.85,
      "grad_norm": 7700.0634765625,
      "learning_rate": 5.745098039215686e-05,
      "loss": 18.6987,
      "step": 8846
    },
    {
      "epoch": 8.86,
      "grad_norm": 8228.970703125,
      "learning_rate": 5.744582043343654e-05,
      "loss": 14.6135,
      "step": 8847
    },
    {
      "epoch": 8.86,
      "grad_norm": 1549.4176025390625,
      "learning_rate": 5.7440660474716204e-05,
      "loss": 24.078,
      "step": 8848
    },
    {
      "epoch": 8.86,
      "grad_norm": 8481.3896484375,
      "learning_rate": 5.743550051599588e-05,
      "loss": 17.5303,
      "step": 8849
    },
    {
      "epoch": 8.86,
      "grad_norm": 6988.37109375,
      "learning_rate": 5.743034055727554e-05,
      "loss": 14.7243,
      "step": 8850
    },
    {
      "epoch": 8.86,
      "grad_norm": 1588.977783203125,
      "learning_rate": 5.7425180598555214e-05,
      "loss": 14.9637,
      "step": 8851
    },
    {
      "epoch": 8.86,
      "grad_norm": 15166.900390625,
      "learning_rate": 5.742002063983488e-05,
      "loss": 14.8333,
      "step": 8852
    },
    {
      "epoch": 8.86,
      "grad_norm": 18007.56640625,
      "learning_rate": 5.741486068111456e-05,
      "loss": 17.2255,
      "step": 8853
    },
    {
      "epoch": 8.86,
      "grad_norm": 3121.502197265625,
      "learning_rate": 5.7409700722394224e-05,
      "loss": 14.2517,
      "step": 8854
    },
    {
      "epoch": 8.86,
      "grad_norm": 4138.20458984375,
      "learning_rate": 5.74045407636739e-05,
      "loss": 16.6346,
      "step": 8855
    },
    {
      "epoch": 8.86,
      "grad_norm": 10182.044921875,
      "learning_rate": 5.739938080495356e-05,
      "loss": 16.5793,
      "step": 8856
    },
    {
      "epoch": 8.87,
      "grad_norm": 2661.024169921875,
      "learning_rate": 5.7394220846233234e-05,
      "loss": 13.6409,
      "step": 8857
    },
    {
      "epoch": 8.87,
      "grad_norm": 5404.4892578125,
      "learning_rate": 5.73890608875129e-05,
      "loss": 16.7707,
      "step": 8858
    },
    {
      "epoch": 8.87,
      "grad_norm": 3263.3857421875,
      "learning_rate": 5.738390092879258e-05,
      "loss": 12.6109,
      "step": 8859
    },
    {
      "epoch": 8.87,
      "grad_norm": 29341.923828125,
      "learning_rate": 5.7378740970072245e-05,
      "loss": 12.9383,
      "step": 8860
    },
    {
      "epoch": 8.87,
      "grad_norm": 1851.2880859375,
      "learning_rate": 5.7373581011351905e-05,
      "loss": 12.5728,
      "step": 8861
    },
    {
      "epoch": 8.87,
      "grad_norm": 9071.25,
      "learning_rate": 5.736842105263158e-05,
      "loss": 15.8185,
      "step": 8862
    },
    {
      "epoch": 8.87,
      "grad_norm": 846.7598266601562,
      "learning_rate": 5.736326109391125e-05,
      "loss": 12.706,
      "step": 8863
    },
    {
      "epoch": 8.87,
      "grad_norm": 1955.71484375,
      "learning_rate": 5.735810113519092e-05,
      "loss": 13.6578,
      "step": 8864
    },
    {
      "epoch": 8.87,
      "grad_norm": 296.30120849609375,
      "learning_rate": 5.735294117647059e-05,
      "loss": 12.7577,
      "step": 8865
    },
    {
      "epoch": 8.87,
      "grad_norm": 2222.99267578125,
      "learning_rate": 5.7347781217750265e-05,
      "loss": 13.2669,
      "step": 8866
    },
    {
      "epoch": 8.88,
      "grad_norm": 3344.2783203125,
      "learning_rate": 5.7342621259029925e-05,
      "loss": 14.6316,
      "step": 8867
    },
    {
      "epoch": 8.88,
      "grad_norm": 8331.412109375,
      "learning_rate": 5.73374613003096e-05,
      "loss": 12.2178,
      "step": 8868
    },
    {
      "epoch": 8.88,
      "grad_norm": 8482.529296875,
      "learning_rate": 5.733230134158927e-05,
      "loss": 14.514,
      "step": 8869
    },
    {
      "epoch": 8.88,
      "grad_norm": 4021.71240234375,
      "learning_rate": 5.732714138286894e-05,
      "loss": 14.7017,
      "step": 8870
    },
    {
      "epoch": 8.88,
      "grad_norm": 13538.13671875,
      "learning_rate": 5.732198142414861e-05,
      "loss": 15.294,
      "step": 8871
    },
    {
      "epoch": 8.88,
      "grad_norm": 2990.62646484375,
      "learning_rate": 5.7316821465428285e-05,
      "loss": 14.7501,
      "step": 8872
    },
    {
      "epoch": 8.88,
      "grad_norm": 27353.896484375,
      "learning_rate": 5.7311661506707945e-05,
      "loss": 16.0504,
      "step": 8873
    },
    {
      "epoch": 8.88,
      "grad_norm": 3667.796630859375,
      "learning_rate": 5.730650154798762e-05,
      "loss": 11.8027,
      "step": 8874
    },
    {
      "epoch": 8.88,
      "grad_norm": 1048.064208984375,
      "learning_rate": 5.730134158926729e-05,
      "loss": 14.9247,
      "step": 8875
    },
    {
      "epoch": 8.88,
      "grad_norm": 2908.28515625,
      "learning_rate": 5.729618163054696e-05,
      "loss": 13.5978,
      "step": 8876
    },
    {
      "epoch": 8.89,
      "grad_norm": 2951.75244140625,
      "learning_rate": 5.729102167182663e-05,
      "loss": 17.544,
      "step": 8877
    },
    {
      "epoch": 8.89,
      "grad_norm": 14820.0927734375,
      "learning_rate": 5.728586171310629e-05,
      "loss": 15.7759,
      "step": 8878
    },
    {
      "epoch": 8.89,
      "grad_norm": 14593.4912109375,
      "learning_rate": 5.7280701754385965e-05,
      "loss": 13.116,
      "step": 8879
    },
    {
      "epoch": 8.89,
      "grad_norm": 1077.434814453125,
      "learning_rate": 5.727554179566563e-05,
      "loss": 11.7794,
      "step": 8880
    },
    {
      "epoch": 8.89,
      "grad_norm": 27486.6796875,
      "learning_rate": 5.727038183694531e-05,
      "loss": 20.6829,
      "step": 8881
    },
    {
      "epoch": 8.89,
      "grad_norm": 3458.9912109375,
      "learning_rate": 5.7265221878224975e-05,
      "loss": 19.978,
      "step": 8882
    },
    {
      "epoch": 8.89,
      "grad_norm": 1735.8636474609375,
      "learning_rate": 5.726006191950465e-05,
      "loss": 14.6836,
      "step": 8883
    },
    {
      "epoch": 8.89,
      "grad_norm": 1854.5003662109375,
      "learning_rate": 5.725490196078431e-05,
      "loss": 12.4498,
      "step": 8884
    },
    {
      "epoch": 8.89,
      "grad_norm": 30436.53125,
      "learning_rate": 5.7249742002063985e-05,
      "loss": 12.1354,
      "step": 8885
    },
    {
      "epoch": 8.89,
      "grad_norm": 8685.7958984375,
      "learning_rate": 5.724458204334365e-05,
      "loss": 13.7785,
      "step": 8886
    },
    {
      "epoch": 8.9,
      "grad_norm": 7225.59130859375,
      "learning_rate": 5.723942208462333e-05,
      "loss": 15.5974,
      "step": 8887
    },
    {
      "epoch": 8.9,
      "grad_norm": 7977.6484375,
      "learning_rate": 5.7234262125902995e-05,
      "loss": 18.304,
      "step": 8888
    },
    {
      "epoch": 8.9,
      "grad_norm": 14964.529296875,
      "learning_rate": 5.722910216718267e-05,
      "loss": 14.727,
      "step": 8889
    },
    {
      "epoch": 8.9,
      "grad_norm": 3160.21923828125,
      "learning_rate": 5.722394220846233e-05,
      "loss": 22.3081,
      "step": 8890
    },
    {
      "epoch": 8.9,
      "grad_norm": 8592.2265625,
      "learning_rate": 5.721878224974201e-05,
      "loss": 15.0052,
      "step": 8891
    },
    {
      "epoch": 8.9,
      "grad_norm": 16731.1953125,
      "learning_rate": 5.721362229102167e-05,
      "loss": 17.7332,
      "step": 8892
    },
    {
      "epoch": 8.9,
      "grad_norm": 4349.47900390625,
      "learning_rate": 5.720846233230135e-05,
      "loss": 15.2097,
      "step": 8893
    },
    {
      "epoch": 8.9,
      "grad_norm": 5239.48974609375,
      "learning_rate": 5.7203302373581015e-05,
      "loss": 16.5152,
      "step": 8894
    },
    {
      "epoch": 8.9,
      "grad_norm": 25000.560546875,
      "learning_rate": 5.719814241486069e-05,
      "loss": 15.3549,
      "step": 8895
    },
    {
      "epoch": 8.9,
      "grad_norm": 27784.734375,
      "learning_rate": 5.719298245614035e-05,
      "loss": 13.4628,
      "step": 8896
    },
    {
      "epoch": 8.91,
      "grad_norm": 1938.5216064453125,
      "learning_rate": 5.718782249742002e-05,
      "loss": 14.6177,
      "step": 8897
    },
    {
      "epoch": 8.91,
      "grad_norm": 501.7099914550781,
      "learning_rate": 5.718266253869969e-05,
      "loss": 16.3658,
      "step": 8898
    },
    {
      "epoch": 8.91,
      "grad_norm": 3113.29052734375,
      "learning_rate": 5.717750257997936e-05,
      "loss": 13.342,
      "step": 8899
    },
    {
      "epoch": 8.91,
      "grad_norm": 2691.624755859375,
      "learning_rate": 5.7172342621259035e-05,
      "loss": 13.5359,
      "step": 8900
    },
    {
      "epoch": 8.91,
      "grad_norm": 6492.04736328125,
      "learning_rate": 5.7167182662538696e-05,
      "loss": 13.253,
      "step": 8901
    },
    {
      "epoch": 8.91,
      "grad_norm": 4827.546875,
      "learning_rate": 5.716202270381837e-05,
      "loss": 12.675,
      "step": 8902
    },
    {
      "epoch": 8.91,
      "grad_norm": 660.52880859375,
      "learning_rate": 5.715686274509804e-05,
      "loss": 17.7554,
      "step": 8903
    },
    {
      "epoch": 8.91,
      "grad_norm": 374215.40625,
      "learning_rate": 5.715170278637771e-05,
      "loss": 13.0283,
      "step": 8904
    },
    {
      "epoch": 8.91,
      "grad_norm": 7726.00830078125,
      "learning_rate": 5.714654282765738e-05,
      "loss": 13.3917,
      "step": 8905
    },
    {
      "epoch": 8.91,
      "grad_norm": 42394.5546875,
      "learning_rate": 5.7141382868937056e-05,
      "loss": 13.5023,
      "step": 8906
    },
    {
      "epoch": 8.92,
      "grad_norm": 3466.060302734375,
      "learning_rate": 5.7136222910216716e-05,
      "loss": 12.6547,
      "step": 8907
    },
    {
      "epoch": 8.92,
      "grad_norm": 1579.0867919921875,
      "learning_rate": 5.71310629514964e-05,
      "loss": 15.5973,
      "step": 8908
    },
    {
      "epoch": 8.92,
      "grad_norm": 6575.279296875,
      "learning_rate": 5.712590299277606e-05,
      "loss": 21.5974,
      "step": 8909
    },
    {
      "epoch": 8.92,
      "grad_norm": 3128.010986328125,
      "learning_rate": 5.712074303405573e-05,
      "loss": 19.7613,
      "step": 8910
    },
    {
      "epoch": 8.92,
      "grad_norm": 2212.562744140625,
      "learning_rate": 5.71155830753354e-05,
      "loss": 14.0278,
      "step": 8911
    },
    {
      "epoch": 8.92,
      "grad_norm": 23277.22265625,
      "learning_rate": 5.7110423116615076e-05,
      "loss": 32.6342,
      "step": 8912
    },
    {
      "epoch": 8.92,
      "grad_norm": 2538.06201171875,
      "learning_rate": 5.7105263157894736e-05,
      "loss": 15.4168,
      "step": 8913
    },
    {
      "epoch": 8.92,
      "grad_norm": 2800.602294921875,
      "learning_rate": 5.7100103199174404e-05,
      "loss": 13.2796,
      "step": 8914
    },
    {
      "epoch": 8.92,
      "grad_norm": 743.4528198242188,
      "learning_rate": 5.709494324045408e-05,
      "loss": 13.2976,
      "step": 8915
    },
    {
      "epoch": 8.92,
      "grad_norm": 18099.51953125,
      "learning_rate": 5.7089783281733746e-05,
      "loss": 13.5461,
      "step": 8916
    },
    {
      "epoch": 8.93,
      "grad_norm": 13725.6826171875,
      "learning_rate": 5.708462332301342e-05,
      "loss": 13.177,
      "step": 8917
    },
    {
      "epoch": 8.93,
      "grad_norm": 49048.03515625,
      "learning_rate": 5.707946336429308e-05,
      "loss": 15.1568,
      "step": 8918
    },
    {
      "epoch": 8.93,
      "grad_norm": 16329.2314453125,
      "learning_rate": 5.707430340557276e-05,
      "loss": 14.091,
      "step": 8919
    },
    {
      "epoch": 8.93,
      "grad_norm": 3065.39794921875,
      "learning_rate": 5.7069143446852424e-05,
      "loss": 14.6375,
      "step": 8920
    },
    {
      "epoch": 8.93,
      "grad_norm": 1474.77783203125,
      "learning_rate": 5.70639834881321e-05,
      "loss": 14.958,
      "step": 8921
    },
    {
      "epoch": 8.93,
      "grad_norm": 7296.9150390625,
      "learning_rate": 5.7058823529411766e-05,
      "loss": 15.0292,
      "step": 8922
    },
    {
      "epoch": 8.93,
      "grad_norm": 15437.5048828125,
      "learning_rate": 5.705366357069144e-05,
      "loss": 13.8988,
      "step": 8923
    },
    {
      "epoch": 8.93,
      "grad_norm": 1511.9337158203125,
      "learning_rate": 5.70485036119711e-05,
      "loss": 12.4064,
      "step": 8924
    },
    {
      "epoch": 8.93,
      "grad_norm": 4869.41748046875,
      "learning_rate": 5.704334365325078e-05,
      "loss": 12.3275,
      "step": 8925
    },
    {
      "epoch": 8.93,
      "grad_norm": 41635.671875,
      "learning_rate": 5.7038183694530444e-05,
      "loss": 20.3366,
      "step": 8926
    },
    {
      "epoch": 8.94,
      "grad_norm": 7244.43115234375,
      "learning_rate": 5.703302373581012e-05,
      "loss": 11.9614,
      "step": 8927
    },
    {
      "epoch": 8.94,
      "grad_norm": 13112.619140625,
      "learning_rate": 5.7027863777089786e-05,
      "loss": 14.0913,
      "step": 8928
    },
    {
      "epoch": 8.94,
      "grad_norm": 6705.3369140625,
      "learning_rate": 5.702270381836946e-05,
      "loss": 21.1943,
      "step": 8929
    },
    {
      "epoch": 8.94,
      "grad_norm": 545.8599243164062,
      "learning_rate": 5.701754385964912e-05,
      "loss": 12.8078,
      "step": 8930
    },
    {
      "epoch": 8.94,
      "grad_norm": 20163.69140625,
      "learning_rate": 5.70123839009288e-05,
      "loss": 14.6624,
      "step": 8931
    },
    {
      "epoch": 8.94,
      "grad_norm": 7942.583984375,
      "learning_rate": 5.7007223942208464e-05,
      "loss": 15.8261,
      "step": 8932
    },
    {
      "epoch": 8.94,
      "grad_norm": 2909.890625,
      "learning_rate": 5.700206398348813e-05,
      "loss": 16.11,
      "step": 8933
    },
    {
      "epoch": 8.94,
      "grad_norm": 1576.4034423828125,
      "learning_rate": 5.6996904024767806e-05,
      "loss": 11.8074,
      "step": 8934
    },
    {
      "epoch": 8.94,
      "grad_norm": 23135.96875,
      "learning_rate": 5.699174406604747e-05,
      "loss": 14.3243,
      "step": 8935
    },
    {
      "epoch": 8.94,
      "grad_norm": 5198.11865234375,
      "learning_rate": 5.698658410732715e-05,
      "loss": 16.3492,
      "step": 8936
    },
    {
      "epoch": 8.95,
      "grad_norm": 6747.40478515625,
      "learning_rate": 5.698142414860681e-05,
      "loss": 12.8673,
      "step": 8937
    },
    {
      "epoch": 8.95,
      "grad_norm": 2020.051513671875,
      "learning_rate": 5.6976264189886484e-05,
      "loss": 16.8509,
      "step": 8938
    },
    {
      "epoch": 8.95,
      "grad_norm": 651.9579467773438,
      "learning_rate": 5.697110423116615e-05,
      "loss": 12.7189,
      "step": 8939
    },
    {
      "epoch": 8.95,
      "grad_norm": 3379.325439453125,
      "learning_rate": 5.6965944272445826e-05,
      "loss": 13.6706,
      "step": 8940
    },
    {
      "epoch": 8.95,
      "grad_norm": 3379.582275390625,
      "learning_rate": 5.696078431372549e-05,
      "loss": 11.5436,
      "step": 8941
    },
    {
      "epoch": 8.95,
      "grad_norm": 1749.0361328125,
      "learning_rate": 5.695562435500517e-05,
      "loss": 13.143,
      "step": 8942
    },
    {
      "epoch": 8.95,
      "grad_norm": 15624.9619140625,
      "learning_rate": 5.695046439628483e-05,
      "loss": 17.2411,
      "step": 8943
    },
    {
      "epoch": 8.95,
      "grad_norm": 6111.06201171875,
      "learning_rate": 5.6945304437564504e-05,
      "loss": 12.0459,
      "step": 8944
    },
    {
      "epoch": 8.95,
      "grad_norm": 2514.9814453125,
      "learning_rate": 5.694014447884417e-05,
      "loss": 13.7882,
      "step": 8945
    },
    {
      "epoch": 8.95,
      "grad_norm": 15291.2412109375,
      "learning_rate": 5.6934984520123847e-05,
      "loss": 15.749,
      "step": 8946
    },
    {
      "epoch": 8.96,
      "grad_norm": 8493.7275390625,
      "learning_rate": 5.692982456140351e-05,
      "loss": 15.8359,
      "step": 8947
    },
    {
      "epoch": 8.96,
      "grad_norm": 30979.173828125,
      "learning_rate": 5.692466460268319e-05,
      "loss": 14.0628,
      "step": 8948
    },
    {
      "epoch": 8.96,
      "grad_norm": 2925.996826171875,
      "learning_rate": 5.691950464396285e-05,
      "loss": 12.1404,
      "step": 8949
    },
    {
      "epoch": 8.96,
      "grad_norm": 3617.992919921875,
      "learning_rate": 5.691434468524252e-05,
      "loss": 15.9239,
      "step": 8950
    },
    {
      "epoch": 8.96,
      "grad_norm": 4284.52734375,
      "learning_rate": 5.690918472652219e-05,
      "loss": 17.0596,
      "step": 8951
    },
    {
      "epoch": 8.96,
      "grad_norm": 14572.228515625,
      "learning_rate": 5.690402476780185e-05,
      "loss": 20.8426,
      "step": 8952
    },
    {
      "epoch": 8.96,
      "grad_norm": 2443.756103515625,
      "learning_rate": 5.6898864809081534e-05,
      "loss": 16.0773,
      "step": 8953
    },
    {
      "epoch": 8.96,
      "grad_norm": 524.720947265625,
      "learning_rate": 5.6893704850361195e-05,
      "loss": 14.2543,
      "step": 8954
    },
    {
      "epoch": 8.96,
      "grad_norm": 1318.9100341796875,
      "learning_rate": 5.688854489164087e-05,
      "loss": 12.1243,
      "step": 8955
    },
    {
      "epoch": 8.96,
      "grad_norm": 2134.6416015625,
      "learning_rate": 5.688338493292054e-05,
      "loss": 14.5279,
      "step": 8956
    },
    {
      "epoch": 8.97,
      "grad_norm": 2377.79052734375,
      "learning_rate": 5.687822497420021e-05,
      "loss": 13.5211,
      "step": 8957
    },
    {
      "epoch": 8.97,
      "grad_norm": 2434.9765625,
      "learning_rate": 5.687306501547987e-05,
      "loss": 13.2761,
      "step": 8958
    },
    {
      "epoch": 8.97,
      "grad_norm": 6792.61865234375,
      "learning_rate": 5.6867905056759554e-05,
      "loss": 15.1306,
      "step": 8959
    },
    {
      "epoch": 8.97,
      "grad_norm": 20691.583984375,
      "learning_rate": 5.6862745098039215e-05,
      "loss": 12.9712,
      "step": 8960
    },
    {
      "epoch": 8.97,
      "grad_norm": 8199.0791015625,
      "learning_rate": 5.685758513931889e-05,
      "loss": 10.8311,
      "step": 8961
    },
    {
      "epoch": 8.97,
      "grad_norm": 3926.328857421875,
      "learning_rate": 5.685242518059856e-05,
      "loss": 14.2503,
      "step": 8962
    },
    {
      "epoch": 8.97,
      "grad_norm": 4733.99267578125,
      "learning_rate": 5.684726522187823e-05,
      "loss": 12.6112,
      "step": 8963
    },
    {
      "epoch": 8.97,
      "grad_norm": 2300.21044921875,
      "learning_rate": 5.68421052631579e-05,
      "loss": 11.4791,
      "step": 8964
    },
    {
      "epoch": 8.97,
      "grad_norm": 1545.2601318359375,
      "learning_rate": 5.6836945304437574e-05,
      "loss": 14.2441,
      "step": 8965
    },
    {
      "epoch": 8.97,
      "grad_norm": 16814.919921875,
      "learning_rate": 5.6831785345717235e-05,
      "loss": 13.3812,
      "step": 8966
    },
    {
      "epoch": 8.98,
      "grad_norm": 5223.2099609375,
      "learning_rate": 5.682662538699691e-05,
      "loss": 11.6018,
      "step": 8967
    },
    {
      "epoch": 8.98,
      "grad_norm": 182243.671875,
      "learning_rate": 5.682146542827658e-05,
      "loss": 16.9425,
      "step": 8968
    },
    {
      "epoch": 8.98,
      "grad_norm": 5806.87744140625,
      "learning_rate": 5.681630546955624e-05,
      "loss": 14.1006,
      "step": 8969
    },
    {
      "epoch": 8.98,
      "grad_norm": 3024.534912109375,
      "learning_rate": 5.681114551083592e-05,
      "loss": 14.7368,
      "step": 8970
    },
    {
      "epoch": 8.98,
      "grad_norm": 1905.1865234375,
      "learning_rate": 5.680598555211558e-05,
      "loss": 17.4258,
      "step": 8971
    },
    {
      "epoch": 8.98,
      "grad_norm": 5320.32177734375,
      "learning_rate": 5.6800825593395255e-05,
      "loss": 13.0377,
      "step": 8972
    },
    {
      "epoch": 8.98,
      "grad_norm": 5257.53369140625,
      "learning_rate": 5.679566563467492e-05,
      "loss": 16.6265,
      "step": 8973
    },
    {
      "epoch": 8.98,
      "grad_norm": 2527.3828125,
      "learning_rate": 5.67905056759546e-05,
      "loss": 14.4216,
      "step": 8974
    },
    {
      "epoch": 8.98,
      "grad_norm": 5000.41455078125,
      "learning_rate": 5.678534571723426e-05,
      "loss": 15.4853,
      "step": 8975
    },
    {
      "epoch": 8.98,
      "grad_norm": 12612.3212890625,
      "learning_rate": 5.678018575851394e-05,
      "loss": 12.641,
      "step": 8976
    },
    {
      "epoch": 8.99,
      "grad_norm": 1760.1214599609375,
      "learning_rate": 5.67750257997936e-05,
      "loss": 14.3689,
      "step": 8977
    },
    {
      "epoch": 8.99,
      "grad_norm": 2059.052978515625,
      "learning_rate": 5.6769865841073275e-05,
      "loss": 15.463,
      "step": 8978
    },
    {
      "epoch": 8.99,
      "grad_norm": 3483.119384765625,
      "learning_rate": 5.676470588235294e-05,
      "loss": 10.8075,
      "step": 8979
    },
    {
      "epoch": 8.99,
      "grad_norm": 9839.2197265625,
      "learning_rate": 5.675954592363262e-05,
      "loss": 16.8314,
      "step": 8980
    },
    {
      "epoch": 8.99,
      "grad_norm": 46860.4140625,
      "learning_rate": 5.6754385964912285e-05,
      "loss": 15.7543,
      "step": 8981
    },
    {
      "epoch": 8.99,
      "grad_norm": 1854.9052734375,
      "learning_rate": 5.674922600619196e-05,
      "loss": 13.4597,
      "step": 8982
    },
    {
      "epoch": 8.99,
      "grad_norm": 5039.533203125,
      "learning_rate": 5.674406604747162e-05,
      "loss": 13.7826,
      "step": 8983
    },
    {
      "epoch": 8.99,
      "grad_norm": 4036.3310546875,
      "learning_rate": 5.6738906088751295e-05,
      "loss": 19.2566,
      "step": 8984
    },
    {
      "epoch": 8.99,
      "grad_norm": 2821.500244140625,
      "learning_rate": 5.673374613003096e-05,
      "loss": 13.0242,
      "step": 8985
    },
    {
      "epoch": 8.99,
      "grad_norm": 6856.490234375,
      "learning_rate": 5.6728586171310624e-05,
      "loss": 14.5092,
      "step": 8986
    },
    {
      "epoch": 9.0,
      "grad_norm": 23425.939453125,
      "learning_rate": 5.6723426212590305e-05,
      "loss": 18.0772,
      "step": 8987
    },
    {
      "epoch": 9.0,
      "grad_norm": 6148.69140625,
      "learning_rate": 5.6718266253869966e-05,
      "loss": 16.1393,
      "step": 8988
    },
    {
      "epoch": 9.0,
      "grad_norm": 1378.1712646484375,
      "learning_rate": 5.671310629514964e-05,
      "loss": 15.6747,
      "step": 8989
    },
    {
      "epoch": 9.0,
      "grad_norm": 2983.12158203125,
      "learning_rate": 5.670794633642931e-05,
      "loss": 12.6884,
      "step": 8990
    },
    {
      "epoch": 9.0,
      "grad_norm": 265444.6875,
      "learning_rate": 5.670278637770898e-05,
      "loss": 14.7495,
      "step": 8991
    },
    {
      "epoch": 9.0,
      "grad_norm": 6649.62353515625,
      "learning_rate": 5.669762641898865e-05,
      "loss": 14.2581,
      "step": 8992
    },
    {
      "epoch": 9.0,
      "grad_norm": 10210.0478515625,
      "learning_rate": 5.6692466460268325e-05,
      "loss": 14.8318,
      "step": 8993
    },
    {
      "epoch": 9.0,
      "grad_norm": 7802.302734375,
      "learning_rate": 5.6687306501547986e-05,
      "loss": 13.7083,
      "step": 8994
    },
    {
      "epoch": 9.0,
      "grad_norm": 18026.447265625,
      "learning_rate": 5.668214654282766e-05,
      "loss": 15.9079,
      "step": 8995
    },
    {
      "epoch": 9.01,
      "grad_norm": 7171.64794921875,
      "learning_rate": 5.667698658410733e-05,
      "loss": 13.6041,
      "step": 8996
    },
    {
      "epoch": 9.01,
      "grad_norm": 14915.87109375,
      "learning_rate": 5.6671826625387e-05,
      "loss": 15.4378,
      "step": 8997
    },
    {
      "epoch": 9.01,
      "grad_norm": 13158.6162109375,
      "learning_rate": 5.666666666666667e-05,
      "loss": 13.8838,
      "step": 8998
    },
    {
      "epoch": 9.01,
      "grad_norm": 39479.68359375,
      "learning_rate": 5.6661506707946345e-05,
      "loss": 13.8417,
      "step": 8999
    },
    {
      "epoch": 9.01,
      "grad_norm": 18585.734375,
      "learning_rate": 5.6656346749226006e-05,
      "loss": 14.2875,
      "step": 9000
    },
    {
      "epoch": 9.01,
      "grad_norm": 3743.55712890625,
      "learning_rate": 5.665118679050568e-05,
      "loss": 12.3064,
      "step": 9001
    },
    {
      "epoch": 9.01,
      "grad_norm": 1561.760498046875,
      "learning_rate": 5.664602683178535e-05,
      "loss": 14.6437,
      "step": 9002
    },
    {
      "epoch": 9.01,
      "grad_norm": 4585.57763671875,
      "learning_rate": 5.664086687306501e-05,
      "loss": 11.5282,
      "step": 9003
    },
    {
      "epoch": 9.01,
      "grad_norm": 6014.58740234375,
      "learning_rate": 5.663570691434469e-05,
      "loss": 14.8843,
      "step": 9004
    },
    {
      "epoch": 9.01,
      "grad_norm": 13263.46875,
      "learning_rate": 5.663054695562435e-05,
      "loss": 16.1881,
      "step": 9005
    },
    {
      "epoch": 9.02,
      "grad_norm": 16276.9013671875,
      "learning_rate": 5.6625386996904026e-05,
      "loss": 13.1526,
      "step": 9006
    },
    {
      "epoch": 9.02,
      "grad_norm": 3592.31591796875,
      "learning_rate": 5.6620227038183694e-05,
      "loss": 16.9031,
      "step": 9007
    },
    {
      "epoch": 9.02,
      "grad_norm": 13181.294921875,
      "learning_rate": 5.661506707946337e-05,
      "loss": 18.0701,
      "step": 9008
    },
    {
      "epoch": 9.02,
      "grad_norm": 3675.127685546875,
      "learning_rate": 5.6609907120743036e-05,
      "loss": 12.4254,
      "step": 9009
    },
    {
      "epoch": 9.02,
      "grad_norm": 3268.488037109375,
      "learning_rate": 5.660474716202271e-05,
      "loss": 14.4518,
      "step": 9010
    },
    {
      "epoch": 9.02,
      "grad_norm": 2598.411376953125,
      "learning_rate": 5.659958720330237e-05,
      "loss": 11.6472,
      "step": 9011
    },
    {
      "epoch": 9.02,
      "grad_norm": 5478.3701171875,
      "learning_rate": 5.6594427244582046e-05,
      "loss": 14.9536,
      "step": 9012
    },
    {
      "epoch": 9.02,
      "grad_norm": 1665.6060791015625,
      "learning_rate": 5.6589267285861714e-05,
      "loss": 13.7824,
      "step": 9013
    },
    {
      "epoch": 9.02,
      "grad_norm": 5994.37255859375,
      "learning_rate": 5.658410732714139e-05,
      "loss": 14.2758,
      "step": 9014
    },
    {
      "epoch": 9.02,
      "grad_norm": 1074.96484375,
      "learning_rate": 5.6578947368421056e-05,
      "loss": 10.5779,
      "step": 9015
    },
    {
      "epoch": 9.03,
      "grad_norm": 550.911865234375,
      "learning_rate": 5.657378740970073e-05,
      "loss": 13.4374,
      "step": 9016
    },
    {
      "epoch": 9.03,
      "grad_norm": 20783.236328125,
      "learning_rate": 5.656862745098039e-05,
      "loss": 14.3429,
      "step": 9017
    },
    {
      "epoch": 9.03,
      "grad_norm": 3415.971923828125,
      "learning_rate": 5.6563467492260066e-05,
      "loss": 12.6738,
      "step": 9018
    },
    {
      "epoch": 9.03,
      "grad_norm": 11877.9052734375,
      "learning_rate": 5.6558307533539734e-05,
      "loss": 11.676,
      "step": 9019
    },
    {
      "epoch": 9.03,
      "grad_norm": 1220.3555908203125,
      "learning_rate": 5.655314757481941e-05,
      "loss": 17.9888,
      "step": 9020
    },
    {
      "epoch": 9.03,
      "grad_norm": 13049.8046875,
      "learning_rate": 5.6547987616099076e-05,
      "loss": 15.6091,
      "step": 9021
    },
    {
      "epoch": 9.03,
      "grad_norm": 1487.983642578125,
      "learning_rate": 5.654282765737874e-05,
      "loss": 12.5385,
      "step": 9022
    },
    {
      "epoch": 9.03,
      "grad_norm": 2337.20703125,
      "learning_rate": 5.653766769865841e-05,
      "loss": 15.0611,
      "step": 9023
    },
    {
      "epoch": 9.03,
      "grad_norm": 4068.17431640625,
      "learning_rate": 5.653250773993808e-05,
      "loss": 12.1058,
      "step": 9024
    },
    {
      "epoch": 9.03,
      "grad_norm": 3621.422119140625,
      "learning_rate": 5.6527347781217754e-05,
      "loss": 13.48,
      "step": 9025
    },
    {
      "epoch": 9.04,
      "grad_norm": 6518.65087890625,
      "learning_rate": 5.652218782249742e-05,
      "loss": 13.8187,
      "step": 9026
    },
    {
      "epoch": 9.04,
      "grad_norm": 5720.40966796875,
      "learning_rate": 5.6517027863777096e-05,
      "loss": 15.8082,
      "step": 9027
    },
    {
      "epoch": 9.04,
      "grad_norm": 4414.92626953125,
      "learning_rate": 5.651186790505676e-05,
      "loss": 13.7878,
      "step": 9028
    },
    {
      "epoch": 9.04,
      "grad_norm": 11220.1650390625,
      "learning_rate": 5.650670794633643e-05,
      "loss": 13.1796,
      "step": 9029
    },
    {
      "epoch": 9.04,
      "grad_norm": 6512.0849609375,
      "learning_rate": 5.65015479876161e-05,
      "loss": 29.3053,
      "step": 9030
    },
    {
      "epoch": 9.04,
      "grad_norm": 3587.092041015625,
      "learning_rate": 5.6496388028895774e-05,
      "loss": 14.8254,
      "step": 9031
    },
    {
      "epoch": 9.04,
      "grad_norm": 4792.357421875,
      "learning_rate": 5.649122807017544e-05,
      "loss": 16.8506,
      "step": 9032
    },
    {
      "epoch": 9.04,
      "grad_norm": 4830.83056640625,
      "learning_rate": 5.6486068111455116e-05,
      "loss": 11.9295,
      "step": 9033
    },
    {
      "epoch": 9.04,
      "grad_norm": 12046.359375,
      "learning_rate": 5.648090815273478e-05,
      "loss": 15.5039,
      "step": 9034
    },
    {
      "epoch": 9.04,
      "grad_norm": 8905.3583984375,
      "learning_rate": 5.647574819401445e-05,
      "loss": 16.9551,
      "step": 9035
    },
    {
      "epoch": 9.05,
      "grad_norm": 16564.5859375,
      "learning_rate": 5.647058823529412e-05,
      "loss": 12.1611,
      "step": 9036
    },
    {
      "epoch": 9.05,
      "grad_norm": 18113.9609375,
      "learning_rate": 5.6465428276573794e-05,
      "loss": 16.9611,
      "step": 9037
    },
    {
      "epoch": 9.05,
      "grad_norm": 13711.1123046875,
      "learning_rate": 5.646026831785346e-05,
      "loss": 14.0791,
      "step": 9038
    },
    {
      "epoch": 9.05,
      "grad_norm": 19196.31640625,
      "learning_rate": 5.645510835913312e-05,
      "loss": 16.9011,
      "step": 9039
    },
    {
      "epoch": 9.05,
      "grad_norm": 2659.195068359375,
      "learning_rate": 5.64499484004128e-05,
      "loss": 13.3323,
      "step": 9040
    },
    {
      "epoch": 9.05,
      "grad_norm": 941.0360107421875,
      "learning_rate": 5.6444788441692465e-05,
      "loss": 14.5887,
      "step": 9041
    },
    {
      "epoch": 9.05,
      "grad_norm": 48413.58984375,
      "learning_rate": 5.643962848297214e-05,
      "loss": 17.5029,
      "step": 9042
    },
    {
      "epoch": 9.05,
      "grad_norm": 16183.27734375,
      "learning_rate": 5.643446852425181e-05,
      "loss": 12.5107,
      "step": 9043
    },
    {
      "epoch": 9.05,
      "grad_norm": 1855.3106689453125,
      "learning_rate": 5.642930856553148e-05,
      "loss": 17.7589,
      "step": 9044
    },
    {
      "epoch": 9.05,
      "grad_norm": 11954.7958984375,
      "learning_rate": 5.642414860681114e-05,
      "loss": 13.9636,
      "step": 9045
    },
    {
      "epoch": 9.06,
      "grad_norm": 17510.49609375,
      "learning_rate": 5.641898864809082e-05,
      "loss": 15.0532,
      "step": 9046
    },
    {
      "epoch": 9.06,
      "grad_norm": 1339.8143310546875,
      "learning_rate": 5.6413828689370485e-05,
      "loss": 12.9229,
      "step": 9047
    },
    {
      "epoch": 9.06,
      "grad_norm": 3393.899169921875,
      "learning_rate": 5.640866873065016e-05,
      "loss": 13.799,
      "step": 9048
    },
    {
      "epoch": 9.06,
      "grad_norm": 1615.5020751953125,
      "learning_rate": 5.640350877192983e-05,
      "loss": 12.5147,
      "step": 9049
    },
    {
      "epoch": 9.06,
      "grad_norm": 10573.466796875,
      "learning_rate": 5.63983488132095e-05,
      "loss": 17.9538,
      "step": 9050
    },
    {
      "epoch": 9.06,
      "grad_norm": 3648.062255859375,
      "learning_rate": 5.639318885448916e-05,
      "loss": 13.1726,
      "step": 9051
    },
    {
      "epoch": 9.06,
      "grad_norm": 16098.4521484375,
      "learning_rate": 5.6388028895768844e-05,
      "loss": 15.6868,
      "step": 9052
    },
    {
      "epoch": 9.06,
      "grad_norm": 22842.759765625,
      "learning_rate": 5.6382868937048505e-05,
      "loss": 16.2259,
      "step": 9053
    },
    {
      "epoch": 9.06,
      "grad_norm": 7726.3603515625,
      "learning_rate": 5.637770897832818e-05,
      "loss": 14.5276,
      "step": 9054
    },
    {
      "epoch": 9.06,
      "grad_norm": 3528.30517578125,
      "learning_rate": 5.637254901960785e-05,
      "loss": 20.2747,
      "step": 9055
    },
    {
      "epoch": 9.07,
      "grad_norm": 9036.55859375,
      "learning_rate": 5.636738906088752e-05,
      "loss": 13.806,
      "step": 9056
    },
    {
      "epoch": 9.07,
      "grad_norm": 3066.867919921875,
      "learning_rate": 5.636222910216718e-05,
      "loss": 14.246,
      "step": 9057
    },
    {
      "epoch": 9.07,
      "grad_norm": 4739.1640625,
      "learning_rate": 5.635706914344685e-05,
      "loss": 12.8541,
      "step": 9058
    },
    {
      "epoch": 9.07,
      "grad_norm": 14436.6689453125,
      "learning_rate": 5.6351909184726525e-05,
      "loss": 12.9071,
      "step": 9059
    },
    {
      "epoch": 9.07,
      "grad_norm": 3215.115966796875,
      "learning_rate": 5.634674922600619e-05,
      "loss": 15.3948,
      "step": 9060
    },
    {
      "epoch": 9.07,
      "grad_norm": 2667.220458984375,
      "learning_rate": 5.634158926728587e-05,
      "loss": 16.328,
      "step": 9061
    },
    {
      "epoch": 9.07,
      "grad_norm": 1098.5164794921875,
      "learning_rate": 5.633642930856553e-05,
      "loss": 13.3389,
      "step": 9062
    },
    {
      "epoch": 9.07,
      "grad_norm": 17533.841796875,
      "learning_rate": 5.63312693498452e-05,
      "loss": 16.8337,
      "step": 9063
    },
    {
      "epoch": 9.07,
      "grad_norm": 3493.11572265625,
      "learning_rate": 5.632610939112487e-05,
      "loss": 13.7578,
      "step": 9064
    },
    {
      "epoch": 9.07,
      "grad_norm": 954.2485961914062,
      "learning_rate": 5.6320949432404545e-05,
      "loss": 13.7214,
      "step": 9065
    },
    {
      "epoch": 9.08,
      "grad_norm": 2483.157958984375,
      "learning_rate": 5.631578947368421e-05,
      "loss": 12.9703,
      "step": 9066
    },
    {
      "epoch": 9.08,
      "grad_norm": 1680.9793701171875,
      "learning_rate": 5.631062951496389e-05,
      "loss": 14.3096,
      "step": 9067
    },
    {
      "epoch": 9.08,
      "grad_norm": 25692.712890625,
      "learning_rate": 5.630546955624355e-05,
      "loss": 16.2047,
      "step": 9068
    },
    {
      "epoch": 9.08,
      "grad_norm": 6960.921875,
      "learning_rate": 5.630030959752323e-05,
      "loss": 17.2018,
      "step": 9069
    },
    {
      "epoch": 9.08,
      "grad_norm": 3404.291259765625,
      "learning_rate": 5.629514963880289e-05,
      "loss": 16.7116,
      "step": 9070
    },
    {
      "epoch": 9.08,
      "grad_norm": 3526.662841796875,
      "learning_rate": 5.6289989680082565e-05,
      "loss": 18.2635,
      "step": 9071
    },
    {
      "epoch": 9.08,
      "grad_norm": 1805.311767578125,
      "learning_rate": 5.628482972136223e-05,
      "loss": 14.4451,
      "step": 9072
    },
    {
      "epoch": 9.08,
      "grad_norm": 2547.06103515625,
      "learning_rate": 5.627966976264191e-05,
      "loss": 13.1151,
      "step": 9073
    },
    {
      "epoch": 9.08,
      "grad_norm": 3554.1064453125,
      "learning_rate": 5.627450980392157e-05,
      "loss": 18.3673,
      "step": 9074
    },
    {
      "epoch": 9.08,
      "grad_norm": 5358.33740234375,
      "learning_rate": 5.6269349845201236e-05,
      "loss": 15.797,
      "step": 9075
    },
    {
      "epoch": 9.09,
      "grad_norm": 15215.56640625,
      "learning_rate": 5.626418988648091e-05,
      "loss": 13.66,
      "step": 9076
    },
    {
      "epoch": 9.09,
      "grad_norm": 908.16650390625,
      "learning_rate": 5.625902992776058e-05,
      "loss": 14.2315,
      "step": 9077
    },
    {
      "epoch": 9.09,
      "grad_norm": 5906.03662109375,
      "learning_rate": 5.625386996904025e-05,
      "loss": 17.6309,
      "step": 9078
    },
    {
      "epoch": 9.09,
      "grad_norm": 3710.270263671875,
      "learning_rate": 5.6248710010319914e-05,
      "loss": 14.6353,
      "step": 9079
    },
    {
      "epoch": 9.09,
      "grad_norm": 4156.80517578125,
      "learning_rate": 5.6243550051599595e-05,
      "loss": 12.3397,
      "step": 9080
    },
    {
      "epoch": 9.09,
      "grad_norm": 66098.1640625,
      "learning_rate": 5.6238390092879256e-05,
      "loss": 17.9934,
      "step": 9081
    },
    {
      "epoch": 9.09,
      "grad_norm": 4790.798828125,
      "learning_rate": 5.623323013415893e-05,
      "loss": 12.1948,
      "step": 9082
    },
    {
      "epoch": 9.09,
      "grad_norm": 3180.321533203125,
      "learning_rate": 5.62280701754386e-05,
      "loss": 13.7102,
      "step": 9083
    },
    {
      "epoch": 9.09,
      "grad_norm": 10563.8984375,
      "learning_rate": 5.622291021671827e-05,
      "loss": 16.345,
      "step": 9084
    },
    {
      "epoch": 9.09,
      "grad_norm": 19199.66015625,
      "learning_rate": 5.6217750257997934e-05,
      "loss": 16.1997,
      "step": 9085
    },
    {
      "epoch": 9.1,
      "grad_norm": 2766.21533203125,
      "learning_rate": 5.6212590299277615e-05,
      "loss": 12.4998,
      "step": 9086
    },
    {
      "epoch": 9.1,
      "grad_norm": 5120.3369140625,
      "learning_rate": 5.6207430340557276e-05,
      "loss": 9.9452,
      "step": 9087
    },
    {
      "epoch": 9.1,
      "grad_norm": 2529.2373046875,
      "learning_rate": 5.620227038183695e-05,
      "loss": 11.1811,
      "step": 9088
    },
    {
      "epoch": 9.1,
      "grad_norm": 31118.798828125,
      "learning_rate": 5.619711042311662e-05,
      "loss": 13.3076,
      "step": 9089
    },
    {
      "epoch": 9.1,
      "grad_norm": 541.446044921875,
      "learning_rate": 5.619195046439629e-05,
      "loss": 17.2671,
      "step": 9090
    },
    {
      "epoch": 9.1,
      "grad_norm": 8977.9794921875,
      "learning_rate": 5.6186790505675954e-05,
      "loss": 14.5012,
      "step": 9091
    },
    {
      "epoch": 9.1,
      "grad_norm": 12445.1259765625,
      "learning_rate": 5.6181630546955635e-05,
      "loss": 19.0271,
      "step": 9092
    },
    {
      "epoch": 9.1,
      "grad_norm": 6457.068359375,
      "learning_rate": 5.6176470588235296e-05,
      "loss": 21.4933,
      "step": 9093
    },
    {
      "epoch": 9.1,
      "grad_norm": 18329.650390625,
      "learning_rate": 5.6171310629514964e-05,
      "loss": 13.7463,
      "step": 9094
    },
    {
      "epoch": 9.1,
      "grad_norm": 6447.99853515625,
      "learning_rate": 5.616615067079464e-05,
      "loss": 15.1103,
      "step": 9095
    },
    {
      "epoch": 9.11,
      "grad_norm": 3249.645751953125,
      "learning_rate": 5.61609907120743e-05,
      "loss": 16.719,
      "step": 9096
    },
    {
      "epoch": 9.11,
      "grad_norm": 8069.43603515625,
      "learning_rate": 5.615583075335398e-05,
      "loss": 21.1303,
      "step": 9097
    },
    {
      "epoch": 9.11,
      "grad_norm": 41269.54296875,
      "learning_rate": 5.615067079463364e-05,
      "loss": 17.3024,
      "step": 9098
    },
    {
      "epoch": 9.11,
      "grad_norm": 95328.046875,
      "learning_rate": 5.6145510835913316e-05,
      "loss": 16.1266,
      "step": 9099
    },
    {
      "epoch": 9.11,
      "grad_norm": 2433.012939453125,
      "learning_rate": 5.6140350877192984e-05,
      "loss": 16.9262,
      "step": 9100
    },
    {
      "epoch": 9.11,
      "grad_norm": 9683.951171875,
      "learning_rate": 5.613519091847266e-05,
      "loss": 13.0601,
      "step": 9101
    },
    {
      "epoch": 9.11,
      "grad_norm": 1382.7294921875,
      "learning_rate": 5.613003095975232e-05,
      "loss": 12.0987,
      "step": 9102
    },
    {
      "epoch": 9.11,
      "grad_norm": 2666.540771484375,
      "learning_rate": 5.6124871001032e-05,
      "loss": 17.2155,
      "step": 9103
    },
    {
      "epoch": 9.11,
      "grad_norm": 9986.330078125,
      "learning_rate": 5.611971104231166e-05,
      "loss": 11.4979,
      "step": 9104
    },
    {
      "epoch": 9.11,
      "grad_norm": 2074.410888671875,
      "learning_rate": 5.6114551083591336e-05,
      "loss": 14.0611,
      "step": 9105
    },
    {
      "epoch": 9.12,
      "grad_norm": 1478.9957275390625,
      "learning_rate": 5.6109391124871004e-05,
      "loss": 18.6689,
      "step": 9106
    },
    {
      "epoch": 9.12,
      "grad_norm": 23792.00390625,
      "learning_rate": 5.610423116615068e-05,
      "loss": 16.056,
      "step": 9107
    },
    {
      "epoch": 9.12,
      "grad_norm": 13856.2451171875,
      "learning_rate": 5.6099071207430346e-05,
      "loss": 17.651,
      "step": 9108
    },
    {
      "epoch": 9.12,
      "grad_norm": 4435.515625,
      "learning_rate": 5.609391124871002e-05,
      "loss": 14.4539,
      "step": 9109
    },
    {
      "epoch": 9.12,
      "grad_norm": 49522.22265625,
      "learning_rate": 5.608875128998968e-05,
      "loss": 16.7915,
      "step": 9110
    },
    {
      "epoch": 9.12,
      "grad_norm": 7385.71533203125,
      "learning_rate": 5.608359133126935e-05,
      "loss": 12.8174,
      "step": 9111
    },
    {
      "epoch": 9.12,
      "grad_norm": 713.3353881835938,
      "learning_rate": 5.6078431372549024e-05,
      "loss": 11.9973,
      "step": 9112
    },
    {
      "epoch": 9.12,
      "grad_norm": 43533.82421875,
      "learning_rate": 5.6073271413828685e-05,
      "loss": 13.1226,
      "step": 9113
    },
    {
      "epoch": 9.12,
      "grad_norm": 2275.047607421875,
      "learning_rate": 5.6068111455108366e-05,
      "loss": 11.8219,
      "step": 9114
    },
    {
      "epoch": 9.12,
      "grad_norm": 1741.8671875,
      "learning_rate": 5.606295149638803e-05,
      "loss": 13.4284,
      "step": 9115
    },
    {
      "epoch": 9.13,
      "grad_norm": 45807.14453125,
      "learning_rate": 5.60577915376677e-05,
      "loss": 18.2909,
      "step": 9116
    },
    {
      "epoch": 9.13,
      "grad_norm": 7813.65478515625,
      "learning_rate": 5.605263157894737e-05,
      "loss": 14.8116,
      "step": 9117
    },
    {
      "epoch": 9.13,
      "grad_norm": 2572.42578125,
      "learning_rate": 5.6047471620227044e-05,
      "loss": 15.8868,
      "step": 9118
    },
    {
      "epoch": 9.13,
      "grad_norm": 2839.23046875,
      "learning_rate": 5.6042311661506705e-05,
      "loss": 15.6984,
      "step": 9119
    },
    {
      "epoch": 9.13,
      "grad_norm": 1836.12646484375,
      "learning_rate": 5.6037151702786386e-05,
      "loss": 13.798,
      "step": 9120
    },
    {
      "epoch": 9.13,
      "grad_norm": 43195.2578125,
      "learning_rate": 5.603199174406605e-05,
      "loss": 16.6928,
      "step": 9121
    },
    {
      "epoch": 9.13,
      "grad_norm": 10434.3515625,
      "learning_rate": 5.602683178534572e-05,
      "loss": 14.6954,
      "step": 9122
    },
    {
      "epoch": 9.13,
      "grad_norm": 1773.5001220703125,
      "learning_rate": 5.602167182662539e-05,
      "loss": 12.0388,
      "step": 9123
    },
    {
      "epoch": 9.13,
      "grad_norm": 7552.283203125,
      "learning_rate": 5.6016511867905064e-05,
      "loss": 14.8382,
      "step": 9124
    },
    {
      "epoch": 9.13,
      "grad_norm": 25280.83203125,
      "learning_rate": 5.601135190918473e-05,
      "loss": 14.3128,
      "step": 9125
    },
    {
      "epoch": 9.14,
      "grad_norm": 3443.6630859375,
      "learning_rate": 5.6006191950464406e-05,
      "loss": 13.5858,
      "step": 9126
    },
    {
      "epoch": 9.14,
      "grad_norm": 257.9892272949219,
      "learning_rate": 5.600103199174407e-05,
      "loss": 14.8651,
      "step": 9127
    },
    {
      "epoch": 9.14,
      "grad_norm": 3561.058349609375,
      "learning_rate": 5.5995872033023735e-05,
      "loss": 12.4017,
      "step": 9128
    },
    {
      "epoch": 9.14,
      "grad_norm": 7394.36279296875,
      "learning_rate": 5.599071207430341e-05,
      "loss": 14.2094,
      "step": 9129
    },
    {
      "epoch": 9.14,
      "grad_norm": 732.8795166015625,
      "learning_rate": 5.598555211558307e-05,
      "loss": 13.425,
      "step": 9130
    },
    {
      "epoch": 9.14,
      "grad_norm": 1596.380859375,
      "learning_rate": 5.598039215686275e-05,
      "loss": 12.8733,
      "step": 9131
    },
    {
      "epoch": 9.14,
      "grad_norm": 5579.59228515625,
      "learning_rate": 5.597523219814241e-05,
      "loss": 15.323,
      "step": 9132
    },
    {
      "epoch": 9.14,
      "grad_norm": 7774.14306640625,
      "learning_rate": 5.597007223942209e-05,
      "loss": 15.2304,
      "step": 9133
    },
    {
      "epoch": 9.14,
      "grad_norm": 19392.751953125,
      "learning_rate": 5.5964912280701755e-05,
      "loss": 18.6009,
      "step": 9134
    },
    {
      "epoch": 9.14,
      "grad_norm": 580.3104248046875,
      "learning_rate": 5.595975232198143e-05,
      "loss": 13.0035,
      "step": 9135
    },
    {
      "epoch": 9.15,
      "grad_norm": 5480.4716796875,
      "learning_rate": 5.595459236326109e-05,
      "loss": 13.1448,
      "step": 9136
    },
    {
      "epoch": 9.15,
      "grad_norm": 567.1429443359375,
      "learning_rate": 5.594943240454077e-05,
      "loss": 11.7765,
      "step": 9137
    },
    {
      "epoch": 9.15,
      "grad_norm": 919.824462890625,
      "learning_rate": 5.594427244582043e-05,
      "loss": 12.5469,
      "step": 9138
    },
    {
      "epoch": 9.15,
      "grad_norm": 12496.89453125,
      "learning_rate": 5.593911248710011e-05,
      "loss": 14.9491,
      "step": 9139
    },
    {
      "epoch": 9.15,
      "grad_norm": 5003.76611328125,
      "learning_rate": 5.5933952528379775e-05,
      "loss": 14.4476,
      "step": 9140
    },
    {
      "epoch": 9.15,
      "grad_norm": 4224.4033203125,
      "learning_rate": 5.592879256965945e-05,
      "loss": 14.831,
      "step": 9141
    },
    {
      "epoch": 9.15,
      "grad_norm": 1826.072998046875,
      "learning_rate": 5.592363261093912e-05,
      "loss": 12.8561,
      "step": 9142
    },
    {
      "epoch": 9.15,
      "grad_norm": 2101.14599609375,
      "learning_rate": 5.591847265221879e-05,
      "loss": 12.5473,
      "step": 9143
    },
    {
      "epoch": 9.15,
      "grad_norm": 10882.314453125,
      "learning_rate": 5.591331269349845e-05,
      "loss": 13.7794,
      "step": 9144
    },
    {
      "epoch": 9.15,
      "grad_norm": 1349.306640625,
      "learning_rate": 5.590815273477813e-05,
      "loss": 16.0024,
      "step": 9145
    },
    {
      "epoch": 9.16,
      "grad_norm": 1225.1103515625,
      "learning_rate": 5.5902992776057795e-05,
      "loss": 11.3193,
      "step": 9146
    },
    {
      "epoch": 9.16,
      "grad_norm": 12994.8349609375,
      "learning_rate": 5.5897832817337456e-05,
      "loss": 14.5956,
      "step": 9147
    },
    {
      "epoch": 9.16,
      "grad_norm": 6842.818359375,
      "learning_rate": 5.589267285861714e-05,
      "loss": 14.3159,
      "step": 9148
    },
    {
      "epoch": 9.16,
      "grad_norm": 4322.7900390625,
      "learning_rate": 5.58875128998968e-05,
      "loss": 16.0288,
      "step": 9149
    },
    {
      "epoch": 9.16,
      "grad_norm": 3496.917236328125,
      "learning_rate": 5.588235294117647e-05,
      "loss": 16.594,
      "step": 9150
    },
    {
      "epoch": 9.16,
      "grad_norm": 1380.030517578125,
      "learning_rate": 5.587719298245614e-05,
      "loss": 17.1276,
      "step": 9151
    },
    {
      "epoch": 9.16,
      "grad_norm": 1148.60302734375,
      "learning_rate": 5.5872033023735815e-05,
      "loss": 12.9751,
      "step": 9152
    },
    {
      "epoch": 9.16,
      "grad_norm": 4001.658935546875,
      "learning_rate": 5.586687306501548e-05,
      "loss": 12.0947,
      "step": 9153
    },
    {
      "epoch": 9.16,
      "grad_norm": 5648.55078125,
      "learning_rate": 5.586171310629516e-05,
      "loss": 16.1934,
      "step": 9154
    },
    {
      "epoch": 9.16,
      "grad_norm": 2802.25341796875,
      "learning_rate": 5.585655314757482e-05,
      "loss": 16.0639,
      "step": 9155
    },
    {
      "epoch": 9.17,
      "grad_norm": 26296.75,
      "learning_rate": 5.585139318885449e-05,
      "loss": 17.2985,
      "step": 9156
    },
    {
      "epoch": 9.17,
      "grad_norm": 9162.83203125,
      "learning_rate": 5.584623323013416e-05,
      "loss": 16.9166,
      "step": 9157
    },
    {
      "epoch": 9.17,
      "grad_norm": 7466.197265625,
      "learning_rate": 5.5841073271413835e-05,
      "loss": 14.3258,
      "step": 9158
    },
    {
      "epoch": 9.17,
      "grad_norm": 15086.228515625,
      "learning_rate": 5.58359133126935e-05,
      "loss": 14.3224,
      "step": 9159
    },
    {
      "epoch": 9.17,
      "grad_norm": 41564.3515625,
      "learning_rate": 5.583075335397318e-05,
      "loss": 17.9736,
      "step": 9160
    },
    {
      "epoch": 9.17,
      "grad_norm": 20762.154296875,
      "learning_rate": 5.582559339525284e-05,
      "loss": 14.887,
      "step": 9161
    },
    {
      "epoch": 9.17,
      "grad_norm": 7422.04052734375,
      "learning_rate": 5.582043343653251e-05,
      "loss": 14.3261,
      "step": 9162
    },
    {
      "epoch": 9.17,
      "grad_norm": 4791.1708984375,
      "learning_rate": 5.581527347781218e-05,
      "loss": 11.5883,
      "step": 9163
    },
    {
      "epoch": 9.17,
      "grad_norm": 3357.27099609375,
      "learning_rate": 5.581011351909184e-05,
      "loss": 13.4452,
      "step": 9164
    },
    {
      "epoch": 9.17,
      "grad_norm": 10088.5205078125,
      "learning_rate": 5.580495356037152e-05,
      "loss": 13.5852,
      "step": 9165
    },
    {
      "epoch": 9.18,
      "grad_norm": 2793.0908203125,
      "learning_rate": 5.5799793601651183e-05,
      "loss": 14.0202,
      "step": 9166
    },
    {
      "epoch": 9.18,
      "grad_norm": 1324.124267578125,
      "learning_rate": 5.579463364293086e-05,
      "loss": 13.6892,
      "step": 9167
    },
    {
      "epoch": 9.18,
      "grad_norm": 9486.1923828125,
      "learning_rate": 5.5789473684210526e-05,
      "loss": 19.5636,
      "step": 9168
    },
    {
      "epoch": 9.18,
      "grad_norm": 15371.94140625,
      "learning_rate": 5.57843137254902e-05,
      "loss": 14.6018,
      "step": 9169
    },
    {
      "epoch": 9.18,
      "grad_norm": 2319.389892578125,
      "learning_rate": 5.577915376676987e-05,
      "loss": 12.056,
      "step": 9170
    },
    {
      "epoch": 9.18,
      "grad_norm": 3472.950927734375,
      "learning_rate": 5.577399380804954e-05,
      "loss": 13.1453,
      "step": 9171
    },
    {
      "epoch": 9.18,
      "grad_norm": 3892.2294921875,
      "learning_rate": 5.5768833849329203e-05,
      "loss": 19.6495,
      "step": 9172
    },
    {
      "epoch": 9.18,
      "grad_norm": 6400.9619140625,
      "learning_rate": 5.576367389060888e-05,
      "loss": 14.6935,
      "step": 9173
    },
    {
      "epoch": 9.18,
      "grad_norm": 46341.3828125,
      "learning_rate": 5.5758513931888546e-05,
      "loss": 15.9002,
      "step": 9174
    },
    {
      "epoch": 9.18,
      "grad_norm": 4529.32861328125,
      "learning_rate": 5.575335397316822e-05,
      "loss": 17.271,
      "step": 9175
    },
    {
      "epoch": 9.19,
      "grad_norm": 2138.040283203125,
      "learning_rate": 5.574819401444789e-05,
      "loss": 13.4141,
      "step": 9176
    },
    {
      "epoch": 9.19,
      "grad_norm": 23442.732421875,
      "learning_rate": 5.574303405572756e-05,
      "loss": 15.5143,
      "step": 9177
    },
    {
      "epoch": 9.19,
      "grad_norm": 2722.607666015625,
      "learning_rate": 5.5737874097007223e-05,
      "loss": 13.5191,
      "step": 9178
    },
    {
      "epoch": 9.19,
      "grad_norm": 3864.1962890625,
      "learning_rate": 5.57327141382869e-05,
      "loss": 11.9269,
      "step": 9179
    },
    {
      "epoch": 9.19,
      "grad_norm": 11783.3876953125,
      "learning_rate": 5.5727554179566566e-05,
      "loss": 12.9262,
      "step": 9180
    },
    {
      "epoch": 9.19,
      "grad_norm": 3422.669921875,
      "learning_rate": 5.572239422084624e-05,
      "loss": 14.1697,
      "step": 9181
    },
    {
      "epoch": 9.19,
      "grad_norm": 18742.708984375,
      "learning_rate": 5.571723426212591e-05,
      "loss": 11.2091,
      "step": 9182
    },
    {
      "epoch": 9.19,
      "grad_norm": 23290.412109375,
      "learning_rate": 5.571207430340557e-05,
      "loss": 14.0452,
      "step": 9183
    },
    {
      "epoch": 9.19,
      "grad_norm": 1685.6351318359375,
      "learning_rate": 5.5706914344685243e-05,
      "loss": 14.2827,
      "step": 9184
    },
    {
      "epoch": 9.19,
      "grad_norm": 4298.060546875,
      "learning_rate": 5.570175438596491e-05,
      "loss": 14.2752,
      "step": 9185
    },
    {
      "epoch": 9.2,
      "grad_norm": 6312.658203125,
      "learning_rate": 5.5696594427244586e-05,
      "loss": 16.9793,
      "step": 9186
    },
    {
      "epoch": 9.2,
      "grad_norm": 9328.4423828125,
      "learning_rate": 5.5691434468524253e-05,
      "loss": 13.847,
      "step": 9187
    },
    {
      "epoch": 9.2,
      "grad_norm": 1015.3974609375,
      "learning_rate": 5.568627450980393e-05,
      "loss": 14.8041,
      "step": 9188
    },
    {
      "epoch": 9.2,
      "grad_norm": 3081.178466796875,
      "learning_rate": 5.568111455108359e-05,
      "loss": 16.2473,
      "step": 9189
    },
    {
      "epoch": 9.2,
      "grad_norm": 2902.103759765625,
      "learning_rate": 5.5675954592363263e-05,
      "loss": 14.6649,
      "step": 9190
    },
    {
      "epoch": 9.2,
      "grad_norm": 9802.0673828125,
      "learning_rate": 5.567079463364293e-05,
      "loss": 12.2201,
      "step": 9191
    },
    {
      "epoch": 9.2,
      "grad_norm": 25834.869140625,
      "learning_rate": 5.5665634674922606e-05,
      "loss": 11.8867,
      "step": 9192
    },
    {
      "epoch": 9.2,
      "grad_norm": 59371.0546875,
      "learning_rate": 5.5660474716202273e-05,
      "loss": 11.4435,
      "step": 9193
    },
    {
      "epoch": 9.2,
      "grad_norm": 17233.296875,
      "learning_rate": 5.565531475748195e-05,
      "loss": 15.387,
      "step": 9194
    },
    {
      "epoch": 9.2,
      "grad_norm": 21428.7890625,
      "learning_rate": 5.565015479876161e-05,
      "loss": 12.9731,
      "step": 9195
    },
    {
      "epoch": 9.21,
      "grad_norm": 7593.14013671875,
      "learning_rate": 5.5644994840041283e-05,
      "loss": 15.4597,
      "step": 9196
    },
    {
      "epoch": 9.21,
      "grad_norm": 2780.549560546875,
      "learning_rate": 5.563983488132095e-05,
      "loss": 15.2148,
      "step": 9197
    },
    {
      "epoch": 9.21,
      "grad_norm": 13372.9716796875,
      "learning_rate": 5.5634674922600626e-05,
      "loss": 16.5069,
      "step": 9198
    },
    {
      "epoch": 9.21,
      "grad_norm": 4092.892822265625,
      "learning_rate": 5.5629514963880293e-05,
      "loss": 12.8305,
      "step": 9199
    },
    {
      "epoch": 9.21,
      "grad_norm": 9692.7333984375,
      "learning_rate": 5.5624355005159954e-05,
      "loss": 20.3808,
      "step": 9200
    },
    {
      "epoch": 9.21,
      "grad_norm": 8713.40234375,
      "learning_rate": 5.561919504643963e-05,
      "loss": 15.5141,
      "step": 9201
    },
    {
      "epoch": 9.21,
      "grad_norm": 3226.810302734375,
      "learning_rate": 5.56140350877193e-05,
      "loss": 14.0914,
      "step": 9202
    },
    {
      "epoch": 9.21,
      "grad_norm": 11180.5634765625,
      "learning_rate": 5.560887512899897e-05,
      "loss": 17.537,
      "step": 9203
    },
    {
      "epoch": 9.21,
      "grad_norm": 4029.981689453125,
      "learning_rate": 5.560371517027864e-05,
      "loss": 12.622,
      "step": 9204
    },
    {
      "epoch": 9.21,
      "grad_norm": 2911.4560546875,
      "learning_rate": 5.5598555211558313e-05,
      "loss": 14.0748,
      "step": 9205
    },
    {
      "epoch": 9.22,
      "grad_norm": 20107.84765625,
      "learning_rate": 5.5593395252837974e-05,
      "loss": 19.3633,
      "step": 9206
    },
    {
      "epoch": 9.22,
      "grad_norm": 2612.52294921875,
      "learning_rate": 5.558823529411765e-05,
      "loss": 15.5521,
      "step": 9207
    },
    {
      "epoch": 9.22,
      "grad_norm": 3229.87255859375,
      "learning_rate": 5.558307533539732e-05,
      "loss": 14.5747,
      "step": 9208
    },
    {
      "epoch": 9.22,
      "grad_norm": 16924.634765625,
      "learning_rate": 5.557791537667699e-05,
      "loss": 15.7293,
      "step": 9209
    },
    {
      "epoch": 9.22,
      "grad_norm": 3638.75537109375,
      "learning_rate": 5.557275541795666e-05,
      "loss": 14.6193,
      "step": 9210
    },
    {
      "epoch": 9.22,
      "grad_norm": 10808.30859375,
      "learning_rate": 5.5567595459236333e-05,
      "loss": 15.3028,
      "step": 9211
    },
    {
      "epoch": 9.22,
      "grad_norm": 1887.5731201171875,
      "learning_rate": 5.5562435500515994e-05,
      "loss": 20.6786,
      "step": 9212
    },
    {
      "epoch": 9.22,
      "grad_norm": 27228.802734375,
      "learning_rate": 5.5557275541795676e-05,
      "loss": 17.8444,
      "step": 9213
    },
    {
      "epoch": 9.22,
      "grad_norm": 7449.17333984375,
      "learning_rate": 5.555211558307534e-05,
      "loss": 15.3703,
      "step": 9214
    },
    {
      "epoch": 9.22,
      "grad_norm": 2670.524658203125,
      "learning_rate": 5.554695562435501e-05,
      "loss": 14.8646,
      "step": 9215
    },
    {
      "epoch": 9.23,
      "grad_norm": 4096.7939453125,
      "learning_rate": 5.554179566563468e-05,
      "loss": 15.8857,
      "step": 9216
    },
    {
      "epoch": 9.23,
      "grad_norm": 2890.927734375,
      "learning_rate": 5.5536635706914353e-05,
      "loss": 15.3353,
      "step": 9217
    },
    {
      "epoch": 9.23,
      "grad_norm": 4818.1767578125,
      "learning_rate": 5.5531475748194014e-05,
      "loss": 12.7542,
      "step": 9218
    },
    {
      "epoch": 9.23,
      "grad_norm": 6606.42041015625,
      "learning_rate": 5.552631578947368e-05,
      "loss": 15.8941,
      "step": 9219
    },
    {
      "epoch": 9.23,
      "grad_norm": 3502.23681640625,
      "learning_rate": 5.552115583075336e-05,
      "loss": 13.7005,
      "step": 9220
    },
    {
      "epoch": 9.23,
      "grad_norm": 63766.96484375,
      "learning_rate": 5.5515995872033024e-05,
      "loss": 16.6048,
      "step": 9221
    },
    {
      "epoch": 9.23,
      "grad_norm": 3247.4736328125,
      "learning_rate": 5.55108359133127e-05,
      "loss": 12.9374,
      "step": 9222
    },
    {
      "epoch": 9.23,
      "grad_norm": 20712.275390625,
      "learning_rate": 5.550567595459236e-05,
      "loss": 16.7012,
      "step": 9223
    },
    {
      "epoch": 9.23,
      "grad_norm": 1427.61083984375,
      "learning_rate": 5.5500515995872034e-05,
      "loss": 14.8962,
      "step": 9224
    },
    {
      "epoch": 9.23,
      "grad_norm": 18281.86328125,
      "learning_rate": 5.54953560371517e-05,
      "loss": 13.9832,
      "step": 9225
    },
    {
      "epoch": 9.24,
      "grad_norm": 4332.26904296875,
      "learning_rate": 5.549019607843138e-05,
      "loss": 13.6871,
      "step": 9226
    },
    {
      "epoch": 9.24,
      "grad_norm": 6027.27880859375,
      "learning_rate": 5.5485036119711044e-05,
      "loss": 13.5154,
      "step": 9227
    },
    {
      "epoch": 9.24,
      "grad_norm": 3657.60693359375,
      "learning_rate": 5.547987616099072e-05,
      "loss": 14.4694,
      "step": 9228
    },
    {
      "epoch": 9.24,
      "grad_norm": 6574.81298828125,
      "learning_rate": 5.547471620227038e-05,
      "loss": 16.5292,
      "step": 9229
    },
    {
      "epoch": 9.24,
      "grad_norm": 2915.759521484375,
      "learning_rate": 5.546955624355006e-05,
      "loss": 16.3651,
      "step": 9230
    },
    {
      "epoch": 9.24,
      "grad_norm": 17549.90234375,
      "learning_rate": 5.546439628482972e-05,
      "loss": 17.2916,
      "step": 9231
    },
    {
      "epoch": 9.24,
      "grad_norm": 6669.84375,
      "learning_rate": 5.54592363261094e-05,
      "loss": 12.4609,
      "step": 9232
    },
    {
      "epoch": 9.24,
      "grad_norm": 15820.9072265625,
      "learning_rate": 5.5454076367389064e-05,
      "loss": 14.6686,
      "step": 9233
    },
    {
      "epoch": 9.24,
      "grad_norm": 15236.068359375,
      "learning_rate": 5.544891640866874e-05,
      "loss": 20.4067,
      "step": 9234
    },
    {
      "epoch": 9.24,
      "grad_norm": 11811.283203125,
      "learning_rate": 5.54437564499484e-05,
      "loss": 14.2998,
      "step": 9235
    },
    {
      "epoch": 9.25,
      "grad_norm": 14541.828125,
      "learning_rate": 5.543859649122807e-05,
      "loss": 13.8649,
      "step": 9236
    },
    {
      "epoch": 9.25,
      "grad_norm": 24518.439453125,
      "learning_rate": 5.543343653250774e-05,
      "loss": 15.1263,
      "step": 9237
    },
    {
      "epoch": 9.25,
      "grad_norm": 18286.9375,
      "learning_rate": 5.542827657378741e-05,
      "loss": 13.9909,
      "step": 9238
    },
    {
      "epoch": 9.25,
      "grad_norm": 49127.1484375,
      "learning_rate": 5.5423116615067084e-05,
      "loss": 17.9133,
      "step": 9239
    },
    {
      "epoch": 9.25,
      "grad_norm": 31506.107421875,
      "learning_rate": 5.5417956656346745e-05,
      "loss": 16.5682,
      "step": 9240
    },
    {
      "epoch": 9.25,
      "grad_norm": 6226.64306640625,
      "learning_rate": 5.541279669762643e-05,
      "loss": 13.9553,
      "step": 9241
    },
    {
      "epoch": 9.25,
      "grad_norm": 37788.71484375,
      "learning_rate": 5.540763673890609e-05,
      "loss": 14.0943,
      "step": 9242
    },
    {
      "epoch": 9.25,
      "grad_norm": 4308.74462890625,
      "learning_rate": 5.540247678018576e-05,
      "loss": 12.9927,
      "step": 9243
    },
    {
      "epoch": 9.25,
      "grad_norm": 3761.8359375,
      "learning_rate": 5.539731682146543e-05,
      "loss": 19.1343,
      "step": 9244
    },
    {
      "epoch": 9.25,
      "grad_norm": 2339.003662109375,
      "learning_rate": 5.5392156862745104e-05,
      "loss": 13.4842,
      "step": 9245
    },
    {
      "epoch": 9.26,
      "grad_norm": 14660.220703125,
      "learning_rate": 5.5386996904024765e-05,
      "loss": 14.3073,
      "step": 9246
    },
    {
      "epoch": 9.26,
      "grad_norm": 8083.36474609375,
      "learning_rate": 5.538183694530445e-05,
      "loss": 14.7916,
      "step": 9247
    },
    {
      "epoch": 9.26,
      "grad_norm": 1623.28466796875,
      "learning_rate": 5.537667698658411e-05,
      "loss": 14.768,
      "step": 9248
    },
    {
      "epoch": 9.26,
      "grad_norm": 10514.7763671875,
      "learning_rate": 5.537151702786378e-05,
      "loss": 15.9194,
      "step": 9249
    },
    {
      "epoch": 9.26,
      "grad_norm": 5417.27001953125,
      "learning_rate": 5.536635706914345e-05,
      "loss": 25.9069,
      "step": 9250
    },
    {
      "epoch": 9.26,
      "grad_norm": 2733.889892578125,
      "learning_rate": 5.5361197110423124e-05,
      "loss": 12.5135,
      "step": 9251
    },
    {
      "epoch": 9.26,
      "grad_norm": 1823.6505126953125,
      "learning_rate": 5.5356037151702785e-05,
      "loss": 13.2495,
      "step": 9252
    },
    {
      "epoch": 9.26,
      "grad_norm": 5578.224609375,
      "learning_rate": 5.535087719298247e-05,
      "loss": 15.729,
      "step": 9253
    },
    {
      "epoch": 9.26,
      "grad_norm": 74166.6875,
      "learning_rate": 5.534571723426213e-05,
      "loss": 18.7452,
      "step": 9254
    },
    {
      "epoch": 9.26,
      "grad_norm": 21619.267578125,
      "learning_rate": 5.5340557275541795e-05,
      "loss": 14.7979,
      "step": 9255
    },
    {
      "epoch": 9.27,
      "grad_norm": 6400.8876953125,
      "learning_rate": 5.533539731682147e-05,
      "loss": 17.765,
      "step": 9256
    },
    {
      "epoch": 9.27,
      "grad_norm": 2204.0380859375,
      "learning_rate": 5.533023735810113e-05,
      "loss": 18.5544,
      "step": 9257
    },
    {
      "epoch": 9.27,
      "grad_norm": 33938.79296875,
      "learning_rate": 5.532507739938081e-05,
      "loss": 18.0579,
      "step": 9258
    },
    {
      "epoch": 9.27,
      "grad_norm": 3091.18017578125,
      "learning_rate": 5.531991744066047e-05,
      "loss": 15.601,
      "step": 9259
    },
    {
      "epoch": 9.27,
      "grad_norm": 41490.78125,
      "learning_rate": 5.531475748194015e-05,
      "loss": 16.5219,
      "step": 9260
    },
    {
      "epoch": 9.27,
      "grad_norm": 2556.2900390625,
      "learning_rate": 5.5309597523219815e-05,
      "loss": 12.9283,
      "step": 9261
    },
    {
      "epoch": 9.27,
      "grad_norm": 10793.494140625,
      "learning_rate": 5.530443756449949e-05,
      "loss": 14.9284,
      "step": 9262
    },
    {
      "epoch": 9.27,
      "grad_norm": 4365.87646484375,
      "learning_rate": 5.529927760577915e-05,
      "loss": 16.4398,
      "step": 9263
    },
    {
      "epoch": 9.27,
      "grad_norm": 9322.771484375,
      "learning_rate": 5.529411764705883e-05,
      "loss": 25.2812,
      "step": 9264
    },
    {
      "epoch": 9.27,
      "grad_norm": 3656.763671875,
      "learning_rate": 5.528895768833849e-05,
      "loss": 12.1167,
      "step": 9265
    },
    {
      "epoch": 9.28,
      "grad_norm": 7928.732421875,
      "learning_rate": 5.528379772961817e-05,
      "loss": 15.4032,
      "step": 9266
    },
    {
      "epoch": 9.28,
      "grad_norm": 2262.879150390625,
      "learning_rate": 5.5278637770897835e-05,
      "loss": 13.9297,
      "step": 9267
    },
    {
      "epoch": 9.28,
      "grad_norm": 1855.6544189453125,
      "learning_rate": 5.527347781217751e-05,
      "loss": 14.5888,
      "step": 9268
    },
    {
      "epoch": 9.28,
      "grad_norm": 4633.830078125,
      "learning_rate": 5.526831785345718e-05,
      "loss": 14.0102,
      "step": 9269
    },
    {
      "epoch": 9.28,
      "grad_norm": 7972.42822265625,
      "learning_rate": 5.526315789473685e-05,
      "loss": 13.835,
      "step": 9270
    },
    {
      "epoch": 9.28,
      "grad_norm": 2180.382080078125,
      "learning_rate": 5.525799793601651e-05,
      "loss": 15.4915,
      "step": 9271
    },
    {
      "epoch": 9.28,
      "grad_norm": 8516.4716796875,
      "learning_rate": 5.525283797729618e-05,
      "loss": 16.4768,
      "step": 9272
    },
    {
      "epoch": 9.28,
      "grad_norm": 3190.05712890625,
      "learning_rate": 5.5247678018575855e-05,
      "loss": 22.6134,
      "step": 9273
    },
    {
      "epoch": 9.28,
      "grad_norm": 13396.0166015625,
      "learning_rate": 5.5242518059855516e-05,
      "loss": 21.9054,
      "step": 9274
    },
    {
      "epoch": 9.28,
      "grad_norm": 11023.37109375,
      "learning_rate": 5.52373581011352e-05,
      "loss": 14.516,
      "step": 9275
    },
    {
      "epoch": 9.29,
      "grad_norm": 5553.99072265625,
      "learning_rate": 5.523219814241486e-05,
      "loss": 14.2468,
      "step": 9276
    },
    {
      "epoch": 9.29,
      "grad_norm": 10435.28125,
      "learning_rate": 5.522703818369453e-05,
      "loss": 14.1589,
      "step": 9277
    },
    {
      "epoch": 9.29,
      "grad_norm": 41876.953125,
      "learning_rate": 5.52218782249742e-05,
      "loss": 18.2193,
      "step": 9278
    },
    {
      "epoch": 9.29,
      "grad_norm": 1377.0478515625,
      "learning_rate": 5.5216718266253875e-05,
      "loss": 11.9669,
      "step": 9279
    },
    {
      "epoch": 9.29,
      "grad_norm": 7173.6865234375,
      "learning_rate": 5.5211558307533536e-05,
      "loss": 13.5763,
      "step": 9280
    },
    {
      "epoch": 9.29,
      "grad_norm": 65400.30078125,
      "learning_rate": 5.520639834881322e-05,
      "loss": 15.3449,
      "step": 9281
    },
    {
      "epoch": 9.29,
      "grad_norm": 1180.309814453125,
      "learning_rate": 5.520123839009288e-05,
      "loss": 11.9902,
      "step": 9282
    },
    {
      "epoch": 9.29,
      "grad_norm": 36314.75,
      "learning_rate": 5.519607843137255e-05,
      "loss": 16.9553,
      "step": 9283
    },
    {
      "epoch": 9.29,
      "grad_norm": 8387.1142578125,
      "learning_rate": 5.519091847265222e-05,
      "loss": 20.6215,
      "step": 9284
    },
    {
      "epoch": 9.29,
      "grad_norm": 17048.94140625,
      "learning_rate": 5.5185758513931895e-05,
      "loss": 14.8772,
      "step": 9285
    },
    {
      "epoch": 9.3,
      "grad_norm": 22503.083984375,
      "learning_rate": 5.518059855521156e-05,
      "loss": 16.4094,
      "step": 9286
    },
    {
      "epoch": 9.3,
      "grad_norm": 82282.3984375,
      "learning_rate": 5.517543859649124e-05,
      "loss": 16.8799,
      "step": 9287
    },
    {
      "epoch": 9.3,
      "grad_norm": 14387.3974609375,
      "learning_rate": 5.51702786377709e-05,
      "loss": 15.5176,
      "step": 9288
    },
    {
      "epoch": 9.3,
      "grad_norm": 112444.046875,
      "learning_rate": 5.5165118679050566e-05,
      "loss": 17.0924,
      "step": 9289
    },
    {
      "epoch": 9.3,
      "grad_norm": 23428.642578125,
      "learning_rate": 5.515995872033024e-05,
      "loss": 13.8524,
      "step": 9290
    },
    {
      "epoch": 9.3,
      "grad_norm": 61606.18359375,
      "learning_rate": 5.51547987616099e-05,
      "loss": 14.2158,
      "step": 9291
    },
    {
      "epoch": 9.3,
      "grad_norm": 6995.794921875,
      "learning_rate": 5.514963880288958e-05,
      "loss": 20.6568,
      "step": 9292
    },
    {
      "epoch": 9.3,
      "grad_norm": 15719.7705078125,
      "learning_rate": 5.5144478844169244e-05,
      "loss": 14.059,
      "step": 9293
    },
    {
      "epoch": 9.3,
      "grad_norm": 1428.857421875,
      "learning_rate": 5.513931888544892e-05,
      "loss": 19.2357,
      "step": 9294
    },
    {
      "epoch": 9.3,
      "grad_norm": 1105.3154296875,
      "learning_rate": 5.5134158926728586e-05,
      "loss": 13.9728,
      "step": 9295
    },
    {
      "epoch": 9.31,
      "grad_norm": 13735.2822265625,
      "learning_rate": 5.512899896800826e-05,
      "loss": 15.1787,
      "step": 9296
    },
    {
      "epoch": 9.31,
      "grad_norm": 35311.7421875,
      "learning_rate": 5.512383900928793e-05,
      "loss": 16.5718,
      "step": 9297
    },
    {
      "epoch": 9.31,
      "grad_norm": 2676.799560546875,
      "learning_rate": 5.51186790505676e-05,
      "loss": 16.7761,
      "step": 9298
    },
    {
      "epoch": 9.31,
      "grad_norm": 76420.8984375,
      "learning_rate": 5.5113519091847264e-05,
      "loss": 20.9591,
      "step": 9299
    },
    {
      "epoch": 9.31,
      "grad_norm": 1433.818603515625,
      "learning_rate": 5.510835913312694e-05,
      "loss": 15.5699,
      "step": 9300
    },
    {
      "epoch": 9.31,
      "grad_norm": 19406.2578125,
      "learning_rate": 5.5103199174406606e-05,
      "loss": 17.7033,
      "step": 9301
    },
    {
      "epoch": 9.31,
      "grad_norm": 5325.51611328125,
      "learning_rate": 5.509803921568628e-05,
      "loss": 14.5477,
      "step": 9302
    },
    {
      "epoch": 9.31,
      "grad_norm": 15169.0048828125,
      "learning_rate": 5.509287925696595e-05,
      "loss": 17.19,
      "step": 9303
    },
    {
      "epoch": 9.31,
      "grad_norm": 3910.309814453125,
      "learning_rate": 5.508771929824562e-05,
      "loss": 13.7479,
      "step": 9304
    },
    {
      "epoch": 9.31,
      "grad_norm": 12850.9619140625,
      "learning_rate": 5.5082559339525284e-05,
      "loss": 18.7108,
      "step": 9305
    },
    {
      "epoch": 9.32,
      "grad_norm": 8304.80078125,
      "learning_rate": 5.507739938080496e-05,
      "loss": 14.5671,
      "step": 9306
    },
    {
      "epoch": 9.32,
      "grad_norm": 12840.8408203125,
      "learning_rate": 5.5072239422084626e-05,
      "loss": 14.9606,
      "step": 9307
    },
    {
      "epoch": 9.32,
      "grad_norm": 3026.71435546875,
      "learning_rate": 5.506707946336429e-05,
      "loss": 16.2571,
      "step": 9308
    },
    {
      "epoch": 9.32,
      "grad_norm": 3295.992919921875,
      "learning_rate": 5.506191950464397e-05,
      "loss": 15.1654,
      "step": 9309
    },
    {
      "epoch": 9.32,
      "grad_norm": 1694.609130859375,
      "learning_rate": 5.505675954592363e-05,
      "loss": 17.5612,
      "step": 9310
    },
    {
      "epoch": 9.32,
      "grad_norm": 23831.1171875,
      "learning_rate": 5.5051599587203304e-05,
      "loss": 17.008,
      "step": 9311
    },
    {
      "epoch": 9.32,
      "grad_norm": 4085.561279296875,
      "learning_rate": 5.504643962848297e-05,
      "loss": 13.0833,
      "step": 9312
    },
    {
      "epoch": 9.32,
      "grad_norm": 4759.537109375,
      "learning_rate": 5.5041279669762646e-05,
      "loss": 12.2709,
      "step": 9313
    },
    {
      "epoch": 9.32,
      "grad_norm": 1776.274169921875,
      "learning_rate": 5.5036119711042314e-05,
      "loss": 16.1881,
      "step": 9314
    },
    {
      "epoch": 9.32,
      "grad_norm": 1621.9698486328125,
      "learning_rate": 5.503095975232199e-05,
      "loss": 13.7263,
      "step": 9315
    },
    {
      "epoch": 9.33,
      "grad_norm": 1125.4324951171875,
      "learning_rate": 5.502579979360165e-05,
      "loss": 13.94,
      "step": 9316
    },
    {
      "epoch": 9.33,
      "grad_norm": 3830.64794921875,
      "learning_rate": 5.5020639834881324e-05,
      "loss": 13.2514,
      "step": 9317
    },
    {
      "epoch": 9.33,
      "grad_norm": 14903.0859375,
      "learning_rate": 5.501547987616099e-05,
      "loss": 13.3514,
      "step": 9318
    },
    {
      "epoch": 9.33,
      "grad_norm": 4711.8251953125,
      "learning_rate": 5.5010319917440666e-05,
      "loss": 16.1414,
      "step": 9319
    },
    {
      "epoch": 9.33,
      "grad_norm": 3398.8818359375,
      "learning_rate": 5.5005159958720334e-05,
      "loss": 13.6552,
      "step": 9320
    },
    {
      "epoch": 9.33,
      "grad_norm": 19775.91796875,
      "learning_rate": 5.500000000000001e-05,
      "loss": 13.687,
      "step": 9321
    },
    {
      "epoch": 9.33,
      "grad_norm": 9174.265625,
      "learning_rate": 5.499484004127967e-05,
      "loss": 18.8338,
      "step": 9322
    },
    {
      "epoch": 9.33,
      "grad_norm": 6793.42529296875,
      "learning_rate": 5.4989680082559344e-05,
      "loss": 14.2839,
      "step": 9323
    },
    {
      "epoch": 9.33,
      "grad_norm": 6089.419921875,
      "learning_rate": 5.498452012383901e-05,
      "loss": 13.9079,
      "step": 9324
    },
    {
      "epoch": 9.33,
      "grad_norm": 39167.453125,
      "learning_rate": 5.497936016511867e-05,
      "loss": 15.7926,
      "step": 9325
    },
    {
      "epoch": 9.34,
      "grad_norm": 12924.1923828125,
      "learning_rate": 5.4974200206398354e-05,
      "loss": 17.5725,
      "step": 9326
    },
    {
      "epoch": 9.34,
      "grad_norm": 1802.53076171875,
      "learning_rate": 5.4969040247678015e-05,
      "loss": 11.6203,
      "step": 9327
    },
    {
      "epoch": 9.34,
      "grad_norm": 2299.982421875,
      "learning_rate": 5.496388028895769e-05,
      "loss": 12.2644,
      "step": 9328
    },
    {
      "epoch": 9.34,
      "grad_norm": 2275.6328125,
      "learning_rate": 5.495872033023736e-05,
      "loss": 13.2901,
      "step": 9329
    },
    {
      "epoch": 9.34,
      "grad_norm": 45008.75,
      "learning_rate": 5.495356037151703e-05,
      "loss": 13.9185,
      "step": 9330
    },
    {
      "epoch": 9.34,
      "grad_norm": 2130.4892578125,
      "learning_rate": 5.49484004127967e-05,
      "loss": 11.9234,
      "step": 9331
    },
    {
      "epoch": 9.34,
      "grad_norm": 2794.00244140625,
      "learning_rate": 5.4943240454076374e-05,
      "loss": 15.6713,
      "step": 9332
    },
    {
      "epoch": 9.34,
      "grad_norm": 10212.572265625,
      "learning_rate": 5.4938080495356035e-05,
      "loss": 18.8551,
      "step": 9333
    },
    {
      "epoch": 9.34,
      "grad_norm": 1808.9044189453125,
      "learning_rate": 5.493292053663571e-05,
      "loss": 21.9353,
      "step": 9334
    },
    {
      "epoch": 9.34,
      "grad_norm": 5211.85498046875,
      "learning_rate": 5.492776057791538e-05,
      "loss": 18.72,
      "step": 9335
    },
    {
      "epoch": 9.35,
      "grad_norm": 2582.5263671875,
      "learning_rate": 5.492260061919505e-05,
      "loss": 14.7419,
      "step": 9336
    },
    {
      "epoch": 9.35,
      "grad_norm": 11638.953125,
      "learning_rate": 5.491744066047472e-05,
      "loss": 16.1158,
      "step": 9337
    },
    {
      "epoch": 9.35,
      "grad_norm": 9760.2392578125,
      "learning_rate": 5.4912280701754394e-05,
      "loss": 16.9384,
      "step": 9338
    },
    {
      "epoch": 9.35,
      "grad_norm": 14568.96875,
      "learning_rate": 5.4907120743034055e-05,
      "loss": 13.8795,
      "step": 9339
    },
    {
      "epoch": 9.35,
      "grad_norm": 20060.5546875,
      "learning_rate": 5.490196078431373e-05,
      "loss": 15.5901,
      "step": 9340
    },
    {
      "epoch": 9.35,
      "grad_norm": 5647.60595703125,
      "learning_rate": 5.48968008255934e-05,
      "loss": 19.3007,
      "step": 9341
    },
    {
      "epoch": 9.35,
      "grad_norm": 13048.0390625,
      "learning_rate": 5.489164086687307e-05,
      "loss": 13.994,
      "step": 9342
    },
    {
      "epoch": 9.35,
      "grad_norm": 19182.48046875,
      "learning_rate": 5.488648090815274e-05,
      "loss": 13.6565,
      "step": 9343
    },
    {
      "epoch": 9.35,
      "grad_norm": 923.52197265625,
      "learning_rate": 5.48813209494324e-05,
      "loss": 14.1674,
      "step": 9344
    },
    {
      "epoch": 9.35,
      "grad_norm": 2967.34619140625,
      "learning_rate": 5.4876160990712075e-05,
      "loss": 12.2695,
      "step": 9345
    },
    {
      "epoch": 9.36,
      "grad_norm": 5672.03076171875,
      "learning_rate": 5.487100103199174e-05,
      "loss": 12.5225,
      "step": 9346
    },
    {
      "epoch": 9.36,
      "grad_norm": 3885.051513671875,
      "learning_rate": 5.486584107327142e-05,
      "loss": 14.5584,
      "step": 9347
    },
    {
      "epoch": 9.36,
      "grad_norm": 16896.35546875,
      "learning_rate": 5.4860681114551085e-05,
      "loss": 16.0346,
      "step": 9348
    },
    {
      "epoch": 9.36,
      "grad_norm": 6513.57275390625,
      "learning_rate": 5.485552115583076e-05,
      "loss": 14.2573,
      "step": 9349
    },
    {
      "epoch": 9.36,
      "grad_norm": 2618.00341796875,
      "learning_rate": 5.485036119711042e-05,
      "loss": 16.8849,
      "step": 9350
    },
    {
      "epoch": 9.36,
      "grad_norm": 1798.6239013671875,
      "learning_rate": 5.4845201238390095e-05,
      "loss": 15.5139,
      "step": 9351
    },
    {
      "epoch": 9.36,
      "grad_norm": 9850.7822265625,
      "learning_rate": 5.484004127966976e-05,
      "loss": 14.4042,
      "step": 9352
    },
    {
      "epoch": 9.36,
      "grad_norm": 4646.11181640625,
      "learning_rate": 5.483488132094944e-05,
      "loss": 18.2155,
      "step": 9353
    },
    {
      "epoch": 9.36,
      "grad_norm": 9827.7919921875,
      "learning_rate": 5.4829721362229105e-05,
      "loss": 12.7641,
      "step": 9354
    },
    {
      "epoch": 9.36,
      "grad_norm": 12589.7978515625,
      "learning_rate": 5.482456140350878e-05,
      "loss": 13.7787,
      "step": 9355
    },
    {
      "epoch": 9.37,
      "grad_norm": 14176.314453125,
      "learning_rate": 5.481940144478844e-05,
      "loss": 18.8674,
      "step": 9356
    },
    {
      "epoch": 9.37,
      "grad_norm": 37166.09375,
      "learning_rate": 5.4814241486068115e-05,
      "loss": 20.5772,
      "step": 9357
    },
    {
      "epoch": 9.37,
      "grad_norm": 6971.8701171875,
      "learning_rate": 5.480908152734778e-05,
      "loss": 14.2372,
      "step": 9358
    },
    {
      "epoch": 9.37,
      "grad_norm": 136209.265625,
      "learning_rate": 5.480392156862746e-05,
      "loss": 19.5809,
      "step": 9359
    },
    {
      "epoch": 9.37,
      "grad_norm": 2441.103515625,
      "learning_rate": 5.4798761609907125e-05,
      "loss": 17.9266,
      "step": 9360
    },
    {
      "epoch": 9.37,
      "grad_norm": 4591.95654296875,
      "learning_rate": 5.4793601651186786e-05,
      "loss": 13.7819,
      "step": 9361
    },
    {
      "epoch": 9.37,
      "grad_norm": 2905.354248046875,
      "learning_rate": 5.478844169246646e-05,
      "loss": 22.7575,
      "step": 9362
    },
    {
      "epoch": 9.37,
      "grad_norm": 2946.751708984375,
      "learning_rate": 5.478328173374613e-05,
      "loss": 14.0578,
      "step": 9363
    },
    {
      "epoch": 9.37,
      "grad_norm": 7608.87646484375,
      "learning_rate": 5.47781217750258e-05,
      "loss": 15.3978,
      "step": 9364
    },
    {
      "epoch": 9.37,
      "grad_norm": 4291.458984375,
      "learning_rate": 5.477296181630547e-05,
      "loss": 20.0008,
      "step": 9365
    },
    {
      "epoch": 9.38,
      "grad_norm": 8157.12646484375,
      "learning_rate": 5.4767801857585145e-05,
      "loss": 13.9731,
      "step": 9366
    },
    {
      "epoch": 9.38,
      "grad_norm": 6544.7802734375,
      "learning_rate": 5.4762641898864806e-05,
      "loss": 13.5403,
      "step": 9367
    },
    {
      "epoch": 9.38,
      "grad_norm": 6607.81298828125,
      "learning_rate": 5.475748194014448e-05,
      "loss": 13.0781,
      "step": 9368
    },
    {
      "epoch": 9.38,
      "grad_norm": 5820.91064453125,
      "learning_rate": 5.475232198142415e-05,
      "loss": 14.8583,
      "step": 9369
    },
    {
      "epoch": 9.38,
      "grad_norm": 1936.6595458984375,
      "learning_rate": 5.474716202270382e-05,
      "loss": 13.1722,
      "step": 9370
    },
    {
      "epoch": 9.38,
      "grad_norm": 41617.01171875,
      "learning_rate": 5.474200206398349e-05,
      "loss": 16.6572,
      "step": 9371
    },
    {
      "epoch": 9.38,
      "grad_norm": 6505.9150390625,
      "learning_rate": 5.4736842105263165e-05,
      "loss": 14.6756,
      "step": 9372
    },
    {
      "epoch": 9.38,
      "grad_norm": 2277.606201171875,
      "learning_rate": 5.4731682146542826e-05,
      "loss": 15.9168,
      "step": 9373
    },
    {
      "epoch": 9.38,
      "grad_norm": 1257.4127197265625,
      "learning_rate": 5.472652218782251e-05,
      "loss": 13.5345,
      "step": 9374
    },
    {
      "epoch": 9.38,
      "grad_norm": 15634.994140625,
      "learning_rate": 5.472136222910217e-05,
      "loss": 20.3783,
      "step": 9375
    },
    {
      "epoch": 9.39,
      "grad_norm": 4167.279296875,
      "learning_rate": 5.471620227038184e-05,
      "loss": 16.4216,
      "step": 9376
    },
    {
      "epoch": 9.39,
      "grad_norm": 19706.615234375,
      "learning_rate": 5.471104231166151e-05,
      "loss": 17.7376,
      "step": 9377
    },
    {
      "epoch": 9.39,
      "grad_norm": 2197.187744140625,
      "learning_rate": 5.4705882352941185e-05,
      "loss": 14.2576,
      "step": 9378
    },
    {
      "epoch": 9.39,
      "grad_norm": 3651.687744140625,
      "learning_rate": 5.4700722394220846e-05,
      "loss": 16.4868,
      "step": 9379
    },
    {
      "epoch": 9.39,
      "grad_norm": 7322.61083984375,
      "learning_rate": 5.4695562435500514e-05,
      "loss": 19.3523,
      "step": 9380
    },
    {
      "epoch": 9.39,
      "grad_norm": 6633.228515625,
      "learning_rate": 5.469040247678019e-05,
      "loss": 18.9554,
      "step": 9381
    },
    {
      "epoch": 9.39,
      "grad_norm": 9700.154296875,
      "learning_rate": 5.4685242518059856e-05,
      "loss": 14.6294,
      "step": 9382
    },
    {
      "epoch": 9.39,
      "grad_norm": 6426.03369140625,
      "learning_rate": 5.468008255933953e-05,
      "loss": 21.3485,
      "step": 9383
    },
    {
      "epoch": 9.39,
      "grad_norm": 3481.441650390625,
      "learning_rate": 5.467492260061919e-05,
      "loss": 15.1781,
      "step": 9384
    },
    {
      "epoch": 9.39,
      "grad_norm": 3902.892578125,
      "learning_rate": 5.4669762641898866e-05,
      "loss": 16.146,
      "step": 9385
    },
    {
      "epoch": 9.4,
      "grad_norm": 20000.01171875,
      "learning_rate": 5.4664602683178534e-05,
      "loss": 17.9981,
      "step": 9386
    },
    {
      "epoch": 9.4,
      "grad_norm": 8610.8359375,
      "learning_rate": 5.465944272445821e-05,
      "loss": 16.6995,
      "step": 9387
    },
    {
      "epoch": 9.4,
      "grad_norm": 2959.644775390625,
      "learning_rate": 5.4654282765737876e-05,
      "loss": 12.2249,
      "step": 9388
    },
    {
      "epoch": 9.4,
      "grad_norm": 11398.1455078125,
      "learning_rate": 5.464912280701755e-05,
      "loss": 17.4674,
      "step": 9389
    },
    {
      "epoch": 9.4,
      "grad_norm": 2469.92431640625,
      "learning_rate": 5.464396284829721e-05,
      "loss": 11.6282,
      "step": 9390
    },
    {
      "epoch": 9.4,
      "grad_norm": 4731.1357421875,
      "learning_rate": 5.463880288957689e-05,
      "loss": 15.3245,
      "step": 9391
    },
    {
      "epoch": 9.4,
      "grad_norm": 10284.7548828125,
      "learning_rate": 5.4633642930856554e-05,
      "loss": 18.1108,
      "step": 9392
    },
    {
      "epoch": 9.4,
      "grad_norm": 5745.83642578125,
      "learning_rate": 5.462848297213623e-05,
      "loss": 14.8953,
      "step": 9393
    },
    {
      "epoch": 9.4,
      "grad_norm": 1788.9775390625,
      "learning_rate": 5.4623323013415896e-05,
      "loss": 16.7722,
      "step": 9394
    },
    {
      "epoch": 9.4,
      "grad_norm": 14830.3984375,
      "learning_rate": 5.461816305469557e-05,
      "loss": 14.2257,
      "step": 9395
    },
    {
      "epoch": 9.41,
      "grad_norm": 33992.36328125,
      "learning_rate": 5.461300309597523e-05,
      "loss": 16.5583,
      "step": 9396
    },
    {
      "epoch": 9.41,
      "grad_norm": 3841.482666015625,
      "learning_rate": 5.46078431372549e-05,
      "loss": 14.5804,
      "step": 9397
    },
    {
      "epoch": 9.41,
      "grad_norm": 38189.515625,
      "learning_rate": 5.4602683178534574e-05,
      "loss": 15.2955,
      "step": 9398
    },
    {
      "epoch": 9.41,
      "grad_norm": 2330.867431640625,
      "learning_rate": 5.459752321981424e-05,
      "loss": 13.5783,
      "step": 9399
    },
    {
      "epoch": 9.41,
      "grad_norm": 2334.58447265625,
      "learning_rate": 5.4592363261093916e-05,
      "loss": 13.3291,
      "step": 9400
    },
    {
      "epoch": 9.41,
      "grad_norm": 2290.720947265625,
      "learning_rate": 5.458720330237358e-05,
      "loss": 14.1792,
      "step": 9401
    },
    {
      "epoch": 9.41,
      "grad_norm": 1884.7603759765625,
      "learning_rate": 5.458204334365326e-05,
      "loss": 14.2337,
      "step": 9402
    },
    {
      "epoch": 9.41,
      "grad_norm": 1133.89697265625,
      "learning_rate": 5.457688338493292e-05,
      "loss": 13.2569,
      "step": 9403
    },
    {
      "epoch": 9.41,
      "grad_norm": 3408.04931640625,
      "learning_rate": 5.4571723426212594e-05,
      "loss": 13.6376,
      "step": 9404
    },
    {
      "epoch": 9.41,
      "grad_norm": 12941.7900390625,
      "learning_rate": 5.456656346749226e-05,
      "loss": 14.8559,
      "step": 9405
    },
    {
      "epoch": 9.42,
      "grad_norm": 875.6190185546875,
      "learning_rate": 5.4561403508771936e-05,
      "loss": 11.1629,
      "step": 9406
    },
    {
      "epoch": 9.42,
      "grad_norm": 4451.08154296875,
      "learning_rate": 5.45562435500516e-05,
      "loss": 13.1058,
      "step": 9407
    },
    {
      "epoch": 9.42,
      "grad_norm": 6261.8486328125,
      "learning_rate": 5.455108359133128e-05,
      "loss": 13.2316,
      "step": 9408
    },
    {
      "epoch": 9.42,
      "grad_norm": 7256.86279296875,
      "learning_rate": 5.454592363261094e-05,
      "loss": 14.4493,
      "step": 9409
    },
    {
      "epoch": 9.42,
      "grad_norm": 23782.03515625,
      "learning_rate": 5.4540763673890614e-05,
      "loss": 17.6337,
      "step": 9410
    },
    {
      "epoch": 9.42,
      "grad_norm": 6713.99560546875,
      "learning_rate": 5.453560371517028e-05,
      "loss": 18.0394,
      "step": 9411
    },
    {
      "epoch": 9.42,
      "grad_norm": 1689.8067626953125,
      "learning_rate": 5.4530443756449956e-05,
      "loss": 11.7923,
      "step": 9412
    },
    {
      "epoch": 9.42,
      "grad_norm": 4370.73583984375,
      "learning_rate": 5.452528379772962e-05,
      "loss": 12.5861,
      "step": 9413
    },
    {
      "epoch": 9.42,
      "grad_norm": 33711.6484375,
      "learning_rate": 5.4520123839009285e-05,
      "loss": 19.5998,
      "step": 9414
    },
    {
      "epoch": 9.42,
      "grad_norm": 12840.921875,
      "learning_rate": 5.451496388028896e-05,
      "loss": 16.1727,
      "step": 9415
    },
    {
      "epoch": 9.43,
      "grad_norm": 3373.83154296875,
      "learning_rate": 5.450980392156863e-05,
      "loss": 17.4173,
      "step": 9416
    },
    {
      "epoch": 9.43,
      "grad_norm": 8442.6201171875,
      "learning_rate": 5.45046439628483e-05,
      "loss": 15.922,
      "step": 9417
    },
    {
      "epoch": 9.43,
      "grad_norm": 11152.9775390625,
      "learning_rate": 5.449948400412796e-05,
      "loss": 17.948,
      "step": 9418
    },
    {
      "epoch": 9.43,
      "grad_norm": 7172.12646484375,
      "learning_rate": 5.4494324045407644e-05,
      "loss": 13.8882,
      "step": 9419
    },
    {
      "epoch": 9.43,
      "grad_norm": 4569.19384765625,
      "learning_rate": 5.4489164086687305e-05,
      "loss": 16.7048,
      "step": 9420
    },
    {
      "epoch": 9.43,
      "grad_norm": 6633.2431640625,
      "learning_rate": 5.448400412796698e-05,
      "loss": 15.8152,
      "step": 9421
    },
    {
      "epoch": 9.43,
      "grad_norm": 3167.085205078125,
      "learning_rate": 5.447884416924665e-05,
      "loss": 12.9555,
      "step": 9422
    },
    {
      "epoch": 9.43,
      "grad_norm": 9820.1005859375,
      "learning_rate": 5.447368421052632e-05,
      "loss": 17.8651,
      "step": 9423
    },
    {
      "epoch": 9.43,
      "grad_norm": 3520.9970703125,
      "learning_rate": 5.446852425180598e-05,
      "loss": 13.8372,
      "step": 9424
    },
    {
      "epoch": 9.43,
      "grad_norm": 881.4415283203125,
      "learning_rate": 5.4463364293085664e-05,
      "loss": 14.2516,
      "step": 9425
    },
    {
      "epoch": 9.44,
      "grad_norm": 4306.39111328125,
      "learning_rate": 5.4458204334365325e-05,
      "loss": 12.5236,
      "step": 9426
    },
    {
      "epoch": 9.44,
      "grad_norm": 11153.830078125,
      "learning_rate": 5.4453044375645e-05,
      "loss": 16.2507,
      "step": 9427
    },
    {
      "epoch": 9.44,
      "grad_norm": 7092.36083984375,
      "learning_rate": 5.444788441692467e-05,
      "loss": 21.6065,
      "step": 9428
    },
    {
      "epoch": 9.44,
      "grad_norm": 2116.670654296875,
      "learning_rate": 5.444272445820434e-05,
      "loss": 13.4812,
      "step": 9429
    },
    {
      "epoch": 9.44,
      "grad_norm": 8185.888671875,
      "learning_rate": 5.443756449948401e-05,
      "loss": 14.0948,
      "step": 9430
    },
    {
      "epoch": 9.44,
      "grad_norm": 9025.455078125,
      "learning_rate": 5.4432404540763684e-05,
      "loss": 14.6294,
      "step": 9431
    },
    {
      "epoch": 9.44,
      "grad_norm": 7558.330078125,
      "learning_rate": 5.4427244582043345e-05,
      "loss": 17.3673,
      "step": 9432
    },
    {
      "epoch": 9.44,
      "grad_norm": 8103.4765625,
      "learning_rate": 5.442208462332301e-05,
      "loss": 14.5331,
      "step": 9433
    },
    {
      "epoch": 9.44,
      "grad_norm": 1347.44921875,
      "learning_rate": 5.441692466460269e-05,
      "loss": 14.2211,
      "step": 9434
    },
    {
      "epoch": 9.44,
      "grad_norm": 62390.5390625,
      "learning_rate": 5.441176470588235e-05,
      "loss": 12.445,
      "step": 9435
    },
    {
      "epoch": 9.45,
      "grad_norm": 3448.72314453125,
      "learning_rate": 5.440660474716203e-05,
      "loss": 16.0372,
      "step": 9436
    },
    {
      "epoch": 9.45,
      "grad_norm": 4081.176025390625,
      "learning_rate": 5.440144478844169e-05,
      "loss": 13.1795,
      "step": 9437
    },
    {
      "epoch": 9.45,
      "grad_norm": 2579.28515625,
      "learning_rate": 5.4396284829721365e-05,
      "loss": 13.9048,
      "step": 9438
    },
    {
      "epoch": 9.45,
      "grad_norm": 3937.191650390625,
      "learning_rate": 5.439112487100103e-05,
      "loss": 14.9227,
      "step": 9439
    },
    {
      "epoch": 9.45,
      "grad_norm": 5198.14404296875,
      "learning_rate": 5.438596491228071e-05,
      "loss": 14.5736,
      "step": 9440
    },
    {
      "epoch": 9.45,
      "grad_norm": 7184.529296875,
      "learning_rate": 5.438080495356037e-05,
      "loss": 19.5289,
      "step": 9441
    },
    {
      "epoch": 9.45,
      "grad_norm": 28620.169921875,
      "learning_rate": 5.437564499484005e-05,
      "loss": 17.7908,
      "step": 9442
    },
    {
      "epoch": 9.45,
      "grad_norm": 10877.419921875,
      "learning_rate": 5.437048503611971e-05,
      "loss": 17.3513,
      "step": 9443
    },
    {
      "epoch": 9.45,
      "grad_norm": 2238.603515625,
      "learning_rate": 5.4365325077399385e-05,
      "loss": 14.8402,
      "step": 9444
    },
    {
      "epoch": 9.45,
      "grad_norm": 2057.027099609375,
      "learning_rate": 5.436016511867905e-05,
      "loss": 12.6156,
      "step": 9445
    },
    {
      "epoch": 9.46,
      "grad_norm": 1420.86865234375,
      "learning_rate": 5.435500515995873e-05,
      "loss": 13.3765,
      "step": 9446
    },
    {
      "epoch": 9.46,
      "grad_norm": 1594.2791748046875,
      "learning_rate": 5.4349845201238395e-05,
      "loss": 18.299,
      "step": 9447
    },
    {
      "epoch": 9.46,
      "grad_norm": 5257.63623046875,
      "learning_rate": 5.434468524251807e-05,
      "loss": 12.7392,
      "step": 9448
    },
    {
      "epoch": 9.46,
      "grad_norm": 5470.3896484375,
      "learning_rate": 5.433952528379773e-05,
      "loss": 16.6028,
      "step": 9449
    },
    {
      "epoch": 9.46,
      "grad_norm": 2222.425048828125,
      "learning_rate": 5.43343653250774e-05,
      "loss": 16.6471,
      "step": 9450
    },
    {
      "epoch": 9.46,
      "grad_norm": 9977.62109375,
      "learning_rate": 5.432920536635707e-05,
      "loss": 18.5357,
      "step": 9451
    },
    {
      "epoch": 9.46,
      "grad_norm": 8749.978515625,
      "learning_rate": 5.4324045407636734e-05,
      "loss": 16.0604,
      "step": 9452
    },
    {
      "epoch": 9.46,
      "grad_norm": 10740.4873046875,
      "learning_rate": 5.4318885448916415e-05,
      "loss": 16.4829,
      "step": 9453
    },
    {
      "epoch": 9.46,
      "grad_norm": 4728.91064453125,
      "learning_rate": 5.4313725490196076e-05,
      "loss": 13.7421,
      "step": 9454
    },
    {
      "epoch": 9.46,
      "grad_norm": 4792.716796875,
      "learning_rate": 5.430856553147575e-05,
      "loss": 15.6593,
      "step": 9455
    },
    {
      "epoch": 9.47,
      "grad_norm": 5953.29443359375,
      "learning_rate": 5.430340557275542e-05,
      "loss": 12.7277,
      "step": 9456
    },
    {
      "epoch": 9.47,
      "grad_norm": 4086.953369140625,
      "learning_rate": 5.429824561403509e-05,
      "loss": 14.591,
      "step": 9457
    },
    {
      "epoch": 9.47,
      "grad_norm": 8130.69580078125,
      "learning_rate": 5.429308565531476e-05,
      "loss": 16.9164,
      "step": 9458
    },
    {
      "epoch": 9.47,
      "grad_norm": 1711.613525390625,
      "learning_rate": 5.4287925696594435e-05,
      "loss": 13.9872,
      "step": 9459
    },
    {
      "epoch": 9.47,
      "grad_norm": 8160.85693359375,
      "learning_rate": 5.4282765737874096e-05,
      "loss": 15.7629,
      "step": 9460
    },
    {
      "epoch": 9.47,
      "grad_norm": 20375.126953125,
      "learning_rate": 5.427760577915377e-05,
      "loss": 16.6948,
      "step": 9461
    },
    {
      "epoch": 9.47,
      "grad_norm": 3657.579345703125,
      "learning_rate": 5.427244582043344e-05,
      "loss": 15.2928,
      "step": 9462
    },
    {
      "epoch": 9.47,
      "grad_norm": 26818.076171875,
      "learning_rate": 5.426728586171311e-05,
      "loss": 18.049,
      "step": 9463
    },
    {
      "epoch": 9.47,
      "grad_norm": 9784.2158203125,
      "learning_rate": 5.426212590299278e-05,
      "loss": 17.2361,
      "step": 9464
    },
    {
      "epoch": 9.47,
      "grad_norm": 12016.0703125,
      "learning_rate": 5.4256965944272455e-05,
      "loss": 15.937,
      "step": 9465
    },
    {
      "epoch": 9.48,
      "grad_norm": 10786.8876953125,
      "learning_rate": 5.4251805985552116e-05,
      "loss": 17.5316,
      "step": 9466
    },
    {
      "epoch": 9.48,
      "grad_norm": 20263.23046875,
      "learning_rate": 5.424664602683179e-05,
      "loss": 12.6222,
      "step": 9467
    },
    {
      "epoch": 9.48,
      "grad_norm": 18513.765625,
      "learning_rate": 5.424148606811146e-05,
      "loss": 18.122,
      "step": 9468
    },
    {
      "epoch": 9.48,
      "grad_norm": 3164.634033203125,
      "learning_rate": 5.423632610939112e-05,
      "loss": 15.3859,
      "step": 9469
    },
    {
      "epoch": 9.48,
      "grad_norm": 440189.03125,
      "learning_rate": 5.42311661506708e-05,
      "loss": 13.7297,
      "step": 9470
    },
    {
      "epoch": 9.48,
      "grad_norm": 28066.71875,
      "learning_rate": 5.422600619195046e-05,
      "loss": 14.2032,
      "step": 9471
    },
    {
      "epoch": 9.48,
      "grad_norm": 9447.111328125,
      "learning_rate": 5.4220846233230136e-05,
      "loss": 20.9983,
      "step": 9472
    },
    {
      "epoch": 9.48,
      "grad_norm": 1652.22998046875,
      "learning_rate": 5.4215686274509804e-05,
      "loss": 14.5646,
      "step": 9473
    },
    {
      "epoch": 9.48,
      "grad_norm": 4416.62158203125,
      "learning_rate": 5.421052631578948e-05,
      "loss": 14.1005,
      "step": 9474
    },
    {
      "epoch": 9.48,
      "grad_norm": 16703.69921875,
      "learning_rate": 5.4205366357069146e-05,
      "loss": 16.8417,
      "step": 9475
    },
    {
      "epoch": 9.49,
      "grad_norm": 13383.6025390625,
      "learning_rate": 5.420020639834882e-05,
      "loss": 19.5797,
      "step": 9476
    },
    {
      "epoch": 9.49,
      "grad_norm": 8426.7451171875,
      "learning_rate": 5.419504643962848e-05,
      "loss": 15.0718,
      "step": 9477
    },
    {
      "epoch": 9.49,
      "grad_norm": 10826.4345703125,
      "learning_rate": 5.4189886480908156e-05,
      "loss": 15.0779,
      "step": 9478
    },
    {
      "epoch": 9.49,
      "grad_norm": 22814.658203125,
      "learning_rate": 5.4184726522187824e-05,
      "loss": 19.3355,
      "step": 9479
    },
    {
      "epoch": 9.49,
      "grad_norm": 5277.97705078125,
      "learning_rate": 5.41795665634675e-05,
      "loss": 17.1213,
      "step": 9480
    },
    {
      "epoch": 9.49,
      "grad_norm": 48619.9765625,
      "learning_rate": 5.4174406604747166e-05,
      "loss": 13.8795,
      "step": 9481
    },
    {
      "epoch": 9.49,
      "grad_norm": 8194.6318359375,
      "learning_rate": 5.416924664602684e-05,
      "loss": 15.0155,
      "step": 9482
    },
    {
      "epoch": 9.49,
      "grad_norm": 4990.84765625,
      "learning_rate": 5.41640866873065e-05,
      "loss": 14.3566,
      "step": 9483
    },
    {
      "epoch": 9.49,
      "grad_norm": 9564.7060546875,
      "learning_rate": 5.4158926728586176e-05,
      "loss": 14.9343,
      "step": 9484
    },
    {
      "epoch": 9.49,
      "grad_norm": 2150.93212890625,
      "learning_rate": 5.4153766769865844e-05,
      "loss": 18.5948,
      "step": 9485
    },
    {
      "epoch": 9.5,
      "grad_norm": 958.1478271484375,
      "learning_rate": 5.4148606811145505e-05,
      "loss": 12.0407,
      "step": 9486
    },
    {
      "epoch": 9.5,
      "grad_norm": 16963.708984375,
      "learning_rate": 5.4143446852425186e-05,
      "loss": 20.0015,
      "step": 9487
    },
    {
      "epoch": 9.5,
      "grad_norm": 4344.04931640625,
      "learning_rate": 5.413828689370485e-05,
      "loss": 15.7897,
      "step": 9488
    },
    {
      "epoch": 9.5,
      "grad_norm": 18205.51171875,
      "learning_rate": 5.413312693498452e-05,
      "loss": 21.0751,
      "step": 9489
    },
    {
      "epoch": 9.5,
      "grad_norm": 16285.7900390625,
      "learning_rate": 5.412796697626419e-05,
      "loss": 15.5305,
      "step": 9490
    },
    {
      "epoch": 9.5,
      "grad_norm": 46410.92578125,
      "learning_rate": 5.4122807017543864e-05,
      "loss": 19.4681,
      "step": 9491
    },
    {
      "epoch": 9.5,
      "grad_norm": 2376.039306640625,
      "learning_rate": 5.411764705882353e-05,
      "loss": 16.7392,
      "step": 9492
    },
    {
      "epoch": 9.5,
      "grad_norm": 19789.8671875,
      "learning_rate": 5.4112487100103206e-05,
      "loss": 17.1449,
      "step": 9493
    },
    {
      "epoch": 9.5,
      "grad_norm": 14928.701171875,
      "learning_rate": 5.410732714138287e-05,
      "loss": 12.2476,
      "step": 9494
    },
    {
      "epoch": 9.5,
      "grad_norm": 5045.10400390625,
      "learning_rate": 5.410216718266254e-05,
      "loss": 16.366,
      "step": 9495
    },
    {
      "epoch": 9.51,
      "grad_norm": 11980.005859375,
      "learning_rate": 5.409700722394221e-05,
      "loss": 17.1218,
      "step": 9496
    },
    {
      "epoch": 9.51,
      "grad_norm": 7408.7578125,
      "learning_rate": 5.4091847265221884e-05,
      "loss": 11.9865,
      "step": 9497
    },
    {
      "epoch": 9.51,
      "grad_norm": 7047.986328125,
      "learning_rate": 5.408668730650155e-05,
      "loss": 15.9783,
      "step": 9498
    },
    {
      "epoch": 9.51,
      "grad_norm": 108335.1953125,
      "learning_rate": 5.4081527347781226e-05,
      "loss": 20.4439,
      "step": 9499
    },
    {
      "epoch": 9.51,
      "grad_norm": 2838.624755859375,
      "learning_rate": 5.407636738906089e-05,
      "loss": 13.8799,
      "step": 9500
    },
    {
      "epoch": 9.51,
      "grad_norm": 19856.796875,
      "learning_rate": 5.407120743034056e-05,
      "loss": 16.9601,
      "step": 9501
    },
    {
      "epoch": 9.51,
      "grad_norm": 12310.3935546875,
      "learning_rate": 5.406604747162023e-05,
      "loss": 19.2272,
      "step": 9502
    },
    {
      "epoch": 9.51,
      "grad_norm": 18623.55078125,
      "learning_rate": 5.4060887512899904e-05,
      "loss": 19.6218,
      "step": 9503
    },
    {
      "epoch": 9.51,
      "grad_norm": 37959.796875,
      "learning_rate": 5.405572755417957e-05,
      "loss": 14.4044,
      "step": 9504
    },
    {
      "epoch": 9.51,
      "grad_norm": 1157.396728515625,
      "learning_rate": 5.405056759545923e-05,
      "loss": 16.9966,
      "step": 9505
    },
    {
      "epoch": 9.52,
      "grad_norm": 4392.26171875,
      "learning_rate": 5.404540763673891e-05,
      "loss": 16.273,
      "step": 9506
    },
    {
      "epoch": 9.52,
      "grad_norm": 3834.660400390625,
      "learning_rate": 5.4040247678018575e-05,
      "loss": 12.1542,
      "step": 9507
    },
    {
      "epoch": 9.52,
      "grad_norm": 18426.55078125,
      "learning_rate": 5.403508771929825e-05,
      "loss": 15.4754,
      "step": 9508
    },
    {
      "epoch": 9.52,
      "grad_norm": 5682.81494140625,
      "learning_rate": 5.402992776057792e-05,
      "loss": 19.0308,
      "step": 9509
    },
    {
      "epoch": 9.52,
      "grad_norm": 8170.1845703125,
      "learning_rate": 5.402476780185759e-05,
      "loss": 13.9559,
      "step": 9510
    },
    {
      "epoch": 9.52,
      "grad_norm": 19994.3515625,
      "learning_rate": 5.401960784313725e-05,
      "loss": 18.4784,
      "step": 9511
    },
    {
      "epoch": 9.52,
      "grad_norm": 6141.8212890625,
      "learning_rate": 5.401444788441693e-05,
      "loss": 13.5254,
      "step": 9512
    },
    {
      "epoch": 9.52,
      "grad_norm": 2988.96337890625,
      "learning_rate": 5.4009287925696595e-05,
      "loss": 15.7673,
      "step": 9513
    },
    {
      "epoch": 9.52,
      "grad_norm": 1661.993408203125,
      "learning_rate": 5.400412796697627e-05,
      "loss": 15.9222,
      "step": 9514
    },
    {
      "epoch": 9.52,
      "grad_norm": 8457.5234375,
      "learning_rate": 5.399896800825594e-05,
      "loss": 15.436,
      "step": 9515
    },
    {
      "epoch": 9.53,
      "grad_norm": 3477.800048828125,
      "learning_rate": 5.399380804953561e-05,
      "loss": 14.2925,
      "step": 9516
    },
    {
      "epoch": 9.53,
      "grad_norm": 14167.896484375,
      "learning_rate": 5.398864809081527e-05,
      "loss": 19.1975,
      "step": 9517
    },
    {
      "epoch": 9.53,
      "grad_norm": 7262.71044921875,
      "learning_rate": 5.3983488132094954e-05,
      "loss": 16.3615,
      "step": 9518
    },
    {
      "epoch": 9.53,
      "grad_norm": 16530.90625,
      "learning_rate": 5.3978328173374615e-05,
      "loss": 18.1963,
      "step": 9519
    },
    {
      "epoch": 9.53,
      "grad_norm": 3222.1884765625,
      "learning_rate": 5.397316821465429e-05,
      "loss": 15.3084,
      "step": 9520
    },
    {
      "epoch": 9.53,
      "grad_norm": 2131.465576171875,
      "learning_rate": 5.396800825593396e-05,
      "loss": 16.0362,
      "step": 9521
    },
    {
      "epoch": 9.53,
      "grad_norm": 3995.77880859375,
      "learning_rate": 5.396284829721362e-05,
      "loss": 14.1997,
      "step": 9522
    },
    {
      "epoch": 9.53,
      "grad_norm": 4402.68896484375,
      "learning_rate": 5.395768833849329e-05,
      "loss": 15.1497,
      "step": 9523
    },
    {
      "epoch": 9.53,
      "grad_norm": 6940.56298828125,
      "learning_rate": 5.395252837977296e-05,
      "loss": 13.9326,
      "step": 9524
    },
    {
      "epoch": 9.53,
      "grad_norm": 396.5004577636719,
      "learning_rate": 5.3947368421052635e-05,
      "loss": 11.7874,
      "step": 9525
    },
    {
      "epoch": 9.54,
      "grad_norm": 14344.810546875,
      "learning_rate": 5.39422084623323e-05,
      "loss": 14.9952,
      "step": 9526
    },
    {
      "epoch": 9.54,
      "grad_norm": 2813.94287109375,
      "learning_rate": 5.393704850361198e-05,
      "loss": 12.9967,
      "step": 9527
    },
    {
      "epoch": 9.54,
      "grad_norm": 4310.7294921875,
      "learning_rate": 5.393188854489164e-05,
      "loss": 14.2749,
      "step": 9528
    },
    {
      "epoch": 9.54,
      "grad_norm": 35924.41796875,
      "learning_rate": 5.392672858617131e-05,
      "loss": 16.5649,
      "step": 9529
    },
    {
      "epoch": 9.54,
      "grad_norm": 3226.424072265625,
      "learning_rate": 5.392156862745098e-05,
      "loss": 15.308,
      "step": 9530
    },
    {
      "epoch": 9.54,
      "grad_norm": 2033.3740234375,
      "learning_rate": 5.3916408668730655e-05,
      "loss": 13.9395,
      "step": 9531
    },
    {
      "epoch": 9.54,
      "grad_norm": 18132.4921875,
      "learning_rate": 5.391124871001032e-05,
      "loss": 21.1812,
      "step": 9532
    },
    {
      "epoch": 9.54,
      "grad_norm": 3655.122314453125,
      "learning_rate": 5.390608875129e-05,
      "loss": 15.7123,
      "step": 9533
    },
    {
      "epoch": 9.54,
      "grad_norm": 3810.5966796875,
      "learning_rate": 5.390092879256966e-05,
      "loss": 13.7623,
      "step": 9534
    },
    {
      "epoch": 9.54,
      "grad_norm": 1144.71826171875,
      "learning_rate": 5.389576883384934e-05,
      "loss": 13.9853,
      "step": 9535
    },
    {
      "epoch": 9.55,
      "grad_norm": 8406.48828125,
      "learning_rate": 5.3890608875129e-05,
      "loss": 12.6597,
      "step": 9536
    },
    {
      "epoch": 9.55,
      "grad_norm": 9835.2841796875,
      "learning_rate": 5.3885448916408675e-05,
      "loss": 14.7059,
      "step": 9537
    },
    {
      "epoch": 9.55,
      "grad_norm": 6615.35107421875,
      "learning_rate": 5.388028895768834e-05,
      "loss": 16.1251,
      "step": 9538
    },
    {
      "epoch": 9.55,
      "grad_norm": 32735.7890625,
      "learning_rate": 5.387512899896802e-05,
      "loss": 18.8189,
      "step": 9539
    },
    {
      "epoch": 9.55,
      "grad_norm": 39748.33984375,
      "learning_rate": 5.386996904024768e-05,
      "loss": 14.7968,
      "step": 9540
    },
    {
      "epoch": 9.55,
      "grad_norm": 4264.90185546875,
      "learning_rate": 5.3864809081527346e-05,
      "loss": 14.1582,
      "step": 9541
    },
    {
      "epoch": 9.55,
      "grad_norm": 11019.0390625,
      "learning_rate": 5.385964912280702e-05,
      "loss": 19.872,
      "step": 9542
    },
    {
      "epoch": 9.55,
      "grad_norm": 54783.171875,
      "learning_rate": 5.385448916408669e-05,
      "loss": 14.5923,
      "step": 9543
    },
    {
      "epoch": 9.55,
      "grad_norm": 7830.732421875,
      "learning_rate": 5.384932920536636e-05,
      "loss": 13.9382,
      "step": 9544
    },
    {
      "epoch": 9.55,
      "grad_norm": 10657.5283203125,
      "learning_rate": 5.3844169246646023e-05,
      "loss": 15.3527,
      "step": 9545
    },
    {
      "epoch": 9.56,
      "grad_norm": 14989.2861328125,
      "learning_rate": 5.38390092879257e-05,
      "loss": 13.951,
      "step": 9546
    },
    {
      "epoch": 9.56,
      "grad_norm": 4494.8388671875,
      "learning_rate": 5.3833849329205366e-05,
      "loss": 14.8807,
      "step": 9547
    },
    {
      "epoch": 9.56,
      "grad_norm": 5992.47998046875,
      "learning_rate": 5.382868937048504e-05,
      "loss": 12.6412,
      "step": 9548
    },
    {
      "epoch": 9.56,
      "grad_norm": 13598.9921875,
      "learning_rate": 5.382352941176471e-05,
      "loss": 15.8521,
      "step": 9549
    },
    {
      "epoch": 9.56,
      "grad_norm": 1947.557861328125,
      "learning_rate": 5.381836945304438e-05,
      "loss": 16.8496,
      "step": 9550
    },
    {
      "epoch": 9.56,
      "grad_norm": 1179.8887939453125,
      "learning_rate": 5.3813209494324043e-05,
      "loss": 14.9368,
      "step": 9551
    },
    {
      "epoch": 9.56,
      "grad_norm": 4715.5966796875,
      "learning_rate": 5.3808049535603725e-05,
      "loss": 17.759,
      "step": 9552
    },
    {
      "epoch": 9.56,
      "grad_norm": 7604.91943359375,
      "learning_rate": 5.3802889576883386e-05,
      "loss": 19.5985,
      "step": 9553
    },
    {
      "epoch": 9.56,
      "grad_norm": 5618.5732421875,
      "learning_rate": 5.379772961816306e-05,
      "loss": 14.6392,
      "step": 9554
    },
    {
      "epoch": 9.56,
      "grad_norm": 2392.451416015625,
      "learning_rate": 5.379256965944273e-05,
      "loss": 14.197,
      "step": 9555
    },
    {
      "epoch": 9.57,
      "grad_norm": 6501.6748046875,
      "learning_rate": 5.37874097007224e-05,
      "loss": 15.3463,
      "step": 9556
    },
    {
      "epoch": 9.57,
      "grad_norm": 2067.61279296875,
      "learning_rate": 5.3782249742002063e-05,
      "loss": 12.9777,
      "step": 9557
    },
    {
      "epoch": 9.57,
      "grad_norm": 109919.375,
      "learning_rate": 5.377708978328173e-05,
      "loss": 17.7632,
      "step": 9558
    },
    {
      "epoch": 9.57,
      "grad_norm": 1077.1536865234375,
      "learning_rate": 5.3771929824561406e-05,
      "loss": 14.4784,
      "step": 9559
    },
    {
      "epoch": 9.57,
      "grad_norm": 1320.9642333984375,
      "learning_rate": 5.3766769865841073e-05,
      "loss": 13.5552,
      "step": 9560
    },
    {
      "epoch": 9.57,
      "grad_norm": 3365.272705078125,
      "learning_rate": 5.376160990712075e-05,
      "loss": 22.2574,
      "step": 9561
    },
    {
      "epoch": 9.57,
      "grad_norm": 3588.375,
      "learning_rate": 5.375644994840041e-05,
      "loss": 17.7211,
      "step": 9562
    },
    {
      "epoch": 9.57,
      "grad_norm": 158801.0,
      "learning_rate": 5.375128998968009e-05,
      "loss": 16.522,
      "step": 9563
    },
    {
      "epoch": 9.57,
      "grad_norm": 3435.690185546875,
      "learning_rate": 5.374613003095975e-05,
      "loss": 15.8759,
      "step": 9564
    },
    {
      "epoch": 9.57,
      "grad_norm": 2543.298828125,
      "learning_rate": 5.3740970072239426e-05,
      "loss": 14.604,
      "step": 9565
    },
    {
      "epoch": 9.58,
      "grad_norm": 4886.05078125,
      "learning_rate": 5.3735810113519093e-05,
      "loss": 15.5852,
      "step": 9566
    },
    {
      "epoch": 9.58,
      "grad_norm": 132049.96875,
      "learning_rate": 5.373065015479877e-05,
      "loss": 18.1778,
      "step": 9567
    },
    {
      "epoch": 9.58,
      "grad_norm": 3660.83251953125,
      "learning_rate": 5.372549019607843e-05,
      "loss": 19.5435,
      "step": 9568
    },
    {
      "epoch": 9.58,
      "grad_norm": 6187.83935546875,
      "learning_rate": 5.372033023735811e-05,
      "loss": 14.8353,
      "step": 9569
    },
    {
      "epoch": 9.58,
      "grad_norm": 2178.4619140625,
      "learning_rate": 5.371517027863777e-05,
      "loss": 17.1378,
      "step": 9570
    },
    {
      "epoch": 9.58,
      "grad_norm": 3294.28759765625,
      "learning_rate": 5.3710010319917446e-05,
      "loss": 15.4725,
      "step": 9571
    },
    {
      "epoch": 9.58,
      "grad_norm": 1746.3731689453125,
      "learning_rate": 5.3704850361197113e-05,
      "loss": 15.5403,
      "step": 9572
    },
    {
      "epoch": 9.58,
      "grad_norm": 1268.5633544921875,
      "learning_rate": 5.369969040247679e-05,
      "loss": 13.498,
      "step": 9573
    },
    {
      "epoch": 9.58,
      "grad_norm": 1571.28955078125,
      "learning_rate": 5.369453044375645e-05,
      "loss": 14.1824,
      "step": 9574
    },
    {
      "epoch": 9.58,
      "grad_norm": 1072.82275390625,
      "learning_rate": 5.368937048503612e-05,
      "loss": 14.7285,
      "step": 9575
    },
    {
      "epoch": 9.59,
      "grad_norm": 9288.8056640625,
      "learning_rate": 5.368421052631579e-05,
      "loss": 14.4571,
      "step": 9576
    },
    {
      "epoch": 9.59,
      "grad_norm": 17821.982421875,
      "learning_rate": 5.367905056759546e-05,
      "loss": 17.5299,
      "step": 9577
    },
    {
      "epoch": 9.59,
      "grad_norm": 9984.048828125,
      "learning_rate": 5.3673890608875133e-05,
      "loss": 13.4125,
      "step": 9578
    },
    {
      "epoch": 9.59,
      "grad_norm": 3676.4599609375,
      "learning_rate": 5.3668730650154794e-05,
      "loss": 12.3063,
      "step": 9579
    },
    {
      "epoch": 9.59,
      "grad_norm": 2947.59912109375,
      "learning_rate": 5.3663570691434476e-05,
      "loss": 18.2222,
      "step": 9580
    },
    {
      "epoch": 9.59,
      "grad_norm": 1817.1087646484375,
      "learning_rate": 5.365841073271414e-05,
      "loss": 15.5002,
      "step": 9581
    },
    {
      "epoch": 9.59,
      "grad_norm": 7754.05126953125,
      "learning_rate": 5.365325077399381e-05,
      "loss": 14.9828,
      "step": 9582
    },
    {
      "epoch": 9.59,
      "grad_norm": 5510.1259765625,
      "learning_rate": 5.364809081527348e-05,
      "loss": 13.8224,
      "step": 9583
    },
    {
      "epoch": 9.59,
      "grad_norm": 4058.37353515625,
      "learning_rate": 5.3642930856553153e-05,
      "loss": 13.2666,
      "step": 9584
    },
    {
      "epoch": 9.59,
      "grad_norm": 9727.5078125,
      "learning_rate": 5.3637770897832814e-05,
      "loss": 18.9909,
      "step": 9585
    },
    {
      "epoch": 9.6,
      "grad_norm": 14601.3916015625,
      "learning_rate": 5.3632610939112496e-05,
      "loss": 16.0738,
      "step": 9586
    },
    {
      "epoch": 9.6,
      "grad_norm": 8655.9560546875,
      "learning_rate": 5.362745098039216e-05,
      "loss": 14.7513,
      "step": 9587
    },
    {
      "epoch": 9.6,
      "grad_norm": 7394.1259765625,
      "learning_rate": 5.362229102167183e-05,
      "loss": 21.2676,
      "step": 9588
    },
    {
      "epoch": 9.6,
      "grad_norm": 2981.76806640625,
      "learning_rate": 5.36171310629515e-05,
      "loss": 15.9727,
      "step": 9589
    },
    {
      "epoch": 9.6,
      "grad_norm": 3873.123779296875,
      "learning_rate": 5.3611971104231173e-05,
      "loss": 14.9611,
      "step": 9590
    },
    {
      "epoch": 9.6,
      "grad_norm": 13551.1875,
      "learning_rate": 5.360681114551084e-05,
      "loss": 14.8588,
      "step": 9591
    },
    {
      "epoch": 9.6,
      "grad_norm": 4016.113037109375,
      "learning_rate": 5.3601651186790516e-05,
      "loss": 17.5902,
      "step": 9592
    },
    {
      "epoch": 9.6,
      "grad_norm": 9541.7041015625,
      "learning_rate": 5.359649122807018e-05,
      "loss": 17.5476,
      "step": 9593
    },
    {
      "epoch": 9.6,
      "grad_norm": 14768.1552734375,
      "learning_rate": 5.3591331269349844e-05,
      "loss": 20.4437,
      "step": 9594
    },
    {
      "epoch": 9.6,
      "grad_norm": 5588.00146484375,
      "learning_rate": 5.358617131062952e-05,
      "loss": 15.5296,
      "step": 9595
    },
    {
      "epoch": 9.61,
      "grad_norm": 49157.79296875,
      "learning_rate": 5.358101135190918e-05,
      "loss": 19.2986,
      "step": 9596
    },
    {
      "epoch": 9.61,
      "grad_norm": 1634.7406005859375,
      "learning_rate": 5.357585139318886e-05,
      "loss": 11.981,
      "step": 9597
    },
    {
      "epoch": 9.61,
      "grad_norm": 53822.98828125,
      "learning_rate": 5.357069143446852e-05,
      "loss": 14.638,
      "step": 9598
    },
    {
      "epoch": 9.61,
      "grad_norm": 8959.7470703125,
      "learning_rate": 5.35655314757482e-05,
      "loss": 12.5125,
      "step": 9599
    },
    {
      "epoch": 9.61,
      "grad_norm": 24288.36328125,
      "learning_rate": 5.3560371517027864e-05,
      "loss": 17.327,
      "step": 9600
    },
    {
      "epoch": 9.61,
      "grad_norm": 1368.470703125,
      "learning_rate": 5.355521155830754e-05,
      "loss": 13.2608,
      "step": 9601
    },
    {
      "epoch": 9.61,
      "grad_norm": 1207.4932861328125,
      "learning_rate": 5.35500515995872e-05,
      "loss": 13.2757,
      "step": 9602
    },
    {
      "epoch": 9.61,
      "grad_norm": 3227.00390625,
      "learning_rate": 5.354489164086688e-05,
      "loss": 19.29,
      "step": 9603
    },
    {
      "epoch": 9.61,
      "grad_norm": 46072.9765625,
      "learning_rate": 5.353973168214654e-05,
      "loss": 30.8272,
      "step": 9604
    },
    {
      "epoch": 9.61,
      "grad_norm": 19427.880859375,
      "learning_rate": 5.353457172342622e-05,
      "loss": 16.0664,
      "step": 9605
    },
    {
      "epoch": 9.62,
      "grad_norm": 14961.4306640625,
      "learning_rate": 5.3529411764705884e-05,
      "loss": 13.9609,
      "step": 9606
    },
    {
      "epoch": 9.62,
      "grad_norm": 4524.60107421875,
      "learning_rate": 5.352425180598556e-05,
      "loss": 15.8817,
      "step": 9607
    },
    {
      "epoch": 9.62,
      "grad_norm": 15655.361328125,
      "learning_rate": 5.351909184726523e-05,
      "loss": 12.7881,
      "step": 9608
    },
    {
      "epoch": 9.62,
      "grad_norm": 6529.38671875,
      "learning_rate": 5.35139318885449e-05,
      "loss": 18.232,
      "step": 9609
    },
    {
      "epoch": 9.62,
      "grad_norm": 4208.25439453125,
      "learning_rate": 5.350877192982456e-05,
      "loss": 24.5518,
      "step": 9610
    },
    {
      "epoch": 9.62,
      "grad_norm": 5451.59130859375,
      "learning_rate": 5.350361197110423e-05,
      "loss": 14.6781,
      "step": 9611
    },
    {
      "epoch": 9.62,
      "grad_norm": 2981.169921875,
      "learning_rate": 5.3498452012383904e-05,
      "loss": 13.5345,
      "step": 9612
    },
    {
      "epoch": 9.62,
      "grad_norm": 9453.3466796875,
      "learning_rate": 5.3493292053663565e-05,
      "loss": 19.3651,
      "step": 9613
    },
    {
      "epoch": 9.62,
      "grad_norm": 10029.7333984375,
      "learning_rate": 5.348813209494325e-05,
      "loss": 16.2154,
      "step": 9614
    },
    {
      "epoch": 9.62,
      "grad_norm": 4933.43408203125,
      "learning_rate": 5.348297213622291e-05,
      "loss": 17.9462,
      "step": 9615
    },
    {
      "epoch": 9.63,
      "grad_norm": 8587.5322265625,
      "learning_rate": 5.347781217750258e-05,
      "loss": 16.9079,
      "step": 9616
    },
    {
      "epoch": 9.63,
      "grad_norm": 147972.234375,
      "learning_rate": 5.347265221878225e-05,
      "loss": 17.1618,
      "step": 9617
    },
    {
      "epoch": 9.63,
      "grad_norm": 12520.5732421875,
      "learning_rate": 5.3467492260061924e-05,
      "loss": 14.2387,
      "step": 9618
    },
    {
      "epoch": 9.63,
      "grad_norm": 2910.13818359375,
      "learning_rate": 5.346233230134159e-05,
      "loss": 14.844,
      "step": 9619
    },
    {
      "epoch": 9.63,
      "grad_norm": 5529.8701171875,
      "learning_rate": 5.345717234262127e-05,
      "loss": 17.993,
      "step": 9620
    },
    {
      "epoch": 9.63,
      "grad_norm": 933.3565063476562,
      "learning_rate": 5.345201238390093e-05,
      "loss": 16.4186,
      "step": 9621
    },
    {
      "epoch": 9.63,
      "grad_norm": 9742.990234375,
      "learning_rate": 5.34468524251806e-05,
      "loss": 17.636,
      "step": 9622
    },
    {
      "epoch": 9.63,
      "grad_norm": 20760.99609375,
      "learning_rate": 5.344169246646027e-05,
      "loss": 14.1306,
      "step": 9623
    },
    {
      "epoch": 9.63,
      "grad_norm": 25519.736328125,
      "learning_rate": 5.3436532507739944e-05,
      "loss": 19.6256,
      "step": 9624
    },
    {
      "epoch": 9.63,
      "grad_norm": 12261.6982421875,
      "learning_rate": 5.343137254901961e-05,
      "loss": 14.5673,
      "step": 9625
    },
    {
      "epoch": 9.64,
      "grad_norm": 14391.4462890625,
      "learning_rate": 5.342621259029929e-05,
      "loss": 19.0036,
      "step": 9626
    },
    {
      "epoch": 9.64,
      "grad_norm": 163375.65625,
      "learning_rate": 5.342105263157895e-05,
      "loss": 16.3965,
      "step": 9627
    },
    {
      "epoch": 9.64,
      "grad_norm": 4031.41015625,
      "learning_rate": 5.341589267285862e-05,
      "loss": 14.4303,
      "step": 9628
    },
    {
      "epoch": 9.64,
      "grad_norm": 4660.8857421875,
      "learning_rate": 5.341073271413829e-05,
      "loss": 14.5236,
      "step": 9629
    },
    {
      "epoch": 9.64,
      "grad_norm": 11784.5078125,
      "learning_rate": 5.340557275541795e-05,
      "loss": 14.5149,
      "step": 9630
    },
    {
      "epoch": 9.64,
      "grad_norm": 89321.3515625,
      "learning_rate": 5.340041279669763e-05,
      "loss": 20.4451,
      "step": 9631
    },
    {
      "epoch": 9.64,
      "grad_norm": 15960.375,
      "learning_rate": 5.339525283797729e-05,
      "loss": 13.6969,
      "step": 9632
    },
    {
      "epoch": 9.64,
      "grad_norm": 4047.15380859375,
      "learning_rate": 5.339009287925697e-05,
      "loss": 25.6712,
      "step": 9633
    },
    {
      "epoch": 9.64,
      "grad_norm": 3758.00341796875,
      "learning_rate": 5.3384932920536635e-05,
      "loss": 15.4431,
      "step": 9634
    },
    {
      "epoch": 9.64,
      "grad_norm": 10491.2587890625,
      "learning_rate": 5.337977296181631e-05,
      "loss": 15.6627,
      "step": 9635
    },
    {
      "epoch": 9.65,
      "grad_norm": 7110.60302734375,
      "learning_rate": 5.337461300309598e-05,
      "loss": 16.5163,
      "step": 9636
    },
    {
      "epoch": 9.65,
      "grad_norm": 4382.005859375,
      "learning_rate": 5.336945304437565e-05,
      "loss": 18.0211,
      "step": 9637
    },
    {
      "epoch": 9.65,
      "grad_norm": 7178.68408203125,
      "learning_rate": 5.336429308565531e-05,
      "loss": 14.6506,
      "step": 9638
    },
    {
      "epoch": 9.65,
      "grad_norm": 2181.9697265625,
      "learning_rate": 5.335913312693499e-05,
      "loss": 12.4824,
      "step": 9639
    },
    {
      "epoch": 9.65,
      "grad_norm": 4221.74072265625,
      "learning_rate": 5.3353973168214655e-05,
      "loss": 18.3227,
      "step": 9640
    },
    {
      "epoch": 9.65,
      "grad_norm": 2754.11865234375,
      "learning_rate": 5.334881320949433e-05,
      "loss": 20.0626,
      "step": 9641
    },
    {
      "epoch": 9.65,
      "grad_norm": 2071.58935546875,
      "learning_rate": 5.3343653250774e-05,
      "loss": 15.5215,
      "step": 9642
    },
    {
      "epoch": 9.65,
      "grad_norm": 1593.350830078125,
      "learning_rate": 5.333849329205367e-05,
      "loss": 12.4013,
      "step": 9643
    },
    {
      "epoch": 9.65,
      "grad_norm": 13604.927734375,
      "learning_rate": 5.333333333333333e-05,
      "loss": 21.8091,
      "step": 9644
    },
    {
      "epoch": 9.65,
      "grad_norm": 30735.974609375,
      "learning_rate": 5.332817337461301e-05,
      "loss": 16.8522,
      "step": 9645
    },
    {
      "epoch": 9.66,
      "grad_norm": 7469.04150390625,
      "learning_rate": 5.3323013415892675e-05,
      "loss": 17.3267,
      "step": 9646
    },
    {
      "epoch": 9.66,
      "grad_norm": 1603.4093017578125,
      "learning_rate": 5.331785345717234e-05,
      "loss": 11.9337,
      "step": 9647
    },
    {
      "epoch": 9.66,
      "grad_norm": 2357.24267578125,
      "learning_rate": 5.331269349845202e-05,
      "loss": 14.629,
      "step": 9648
    },
    {
      "epoch": 9.66,
      "grad_norm": 8188.63623046875,
      "learning_rate": 5.330753353973168e-05,
      "loss": 15.334,
      "step": 9649
    },
    {
      "epoch": 9.66,
      "grad_norm": 14093.625,
      "learning_rate": 5.330237358101135e-05,
      "loss": 20.1037,
      "step": 9650
    },
    {
      "epoch": 9.66,
      "grad_norm": 1728.466064453125,
      "learning_rate": 5.329721362229102e-05,
      "loss": 14.359,
      "step": 9651
    },
    {
      "epoch": 9.66,
      "grad_norm": 4615.8369140625,
      "learning_rate": 5.3292053663570695e-05,
      "loss": 19.8691,
      "step": 9652
    },
    {
      "epoch": 9.66,
      "grad_norm": 4376.91162109375,
      "learning_rate": 5.328689370485036e-05,
      "loss": 17.3376,
      "step": 9653
    },
    {
      "epoch": 9.66,
      "grad_norm": 8776.01171875,
      "learning_rate": 5.328173374613004e-05,
      "loss": 17.1699,
      "step": 9654
    },
    {
      "epoch": 9.66,
      "grad_norm": 7153.84716796875,
      "learning_rate": 5.32765737874097e-05,
      "loss": 17.2508,
      "step": 9655
    },
    {
      "epoch": 9.67,
      "grad_norm": 8746.2666015625,
      "learning_rate": 5.327141382868937e-05,
      "loss": 15.3772,
      "step": 9656
    },
    {
      "epoch": 9.67,
      "grad_norm": 5562.0732421875,
      "learning_rate": 5.326625386996904e-05,
      "loss": 16.975,
      "step": 9657
    },
    {
      "epoch": 9.67,
      "grad_norm": 2571.497314453125,
      "learning_rate": 5.3261093911248715e-05,
      "loss": 20.4338,
      "step": 9658
    },
    {
      "epoch": 9.67,
      "grad_norm": 14710.5673828125,
      "learning_rate": 5.325593395252838e-05,
      "loss": 22.7176,
      "step": 9659
    },
    {
      "epoch": 9.67,
      "grad_norm": 32061.37109375,
      "learning_rate": 5.325077399380806e-05,
      "loss": 15.734,
      "step": 9660
    },
    {
      "epoch": 9.67,
      "grad_norm": 5281.64892578125,
      "learning_rate": 5.324561403508772e-05,
      "loss": 14.6585,
      "step": 9661
    },
    {
      "epoch": 9.67,
      "grad_norm": 4217.359375,
      "learning_rate": 5.324045407636739e-05,
      "loss": 15.3146,
      "step": 9662
    },
    {
      "epoch": 9.67,
      "grad_norm": 5919.43115234375,
      "learning_rate": 5.323529411764706e-05,
      "loss": 16.4356,
      "step": 9663
    },
    {
      "epoch": 9.67,
      "grad_norm": 5043.595703125,
      "learning_rate": 5.3230134158926735e-05,
      "loss": 12.3156,
      "step": 9664
    },
    {
      "epoch": 9.67,
      "grad_norm": 11136.3388671875,
      "learning_rate": 5.32249742002064e-05,
      "loss": 16.3252,
      "step": 9665
    },
    {
      "epoch": 9.68,
      "grad_norm": 24645.53125,
      "learning_rate": 5.3219814241486064e-05,
      "loss": 19.4659,
      "step": 9666
    },
    {
      "epoch": 9.68,
      "grad_norm": 15416.4365234375,
      "learning_rate": 5.321465428276574e-05,
      "loss": 17.065,
      "step": 9667
    },
    {
      "epoch": 9.68,
      "grad_norm": 3065.44140625,
      "learning_rate": 5.3209494324045406e-05,
      "loss": 14.8213,
      "step": 9668
    },
    {
      "epoch": 9.68,
      "grad_norm": 6518.01025390625,
      "learning_rate": 5.320433436532508e-05,
      "loss": 12.3418,
      "step": 9669
    },
    {
      "epoch": 9.68,
      "grad_norm": 4437.09765625,
      "learning_rate": 5.319917440660475e-05,
      "loss": 13.5788,
      "step": 9670
    },
    {
      "epoch": 9.68,
      "grad_norm": 4669.76708984375,
      "learning_rate": 5.319401444788442e-05,
      "loss": 13.6142,
      "step": 9671
    },
    {
      "epoch": 9.68,
      "grad_norm": 4857.15966796875,
      "learning_rate": 5.3188854489164084e-05,
      "loss": 15.2183,
      "step": 9672
    },
    {
      "epoch": 9.68,
      "grad_norm": 3617.697021484375,
      "learning_rate": 5.318369453044376e-05,
      "loss": 13.4116,
      "step": 9673
    },
    {
      "epoch": 9.68,
      "grad_norm": 1886.70556640625,
      "learning_rate": 5.3178534571723426e-05,
      "loss": 13.1291,
      "step": 9674
    },
    {
      "epoch": 9.68,
      "grad_norm": 15249.4365234375,
      "learning_rate": 5.31733746130031e-05,
      "loss": 18.4255,
      "step": 9675
    },
    {
      "epoch": 9.69,
      "grad_norm": 30040.412109375,
      "learning_rate": 5.316821465428277e-05,
      "loss": 17.2537,
      "step": 9676
    },
    {
      "epoch": 9.69,
      "grad_norm": 5672.54052734375,
      "learning_rate": 5.316305469556244e-05,
      "loss": 13.2808,
      "step": 9677
    },
    {
      "epoch": 9.69,
      "grad_norm": 6203.80712890625,
      "learning_rate": 5.3157894736842104e-05,
      "loss": 14.4457,
      "step": 9678
    },
    {
      "epoch": 9.69,
      "grad_norm": 3199.793212890625,
      "learning_rate": 5.3152734778121785e-05,
      "loss": 15.8663,
      "step": 9679
    },
    {
      "epoch": 9.69,
      "grad_norm": 2562.11328125,
      "learning_rate": 5.3147574819401446e-05,
      "loss": 16.3334,
      "step": 9680
    },
    {
      "epoch": 9.69,
      "grad_norm": 6444.08642578125,
      "learning_rate": 5.314241486068112e-05,
      "loss": 18.933,
      "step": 9681
    },
    {
      "epoch": 9.69,
      "grad_norm": 18541.13671875,
      "learning_rate": 5.313725490196079e-05,
      "loss": 16.3796,
      "step": 9682
    },
    {
      "epoch": 9.69,
      "grad_norm": 17264.62890625,
      "learning_rate": 5.313209494324045e-05,
      "loss": 15.6997,
      "step": 9683
    },
    {
      "epoch": 9.69,
      "grad_norm": 18302.150390625,
      "learning_rate": 5.3126934984520124e-05,
      "loss": 16.8509,
      "step": 9684
    },
    {
      "epoch": 9.69,
      "grad_norm": 33646.19921875,
      "learning_rate": 5.312177502579979e-05,
      "loss": 14.1994,
      "step": 9685
    },
    {
      "epoch": 9.7,
      "grad_norm": 32181.642578125,
      "learning_rate": 5.3116615067079466e-05,
      "loss": 17.72,
      "step": 9686
    },
    {
      "epoch": 9.7,
      "grad_norm": 19578.4140625,
      "learning_rate": 5.3111455108359134e-05,
      "loss": 13.1197,
      "step": 9687
    },
    {
      "epoch": 9.7,
      "grad_norm": 5763.19970703125,
      "learning_rate": 5.310629514963881e-05,
      "loss": 14.6035,
      "step": 9688
    },
    {
      "epoch": 9.7,
      "grad_norm": 12771.9345703125,
      "learning_rate": 5.310113519091847e-05,
      "loss": 12.8686,
      "step": 9689
    },
    {
      "epoch": 9.7,
      "grad_norm": 903.6282958984375,
      "learning_rate": 5.3095975232198144e-05,
      "loss": 13.2113,
      "step": 9690
    },
    {
      "epoch": 9.7,
      "grad_norm": 18839.77734375,
      "learning_rate": 5.309081527347781e-05,
      "loss": 16.2935,
      "step": 9691
    },
    {
      "epoch": 9.7,
      "grad_norm": 167779.40625,
      "learning_rate": 5.3085655314757486e-05,
      "loss": 20.5577,
      "step": 9692
    },
    {
      "epoch": 9.7,
      "grad_norm": 4891.22998046875,
      "learning_rate": 5.3080495356037154e-05,
      "loss": 14.3805,
      "step": 9693
    },
    {
      "epoch": 9.7,
      "grad_norm": 23449.296875,
      "learning_rate": 5.307533539731683e-05,
      "loss": 21.3197,
      "step": 9694
    },
    {
      "epoch": 9.7,
      "grad_norm": 4654.2548828125,
      "learning_rate": 5.307017543859649e-05,
      "loss": 14.5527,
      "step": 9695
    },
    {
      "epoch": 9.71,
      "grad_norm": 21416.091796875,
      "learning_rate": 5.306501547987617e-05,
      "loss": 10.9848,
      "step": 9696
    },
    {
      "epoch": 9.71,
      "grad_norm": 19663.876953125,
      "learning_rate": 5.305985552115583e-05,
      "loss": 14.6559,
      "step": 9697
    },
    {
      "epoch": 9.71,
      "grad_norm": 25789.666015625,
      "learning_rate": 5.3054695562435506e-05,
      "loss": 15.0254,
      "step": 9698
    },
    {
      "epoch": 9.71,
      "grad_norm": 8804.74609375,
      "learning_rate": 5.3049535603715174e-05,
      "loss": 15.1957,
      "step": 9699
    },
    {
      "epoch": 9.71,
      "grad_norm": 6659.455078125,
      "learning_rate": 5.304437564499485e-05,
      "loss": 18.8545,
      "step": 9700
    },
    {
      "epoch": 9.71,
      "grad_norm": 4289.63037109375,
      "learning_rate": 5.303921568627451e-05,
      "loss": 15.8469,
      "step": 9701
    },
    {
      "epoch": 9.71,
      "grad_norm": 5307.021484375,
      "learning_rate": 5.303405572755418e-05,
      "loss": 16.829,
      "step": 9702
    },
    {
      "epoch": 9.71,
      "grad_norm": 6127.39013671875,
      "learning_rate": 5.302889576883385e-05,
      "loss": 14.7339,
      "step": 9703
    },
    {
      "epoch": 9.71,
      "grad_norm": 7135.765625,
      "learning_rate": 5.302373581011352e-05,
      "loss": 16.2319,
      "step": 9704
    },
    {
      "epoch": 9.71,
      "grad_norm": 3827.7509765625,
      "learning_rate": 5.3018575851393194e-05,
      "loss": 16.8766,
      "step": 9705
    },
    {
      "epoch": 9.72,
      "grad_norm": 6794.9423828125,
      "learning_rate": 5.3013415892672855e-05,
      "loss": 17.3294,
      "step": 9706
    },
    {
      "epoch": 9.72,
      "grad_norm": 4101.5263671875,
      "learning_rate": 5.3008255933952536e-05,
      "loss": 14.7823,
      "step": 9707
    },
    {
      "epoch": 9.72,
      "grad_norm": 8878.25390625,
      "learning_rate": 5.30030959752322e-05,
      "loss": 22.2483,
      "step": 9708
    },
    {
      "epoch": 9.72,
      "grad_norm": 13522.435546875,
      "learning_rate": 5.299793601651187e-05,
      "loss": 14.9421,
      "step": 9709
    },
    {
      "epoch": 9.72,
      "grad_norm": 5265.68798828125,
      "learning_rate": 5.299277605779154e-05,
      "loss": 15.5869,
      "step": 9710
    },
    {
      "epoch": 9.72,
      "grad_norm": 4136.1923828125,
      "learning_rate": 5.2987616099071214e-05,
      "loss": 18.9974,
      "step": 9711
    },
    {
      "epoch": 9.72,
      "grad_norm": 31796.71484375,
      "learning_rate": 5.2982456140350875e-05,
      "loss": 22.0011,
      "step": 9712
    },
    {
      "epoch": 9.72,
      "grad_norm": 9019.568359375,
      "learning_rate": 5.2977296181630556e-05,
      "loss": 14.3258,
      "step": 9713
    },
    {
      "epoch": 9.72,
      "grad_norm": 20142.953125,
      "learning_rate": 5.297213622291022e-05,
      "loss": 20.3424,
      "step": 9714
    },
    {
      "epoch": 9.72,
      "grad_norm": 7441.9287109375,
      "learning_rate": 5.296697626418989e-05,
      "loss": 19.249,
      "step": 9715
    },
    {
      "epoch": 9.73,
      "grad_norm": 3266.864013671875,
      "learning_rate": 5.296181630546956e-05,
      "loss": 14.3334,
      "step": 9716
    },
    {
      "epoch": 9.73,
      "grad_norm": 492570.03125,
      "learning_rate": 5.2956656346749234e-05,
      "loss": 15.1385,
      "step": 9717
    },
    {
      "epoch": 9.73,
      "grad_norm": 10677.037109375,
      "learning_rate": 5.2951496388028895e-05,
      "loss": 13.8139,
      "step": 9718
    },
    {
      "epoch": 9.73,
      "grad_norm": 2088.135986328125,
      "learning_rate": 5.294633642930856e-05,
      "loss": 13.6776,
      "step": 9719
    },
    {
      "epoch": 9.73,
      "grad_norm": 6088.4140625,
      "learning_rate": 5.294117647058824e-05,
      "loss": 17.1666,
      "step": 9720
    },
    {
      "epoch": 9.73,
      "grad_norm": 13694.478515625,
      "learning_rate": 5.2936016511867905e-05,
      "loss": 17.6387,
      "step": 9721
    },
    {
      "epoch": 9.73,
      "grad_norm": 4790.13916015625,
      "learning_rate": 5.293085655314758e-05,
      "loss": 18.0234,
      "step": 9722
    },
    {
      "epoch": 9.73,
      "grad_norm": 13879.978515625,
      "learning_rate": 5.292569659442724e-05,
      "loss": 17.0997,
      "step": 9723
    },
    {
      "epoch": 9.73,
      "grad_norm": 1192.6204833984375,
      "learning_rate": 5.292053663570692e-05,
      "loss": 18.5321,
      "step": 9724
    },
    {
      "epoch": 9.73,
      "grad_norm": 3331.3857421875,
      "learning_rate": 5.291537667698658e-05,
      "loss": 13.0505,
      "step": 9725
    },
    {
      "epoch": 9.74,
      "grad_norm": 5889.595703125,
      "learning_rate": 5.291021671826626e-05,
      "loss": 18.1124,
      "step": 9726
    },
    {
      "epoch": 9.74,
      "grad_norm": 11175.912109375,
      "learning_rate": 5.2905056759545925e-05,
      "loss": 14.612,
      "step": 9727
    },
    {
      "epoch": 9.74,
      "grad_norm": 9340.8232421875,
      "learning_rate": 5.28998968008256e-05,
      "loss": 16.5442,
      "step": 9728
    },
    {
      "epoch": 9.74,
      "grad_norm": 4082.293212890625,
      "learning_rate": 5.289473684210526e-05,
      "loss": 13.6982,
      "step": 9729
    },
    {
      "epoch": 9.74,
      "grad_norm": 7872.6552734375,
      "learning_rate": 5.288957688338494e-05,
      "loss": 17.2617,
      "step": 9730
    },
    {
      "epoch": 9.74,
      "grad_norm": 27155.8515625,
      "learning_rate": 5.28844169246646e-05,
      "loss": 18.6966,
      "step": 9731
    },
    {
      "epoch": 9.74,
      "grad_norm": 6136.17333984375,
      "learning_rate": 5.287925696594428e-05,
      "loss": 12.6612,
      "step": 9732
    },
    {
      "epoch": 9.74,
      "grad_norm": 1970.566162109375,
      "learning_rate": 5.2874097007223945e-05,
      "loss": 15.6486,
      "step": 9733
    },
    {
      "epoch": 9.74,
      "grad_norm": 3185.944580078125,
      "learning_rate": 5.286893704850362e-05,
      "loss": 16.7177,
      "step": 9734
    },
    {
      "epoch": 9.74,
      "grad_norm": 5705.29541015625,
      "learning_rate": 5.286377708978328e-05,
      "loss": 16.237,
      "step": 9735
    },
    {
      "epoch": 9.75,
      "grad_norm": 2300.912353515625,
      "learning_rate": 5.285861713106295e-05,
      "loss": 17.37,
      "step": 9736
    },
    {
      "epoch": 9.75,
      "grad_norm": 3505.850341796875,
      "learning_rate": 5.285345717234262e-05,
      "loss": 17.6324,
      "step": 9737
    },
    {
      "epoch": 9.75,
      "grad_norm": 3006.976318359375,
      "learning_rate": 5.284829721362229e-05,
      "loss": 17.5808,
      "step": 9738
    },
    {
      "epoch": 9.75,
      "grad_norm": 10865.22265625,
      "learning_rate": 5.2843137254901965e-05,
      "loss": 22.9285,
      "step": 9739
    },
    {
      "epoch": 9.75,
      "grad_norm": 2425.563232421875,
      "learning_rate": 5.2837977296181626e-05,
      "loss": 12.6659,
      "step": 9740
    },
    {
      "epoch": 9.75,
      "grad_norm": 18975.1640625,
      "learning_rate": 5.283281733746131e-05,
      "loss": 13.4835,
      "step": 9741
    },
    {
      "epoch": 9.75,
      "grad_norm": 6577.72509765625,
      "learning_rate": 5.282765737874097e-05,
      "loss": 19.1735,
      "step": 9742
    },
    {
      "epoch": 9.75,
      "grad_norm": 7337.830078125,
      "learning_rate": 5.282249742002064e-05,
      "loss": 15.9822,
      "step": 9743
    },
    {
      "epoch": 9.75,
      "grad_norm": 1441.0399169921875,
      "learning_rate": 5.281733746130031e-05,
      "loss": 15.7429,
      "step": 9744
    },
    {
      "epoch": 9.75,
      "grad_norm": 6261.75634765625,
      "learning_rate": 5.2812177502579985e-05,
      "loss": 17.4225,
      "step": 9745
    },
    {
      "epoch": 9.76,
      "grad_norm": 17152.4453125,
      "learning_rate": 5.2807017543859646e-05,
      "loss": 16.2178,
      "step": 9746
    },
    {
      "epoch": 9.76,
      "grad_norm": 7077.90966796875,
      "learning_rate": 5.280185758513933e-05,
      "loss": 14.7138,
      "step": 9747
    },
    {
      "epoch": 9.76,
      "grad_norm": 15279.404296875,
      "learning_rate": 5.279669762641899e-05,
      "loss": 16.8222,
      "step": 9748
    },
    {
      "epoch": 9.76,
      "grad_norm": 8787.673828125,
      "learning_rate": 5.279153766769866e-05,
      "loss": 16.7603,
      "step": 9749
    },
    {
      "epoch": 9.76,
      "grad_norm": 2126.00244140625,
      "learning_rate": 5.278637770897833e-05,
      "loss": 17.5144,
      "step": 9750
    },
    {
      "epoch": 9.76,
      "grad_norm": 31492.13671875,
      "learning_rate": 5.2781217750258005e-05,
      "loss": 14.5205,
      "step": 9751
    },
    {
      "epoch": 9.76,
      "grad_norm": 2076.95263671875,
      "learning_rate": 5.277605779153767e-05,
      "loss": 14.5357,
      "step": 9752
    },
    {
      "epoch": 9.76,
      "grad_norm": 19040.61328125,
      "learning_rate": 5.277089783281735e-05,
      "loss": 12.239,
      "step": 9753
    },
    {
      "epoch": 9.76,
      "grad_norm": 53210.67578125,
      "learning_rate": 5.276573787409701e-05,
      "loss": 18.3237,
      "step": 9754
    },
    {
      "epoch": 9.76,
      "grad_norm": 11678.119140625,
      "learning_rate": 5.2760577915376676e-05,
      "loss": 18.3793,
      "step": 9755
    },
    {
      "epoch": 9.77,
      "grad_norm": 36519.15234375,
      "learning_rate": 5.275541795665635e-05,
      "loss": 14.458,
      "step": 9756
    },
    {
      "epoch": 9.77,
      "grad_norm": 5632.01220703125,
      "learning_rate": 5.275025799793601e-05,
      "loss": 15.8466,
      "step": 9757
    },
    {
      "epoch": 9.77,
      "grad_norm": 8188.7822265625,
      "learning_rate": 5.274509803921569e-05,
      "loss": 16.0092,
      "step": 9758
    },
    {
      "epoch": 9.77,
      "grad_norm": 2146.1962890625,
      "learning_rate": 5.2739938080495354e-05,
      "loss": 12.9905,
      "step": 9759
    },
    {
      "epoch": 9.77,
      "grad_norm": 7501.8681640625,
      "learning_rate": 5.273477812177503e-05,
      "loss": 15.7619,
      "step": 9760
    },
    {
      "epoch": 9.77,
      "grad_norm": 9584.2626953125,
      "learning_rate": 5.2729618163054696e-05,
      "loss": 16.7417,
      "step": 9761
    },
    {
      "epoch": 9.77,
      "grad_norm": 25332.0625,
      "learning_rate": 5.272445820433437e-05,
      "loss": 18.827,
      "step": 9762
    },
    {
      "epoch": 9.77,
      "grad_norm": 7915.7021484375,
      "learning_rate": 5.271929824561403e-05,
      "loss": 17.2091,
      "step": 9763
    },
    {
      "epoch": 9.77,
      "grad_norm": 56788.6015625,
      "learning_rate": 5.271413828689371e-05,
      "loss": 23.1534,
      "step": 9764
    },
    {
      "epoch": 9.77,
      "grad_norm": 51925.64453125,
      "learning_rate": 5.2708978328173374e-05,
      "loss": 19.0689,
      "step": 9765
    },
    {
      "epoch": 9.78,
      "grad_norm": 1026.4918212890625,
      "learning_rate": 5.270381836945305e-05,
      "loss": 12.5617,
      "step": 9766
    },
    {
      "epoch": 9.78,
      "grad_norm": 6388.9716796875,
      "learning_rate": 5.2698658410732716e-05,
      "loss": 18.0907,
      "step": 9767
    },
    {
      "epoch": 9.78,
      "grad_norm": 21426.193359375,
      "learning_rate": 5.269349845201239e-05,
      "loss": 16.3283,
      "step": 9768
    },
    {
      "epoch": 9.78,
      "grad_norm": 29906.408203125,
      "learning_rate": 5.268833849329206e-05,
      "loss": 13.5128,
      "step": 9769
    },
    {
      "epoch": 9.78,
      "grad_norm": 7293.2666015625,
      "learning_rate": 5.268317853457173e-05,
      "loss": 14.2573,
      "step": 9770
    },
    {
      "epoch": 9.78,
      "grad_norm": 4443.03271484375,
      "learning_rate": 5.2678018575851394e-05,
      "loss": 21.0697,
      "step": 9771
    },
    {
      "epoch": 9.78,
      "grad_norm": 56712.9375,
      "learning_rate": 5.267285861713106e-05,
      "loss": 15.8233,
      "step": 9772
    },
    {
      "epoch": 9.78,
      "grad_norm": 54877.3046875,
      "learning_rate": 5.2667698658410736e-05,
      "loss": 14.0408,
      "step": 9773
    },
    {
      "epoch": 9.78,
      "grad_norm": 2083.6875,
      "learning_rate": 5.26625386996904e-05,
      "loss": 15.9597,
      "step": 9774
    },
    {
      "epoch": 9.78,
      "grad_norm": 10453.0927734375,
      "learning_rate": 5.265737874097008e-05,
      "loss": 24.4374,
      "step": 9775
    },
    {
      "epoch": 9.79,
      "grad_norm": 7844.1103515625,
      "learning_rate": 5.265221878224974e-05,
      "loss": 13.9107,
      "step": 9776
    },
    {
      "epoch": 9.79,
      "grad_norm": 7575.6572265625,
      "learning_rate": 5.2647058823529414e-05,
      "loss": 13.1008,
      "step": 9777
    },
    {
      "epoch": 9.79,
      "grad_norm": 61347.1875,
      "learning_rate": 5.264189886480908e-05,
      "loss": 17.5456,
      "step": 9778
    },
    {
      "epoch": 9.79,
      "grad_norm": 26085.845703125,
      "learning_rate": 5.2636738906088756e-05,
      "loss": 13.2676,
      "step": 9779
    },
    {
      "epoch": 9.79,
      "grad_norm": 2638.82861328125,
      "learning_rate": 5.2631578947368424e-05,
      "loss": 13.769,
      "step": 9780
    },
    {
      "epoch": 9.79,
      "grad_norm": 5741.64208984375,
      "learning_rate": 5.26264189886481e-05,
      "loss": 15.1892,
      "step": 9781
    },
    {
      "epoch": 9.79,
      "grad_norm": 1982.825439453125,
      "learning_rate": 5.262125902992776e-05,
      "loss": 12.667,
      "step": 9782
    },
    {
      "epoch": 9.79,
      "grad_norm": 3983.56005859375,
      "learning_rate": 5.2616099071207434e-05,
      "loss": 14.7719,
      "step": 9783
    },
    {
      "epoch": 9.79,
      "grad_norm": 1622.7088623046875,
      "learning_rate": 5.26109391124871e-05,
      "loss": 15.4763,
      "step": 9784
    },
    {
      "epoch": 9.79,
      "grad_norm": 4932.576171875,
      "learning_rate": 5.2605779153766776e-05,
      "loss": 18.9072,
      "step": 9785
    },
    {
      "epoch": 9.8,
      "grad_norm": 3093.048583984375,
      "learning_rate": 5.2600619195046444e-05,
      "loss": 17.2002,
      "step": 9786
    },
    {
      "epoch": 9.8,
      "grad_norm": 5235.72509765625,
      "learning_rate": 5.259545923632612e-05,
      "loss": 20.2401,
      "step": 9787
    },
    {
      "epoch": 9.8,
      "grad_norm": 11246.6904296875,
      "learning_rate": 5.259029927760578e-05,
      "loss": 14.074,
      "step": 9788
    },
    {
      "epoch": 9.8,
      "grad_norm": 8932.138671875,
      "learning_rate": 5.2585139318885454e-05,
      "loss": 14.0603,
      "step": 9789
    },
    {
      "epoch": 9.8,
      "grad_norm": 48853.72265625,
      "learning_rate": 5.257997936016512e-05,
      "loss": 13.3613,
      "step": 9790
    },
    {
      "epoch": 9.8,
      "grad_norm": 18721.818359375,
      "learning_rate": 5.257481940144478e-05,
      "loss": 13.7877,
      "step": 9791
    },
    {
      "epoch": 9.8,
      "grad_norm": 11366.6025390625,
      "learning_rate": 5.2569659442724464e-05,
      "loss": 15.3139,
      "step": 9792
    },
    {
      "epoch": 9.8,
      "grad_norm": 14256.75,
      "learning_rate": 5.2564499484004125e-05,
      "loss": 17.1219,
      "step": 9793
    },
    {
      "epoch": 9.8,
      "grad_norm": 6475.99755859375,
      "learning_rate": 5.25593395252838e-05,
      "loss": 18.3564,
      "step": 9794
    },
    {
      "epoch": 9.8,
      "grad_norm": 1029.8931884765625,
      "learning_rate": 5.255417956656347e-05,
      "loss": 15.8694,
      "step": 9795
    },
    {
      "epoch": 9.81,
      "grad_norm": 10632.185546875,
      "learning_rate": 5.254901960784314e-05,
      "loss": 15.9413,
      "step": 9796
    },
    {
      "epoch": 9.81,
      "grad_norm": 809.1578979492188,
      "learning_rate": 5.254385964912281e-05,
      "loss": 13.0287,
      "step": 9797
    },
    {
      "epoch": 9.81,
      "grad_norm": 15612.9541015625,
      "learning_rate": 5.2538699690402484e-05,
      "loss": 15.4167,
      "step": 9798
    },
    {
      "epoch": 9.81,
      "grad_norm": 1708.686279296875,
      "learning_rate": 5.2533539731682145e-05,
      "loss": 12.9009,
      "step": 9799
    },
    {
      "epoch": 9.81,
      "grad_norm": 7795.0126953125,
      "learning_rate": 5.252837977296182e-05,
      "loss": 13.9674,
      "step": 9800
    },
    {
      "epoch": 9.81,
      "grad_norm": 5902.9697265625,
      "learning_rate": 5.252321981424149e-05,
      "loss": 18.3005,
      "step": 9801
    },
    {
      "epoch": 9.81,
      "grad_norm": 7833.5107421875,
      "learning_rate": 5.251805985552116e-05,
      "loss": 12.9841,
      "step": 9802
    },
    {
      "epoch": 9.81,
      "grad_norm": 9536.376953125,
      "learning_rate": 5.251289989680083e-05,
      "loss": 13.6667,
      "step": 9803
    },
    {
      "epoch": 9.81,
      "grad_norm": 2090.15966796875,
      "learning_rate": 5.2507739938080504e-05,
      "loss": 18.3909,
      "step": 9804
    },
    {
      "epoch": 9.81,
      "grad_norm": 63140.55078125,
      "learning_rate": 5.2502579979360165e-05,
      "loss": 14.2934,
      "step": 9805
    },
    {
      "epoch": 9.82,
      "grad_norm": 3738.230224609375,
      "learning_rate": 5.249742002063984e-05,
      "loss": 16.1852,
      "step": 9806
    },
    {
      "epoch": 9.82,
      "grad_norm": 11737.9130859375,
      "learning_rate": 5.249226006191951e-05,
      "loss": 15.2275,
      "step": 9807
    },
    {
      "epoch": 9.82,
      "grad_norm": 6530.74658203125,
      "learning_rate": 5.2487100103199175e-05,
      "loss": 17.2572,
      "step": 9808
    },
    {
      "epoch": 9.82,
      "grad_norm": 1230.444091796875,
      "learning_rate": 5.248194014447885e-05,
      "loss": 13.7783,
      "step": 9809
    },
    {
      "epoch": 9.82,
      "grad_norm": 7236.205078125,
      "learning_rate": 5.247678018575851e-05,
      "loss": 20.362,
      "step": 9810
    },
    {
      "epoch": 9.82,
      "grad_norm": 17079.037109375,
      "learning_rate": 5.2471620227038185e-05,
      "loss": 15.4006,
      "step": 9811
    },
    {
      "epoch": 9.82,
      "grad_norm": 24493.462890625,
      "learning_rate": 5.246646026831785e-05,
      "loss": 15.0102,
      "step": 9812
    },
    {
      "epoch": 9.82,
      "grad_norm": 2694.21435546875,
      "learning_rate": 5.246130030959753e-05,
      "loss": 13.4699,
      "step": 9813
    },
    {
      "epoch": 9.82,
      "grad_norm": 51960.71484375,
      "learning_rate": 5.2456140350877195e-05,
      "loss": 15.4286,
      "step": 9814
    },
    {
      "epoch": 9.82,
      "grad_norm": 7828.24951171875,
      "learning_rate": 5.245098039215687e-05,
      "loss": 15.0434,
      "step": 9815
    },
    {
      "epoch": 9.83,
      "grad_norm": 964.6311645507812,
      "learning_rate": 5.244582043343653e-05,
      "loss": 16.5389,
      "step": 9816
    },
    {
      "epoch": 9.83,
      "grad_norm": 33451.51171875,
      "learning_rate": 5.2440660474716205e-05,
      "loss": 14.9168,
      "step": 9817
    },
    {
      "epoch": 9.83,
      "grad_norm": 115821.734375,
      "learning_rate": 5.243550051599587e-05,
      "loss": 20.5928,
      "step": 9818
    },
    {
      "epoch": 9.83,
      "grad_norm": 15561.0341796875,
      "learning_rate": 5.243034055727555e-05,
      "loss": 13.8497,
      "step": 9819
    },
    {
      "epoch": 9.83,
      "grad_norm": 14321.419921875,
      "learning_rate": 5.2425180598555215e-05,
      "loss": 18.8984,
      "step": 9820
    },
    {
      "epoch": 9.83,
      "grad_norm": 9397.3349609375,
      "learning_rate": 5.242002063983489e-05,
      "loss": 17.83,
      "step": 9821
    },
    {
      "epoch": 9.83,
      "grad_norm": 10262.0205078125,
      "learning_rate": 5.241486068111455e-05,
      "loss": 14.3835,
      "step": 9822
    },
    {
      "epoch": 9.83,
      "grad_norm": 3170.7294921875,
      "learning_rate": 5.2409700722394225e-05,
      "loss": 20.1168,
      "step": 9823
    },
    {
      "epoch": 9.83,
      "grad_norm": 2865.73046875,
      "learning_rate": 5.240454076367389e-05,
      "loss": 19.3863,
      "step": 9824
    },
    {
      "epoch": 9.83,
      "grad_norm": 14813.3798828125,
      "learning_rate": 5.239938080495357e-05,
      "loss": 16.6943,
      "step": 9825
    },
    {
      "epoch": 9.84,
      "grad_norm": 11912.5283203125,
      "learning_rate": 5.2394220846233235e-05,
      "loss": 13.0899,
      "step": 9826
    },
    {
      "epoch": 9.84,
      "grad_norm": 34025.1484375,
      "learning_rate": 5.2389060887512896e-05,
      "loss": 18.8462,
      "step": 9827
    },
    {
      "epoch": 9.84,
      "grad_norm": 29004.193359375,
      "learning_rate": 5.238390092879257e-05,
      "loss": 15.2,
      "step": 9828
    },
    {
      "epoch": 9.84,
      "grad_norm": 17462.791015625,
      "learning_rate": 5.237874097007224e-05,
      "loss": 18.5401,
      "step": 9829
    },
    {
      "epoch": 9.84,
      "grad_norm": 7631.4091796875,
      "learning_rate": 5.237358101135191e-05,
      "loss": 13.6347,
      "step": 9830
    },
    {
      "epoch": 9.84,
      "grad_norm": 7085.06494140625,
      "learning_rate": 5.236842105263158e-05,
      "loss": 20.3923,
      "step": 9831
    },
    {
      "epoch": 9.84,
      "grad_norm": 6498.8564453125,
      "learning_rate": 5.2363261093911255e-05,
      "loss": 17.9193,
      "step": 9832
    },
    {
      "epoch": 9.84,
      "grad_norm": 10085.9990234375,
      "learning_rate": 5.2358101135190916e-05,
      "loss": 18.9349,
      "step": 9833
    },
    {
      "epoch": 9.84,
      "grad_norm": 3518.410888671875,
      "learning_rate": 5.235294117647059e-05,
      "loss": 13.3409,
      "step": 9834
    },
    {
      "epoch": 9.84,
      "grad_norm": 20719.291015625,
      "learning_rate": 5.234778121775026e-05,
      "loss": 14.2511,
      "step": 9835
    },
    {
      "epoch": 9.85,
      "grad_norm": 5187.130859375,
      "learning_rate": 5.234262125902993e-05,
      "loss": 15.2917,
      "step": 9836
    },
    {
      "epoch": 9.85,
      "grad_norm": 10570.296875,
      "learning_rate": 5.23374613003096e-05,
      "loss": 15.9544,
      "step": 9837
    },
    {
      "epoch": 9.85,
      "grad_norm": 5834.0625,
      "learning_rate": 5.2332301341589275e-05,
      "loss": 16.6883,
      "step": 9838
    },
    {
      "epoch": 9.85,
      "grad_norm": 11308.048828125,
      "learning_rate": 5.2327141382868936e-05,
      "loss": 14.6358,
      "step": 9839
    },
    {
      "epoch": 9.85,
      "grad_norm": 11473.0302734375,
      "learning_rate": 5.232198142414862e-05,
      "loss": 18.5049,
      "step": 9840
    },
    {
      "epoch": 9.85,
      "grad_norm": 30432.984375,
      "learning_rate": 5.231682146542828e-05,
      "loss": 15.2285,
      "step": 9841
    },
    {
      "epoch": 9.85,
      "grad_norm": 7267.50390625,
      "learning_rate": 5.231166150670795e-05,
      "loss": 14.0238,
      "step": 9842
    },
    {
      "epoch": 9.85,
      "grad_norm": 12970.2314453125,
      "learning_rate": 5.230650154798762e-05,
      "loss": 12.6332,
      "step": 9843
    },
    {
      "epoch": 9.85,
      "grad_norm": 8887.35546875,
      "learning_rate": 5.230134158926728e-05,
      "loss": 17.0985,
      "step": 9844
    },
    {
      "epoch": 9.85,
      "grad_norm": 6181.74755859375,
      "learning_rate": 5.2296181630546956e-05,
      "loss": 15.2742,
      "step": 9845
    },
    {
      "epoch": 9.86,
      "grad_norm": 10590.6376953125,
      "learning_rate": 5.2291021671826624e-05,
      "loss": 16.7866,
      "step": 9846
    },
    {
      "epoch": 9.86,
      "grad_norm": 3687.96875,
      "learning_rate": 5.22858617131063e-05,
      "loss": 16.258,
      "step": 9847
    },
    {
      "epoch": 9.86,
      "grad_norm": 5908.34375,
      "learning_rate": 5.2280701754385966e-05,
      "loss": 14.4539,
      "step": 9848
    },
    {
      "epoch": 9.86,
      "grad_norm": 2249.138916015625,
      "learning_rate": 5.227554179566564e-05,
      "loss": 25.1449,
      "step": 9849
    },
    {
      "epoch": 9.86,
      "grad_norm": 2891.044677734375,
      "learning_rate": 5.22703818369453e-05,
      "loss": 15.9557,
      "step": 9850
    },
    {
      "epoch": 9.86,
      "grad_norm": 18332.748046875,
      "learning_rate": 5.2265221878224976e-05,
      "loss": 20.2767,
      "step": 9851
    },
    {
      "epoch": 9.86,
      "grad_norm": 15675.166015625,
      "learning_rate": 5.2260061919504644e-05,
      "loss": 17.2439,
      "step": 9852
    },
    {
      "epoch": 9.86,
      "grad_norm": 3010.999755859375,
      "learning_rate": 5.225490196078432e-05,
      "loss": 15.9006,
      "step": 9853
    },
    {
      "epoch": 9.86,
      "grad_norm": 7567.40625,
      "learning_rate": 5.2249742002063986e-05,
      "loss": 15.8888,
      "step": 9854
    },
    {
      "epoch": 9.86,
      "grad_norm": 11537.0390625,
      "learning_rate": 5.224458204334366e-05,
      "loss": 13.4472,
      "step": 9855
    },
    {
      "epoch": 9.87,
      "grad_norm": 12299.5029296875,
      "learning_rate": 5.223942208462332e-05,
      "loss": 16.5046,
      "step": 9856
    },
    {
      "epoch": 9.87,
      "grad_norm": 1266.9974365234375,
      "learning_rate": 5.2234262125903e-05,
      "loss": 20.8319,
      "step": 9857
    },
    {
      "epoch": 9.87,
      "grad_norm": 906.4545288085938,
      "learning_rate": 5.2229102167182664e-05,
      "loss": 14.2502,
      "step": 9858
    },
    {
      "epoch": 9.87,
      "grad_norm": 3036.21435546875,
      "learning_rate": 5.222394220846234e-05,
      "loss": 19.0543,
      "step": 9859
    },
    {
      "epoch": 9.87,
      "grad_norm": 8203.1513671875,
      "learning_rate": 5.2218782249742006e-05,
      "loss": 13.9844,
      "step": 9860
    },
    {
      "epoch": 9.87,
      "grad_norm": 10909.4150390625,
      "learning_rate": 5.221362229102167e-05,
      "loss": 16.319,
      "step": 9861
    },
    {
      "epoch": 9.87,
      "grad_norm": 22470.3984375,
      "learning_rate": 5.220846233230134e-05,
      "loss": 19.5077,
      "step": 9862
    },
    {
      "epoch": 9.87,
      "grad_norm": 12473.4921875,
      "learning_rate": 5.220330237358101e-05,
      "loss": 15.4684,
      "step": 9863
    },
    {
      "epoch": 9.87,
      "grad_norm": 8651.59765625,
      "learning_rate": 5.2198142414860684e-05,
      "loss": 19.4396,
      "step": 9864
    },
    {
      "epoch": 9.87,
      "grad_norm": 26868.447265625,
      "learning_rate": 5.219298245614035e-05,
      "loss": 15.9456,
      "step": 9865
    },
    {
      "epoch": 9.88,
      "grad_norm": 8847.18359375,
      "learning_rate": 5.2187822497420026e-05,
      "loss": 14.2892,
      "step": 9866
    },
    {
      "epoch": 9.88,
      "grad_norm": 10048.3125,
      "learning_rate": 5.218266253869969e-05,
      "loss": 21.2659,
      "step": 9867
    },
    {
      "epoch": 9.88,
      "grad_norm": 4869.6552734375,
      "learning_rate": 5.217750257997937e-05,
      "loss": 14.2679,
      "step": 9868
    },
    {
      "epoch": 9.88,
      "grad_norm": 25040.412109375,
      "learning_rate": 5.217234262125903e-05,
      "loss": 21.9032,
      "step": 9869
    },
    {
      "epoch": 9.88,
      "grad_norm": 3042.419677734375,
      "learning_rate": 5.2167182662538704e-05,
      "loss": 19.5482,
      "step": 9870
    },
    {
      "epoch": 9.88,
      "grad_norm": 1987.1142578125,
      "learning_rate": 5.216202270381837e-05,
      "loss": 15.387,
      "step": 9871
    },
    {
      "epoch": 9.88,
      "grad_norm": 33602.7109375,
      "learning_rate": 5.2156862745098046e-05,
      "loss": 16.8731,
      "step": 9872
    },
    {
      "epoch": 9.88,
      "grad_norm": 7266.076171875,
      "learning_rate": 5.215170278637771e-05,
      "loss": 15.951,
      "step": 9873
    },
    {
      "epoch": 9.88,
      "grad_norm": 3317.156982421875,
      "learning_rate": 5.214654282765739e-05,
      "loss": 15.8915,
      "step": 9874
    },
    {
      "epoch": 9.88,
      "grad_norm": 26448.392578125,
      "learning_rate": 5.214138286893705e-05,
      "loss": 17.5325,
      "step": 9875
    },
    {
      "epoch": 9.89,
      "grad_norm": 8161.7109375,
      "learning_rate": 5.2136222910216724e-05,
      "loss": 16.2255,
      "step": 9876
    },
    {
      "epoch": 9.89,
      "grad_norm": 62399.234375,
      "learning_rate": 5.213106295149639e-05,
      "loss": 16.2389,
      "step": 9877
    },
    {
      "epoch": 9.89,
      "grad_norm": 5593.8193359375,
      "learning_rate": 5.2125902992776066e-05,
      "loss": 17.7373,
      "step": 9878
    },
    {
      "epoch": 9.89,
      "grad_norm": 6434.89208984375,
      "learning_rate": 5.212074303405573e-05,
      "loss": 18.5389,
      "step": 9879
    },
    {
      "epoch": 9.89,
      "grad_norm": 5567.17724609375,
      "learning_rate": 5.2115583075335395e-05,
      "loss": 20.1762,
      "step": 9880
    },
    {
      "epoch": 9.89,
      "grad_norm": 58678.38671875,
      "learning_rate": 5.211042311661507e-05,
      "loss": 23.5918,
      "step": 9881
    },
    {
      "epoch": 9.89,
      "grad_norm": 1274.768798828125,
      "learning_rate": 5.210526315789474e-05,
      "loss": 16.5149,
      "step": 9882
    },
    {
      "epoch": 9.89,
      "grad_norm": 6242.40869140625,
      "learning_rate": 5.210010319917441e-05,
      "loss": 13.2356,
      "step": 9883
    },
    {
      "epoch": 9.89,
      "grad_norm": 2332.40283203125,
      "learning_rate": 5.209494324045407e-05,
      "loss": 16.7877,
      "step": 9884
    },
    {
      "epoch": 9.89,
      "grad_norm": 5056.59912109375,
      "learning_rate": 5.2089783281733754e-05,
      "loss": 16.667,
      "step": 9885
    },
    {
      "epoch": 9.9,
      "grad_norm": 3171.14599609375,
      "learning_rate": 5.2084623323013415e-05,
      "loss": 13.0047,
      "step": 9886
    },
    {
      "epoch": 9.9,
      "grad_norm": 12056.0341796875,
      "learning_rate": 5.207946336429309e-05,
      "loss": 15.4206,
      "step": 9887
    },
    {
      "epoch": 9.9,
      "grad_norm": 7241.24365234375,
      "learning_rate": 5.207430340557276e-05,
      "loss": 20.2299,
      "step": 9888
    },
    {
      "epoch": 9.9,
      "grad_norm": 2224.918212890625,
      "learning_rate": 5.206914344685243e-05,
      "loss": 14.8874,
      "step": 9889
    },
    {
      "epoch": 9.9,
      "grad_norm": 8541.8193359375,
      "learning_rate": 5.206398348813209e-05,
      "loss": 14.4783,
      "step": 9890
    },
    {
      "epoch": 9.9,
      "grad_norm": 29201.060546875,
      "learning_rate": 5.2058823529411774e-05,
      "loss": 16.293,
      "step": 9891
    },
    {
      "epoch": 9.9,
      "grad_norm": 12109.095703125,
      "learning_rate": 5.2053663570691435e-05,
      "loss": 23.0964,
      "step": 9892
    },
    {
      "epoch": 9.9,
      "grad_norm": 68557.8125,
      "learning_rate": 5.204850361197111e-05,
      "loss": 16.9631,
      "step": 9893
    },
    {
      "epoch": 9.9,
      "grad_norm": 4892.85986328125,
      "learning_rate": 5.204334365325078e-05,
      "loss": 20.2478,
      "step": 9894
    },
    {
      "epoch": 9.9,
      "grad_norm": 5118.802734375,
      "learning_rate": 5.203818369453045e-05,
      "loss": 18.5263,
      "step": 9895
    },
    {
      "epoch": 9.91,
      "grad_norm": 34752.61328125,
      "learning_rate": 5.203302373581012e-05,
      "loss": 15.1972,
      "step": 9896
    },
    {
      "epoch": 9.91,
      "grad_norm": 2122.57421875,
      "learning_rate": 5.202786377708978e-05,
      "loss": 14.4827,
      "step": 9897
    },
    {
      "epoch": 9.91,
      "grad_norm": 5242.1552734375,
      "learning_rate": 5.2022703818369455e-05,
      "loss": 17.7999,
      "step": 9898
    },
    {
      "epoch": 9.91,
      "grad_norm": 39183.65234375,
      "learning_rate": 5.201754385964912e-05,
      "loss": 17.8577,
      "step": 9899
    },
    {
      "epoch": 9.91,
      "grad_norm": 10214.6708984375,
      "learning_rate": 5.20123839009288e-05,
      "loss": 19.3169,
      "step": 9900
    },
    {
      "epoch": 9.91,
      "grad_norm": 20127.73046875,
      "learning_rate": 5.200722394220846e-05,
      "loss": 16.1874,
      "step": 9901
    },
    {
      "epoch": 9.91,
      "grad_norm": 1433.263671875,
      "learning_rate": 5.200206398348814e-05,
      "loss": 14.4234,
      "step": 9902
    },
    {
      "epoch": 9.91,
      "grad_norm": 22286.826171875,
      "learning_rate": 5.19969040247678e-05,
      "loss": 17.7107,
      "step": 9903
    },
    {
      "epoch": 9.91,
      "grad_norm": 11260.9169921875,
      "learning_rate": 5.1991744066047475e-05,
      "loss": 13.502,
      "step": 9904
    },
    {
      "epoch": 9.91,
      "grad_norm": 18167.59375,
      "learning_rate": 5.198658410732714e-05,
      "loss": 14.0404,
      "step": 9905
    },
    {
      "epoch": 9.92,
      "grad_norm": 19820.91015625,
      "learning_rate": 5.198142414860682e-05,
      "loss": 19.8645,
      "step": 9906
    },
    {
      "epoch": 9.92,
      "grad_norm": 1096.8780517578125,
      "learning_rate": 5.197626418988648e-05,
      "loss": 19.2524,
      "step": 9907
    },
    {
      "epoch": 9.92,
      "grad_norm": 38844.234375,
      "learning_rate": 5.197110423116616e-05,
      "loss": 15.6183,
      "step": 9908
    },
    {
      "epoch": 9.92,
      "grad_norm": 2357.365234375,
      "learning_rate": 5.196594427244582e-05,
      "loss": 18.0253,
      "step": 9909
    },
    {
      "epoch": 9.92,
      "grad_norm": 9548.841796875,
      "learning_rate": 5.1960784313725495e-05,
      "loss": 17.579,
      "step": 9910
    },
    {
      "epoch": 9.92,
      "grad_norm": 9272.30859375,
      "learning_rate": 5.195562435500516e-05,
      "loss": 18.0973,
      "step": 9911
    },
    {
      "epoch": 9.92,
      "grad_norm": 1902.1728515625,
      "learning_rate": 5.195046439628484e-05,
      "loss": 17.7628,
      "step": 9912
    },
    {
      "epoch": 9.92,
      "grad_norm": 4978.2255859375,
      "learning_rate": 5.1945304437564505e-05,
      "loss": 13.4302,
      "step": 9913
    },
    {
      "epoch": 9.92,
      "grad_norm": 18613.794921875,
      "learning_rate": 5.194014447884418e-05,
      "loss": 13.5419,
      "step": 9914
    },
    {
      "epoch": 9.92,
      "grad_norm": 4067.92041015625,
      "learning_rate": 5.193498452012384e-05,
      "loss": 16.915,
      "step": 9915
    },
    {
      "epoch": 9.93,
      "grad_norm": 26889.412109375,
      "learning_rate": 5.192982456140351e-05,
      "loss": 20.1718,
      "step": 9916
    },
    {
      "epoch": 9.93,
      "grad_norm": 2542.79345703125,
      "learning_rate": 5.192466460268318e-05,
      "loss": 15.4434,
      "step": 9917
    },
    {
      "epoch": 9.93,
      "grad_norm": 20894.560546875,
      "learning_rate": 5.1919504643962843e-05,
      "loss": 14.4472,
      "step": 9918
    },
    {
      "epoch": 9.93,
      "grad_norm": 551.6599731445312,
      "learning_rate": 5.1914344685242525e-05,
      "loss": 12.4605,
      "step": 9919
    },
    {
      "epoch": 9.93,
      "grad_norm": 5208.0244140625,
      "learning_rate": 5.1909184726522186e-05,
      "loss": 15.1411,
      "step": 9920
    },
    {
      "epoch": 9.93,
      "grad_norm": 26524.4296875,
      "learning_rate": 5.190402476780186e-05,
      "loss": 19.0507,
      "step": 9921
    },
    {
      "epoch": 9.93,
      "grad_norm": 13241.9892578125,
      "learning_rate": 5.189886480908153e-05,
      "loss": 14.1253,
      "step": 9922
    },
    {
      "epoch": 9.93,
      "grad_norm": 10222.5361328125,
      "learning_rate": 5.18937048503612e-05,
      "loss": 14.009,
      "step": 9923
    },
    {
      "epoch": 9.93,
      "grad_norm": 2983.978515625,
      "learning_rate": 5.1888544891640863e-05,
      "loss": 17.5201,
      "step": 9924
    },
    {
      "epoch": 9.93,
      "grad_norm": 27017.599609375,
      "learning_rate": 5.1883384932920545e-05,
      "loss": 15.3575,
      "step": 9925
    },
    {
      "epoch": 9.94,
      "grad_norm": 6269.17236328125,
      "learning_rate": 5.1878224974200206e-05,
      "loss": 16.2955,
      "step": 9926
    },
    {
      "epoch": 9.94,
      "grad_norm": 6348.99560546875,
      "learning_rate": 5.187306501547988e-05,
      "loss": 15.1333,
      "step": 9927
    },
    {
      "epoch": 9.94,
      "grad_norm": 13349.2236328125,
      "learning_rate": 5.186790505675955e-05,
      "loss": 16.2298,
      "step": 9928
    },
    {
      "epoch": 9.94,
      "grad_norm": 11796.3359375,
      "learning_rate": 5.186274509803922e-05,
      "loss": 17.9577,
      "step": 9929
    },
    {
      "epoch": 9.94,
      "grad_norm": 6649.3310546875,
      "learning_rate": 5.185758513931889e-05,
      "loss": 15.3662,
      "step": 9930
    },
    {
      "epoch": 9.94,
      "grad_norm": 3835.684814453125,
      "learning_rate": 5.1852425180598565e-05,
      "loss": 15.2833,
      "step": 9931
    },
    {
      "epoch": 9.94,
      "grad_norm": 11081.75390625,
      "learning_rate": 5.1847265221878226e-05,
      "loss": 16.8141,
      "step": 9932
    },
    {
      "epoch": 9.94,
      "grad_norm": 1957.3270263671875,
      "learning_rate": 5.1842105263157893e-05,
      "loss": 18.4127,
      "step": 9933
    },
    {
      "epoch": 9.94,
      "grad_norm": 25810.31640625,
      "learning_rate": 5.183694530443757e-05,
      "loss": 20.2374,
      "step": 9934
    },
    {
      "epoch": 9.94,
      "grad_norm": 5887.70947265625,
      "learning_rate": 5.183178534571723e-05,
      "loss": 19.1524,
      "step": 9935
    },
    {
      "epoch": 9.95,
      "grad_norm": 6829.36669921875,
      "learning_rate": 5.182662538699691e-05,
      "loss": 16.5747,
      "step": 9936
    },
    {
      "epoch": 9.95,
      "grad_norm": 6192.74365234375,
      "learning_rate": 5.182146542827657e-05,
      "loss": 16.3106,
      "step": 9937
    },
    {
      "epoch": 9.95,
      "grad_norm": 4630.173828125,
      "learning_rate": 5.1816305469556246e-05,
      "loss": 21.5365,
      "step": 9938
    },
    {
      "epoch": 9.95,
      "grad_norm": 1145.53466796875,
      "learning_rate": 5.1811145510835913e-05,
      "loss": 14.9292,
      "step": 9939
    },
    {
      "epoch": 9.95,
      "grad_norm": 1318.1473388671875,
      "learning_rate": 5.180598555211559e-05,
      "loss": 14.3776,
      "step": 9940
    },
    {
      "epoch": 9.95,
      "grad_norm": 34063.5390625,
      "learning_rate": 5.1800825593395256e-05,
      "loss": 17.2469,
      "step": 9941
    },
    {
      "epoch": 9.95,
      "grad_norm": 6579.55712890625,
      "learning_rate": 5.179566563467493e-05,
      "loss": 16.3039,
      "step": 9942
    },
    {
      "epoch": 9.95,
      "grad_norm": 6535.73046875,
      "learning_rate": 5.179050567595459e-05,
      "loss": 17.4106,
      "step": 9943
    },
    {
      "epoch": 9.95,
      "grad_norm": 41417.12109375,
      "learning_rate": 5.1785345717234266e-05,
      "loss": 19.2363,
      "step": 9944
    },
    {
      "epoch": 9.95,
      "grad_norm": 1704.2164306640625,
      "learning_rate": 5.1780185758513933e-05,
      "loss": 15.1576,
      "step": 9945
    },
    {
      "epoch": 9.96,
      "grad_norm": 3405.531005859375,
      "learning_rate": 5.177502579979361e-05,
      "loss": 15.7196,
      "step": 9946
    },
    {
      "epoch": 9.96,
      "grad_norm": 11781.634765625,
      "learning_rate": 5.1769865841073276e-05,
      "loss": 16.1038,
      "step": 9947
    },
    {
      "epoch": 9.96,
      "grad_norm": 32864.421875,
      "learning_rate": 5.176470588235295e-05,
      "loss": 16.5791,
      "step": 9948
    },
    {
      "epoch": 9.96,
      "grad_norm": 4576.73828125,
      "learning_rate": 5.175954592363261e-05,
      "loss": 19.3936,
      "step": 9949
    },
    {
      "epoch": 9.96,
      "grad_norm": 6567.69775390625,
      "learning_rate": 5.1754385964912286e-05,
      "loss": 16.5228,
      "step": 9950
    },
    {
      "epoch": 9.96,
      "grad_norm": 3054.58056640625,
      "learning_rate": 5.1749226006191953e-05,
      "loss": 15.2754,
      "step": 9951
    },
    {
      "epoch": 9.96,
      "grad_norm": 3518.039306640625,
      "learning_rate": 5.1744066047471614e-05,
      "loss": 14.9435,
      "step": 9952
    },
    {
      "epoch": 9.96,
      "grad_norm": 2714.766357421875,
      "learning_rate": 5.1738906088751296e-05,
      "loss": 15.5382,
      "step": 9953
    },
    {
      "epoch": 9.96,
      "grad_norm": 2909.171875,
      "learning_rate": 5.173374613003096e-05,
      "loss": 17.0688,
      "step": 9954
    },
    {
      "epoch": 9.96,
      "grad_norm": 17644.515625,
      "learning_rate": 5.172858617131063e-05,
      "loss": 17.978,
      "step": 9955
    },
    {
      "epoch": 9.97,
      "grad_norm": 5755.6357421875,
      "learning_rate": 5.17234262125903e-05,
      "loss": 18.2736,
      "step": 9956
    },
    {
      "epoch": 9.97,
      "grad_norm": 9155.775390625,
      "learning_rate": 5.1718266253869973e-05,
      "loss": 14.6302,
      "step": 9957
    },
    {
      "epoch": 9.97,
      "grad_norm": 3219.638427734375,
      "learning_rate": 5.171310629514964e-05,
      "loss": 15.371,
      "step": 9958
    },
    {
      "epoch": 9.97,
      "grad_norm": 35948.625,
      "learning_rate": 5.1707946336429316e-05,
      "loss": 14.3001,
      "step": 9959
    },
    {
      "epoch": 9.97,
      "grad_norm": 14886.1630859375,
      "learning_rate": 5.170278637770898e-05,
      "loss": 16.2459,
      "step": 9960
    },
    {
      "epoch": 9.97,
      "grad_norm": 19090.5234375,
      "learning_rate": 5.169762641898865e-05,
      "loss": 14.4752,
      "step": 9961
    },
    {
      "epoch": 9.97,
      "grad_norm": 10723.046875,
      "learning_rate": 5.169246646026832e-05,
      "loss": 21.9512,
      "step": 9962
    },
    {
      "epoch": 9.97,
      "grad_norm": 3402.79296875,
      "learning_rate": 5.1687306501547993e-05,
      "loss": 19.7186,
      "step": 9963
    },
    {
      "epoch": 9.97,
      "grad_norm": 8066.24267578125,
      "learning_rate": 5.168214654282766e-05,
      "loss": 16.3652,
      "step": 9964
    },
    {
      "epoch": 9.97,
      "grad_norm": 3352.276611328125,
      "learning_rate": 5.1676986584107336e-05,
      "loss": 15.6816,
      "step": 9965
    },
    {
      "epoch": 9.98,
      "grad_norm": 36564.40625,
      "learning_rate": 5.1671826625387e-05,
      "loss": 18.6822,
      "step": 9966
    },
    {
      "epoch": 9.98,
      "grad_norm": 1407.5584716796875,
      "learning_rate": 5.166666666666667e-05,
      "loss": 16.8271,
      "step": 9967
    },
    {
      "epoch": 9.98,
      "grad_norm": 9694.5927734375,
      "learning_rate": 5.166150670794634e-05,
      "loss": 18.1572,
      "step": 9968
    },
    {
      "epoch": 9.98,
      "grad_norm": 20172.0234375,
      "learning_rate": 5.165634674922601e-05,
      "loss": 18.8418,
      "step": 9969
    },
    {
      "epoch": 9.98,
      "grad_norm": 18772.7890625,
      "learning_rate": 5.165118679050568e-05,
      "loss": 17.1869,
      "step": 9970
    },
    {
      "epoch": 9.98,
      "grad_norm": 1658.4755859375,
      "learning_rate": 5.164602683178534e-05,
      "loss": 12.9707,
      "step": 9971
    },
    {
      "epoch": 9.98,
      "grad_norm": 32740.017578125,
      "learning_rate": 5.164086687306502e-05,
      "loss": 29.8871,
      "step": 9972
    },
    {
      "epoch": 9.98,
      "grad_norm": 7163.81787109375,
      "learning_rate": 5.1635706914344684e-05,
      "loss": 19.4846,
      "step": 9973
    },
    {
      "epoch": 9.98,
      "grad_norm": 4794.05517578125,
      "learning_rate": 5.163054695562436e-05,
      "loss": 18.607,
      "step": 9974
    },
    {
      "epoch": 9.98,
      "grad_norm": 11475.857421875,
      "learning_rate": 5.162538699690403e-05,
      "loss": 31.5894,
      "step": 9975
    },
    {
      "epoch": 9.99,
      "grad_norm": 16936.353515625,
      "learning_rate": 5.16202270381837e-05,
      "loss": 15.184,
      "step": 9976
    },
    {
      "epoch": 9.99,
      "grad_norm": 1644.2603759765625,
      "learning_rate": 5.161506707946336e-05,
      "loss": 15.1031,
      "step": 9977
    },
    {
      "epoch": 9.99,
      "grad_norm": 9708.9892578125,
      "learning_rate": 5.160990712074304e-05,
      "loss": 18.2825,
      "step": 9978
    },
    {
      "epoch": 9.99,
      "grad_norm": 2376.668212890625,
      "learning_rate": 5.1604747162022704e-05,
      "loss": 12.1609,
      "step": 9979
    },
    {
      "epoch": 9.99,
      "grad_norm": 22216.537109375,
      "learning_rate": 5.159958720330238e-05,
      "loss": 14.9312,
      "step": 9980
    },
    {
      "epoch": 9.99,
      "grad_norm": 16823.19921875,
      "learning_rate": 5.159442724458205e-05,
      "loss": 17.8637,
      "step": 9981
    },
    {
      "epoch": 9.99,
      "grad_norm": 3708.22265625,
      "learning_rate": 5.158926728586172e-05,
      "loss": 17.6246,
      "step": 9982
    },
    {
      "epoch": 9.99,
      "grad_norm": 2417.023193359375,
      "learning_rate": 5.158410732714138e-05,
      "loss": 16.0772,
      "step": 9983
    },
    {
      "epoch": 9.99,
      "grad_norm": 1484.5057373046875,
      "learning_rate": 5.157894736842106e-05,
      "loss": 13.3125,
      "step": 9984
    },
    {
      "epoch": 9.99,
      "grad_norm": 3655.00439453125,
      "learning_rate": 5.1573787409700724e-05,
      "loss": 14.2207,
      "step": 9985
    },
    {
      "epoch": 10.0,
      "grad_norm": 3560.882568359375,
      "learning_rate": 5.15686274509804e-05,
      "loss": 13.1501,
      "step": 9986
    },
    {
      "epoch": 10.0,
      "grad_norm": 5808.1875,
      "learning_rate": 5.156346749226007e-05,
      "loss": 14.1363,
      "step": 9987
    },
    {
      "epoch": 10.0,
      "grad_norm": 5964.4521484375,
      "learning_rate": 5.155830753353973e-05,
      "loss": 15.6923,
      "step": 9988
    },
    {
      "epoch": 10.0,
      "grad_norm": 16072.4052734375,
      "learning_rate": 5.15531475748194e-05,
      "loss": 18.436,
      "step": 9989
    },
    {
      "epoch": 10.0,
      "grad_norm": 58352.5234375,
      "learning_rate": 5.154798761609907e-05,
      "loss": 23.1789,
      "step": 9990
    },
    {
      "epoch": 10.0,
      "grad_norm": 15471.9375,
      "learning_rate": 5.1542827657378744e-05,
      "loss": 16.0055,
      "step": 9991
    },
    {
      "epoch": 10.0,
      "grad_norm": 887.1779174804688,
      "learning_rate": 5.153766769865841e-05,
      "loss": 15.0922,
      "step": 9992
    },
    {
      "epoch": 10.0,
      "grad_norm": 4728.24951171875,
      "learning_rate": 5.153250773993809e-05,
      "loss": 14.3814,
      "step": 9993
    },
    {
      "epoch": 10.0,
      "grad_norm": 4699.14990234375,
      "learning_rate": 5.152734778121775e-05,
      "loss": 15.3696,
      "step": 9994
    },
    {
      "epoch": 10.01,
      "grad_norm": 2150.144287109375,
      "learning_rate": 5.152218782249742e-05,
      "loss": 11.0704,
      "step": 9995
    },
    {
      "epoch": 10.01,
      "grad_norm": 1871.83935546875,
      "learning_rate": 5.151702786377709e-05,
      "loss": 14.5381,
      "step": 9996
    },
    {
      "epoch": 10.01,
      "grad_norm": 4546.06494140625,
      "learning_rate": 5.1511867905056764e-05,
      "loss": 15.3629,
      "step": 9997
    },
    {
      "epoch": 10.01,
      "grad_norm": 8450.3427734375,
      "learning_rate": 5.150670794633643e-05,
      "loss": 14.8432,
      "step": 9998
    },
    {
      "epoch": 10.01,
      "grad_norm": 16143.2763671875,
      "learning_rate": 5.150154798761611e-05,
      "loss": 15.9474,
      "step": 9999
    },
    {
      "epoch": 10.01,
      "grad_norm": 18680.216796875,
      "learning_rate": 5.149638802889577e-05,
      "loss": 19.9862,
      "step": 10000
    },
    {
      "epoch": 10.01,
      "grad_norm": 2224.82568359375,
      "learning_rate": 5.149122807017545e-05,
      "loss": 14.2685,
      "step": 10001
    },
    {
      "epoch": 10.01,
      "grad_norm": 37677.640625,
      "learning_rate": 5.148606811145511e-05,
      "loss": 18.8404,
      "step": 10002
    },
    {
      "epoch": 10.01,
      "grad_norm": 2379.822021484375,
      "learning_rate": 5.1480908152734784e-05,
      "loss": 16.4361,
      "step": 10003
    },
    {
      "epoch": 10.01,
      "grad_norm": 1642.2298583984375,
      "learning_rate": 5.147574819401445e-05,
      "loss": 17.3426,
      "step": 10004
    },
    {
      "epoch": 10.02,
      "grad_norm": 4251.58544921875,
      "learning_rate": 5.147058823529411e-05,
      "loss": 13.3942,
      "step": 10005
    },
    {
      "epoch": 10.02,
      "grad_norm": 3150.98095703125,
      "learning_rate": 5.146542827657379e-05,
      "loss": 17.864,
      "step": 10006
    },
    {
      "epoch": 10.02,
      "grad_norm": 796.4848022460938,
      "learning_rate": 5.1460268317853455e-05,
      "loss": 13.7114,
      "step": 10007
    },
    {
      "epoch": 10.02,
      "grad_norm": 8361.6513671875,
      "learning_rate": 5.145510835913313e-05,
      "loss": 13.9975,
      "step": 10008
    },
    {
      "epoch": 10.02,
      "grad_norm": 19497.53515625,
      "learning_rate": 5.14499484004128e-05,
      "loss": 19.9804,
      "step": 10009
    },
    {
      "epoch": 10.02,
      "grad_norm": 2382.215087890625,
      "learning_rate": 5.144478844169247e-05,
      "loss": 14.5466,
      "step": 10010
    },
    {
      "epoch": 10.02,
      "grad_norm": 17128.7734375,
      "learning_rate": 5.143962848297213e-05,
      "loss": 22.3778,
      "step": 10011
    },
    {
      "epoch": 10.02,
      "grad_norm": 17884.17578125,
      "learning_rate": 5.143446852425181e-05,
      "loss": 14.5788,
      "step": 10012
    },
    {
      "epoch": 10.02,
      "grad_norm": 4997.908203125,
      "learning_rate": 5.1429308565531475e-05,
      "loss": 12.3952,
      "step": 10013
    },
    {
      "epoch": 10.02,
      "grad_norm": 1095.638916015625,
      "learning_rate": 5.142414860681115e-05,
      "loss": 14.3543,
      "step": 10014
    },
    {
      "epoch": 10.03,
      "grad_norm": 2807.375244140625,
      "learning_rate": 5.141898864809082e-05,
      "loss": 15.2956,
      "step": 10015
    },
    {
      "epoch": 10.03,
      "grad_norm": 1776.2415771484375,
      "learning_rate": 5.141382868937049e-05,
      "loss": 16.1945,
      "step": 10016
    },
    {
      "epoch": 10.03,
      "grad_norm": 10118.8701171875,
      "learning_rate": 5.140866873065015e-05,
      "loss": 17.6657,
      "step": 10017
    },
    {
      "epoch": 10.03,
      "grad_norm": 3359.70068359375,
      "learning_rate": 5.1403508771929834e-05,
      "loss": 14.3638,
      "step": 10018
    },
    {
      "epoch": 10.03,
      "grad_norm": 1756.786865234375,
      "learning_rate": 5.1398348813209495e-05,
      "loss": 15.8677,
      "step": 10019
    },
    {
      "epoch": 10.03,
      "grad_norm": 4707.4638671875,
      "learning_rate": 5.139318885448917e-05,
      "loss": 15.7673,
      "step": 10020
    },
    {
      "epoch": 10.03,
      "grad_norm": 49842.515625,
      "learning_rate": 5.138802889576884e-05,
      "loss": 20.4878,
      "step": 10021
    },
    {
      "epoch": 10.03,
      "grad_norm": 2886.2158203125,
      "learning_rate": 5.13828689370485e-05,
      "loss": 15.3652,
      "step": 10022
    },
    {
      "epoch": 10.03,
      "grad_norm": 32326.482421875,
      "learning_rate": 5.137770897832817e-05,
      "loss": 15.6586,
      "step": 10023
    },
    {
      "epoch": 10.03,
      "grad_norm": 5738.14697265625,
      "learning_rate": 5.137254901960784e-05,
      "loss": 17.0743,
      "step": 10024
    },
    {
      "epoch": 10.04,
      "grad_norm": 41440.82421875,
      "learning_rate": 5.1367389060887515e-05,
      "loss": 18.4694,
      "step": 10025
    },
    {
      "epoch": 10.04,
      "grad_norm": 24340.673828125,
      "learning_rate": 5.136222910216718e-05,
      "loss": 16.799,
      "step": 10026
    },
    {
      "epoch": 10.04,
      "grad_norm": 1876.73876953125,
      "learning_rate": 5.135706914344686e-05,
      "loss": 14.2828,
      "step": 10027
    },
    {
      "epoch": 10.04,
      "grad_norm": 9277.974609375,
      "learning_rate": 5.135190918472652e-05,
      "loss": 20.1308,
      "step": 10028
    },
    {
      "epoch": 10.04,
      "grad_norm": 3210.059814453125,
      "learning_rate": 5.13467492260062e-05,
      "loss": 16.968,
      "step": 10029
    },
    {
      "epoch": 10.04,
      "grad_norm": 4793.029296875,
      "learning_rate": 5.134158926728586e-05,
      "loss": 16.3979,
      "step": 10030
    },
    {
      "epoch": 10.04,
      "grad_norm": 14352.470703125,
      "learning_rate": 5.1336429308565535e-05,
      "loss": 15.7801,
      "step": 10031
    },
    {
      "epoch": 10.04,
      "grad_norm": 147647.25,
      "learning_rate": 5.13312693498452e-05,
      "loss": 15.6263,
      "step": 10032
    },
    {
      "epoch": 10.04,
      "grad_norm": 1681.19384765625,
      "learning_rate": 5.132610939112488e-05,
      "loss": 14.4787,
      "step": 10033
    },
    {
      "epoch": 10.04,
      "grad_norm": 8148.67578125,
      "learning_rate": 5.132094943240454e-05,
      "loss": 16.7833,
      "step": 10034
    },
    {
      "epoch": 10.05,
      "grad_norm": 9863.87109375,
      "learning_rate": 5.131578947368422e-05,
      "loss": 15.5491,
      "step": 10035
    },
    {
      "epoch": 10.05,
      "grad_norm": 6299.3759765625,
      "learning_rate": 5.131062951496388e-05,
      "loss": 15.7471,
      "step": 10036
    },
    {
      "epoch": 10.05,
      "grad_norm": 5410.3076171875,
      "learning_rate": 5.1305469556243555e-05,
      "loss": 14.7494,
      "step": 10037
    },
    {
      "epoch": 10.05,
      "grad_norm": 19518.6875,
      "learning_rate": 5.130030959752322e-05,
      "loss": 16.8882,
      "step": 10038
    },
    {
      "epoch": 10.05,
      "grad_norm": 30553.5,
      "learning_rate": 5.12951496388029e-05,
      "loss": 15.3442,
      "step": 10039
    },
    {
      "epoch": 10.05,
      "grad_norm": 1153.6297607421875,
      "learning_rate": 5.128998968008256e-05,
      "loss": 17.1555,
      "step": 10040
    },
    {
      "epoch": 10.05,
      "grad_norm": 3314.533447265625,
      "learning_rate": 5.1284829721362226e-05,
      "loss": 14.8852,
      "step": 10041
    },
    {
      "epoch": 10.05,
      "grad_norm": 68718.359375,
      "learning_rate": 5.12796697626419e-05,
      "loss": 13.2352,
      "step": 10042
    },
    {
      "epoch": 10.05,
      "grad_norm": 3243.700439453125,
      "learning_rate": 5.127450980392157e-05,
      "loss": 17.6097,
      "step": 10043
    },
    {
      "epoch": 10.05,
      "grad_norm": 88255.3046875,
      "learning_rate": 5.126934984520124e-05,
      "loss": 18.9575,
      "step": 10044
    },
    {
      "epoch": 10.06,
      "grad_norm": 10157.6748046875,
      "learning_rate": 5.1264189886480904e-05,
      "loss": 20.758,
      "step": 10045
    },
    {
      "epoch": 10.06,
      "grad_norm": 13229.177734375,
      "learning_rate": 5.1259029927760585e-05,
      "loss": 18.0737,
      "step": 10046
    },
    {
      "epoch": 10.06,
      "grad_norm": 6685.10302734375,
      "learning_rate": 5.1253869969040246e-05,
      "loss": 16.3145,
      "step": 10047
    },
    {
      "epoch": 10.06,
      "grad_norm": 4353.07666015625,
      "learning_rate": 5.124871001031992e-05,
      "loss": 16.1857,
      "step": 10048
    },
    {
      "epoch": 10.06,
      "grad_norm": 13121.865234375,
      "learning_rate": 5.124355005159959e-05,
      "loss": 16.9739,
      "step": 10049
    },
    {
      "epoch": 10.06,
      "grad_norm": 600.0911254882812,
      "learning_rate": 5.123839009287926e-05,
      "loss": 12.9904,
      "step": 10050
    },
    {
      "epoch": 10.06,
      "grad_norm": 13444.41015625,
      "learning_rate": 5.1233230134158924e-05,
      "loss": 19.4816,
      "step": 10051
    },
    {
      "epoch": 10.06,
      "grad_norm": 659215.5625,
      "learning_rate": 5.1228070175438605e-05,
      "loss": 14.6683,
      "step": 10052
    },
    {
      "epoch": 10.06,
      "grad_norm": 42036.45703125,
      "learning_rate": 5.1222910216718266e-05,
      "loss": 22.2645,
      "step": 10053
    },
    {
      "epoch": 10.06,
      "grad_norm": 5807.15478515625,
      "learning_rate": 5.121775025799794e-05,
      "loss": 21.5753,
      "step": 10054
    },
    {
      "epoch": 10.07,
      "grad_norm": 8794.7265625,
      "learning_rate": 5.121259029927761e-05,
      "loss": 15.2155,
      "step": 10055
    },
    {
      "epoch": 10.07,
      "grad_norm": 6129.65771484375,
      "learning_rate": 5.120743034055728e-05,
      "loss": 22.0399,
      "step": 10056
    },
    {
      "epoch": 10.07,
      "grad_norm": 4839.53125,
      "learning_rate": 5.120227038183695e-05,
      "loss": 16.7762,
      "step": 10057
    },
    {
      "epoch": 10.07,
      "grad_norm": 4497.52001953125,
      "learning_rate": 5.119711042311661e-05,
      "loss": 13.6979,
      "step": 10058
    },
    {
      "epoch": 10.07,
      "grad_norm": 2929.242431640625,
      "learning_rate": 5.1191950464396286e-05,
      "loss": 18.1538,
      "step": 10059
    },
    {
      "epoch": 10.07,
      "grad_norm": 6278.48779296875,
      "learning_rate": 5.1186790505675954e-05,
      "loss": 17.2654,
      "step": 10060
    },
    {
      "epoch": 10.07,
      "grad_norm": 3740.689697265625,
      "learning_rate": 5.118163054695563e-05,
      "loss": 14.2349,
      "step": 10061
    },
    {
      "epoch": 10.07,
      "grad_norm": 5101.45458984375,
      "learning_rate": 5.117647058823529e-05,
      "loss": 18.9416,
      "step": 10062
    },
    {
      "epoch": 10.07,
      "grad_norm": 2582.2392578125,
      "learning_rate": 5.117131062951497e-05,
      "loss": 17.9064,
      "step": 10063
    },
    {
      "epoch": 10.07,
      "grad_norm": 7482.00341796875,
      "learning_rate": 5.116615067079463e-05,
      "loss": 16.2207,
      "step": 10064
    },
    {
      "epoch": 10.08,
      "grad_norm": 5798.88623046875,
      "learning_rate": 5.1160990712074306e-05,
      "loss": 18.9291,
      "step": 10065
    },
    {
      "epoch": 10.08,
      "grad_norm": 41378.49609375,
      "learning_rate": 5.1155830753353974e-05,
      "loss": 14.0263,
      "step": 10066
    },
    {
      "epoch": 10.08,
      "grad_norm": 3177.89501953125,
      "learning_rate": 5.115067079463365e-05,
      "loss": 15.3247,
      "step": 10067
    },
    {
      "epoch": 10.08,
      "grad_norm": 12276.6416015625,
      "learning_rate": 5.114551083591331e-05,
      "loss": 21.4231,
      "step": 10068
    },
    {
      "epoch": 10.08,
      "grad_norm": 13781.4345703125,
      "learning_rate": 5.114035087719299e-05,
      "loss": 16.2364,
      "step": 10069
    },
    {
      "epoch": 10.08,
      "grad_norm": 3141.1962890625,
      "learning_rate": 5.113519091847265e-05,
      "loss": 18.2313,
      "step": 10070
    },
    {
      "epoch": 10.08,
      "grad_norm": 830.3426513671875,
      "learning_rate": 5.1130030959752326e-05,
      "loss": 16.3499,
      "step": 10071
    },
    {
      "epoch": 10.08,
      "grad_norm": 3441.074462890625,
      "learning_rate": 5.1124871001031994e-05,
      "loss": 14.8687,
      "step": 10072
    },
    {
      "epoch": 10.08,
      "grad_norm": 7924.3486328125,
      "learning_rate": 5.111971104231167e-05,
      "loss": 20.1047,
      "step": 10073
    },
    {
      "epoch": 10.08,
      "grad_norm": 15795.3681640625,
      "learning_rate": 5.1114551083591336e-05,
      "loss": 13.2599,
      "step": 10074
    },
    {
      "epoch": 10.09,
      "grad_norm": 9848.3427734375,
      "learning_rate": 5.110939112487101e-05,
      "loss": 15.2076,
      "step": 10075
    },
    {
      "epoch": 10.09,
      "grad_norm": 42147.58984375,
      "learning_rate": 5.110423116615067e-05,
      "loss": 23.4103,
      "step": 10076
    },
    {
      "epoch": 10.09,
      "grad_norm": 51263.046875,
      "learning_rate": 5.109907120743034e-05,
      "loss": 16.0317,
      "step": 10077
    },
    {
      "epoch": 10.09,
      "grad_norm": 16597.99609375,
      "learning_rate": 5.1093911248710014e-05,
      "loss": 16.9621,
      "step": 10078
    },
    {
      "epoch": 10.09,
      "grad_norm": 59083.26953125,
      "learning_rate": 5.1088751289989675e-05,
      "loss": 22.4042,
      "step": 10079
    },
    {
      "epoch": 10.09,
      "grad_norm": 20548.427734375,
      "learning_rate": 5.1083591331269356e-05,
      "loss": 19.2371,
      "step": 10080
    },
    {
      "epoch": 10.09,
      "grad_norm": 4705.89013671875,
      "learning_rate": 5.107843137254902e-05,
      "loss": 14.3216,
      "step": 10081
    },
    {
      "epoch": 10.09,
      "grad_norm": 4314.5947265625,
      "learning_rate": 5.107327141382869e-05,
      "loss": 17.3277,
      "step": 10082
    },
    {
      "epoch": 10.09,
      "grad_norm": 4272.84228515625,
      "learning_rate": 5.106811145510836e-05,
      "loss": 15.1997,
      "step": 10083
    },
    {
      "epoch": 10.09,
      "grad_norm": 17853.44921875,
      "learning_rate": 5.1062951496388034e-05,
      "loss": 21.5605,
      "step": 10084
    },
    {
      "epoch": 10.1,
      "grad_norm": 22407.01953125,
      "learning_rate": 5.1057791537667695e-05,
      "loss": 14.8935,
      "step": 10085
    },
    {
      "epoch": 10.1,
      "grad_norm": 3567.96533203125,
      "learning_rate": 5.1052631578947376e-05,
      "loss": 15.7281,
      "step": 10086
    },
    {
      "epoch": 10.1,
      "grad_norm": 25946.201171875,
      "learning_rate": 5.104747162022704e-05,
      "loss": 18.8624,
      "step": 10087
    },
    {
      "epoch": 10.1,
      "grad_norm": 60475.84375,
      "learning_rate": 5.104231166150671e-05,
      "loss": 16.558,
      "step": 10088
    },
    {
      "epoch": 10.1,
      "grad_norm": 6877.52294921875,
      "learning_rate": 5.103715170278638e-05,
      "loss": 17.4291,
      "step": 10089
    },
    {
      "epoch": 10.1,
      "grad_norm": 9502.5888671875,
      "learning_rate": 5.1031991744066054e-05,
      "loss": 13.9737,
      "step": 10090
    },
    {
      "epoch": 10.1,
      "grad_norm": 11023.6982421875,
      "learning_rate": 5.102683178534572e-05,
      "loss": 14.2741,
      "step": 10091
    },
    {
      "epoch": 10.1,
      "grad_norm": 124423.734375,
      "learning_rate": 5.1021671826625396e-05,
      "loss": 14.1874,
      "step": 10092
    },
    {
      "epoch": 10.1,
      "grad_norm": 41146.63671875,
      "learning_rate": 5.101651186790506e-05,
      "loss": 15.3544,
      "step": 10093
    },
    {
      "epoch": 10.1,
      "grad_norm": 6057.392578125,
      "learning_rate": 5.1011351909184725e-05,
      "loss": 15.5094,
      "step": 10094
    },
    {
      "epoch": 10.11,
      "grad_norm": 12017.4443359375,
      "learning_rate": 5.10061919504644e-05,
      "loss": 19.0929,
      "step": 10095
    },
    {
      "epoch": 10.11,
      "grad_norm": 9363.931640625,
      "learning_rate": 5.100103199174406e-05,
      "loss": 15.4385,
      "step": 10096
    },
    {
      "epoch": 10.11,
      "grad_norm": 15123.126953125,
      "learning_rate": 5.099587203302374e-05,
      "loss": 14.7512,
      "step": 10097
    },
    {
      "epoch": 10.11,
      "grad_norm": 11196.4638671875,
      "learning_rate": 5.09907120743034e-05,
      "loss": 14.1138,
      "step": 10098
    },
    {
      "epoch": 10.11,
      "grad_norm": 15827.943359375,
      "learning_rate": 5.098555211558308e-05,
      "loss": 13.943,
      "step": 10099
    },
    {
      "epoch": 10.11,
      "grad_norm": 4192.2373046875,
      "learning_rate": 5.0980392156862745e-05,
      "loss": 14.2512,
      "step": 10100
    },
    {
      "epoch": 10.11,
      "grad_norm": 7613.25048828125,
      "learning_rate": 5.097523219814242e-05,
      "loss": 12.3042,
      "step": 10101
    },
    {
      "epoch": 10.11,
      "grad_norm": 13132.66015625,
      "learning_rate": 5.097007223942209e-05,
      "loss": 15.9749,
      "step": 10102
    },
    {
      "epoch": 10.11,
      "grad_norm": 2957.544677734375,
      "learning_rate": 5.096491228070176e-05,
      "loss": 17.2403,
      "step": 10103
    },
    {
      "epoch": 10.11,
      "grad_norm": 30734.349609375,
      "learning_rate": 5.095975232198142e-05,
      "loss": 14.2745,
      "step": 10104
    },
    {
      "epoch": 10.12,
      "grad_norm": 3382.043701171875,
      "learning_rate": 5.09545923632611e-05,
      "loss": 16.1779,
      "step": 10105
    },
    {
      "epoch": 10.12,
      "grad_norm": 22084.01171875,
      "learning_rate": 5.0949432404540765e-05,
      "loss": 16.6493,
      "step": 10106
    },
    {
      "epoch": 10.12,
      "grad_norm": 16735.392578125,
      "learning_rate": 5.094427244582044e-05,
      "loss": 17.4621,
      "step": 10107
    },
    {
      "epoch": 10.12,
      "grad_norm": 9898.017578125,
      "learning_rate": 5.093911248710011e-05,
      "loss": 17.0028,
      "step": 10108
    },
    {
      "epoch": 10.12,
      "grad_norm": 13422.0703125,
      "learning_rate": 5.093395252837978e-05,
      "loss": 18.5013,
      "step": 10109
    },
    {
      "epoch": 10.12,
      "grad_norm": 11317.353515625,
      "learning_rate": 5.092879256965944e-05,
      "loss": 13.6234,
      "step": 10110
    },
    {
      "epoch": 10.12,
      "grad_norm": 18187.3125,
      "learning_rate": 5.092363261093912e-05,
      "loss": 13.6991,
      "step": 10111
    },
    {
      "epoch": 10.12,
      "grad_norm": 86180.5,
      "learning_rate": 5.0918472652218785e-05,
      "loss": 16.975,
      "step": 10112
    },
    {
      "epoch": 10.12,
      "grad_norm": 4997.11865234375,
      "learning_rate": 5.0913312693498446e-05,
      "loss": 16.0443,
      "step": 10113
    },
    {
      "epoch": 10.12,
      "grad_norm": 2487.18115234375,
      "learning_rate": 5.090815273477813e-05,
      "loss": 15.3574,
      "step": 10114
    },
    {
      "epoch": 10.13,
      "grad_norm": 4086.73291015625,
      "learning_rate": 5.090299277605779e-05,
      "loss": 15.6499,
      "step": 10115
    },
    {
      "epoch": 10.13,
      "grad_norm": 2176.099609375,
      "learning_rate": 5.089783281733746e-05,
      "loss": 20.1609,
      "step": 10116
    },
    {
      "epoch": 10.13,
      "grad_norm": 886.6171875,
      "learning_rate": 5.089267285861713e-05,
      "loss": 15.1809,
      "step": 10117
    },
    {
      "epoch": 10.13,
      "grad_norm": 7382.5419921875,
      "learning_rate": 5.0887512899896805e-05,
      "loss": 14.1172,
      "step": 10118
    },
    {
      "epoch": 10.13,
      "grad_norm": 36979.36328125,
      "learning_rate": 5.088235294117647e-05,
      "loss": 14.925,
      "step": 10119
    },
    {
      "epoch": 10.13,
      "grad_norm": 9509.78515625,
      "learning_rate": 5.087719298245615e-05,
      "loss": 15.1824,
      "step": 10120
    },
    {
      "epoch": 10.13,
      "grad_norm": 46873.9296875,
      "learning_rate": 5.087203302373581e-05,
      "loss": 14.7673,
      "step": 10121
    },
    {
      "epoch": 10.13,
      "grad_norm": 4339.0419921875,
      "learning_rate": 5.086687306501548e-05,
      "loss": 21.8102,
      "step": 10122
    },
    {
      "epoch": 10.13,
      "grad_norm": 23388.783203125,
      "learning_rate": 5.086171310629515e-05,
      "loss": 22.1413,
      "step": 10123
    },
    {
      "epoch": 10.13,
      "grad_norm": 10800.021484375,
      "learning_rate": 5.0856553147574825e-05,
      "loss": 21.2743,
      "step": 10124
    },
    {
      "epoch": 10.14,
      "grad_norm": 5032.556640625,
      "learning_rate": 5.085139318885449e-05,
      "loss": 14.5142,
      "step": 10125
    },
    {
      "epoch": 10.14,
      "grad_norm": 2701.64208984375,
      "learning_rate": 5.084623323013417e-05,
      "loss": 14.7097,
      "step": 10126
    },
    {
      "epoch": 10.14,
      "grad_norm": 27164.982421875,
      "learning_rate": 5.084107327141383e-05,
      "loss": 15.1356,
      "step": 10127
    },
    {
      "epoch": 10.14,
      "grad_norm": 43294.30859375,
      "learning_rate": 5.08359133126935e-05,
      "loss": 14.3391,
      "step": 10128
    },
    {
      "epoch": 10.14,
      "grad_norm": 63275.22265625,
      "learning_rate": 5.083075335397317e-05,
      "loss": 17.7559,
      "step": 10129
    },
    {
      "epoch": 10.14,
      "grad_norm": 1787.7685546875,
      "learning_rate": 5.082559339525284e-05,
      "loss": 12.7811,
      "step": 10130
    },
    {
      "epoch": 10.14,
      "grad_norm": 42229.65625,
      "learning_rate": 5.082043343653251e-05,
      "loss": 16.2157,
      "step": 10131
    },
    {
      "epoch": 10.14,
      "grad_norm": 3836.954833984375,
      "learning_rate": 5.0815273477812174e-05,
      "loss": 15.7553,
      "step": 10132
    },
    {
      "epoch": 10.14,
      "grad_norm": 1408.7127685546875,
      "learning_rate": 5.081011351909185e-05,
      "loss": 14.7175,
      "step": 10133
    },
    {
      "epoch": 10.14,
      "grad_norm": 34230.9375,
      "learning_rate": 5.0804953560371516e-05,
      "loss": 16.2973,
      "step": 10134
    },
    {
      "epoch": 10.15,
      "grad_norm": 31849.193359375,
      "learning_rate": 5.079979360165119e-05,
      "loss": 13.8347,
      "step": 10135
    },
    {
      "epoch": 10.15,
      "grad_norm": 5033.70263671875,
      "learning_rate": 5.079463364293086e-05,
      "loss": 15.051,
      "step": 10136
    },
    {
      "epoch": 10.15,
      "grad_norm": 3135.34326171875,
      "learning_rate": 5.078947368421053e-05,
      "loss": 17.2058,
      "step": 10137
    },
    {
      "epoch": 10.15,
      "grad_norm": 19110.0625,
      "learning_rate": 5.0784313725490194e-05,
      "loss": 13.4796,
      "step": 10138
    },
    {
      "epoch": 10.15,
      "grad_norm": 7283.091796875,
      "learning_rate": 5.077915376676987e-05,
      "loss": 16.3086,
      "step": 10139
    },
    {
      "epoch": 10.15,
      "grad_norm": 2453.105712890625,
      "learning_rate": 5.0773993808049536e-05,
      "loss": 16.6846,
      "step": 10140
    },
    {
      "epoch": 10.15,
      "grad_norm": 12863.3671875,
      "learning_rate": 5.076883384932921e-05,
      "loss": 15.2016,
      "step": 10141
    },
    {
      "epoch": 10.15,
      "grad_norm": 11538.3994140625,
      "learning_rate": 5.076367389060888e-05,
      "loss": 16.4821,
      "step": 10142
    },
    {
      "epoch": 10.15,
      "grad_norm": 12347.533203125,
      "learning_rate": 5.075851393188855e-05,
      "loss": 12.058,
      "step": 10143
    },
    {
      "epoch": 10.15,
      "grad_norm": 12604.7392578125,
      "learning_rate": 5.0753353973168214e-05,
      "loss": 13.8818,
      "step": 10144
    },
    {
      "epoch": 10.16,
      "grad_norm": 14351.4501953125,
      "learning_rate": 5.074819401444789e-05,
      "loss": 21.7245,
      "step": 10145
    },
    {
      "epoch": 10.16,
      "grad_norm": 20385.958984375,
      "learning_rate": 5.0743034055727556e-05,
      "loss": 20.7317,
      "step": 10146
    },
    {
      "epoch": 10.16,
      "grad_norm": 14661.330078125,
      "learning_rate": 5.0737874097007224e-05,
      "loss": 18.1273,
      "step": 10147
    },
    {
      "epoch": 10.16,
      "grad_norm": 3437.4599609375,
      "learning_rate": 5.07327141382869e-05,
      "loss": 16.133,
      "step": 10148
    },
    {
      "epoch": 10.16,
      "grad_norm": 5601.3212890625,
      "learning_rate": 5.072755417956656e-05,
      "loss": 19.9357,
      "step": 10149
    },
    {
      "epoch": 10.16,
      "grad_norm": 2779.56201171875,
      "learning_rate": 5.0722394220846234e-05,
      "loss": 13.8594,
      "step": 10150
    },
    {
      "epoch": 10.16,
      "grad_norm": 23979.447265625,
      "learning_rate": 5.07172342621259e-05,
      "loss": 16.6158,
      "step": 10151
    },
    {
      "epoch": 10.16,
      "grad_norm": 4206.73486328125,
      "learning_rate": 5.0712074303405576e-05,
      "loss": 12.555,
      "step": 10152
    },
    {
      "epoch": 10.16,
      "grad_norm": 30228.580078125,
      "learning_rate": 5.0706914344685244e-05,
      "loss": 19.0484,
      "step": 10153
    },
    {
      "epoch": 10.16,
      "grad_norm": 1891.37890625,
      "learning_rate": 5.070175438596492e-05,
      "loss": 14.4227,
      "step": 10154
    },
    {
      "epoch": 10.17,
      "grad_norm": 33059.45703125,
      "learning_rate": 5.069659442724458e-05,
      "loss": 15.8909,
      "step": 10155
    },
    {
      "epoch": 10.17,
      "grad_norm": 3854.663818359375,
      "learning_rate": 5.0691434468524254e-05,
      "loss": 16.305,
      "step": 10156
    },
    {
      "epoch": 10.17,
      "grad_norm": 13332.876953125,
      "learning_rate": 5.068627450980392e-05,
      "loss": 14.8427,
      "step": 10157
    },
    {
      "epoch": 10.17,
      "grad_norm": 26109.232421875,
      "learning_rate": 5.0681114551083596e-05,
      "loss": 16.4353,
      "step": 10158
    },
    {
      "epoch": 10.17,
      "grad_norm": 2437.806396484375,
      "learning_rate": 5.0675954592363264e-05,
      "loss": 12.3479,
      "step": 10159
    },
    {
      "epoch": 10.17,
      "grad_norm": 12823.767578125,
      "learning_rate": 5.067079463364294e-05,
      "loss": 13.1403,
      "step": 10160
    },
    {
      "epoch": 10.17,
      "grad_norm": 11706.40625,
      "learning_rate": 5.06656346749226e-05,
      "loss": 18.6938,
      "step": 10161
    },
    {
      "epoch": 10.17,
      "grad_norm": 18168.83203125,
      "learning_rate": 5.066047471620228e-05,
      "loss": 14.9416,
      "step": 10162
    },
    {
      "epoch": 10.17,
      "grad_norm": 7650.9404296875,
      "learning_rate": 5.065531475748194e-05,
      "loss": 15.8957,
      "step": 10163
    },
    {
      "epoch": 10.17,
      "grad_norm": 8009.19873046875,
      "learning_rate": 5.0650154798761616e-05,
      "loss": 12.7509,
      "step": 10164
    },
    {
      "epoch": 10.18,
      "grad_norm": 2214.30908203125,
      "learning_rate": 5.0644994840041284e-05,
      "loss": 13.9323,
      "step": 10165
    },
    {
      "epoch": 10.18,
      "grad_norm": 3074.216064453125,
      "learning_rate": 5.0639834881320945e-05,
      "loss": 14.0887,
      "step": 10166
    },
    {
      "epoch": 10.18,
      "grad_norm": 22081.892578125,
      "learning_rate": 5.063467492260062e-05,
      "loss": 16.2617,
      "step": 10167
    },
    {
      "epoch": 10.18,
      "grad_norm": 2205.833984375,
      "learning_rate": 5.062951496388029e-05,
      "loss": 18.1874,
      "step": 10168
    },
    {
      "epoch": 10.18,
      "grad_norm": 4138.48974609375,
      "learning_rate": 5.062435500515996e-05,
      "loss": 15.9052,
      "step": 10169
    },
    {
      "epoch": 10.18,
      "grad_norm": 14850.7177734375,
      "learning_rate": 5.061919504643963e-05,
      "loss": 26.5517,
      "step": 10170
    },
    {
      "epoch": 10.18,
      "grad_norm": 5150.0146484375,
      "learning_rate": 5.0614035087719304e-05,
      "loss": 16.8941,
      "step": 10171
    },
    {
      "epoch": 10.18,
      "grad_norm": 6358.22998046875,
      "learning_rate": 5.0608875128998965e-05,
      "loss": 16.1663,
      "step": 10172
    },
    {
      "epoch": 10.18,
      "grad_norm": 57222.83984375,
      "learning_rate": 5.060371517027864e-05,
      "loss": 17.4436,
      "step": 10173
    },
    {
      "epoch": 10.18,
      "grad_norm": 6525.5146484375,
      "learning_rate": 5.059855521155831e-05,
      "loss": 14.2453,
      "step": 10174
    },
    {
      "epoch": 10.19,
      "grad_norm": 4276.60498046875,
      "learning_rate": 5.059339525283798e-05,
      "loss": 19.2618,
      "step": 10175
    },
    {
      "epoch": 10.19,
      "grad_norm": 3221.596923828125,
      "learning_rate": 5.058823529411765e-05,
      "loss": 13.5599,
      "step": 10176
    },
    {
      "epoch": 10.19,
      "grad_norm": 2218.0908203125,
      "learning_rate": 5.0583075335397324e-05,
      "loss": 14.7193,
      "step": 10177
    },
    {
      "epoch": 10.19,
      "grad_norm": 6790.0791015625,
      "learning_rate": 5.0577915376676985e-05,
      "loss": 14.3429,
      "step": 10178
    },
    {
      "epoch": 10.19,
      "grad_norm": 115491.625,
      "learning_rate": 5.0572755417956666e-05,
      "loss": 14.7394,
      "step": 10179
    },
    {
      "epoch": 10.19,
      "grad_norm": 8754.1181640625,
      "learning_rate": 5.056759545923633e-05,
      "loss": 13.4326,
      "step": 10180
    },
    {
      "epoch": 10.19,
      "grad_norm": 2386.33837890625,
      "learning_rate": 5.0562435500516e-05,
      "loss": 15.573,
      "step": 10181
    },
    {
      "epoch": 10.19,
      "grad_norm": 3450.562255859375,
      "learning_rate": 5.055727554179567e-05,
      "loss": 17.3947,
      "step": 10182
    },
    {
      "epoch": 10.19,
      "grad_norm": 52335.5859375,
      "learning_rate": 5.055211558307533e-05,
      "loss": 20.6401,
      "step": 10183
    },
    {
      "epoch": 10.19,
      "grad_norm": 11135.1318359375,
      "learning_rate": 5.0546955624355005e-05,
      "loss": 14.2111,
      "step": 10184
    },
    {
      "epoch": 10.2,
      "grad_norm": 844.8444213867188,
      "learning_rate": 5.054179566563467e-05,
      "loss": 26.5913,
      "step": 10185
    },
    {
      "epoch": 10.2,
      "grad_norm": 27076.275390625,
      "learning_rate": 5.053663570691435e-05,
      "loss": 18.5449,
      "step": 10186
    },
    {
      "epoch": 10.2,
      "grad_norm": 28352.44140625,
      "learning_rate": 5.0531475748194015e-05,
      "loss": 22.8853,
      "step": 10187
    },
    {
      "epoch": 10.2,
      "grad_norm": 7662.6181640625,
      "learning_rate": 5.052631578947369e-05,
      "loss": 15.3401,
      "step": 10188
    },
    {
      "epoch": 10.2,
      "grad_norm": 3210.043701171875,
      "learning_rate": 5.052115583075335e-05,
      "loss": 16.0708,
      "step": 10189
    },
    {
      "epoch": 10.2,
      "grad_norm": 6148.58447265625,
      "learning_rate": 5.051599587203303e-05,
      "loss": 17.1962,
      "step": 10190
    },
    {
      "epoch": 10.2,
      "grad_norm": 10845.2919921875,
      "learning_rate": 5.051083591331269e-05,
      "loss": 16.0848,
      "step": 10191
    },
    {
      "epoch": 10.2,
      "grad_norm": 13180.345703125,
      "learning_rate": 5.050567595459237e-05,
      "loss": 15.6375,
      "step": 10192
    },
    {
      "epoch": 10.2,
      "grad_norm": 9488.51171875,
      "learning_rate": 5.0500515995872035e-05,
      "loss": 15.9674,
      "step": 10193
    },
    {
      "epoch": 10.2,
      "grad_norm": 11162.9990234375,
      "learning_rate": 5.049535603715171e-05,
      "loss": 19.3724,
      "step": 10194
    },
    {
      "epoch": 10.21,
      "grad_norm": 27038.8984375,
      "learning_rate": 5.049019607843137e-05,
      "loss": 16.4098,
      "step": 10195
    },
    {
      "epoch": 10.21,
      "grad_norm": 27649.82421875,
      "learning_rate": 5.048503611971105e-05,
      "loss": 15.6117,
      "step": 10196
    },
    {
      "epoch": 10.21,
      "grad_norm": 34542.2265625,
      "learning_rate": 5.047987616099071e-05,
      "loss": 16.9668,
      "step": 10197
    },
    {
      "epoch": 10.21,
      "grad_norm": 8014.552734375,
      "learning_rate": 5.047471620227039e-05,
      "loss": 16.8783,
      "step": 10198
    },
    {
      "epoch": 10.21,
      "grad_norm": 4024.280517578125,
      "learning_rate": 5.0469556243550055e-05,
      "loss": 15.3312,
      "step": 10199
    },
    {
      "epoch": 10.21,
      "grad_norm": 15614.576171875,
      "learning_rate": 5.046439628482973e-05,
      "loss": 15.5526,
      "step": 10200
    },
    {
      "epoch": 10.21,
      "grad_norm": 1547.35400390625,
      "learning_rate": 5.045923632610939e-05,
      "loss": 16.5786,
      "step": 10201
    },
    {
      "epoch": 10.21,
      "grad_norm": 4108.19189453125,
      "learning_rate": 5.045407636738906e-05,
      "loss": 17.3849,
      "step": 10202
    },
    {
      "epoch": 10.21,
      "grad_norm": 8149.107421875,
      "learning_rate": 5.044891640866873e-05,
      "loss": 17.9786,
      "step": 10203
    },
    {
      "epoch": 10.21,
      "grad_norm": 2063.388916015625,
      "learning_rate": 5.04437564499484e-05,
      "loss": 14.8209,
      "step": 10204
    },
    {
      "epoch": 10.22,
      "grad_norm": 6508.20458984375,
      "learning_rate": 5.0438596491228075e-05,
      "loss": 15.6511,
      "step": 10205
    },
    {
      "epoch": 10.22,
      "grad_norm": 2997.204833984375,
      "learning_rate": 5.0433436532507736e-05,
      "loss": 15.7121,
      "step": 10206
    },
    {
      "epoch": 10.22,
      "grad_norm": 12752.884765625,
      "learning_rate": 5.042827657378742e-05,
      "loss": 19.321,
      "step": 10207
    },
    {
      "epoch": 10.22,
      "grad_norm": 900.97705078125,
      "learning_rate": 5.042311661506708e-05,
      "loss": 13.6407,
      "step": 10208
    },
    {
      "epoch": 10.22,
      "grad_norm": 1991.612060546875,
      "learning_rate": 5.041795665634675e-05,
      "loss": 22.0067,
      "step": 10209
    },
    {
      "epoch": 10.22,
      "grad_norm": 16985.890625,
      "learning_rate": 5.041279669762642e-05,
      "loss": 24.3699,
      "step": 10210
    },
    {
      "epoch": 10.22,
      "grad_norm": 5955.43701171875,
      "learning_rate": 5.0407636738906095e-05,
      "loss": 18.4001,
      "step": 10211
    },
    {
      "epoch": 10.22,
      "grad_norm": 4069.119384765625,
      "learning_rate": 5.0402476780185756e-05,
      "loss": 13.537,
      "step": 10212
    },
    {
      "epoch": 10.22,
      "grad_norm": 794.9126586914062,
      "learning_rate": 5.039731682146544e-05,
      "loss": 16.1872,
      "step": 10213
    },
    {
      "epoch": 10.22,
      "grad_norm": 4296.8193359375,
      "learning_rate": 5.03921568627451e-05,
      "loss": 19.2439,
      "step": 10214
    },
    {
      "epoch": 10.23,
      "grad_norm": 10472.548828125,
      "learning_rate": 5.038699690402477e-05,
      "loss": 17.428,
      "step": 10215
    },
    {
      "epoch": 10.23,
      "grad_norm": 13636.61328125,
      "learning_rate": 5.038183694530444e-05,
      "loss": 16.6061,
      "step": 10216
    },
    {
      "epoch": 10.23,
      "grad_norm": 2015.5263671875,
      "learning_rate": 5.0376676986584115e-05,
      "loss": 15.2052,
      "step": 10217
    },
    {
      "epoch": 10.23,
      "grad_norm": 12172.6455078125,
      "learning_rate": 5.037151702786378e-05,
      "loss": 20.5332,
      "step": 10218
    },
    {
      "epoch": 10.23,
      "grad_norm": 9104.4375,
      "learning_rate": 5.0366357069143444e-05,
      "loss": 13.1213,
      "step": 10219
    },
    {
      "epoch": 10.23,
      "grad_norm": 2679.568359375,
      "learning_rate": 5.036119711042312e-05,
      "loss": 16.244,
      "step": 10220
    },
    {
      "epoch": 10.23,
      "grad_norm": 3487.800537109375,
      "learning_rate": 5.0356037151702786e-05,
      "loss": 18.4335,
      "step": 10221
    },
    {
      "epoch": 10.23,
      "grad_norm": 11911.8837890625,
      "learning_rate": 5.035087719298246e-05,
      "loss": 18.9342,
      "step": 10222
    },
    {
      "epoch": 10.23,
      "grad_norm": 18707.1875,
      "learning_rate": 5.034571723426212e-05,
      "loss": 16.8707,
      "step": 10223
    },
    {
      "epoch": 10.23,
      "grad_norm": 68989.4296875,
      "learning_rate": 5.03405572755418e-05,
      "loss": 15.0066,
      "step": 10224
    },
    {
      "epoch": 10.24,
      "grad_norm": 40313.59765625,
      "learning_rate": 5.0335397316821464e-05,
      "loss": 17.0917,
      "step": 10225
    },
    {
      "epoch": 10.24,
      "grad_norm": 6566.9736328125,
      "learning_rate": 5.033023735810114e-05,
      "loss": 15.2398,
      "step": 10226
    },
    {
      "epoch": 10.24,
      "grad_norm": 12547.8056640625,
      "learning_rate": 5.0325077399380806e-05,
      "loss": 13.8412,
      "step": 10227
    },
    {
      "epoch": 10.24,
      "grad_norm": 4782.6103515625,
      "learning_rate": 5.031991744066048e-05,
      "loss": 14.5211,
      "step": 10228
    },
    {
      "epoch": 10.24,
      "grad_norm": 8176.43359375,
      "learning_rate": 5.031475748194014e-05,
      "loss": 18.5463,
      "step": 10229
    },
    {
      "epoch": 10.24,
      "grad_norm": 8462.02734375,
      "learning_rate": 5.030959752321982e-05,
      "loss": 15.8408,
      "step": 10230
    },
    {
      "epoch": 10.24,
      "grad_norm": 21250.85546875,
      "learning_rate": 5.0304437564499484e-05,
      "loss": 22.3674,
      "step": 10231
    },
    {
      "epoch": 10.24,
      "grad_norm": 18526.029296875,
      "learning_rate": 5.029927760577916e-05,
      "loss": 18.3553,
      "step": 10232
    },
    {
      "epoch": 10.24,
      "grad_norm": 1699.09814453125,
      "learning_rate": 5.0294117647058826e-05,
      "loss": 14.4555,
      "step": 10233
    },
    {
      "epoch": 10.24,
      "grad_norm": 2514.242431640625,
      "learning_rate": 5.02889576883385e-05,
      "loss": 13.6156,
      "step": 10234
    },
    {
      "epoch": 10.25,
      "grad_norm": 3057.841552734375,
      "learning_rate": 5.028379772961817e-05,
      "loss": 15.3032,
      "step": 10235
    },
    {
      "epoch": 10.25,
      "grad_norm": 10044.0615234375,
      "learning_rate": 5.027863777089784e-05,
      "loss": 17.9313,
      "step": 10236
    },
    {
      "epoch": 10.25,
      "grad_norm": 5195.9716796875,
      "learning_rate": 5.0273477812177504e-05,
      "loss": 13.6531,
      "step": 10237
    },
    {
      "epoch": 10.25,
      "grad_norm": 7213.4833984375,
      "learning_rate": 5.026831785345717e-05,
      "loss": 19.6406,
      "step": 10238
    },
    {
      "epoch": 10.25,
      "grad_norm": 13895.2685546875,
      "learning_rate": 5.0263157894736846e-05,
      "loss": 10.9725,
      "step": 10239
    },
    {
      "epoch": 10.25,
      "grad_norm": 10150.8017578125,
      "learning_rate": 5.025799793601651e-05,
      "loss": 13.2225,
      "step": 10240
    },
    {
      "epoch": 10.25,
      "grad_norm": 6840.234375,
      "learning_rate": 5.025283797729619e-05,
      "loss": 14.4635,
      "step": 10241
    },
    {
      "epoch": 10.25,
      "grad_norm": 2698.88671875,
      "learning_rate": 5.024767801857585e-05,
      "loss": 17.2399,
      "step": 10242
    },
    {
      "epoch": 10.25,
      "grad_norm": 70988.015625,
      "learning_rate": 5.0242518059855524e-05,
      "loss": 17.1399,
      "step": 10243
    },
    {
      "epoch": 10.25,
      "grad_norm": 53097.56640625,
      "learning_rate": 5.023735810113519e-05,
      "loss": 16.4489,
      "step": 10244
    },
    {
      "epoch": 10.26,
      "grad_norm": 5387.80517578125,
      "learning_rate": 5.0232198142414866e-05,
      "loss": 15.3315,
      "step": 10245
    },
    {
      "epoch": 10.26,
      "grad_norm": 11604.3974609375,
      "learning_rate": 5.0227038183694534e-05,
      "loss": 19.7153,
      "step": 10246
    },
    {
      "epoch": 10.26,
      "grad_norm": 8344.0185546875,
      "learning_rate": 5.022187822497421e-05,
      "loss": 17.1607,
      "step": 10247
    },
    {
      "epoch": 10.26,
      "grad_norm": 1682.18310546875,
      "learning_rate": 5.021671826625387e-05,
      "loss": 17.1825,
      "step": 10248
    },
    {
      "epoch": 10.26,
      "grad_norm": 41933.03125,
      "learning_rate": 5.0211558307533544e-05,
      "loss": 15.5772,
      "step": 10249
    },
    {
      "epoch": 10.26,
      "grad_norm": 40395.1796875,
      "learning_rate": 5.020639834881321e-05,
      "loss": 16.4641,
      "step": 10250
    },
    {
      "epoch": 10.26,
      "grad_norm": 32957.328125,
      "learning_rate": 5.0201238390092886e-05,
      "loss": 19.4604,
      "step": 10251
    },
    {
      "epoch": 10.26,
      "grad_norm": 12722.7919921875,
      "learning_rate": 5.0196078431372554e-05,
      "loss": 14.5131,
      "step": 10252
    },
    {
      "epoch": 10.26,
      "grad_norm": 6117.15185546875,
      "learning_rate": 5.019091847265223e-05,
      "loss": 17.9635,
      "step": 10253
    },
    {
      "epoch": 10.26,
      "grad_norm": 10378.6142578125,
      "learning_rate": 5.018575851393189e-05,
      "loss": 16.1432,
      "step": 10254
    },
    {
      "epoch": 10.27,
      "grad_norm": 35633.45703125,
      "learning_rate": 5.018059855521156e-05,
      "loss": 18.841,
      "step": 10255
    },
    {
      "epoch": 10.27,
      "grad_norm": 5355.087890625,
      "learning_rate": 5.017543859649123e-05,
      "loss": 14.6606,
      "step": 10256
    },
    {
      "epoch": 10.27,
      "grad_norm": 3994.830322265625,
      "learning_rate": 5.017027863777089e-05,
      "loss": 14.4639,
      "step": 10257
    },
    {
      "epoch": 10.27,
      "grad_norm": 6681.8388671875,
      "learning_rate": 5.0165118679050574e-05,
      "loss": 15.1749,
      "step": 10258
    },
    {
      "epoch": 10.27,
      "grad_norm": 6617.67041015625,
      "learning_rate": 5.0159958720330235e-05,
      "loss": 17.4139,
      "step": 10259
    },
    {
      "epoch": 10.27,
      "grad_norm": 2563.052490234375,
      "learning_rate": 5.015479876160991e-05,
      "loss": 18.5182,
      "step": 10260
    },
    {
      "epoch": 10.27,
      "grad_norm": 4980.40380859375,
      "learning_rate": 5.014963880288958e-05,
      "loss": 15.0925,
      "step": 10261
    },
    {
      "epoch": 10.27,
      "grad_norm": 54485.1015625,
      "learning_rate": 5.014447884416925e-05,
      "loss": 15.9079,
      "step": 10262
    },
    {
      "epoch": 10.27,
      "grad_norm": 5450.80517578125,
      "learning_rate": 5.013931888544892e-05,
      "loss": 23.8946,
      "step": 10263
    },
    {
      "epoch": 10.27,
      "grad_norm": 7089.40478515625,
      "learning_rate": 5.0134158926728594e-05,
      "loss": 16.3258,
      "step": 10264
    },
    {
      "epoch": 10.28,
      "grad_norm": 23414.03125,
      "learning_rate": 5.0128998968008255e-05,
      "loss": 16.8665,
      "step": 10265
    },
    {
      "epoch": 10.28,
      "grad_norm": 15221.962890625,
      "learning_rate": 5.012383900928793e-05,
      "loss": 17.2616,
      "step": 10266
    },
    {
      "epoch": 10.28,
      "grad_norm": 21024.005859375,
      "learning_rate": 5.01186790505676e-05,
      "loss": 11.1585,
      "step": 10267
    },
    {
      "epoch": 10.28,
      "grad_norm": 12126.2421875,
      "learning_rate": 5.011351909184727e-05,
      "loss": 18.0882,
      "step": 10268
    },
    {
      "epoch": 10.28,
      "grad_norm": 5605.630859375,
      "learning_rate": 5.010835913312694e-05,
      "loss": 14.9782,
      "step": 10269
    },
    {
      "epoch": 10.28,
      "grad_norm": 12519.5439453125,
      "learning_rate": 5.0103199174406614e-05,
      "loss": 19.5064,
      "step": 10270
    },
    {
      "epoch": 10.28,
      "grad_norm": 5071.82958984375,
      "learning_rate": 5.0098039215686275e-05,
      "loss": 14.6969,
      "step": 10271
    },
    {
      "epoch": 10.28,
      "grad_norm": 9968.1572265625,
      "learning_rate": 5.009287925696595e-05,
      "loss": 16.3533,
      "step": 10272
    },
    {
      "epoch": 10.28,
      "grad_norm": 6404.1787109375,
      "learning_rate": 5.008771929824562e-05,
      "loss": 16.0,
      "step": 10273
    },
    {
      "epoch": 10.28,
      "grad_norm": 6057.150390625,
      "learning_rate": 5.008255933952528e-05,
      "loss": 12.9629,
      "step": 10274
    },
    {
      "epoch": 10.29,
      "grad_norm": 72994.8359375,
      "learning_rate": 5.007739938080496e-05,
      "loss": 13.5487,
      "step": 10275
    },
    {
      "epoch": 10.29,
      "grad_norm": 3267.0166015625,
      "learning_rate": 5.007223942208462e-05,
      "loss": 17.7707,
      "step": 10276
    },
    {
      "epoch": 10.29,
      "grad_norm": 7609.2607421875,
      "learning_rate": 5.0067079463364295e-05,
      "loss": 20.8203,
      "step": 10277
    },
    {
      "epoch": 10.29,
      "grad_norm": 13294.5537109375,
      "learning_rate": 5.006191950464396e-05,
      "loss": 15.6504,
      "step": 10278
    },
    {
      "epoch": 10.29,
      "grad_norm": 8416.583984375,
      "learning_rate": 5.005675954592364e-05,
      "loss": 16.0439,
      "step": 10279
    },
    {
      "epoch": 10.29,
      "grad_norm": 2963.067626953125,
      "learning_rate": 5.0051599587203305e-05,
      "loss": 15.1249,
      "step": 10280
    },
    {
      "epoch": 10.29,
      "grad_norm": 3589.7890625,
      "learning_rate": 5.004643962848298e-05,
      "loss": 14.8347,
      "step": 10281
    },
    {
      "epoch": 10.29,
      "grad_norm": 21203.919921875,
      "learning_rate": 5.004127966976264e-05,
      "loss": 14.5895,
      "step": 10282
    },
    {
      "epoch": 10.29,
      "grad_norm": 12266.4873046875,
      "learning_rate": 5.0036119711042315e-05,
      "loss": 14.9885,
      "step": 10283
    },
    {
      "epoch": 10.29,
      "grad_norm": 16466.705078125,
      "learning_rate": 5.003095975232198e-05,
      "loss": 17.1493,
      "step": 10284
    },
    {
      "epoch": 10.3,
      "grad_norm": 37943.4375,
      "learning_rate": 5.002579979360166e-05,
      "loss": 17.1327,
      "step": 10285
    },
    {
      "epoch": 10.3,
      "grad_norm": 12062.6005859375,
      "learning_rate": 5.0020639834881325e-05,
      "loss": 15.5691,
      "step": 10286
    },
    {
      "epoch": 10.3,
      "grad_norm": 461.3449401855469,
      "learning_rate": 5.0015479876161e-05,
      "loss": 14.4567,
      "step": 10287
    },
    {
      "epoch": 10.3,
      "grad_norm": 9521.6494140625,
      "learning_rate": 5.001031991744066e-05,
      "loss": 14.834,
      "step": 10288
    },
    {
      "epoch": 10.3,
      "grad_norm": 2667.942626953125,
      "learning_rate": 5.0005159958720335e-05,
      "loss": 14.2561,
      "step": 10289
    },
    {
      "epoch": 10.3,
      "grad_norm": 4153.9375,
      "learning_rate": 5e-05,
      "loss": 15.9505,
      "step": 10290
    },
    {
      "epoch": 10.3,
      "grad_norm": 9408.423828125,
      "learning_rate": 4.999484004127967e-05,
      "loss": 15.1269,
      "step": 10291
    },
    {
      "epoch": 10.3,
      "grad_norm": 8463.2705078125,
      "learning_rate": 4.9989680082559345e-05,
      "loss": 17.4376,
      "step": 10292
    },
    {
      "epoch": 10.3,
      "grad_norm": 2566.33642578125,
      "learning_rate": 4.998452012383901e-05,
      "loss": 15.6303,
      "step": 10293
    },
    {
      "epoch": 10.3,
      "grad_norm": 17280.919921875,
      "learning_rate": 4.997936016511868e-05,
      "loss": 20.6501,
      "step": 10294
    },
    {
      "epoch": 10.31,
      "grad_norm": 12997.0859375,
      "learning_rate": 4.9974200206398355e-05,
      "loss": 15.1319,
      "step": 10295
    },
    {
      "epoch": 10.31,
      "grad_norm": 69938.8046875,
      "learning_rate": 4.996904024767802e-05,
      "loss": 19.1505,
      "step": 10296
    },
    {
      "epoch": 10.31,
      "grad_norm": 20509.763671875,
      "learning_rate": 4.996388028895769e-05,
      "loss": 17.0251,
      "step": 10297
    },
    {
      "epoch": 10.31,
      "grad_norm": 8037.61767578125,
      "learning_rate": 4.9958720330237365e-05,
      "loss": 23.2401,
      "step": 10298
    },
    {
      "epoch": 10.31,
      "grad_norm": 12472.6005859375,
      "learning_rate": 4.9953560371517026e-05,
      "loss": 21.026,
      "step": 10299
    },
    {
      "epoch": 10.31,
      "grad_norm": 6907.08251953125,
      "learning_rate": 4.99484004127967e-05,
      "loss": 19.1935,
      "step": 10300
    },
    {
      "epoch": 10.31,
      "grad_norm": 10023.8818359375,
      "learning_rate": 4.994324045407637e-05,
      "loss": 15.5114,
      "step": 10301
    },
    {
      "epoch": 10.31,
      "grad_norm": 56268.7578125,
      "learning_rate": 4.9938080495356036e-05,
      "loss": 17.9363,
      "step": 10302
    },
    {
      "epoch": 10.31,
      "grad_norm": 14772.837890625,
      "learning_rate": 4.993292053663571e-05,
      "loss": 17.6221,
      "step": 10303
    },
    {
      "epoch": 10.31,
      "grad_norm": 6755.59912109375,
      "learning_rate": 4.992776057791538e-05,
      "loss": 15.0536,
      "step": 10304
    },
    {
      "epoch": 10.32,
      "grad_norm": 3531.971923828125,
      "learning_rate": 4.9922600619195046e-05,
      "loss": 15.481,
      "step": 10305
    },
    {
      "epoch": 10.32,
      "grad_norm": 2036.00439453125,
      "learning_rate": 4.991744066047472e-05,
      "loss": 15.6114,
      "step": 10306
    },
    {
      "epoch": 10.32,
      "grad_norm": 16339.837890625,
      "learning_rate": 4.991228070175439e-05,
      "loss": 20.9168,
      "step": 10307
    },
    {
      "epoch": 10.32,
      "grad_norm": 6344.5634765625,
      "learning_rate": 4.9907120743034056e-05,
      "loss": 13.1353,
      "step": 10308
    },
    {
      "epoch": 10.32,
      "grad_norm": 2434.4150390625,
      "learning_rate": 4.990196078431373e-05,
      "loss": 15.9531,
      "step": 10309
    },
    {
      "epoch": 10.32,
      "grad_norm": 11621.0673828125,
      "learning_rate": 4.98968008255934e-05,
      "loss": 17.3661,
      "step": 10310
    },
    {
      "epoch": 10.32,
      "grad_norm": 5963.7783203125,
      "learning_rate": 4.9891640866873066e-05,
      "loss": 25.698,
      "step": 10311
    },
    {
      "epoch": 10.32,
      "grad_norm": 7351.51416015625,
      "learning_rate": 4.988648090815274e-05,
      "loss": 13.6562,
      "step": 10312
    },
    {
      "epoch": 10.32,
      "grad_norm": 2218.0908203125,
      "learning_rate": 4.988132094943241e-05,
      "loss": 15.2302,
      "step": 10313
    },
    {
      "epoch": 10.32,
      "grad_norm": 4184.25830078125,
      "learning_rate": 4.9876160990712076e-05,
      "loss": 20.6898,
      "step": 10314
    },
    {
      "epoch": 10.33,
      "grad_norm": 13354.62890625,
      "learning_rate": 4.987100103199175e-05,
      "loss": 17.9249,
      "step": 10315
    },
    {
      "epoch": 10.33,
      "grad_norm": 49359.0859375,
      "learning_rate": 4.986584107327142e-05,
      "loss": 18.8448,
      "step": 10316
    },
    {
      "epoch": 10.33,
      "grad_norm": 78223.15625,
      "learning_rate": 4.9860681114551086e-05,
      "loss": 20.9974,
      "step": 10317
    },
    {
      "epoch": 10.33,
      "grad_norm": 3590.18212890625,
      "learning_rate": 4.985552115583075e-05,
      "loss": 18.4848,
      "step": 10318
    },
    {
      "epoch": 10.33,
      "grad_norm": 8801.2138671875,
      "learning_rate": 4.985036119711042e-05,
      "loss": 14.9682,
      "step": 10319
    },
    {
      "epoch": 10.33,
      "grad_norm": 9908.1435546875,
      "learning_rate": 4.9845201238390096e-05,
      "loss": 14.2835,
      "step": 10320
    },
    {
      "epoch": 10.33,
      "grad_norm": 19612.96484375,
      "learning_rate": 4.984004127966976e-05,
      "loss": 15.4169,
      "step": 10321
    },
    {
      "epoch": 10.33,
      "grad_norm": 4977.14111328125,
      "learning_rate": 4.983488132094943e-05,
      "loss": 15.2449,
      "step": 10322
    },
    {
      "epoch": 10.33,
      "grad_norm": 11150.5126953125,
      "learning_rate": 4.9829721362229106e-05,
      "loss": 17.5961,
      "step": 10323
    },
    {
      "epoch": 10.33,
      "grad_norm": 2269.571044921875,
      "learning_rate": 4.9824561403508773e-05,
      "loss": 11.9218,
      "step": 10324
    },
    {
      "epoch": 10.34,
      "grad_norm": 14723.751953125,
      "learning_rate": 4.981940144478844e-05,
      "loss": 18.5201,
      "step": 10325
    },
    {
      "epoch": 10.34,
      "grad_norm": 13721.7548828125,
      "learning_rate": 4.9814241486068116e-05,
      "loss": 14.8758,
      "step": 10326
    },
    {
      "epoch": 10.34,
      "grad_norm": 10968.67578125,
      "learning_rate": 4.9809081527347783e-05,
      "loss": 23.0676,
      "step": 10327
    },
    {
      "epoch": 10.34,
      "grad_norm": 12739.775390625,
      "learning_rate": 4.980392156862745e-05,
      "loss": 19.9169,
      "step": 10328
    },
    {
      "epoch": 10.34,
      "grad_norm": 12941.4248046875,
      "learning_rate": 4.9798761609907126e-05,
      "loss": 19.6168,
      "step": 10329
    },
    {
      "epoch": 10.34,
      "grad_norm": 11735.2666015625,
      "learning_rate": 4.9793601651186793e-05,
      "loss": 17.4158,
      "step": 10330
    },
    {
      "epoch": 10.34,
      "grad_norm": 10607.5263671875,
      "learning_rate": 4.978844169246646e-05,
      "loss": 18.332,
      "step": 10331
    },
    {
      "epoch": 10.34,
      "grad_norm": 31486.810546875,
      "learning_rate": 4.9783281733746136e-05,
      "loss": 16.0335,
      "step": 10332
    },
    {
      "epoch": 10.34,
      "grad_norm": 11491.5751953125,
      "learning_rate": 4.9778121775025803e-05,
      "loss": 14.4602,
      "step": 10333
    },
    {
      "epoch": 10.34,
      "grad_norm": 2641.198486328125,
      "learning_rate": 4.977296181630547e-05,
      "loss": 16.0411,
      "step": 10334
    },
    {
      "epoch": 10.35,
      "grad_norm": 141544.171875,
      "learning_rate": 4.976780185758514e-05,
      "loss": 18.5387,
      "step": 10335
    },
    {
      "epoch": 10.35,
      "grad_norm": 7610.2421875,
      "learning_rate": 4.976264189886481e-05,
      "loss": 26.9686,
      "step": 10336
    },
    {
      "epoch": 10.35,
      "grad_norm": 9888.1904296875,
      "learning_rate": 4.975748194014448e-05,
      "loss": 19.5782,
      "step": 10337
    },
    {
      "epoch": 10.35,
      "grad_norm": 548.5320434570312,
      "learning_rate": 4.975232198142415e-05,
      "loss": 16.2339,
      "step": 10338
    },
    {
      "epoch": 10.35,
      "grad_norm": 7690.986328125,
      "learning_rate": 4.974716202270382e-05,
      "loss": 18.6525,
      "step": 10339
    },
    {
      "epoch": 10.35,
      "grad_norm": 17197.296875,
      "learning_rate": 4.974200206398349e-05,
      "loss": 17.1226,
      "step": 10340
    },
    {
      "epoch": 10.35,
      "grad_norm": 1600.250732421875,
      "learning_rate": 4.973684210526316e-05,
      "loss": 15.5718,
      "step": 10341
    },
    {
      "epoch": 10.35,
      "grad_norm": 4742.39453125,
      "learning_rate": 4.973168214654283e-05,
      "loss": 12.0765,
      "step": 10342
    },
    {
      "epoch": 10.35,
      "grad_norm": 9756.708984375,
      "learning_rate": 4.97265221878225e-05,
      "loss": 15.9666,
      "step": 10343
    },
    {
      "epoch": 10.35,
      "grad_norm": 3567.7060546875,
      "learning_rate": 4.972136222910217e-05,
      "loss": 14.6683,
      "step": 10344
    },
    {
      "epoch": 10.36,
      "grad_norm": 13529.0908203125,
      "learning_rate": 4.971620227038184e-05,
      "loss": 16.2444,
      "step": 10345
    },
    {
      "epoch": 10.36,
      "grad_norm": 2664.067138671875,
      "learning_rate": 4.971104231166151e-05,
      "loss": 15.8444,
      "step": 10346
    },
    {
      "epoch": 10.36,
      "grad_norm": 4805.80419921875,
      "learning_rate": 4.970588235294118e-05,
      "loss": 16.4606,
      "step": 10347
    },
    {
      "epoch": 10.36,
      "grad_norm": 5419.048828125,
      "learning_rate": 4.970072239422085e-05,
      "loss": 13.4706,
      "step": 10348
    },
    {
      "epoch": 10.36,
      "grad_norm": 30650.1015625,
      "learning_rate": 4.969556243550052e-05,
      "loss": 15.2463,
      "step": 10349
    },
    {
      "epoch": 10.36,
      "grad_norm": 3238.806640625,
      "learning_rate": 4.969040247678019e-05,
      "loss": 18.1147,
      "step": 10350
    },
    {
      "epoch": 10.36,
      "grad_norm": 11137.38671875,
      "learning_rate": 4.9685242518059863e-05,
      "loss": 14.1485,
      "step": 10351
    },
    {
      "epoch": 10.36,
      "grad_norm": 11656.595703125,
      "learning_rate": 4.968008255933953e-05,
      "loss": 14.6617,
      "step": 10352
    },
    {
      "epoch": 10.36,
      "grad_norm": 10877.109375,
      "learning_rate": 4.967492260061919e-05,
      "loss": 15.5468,
      "step": 10353
    },
    {
      "epoch": 10.36,
      "grad_norm": 8043.58642578125,
      "learning_rate": 4.966976264189887e-05,
      "loss": 16.5891,
      "step": 10354
    },
    {
      "epoch": 10.37,
      "grad_norm": 17844.625,
      "learning_rate": 4.9664602683178534e-05,
      "loss": 19.9502,
      "step": 10355
    },
    {
      "epoch": 10.37,
      "grad_norm": 42748.4609375,
      "learning_rate": 4.96594427244582e-05,
      "loss": 17.4069,
      "step": 10356
    },
    {
      "epoch": 10.37,
      "grad_norm": 4929.19482421875,
      "learning_rate": 4.965428276573788e-05,
      "loss": 20.2563,
      "step": 10357
    },
    {
      "epoch": 10.37,
      "grad_norm": 2540.841552734375,
      "learning_rate": 4.9649122807017544e-05,
      "loss": 14.0319,
      "step": 10358
    },
    {
      "epoch": 10.37,
      "grad_norm": 8781.6923828125,
      "learning_rate": 4.964396284829721e-05,
      "loss": 12.699,
      "step": 10359
    },
    {
      "epoch": 10.37,
      "grad_norm": 8364.857421875,
      "learning_rate": 4.963880288957689e-05,
      "loss": 17.318,
      "step": 10360
    },
    {
      "epoch": 10.37,
      "grad_norm": 2488.172607421875,
      "learning_rate": 4.9633642930856554e-05,
      "loss": 18.4205,
      "step": 10361
    },
    {
      "epoch": 10.37,
      "grad_norm": 127844.0625,
      "learning_rate": 4.962848297213622e-05,
      "loss": 15.8526,
      "step": 10362
    },
    {
      "epoch": 10.37,
      "grad_norm": 45760.1640625,
      "learning_rate": 4.96233230134159e-05,
      "loss": 16.3818,
      "step": 10363
    },
    {
      "epoch": 10.37,
      "grad_norm": 6856.8798828125,
      "learning_rate": 4.9618163054695564e-05,
      "loss": 18.8489,
      "step": 10364
    },
    {
      "epoch": 10.38,
      "grad_norm": 20080.5234375,
      "learning_rate": 4.961300309597524e-05,
      "loss": 17.202,
      "step": 10365
    },
    {
      "epoch": 10.38,
      "grad_norm": 13323.052734375,
      "learning_rate": 4.960784313725491e-05,
      "loss": 13.7679,
      "step": 10366
    },
    {
      "epoch": 10.38,
      "grad_norm": 77912.6171875,
      "learning_rate": 4.9602683178534574e-05,
      "loss": 16.8528,
      "step": 10367
    },
    {
      "epoch": 10.38,
      "grad_norm": 3799.07470703125,
      "learning_rate": 4.959752321981425e-05,
      "loss": 15.6116,
      "step": 10368
    },
    {
      "epoch": 10.38,
      "grad_norm": 6323.96044921875,
      "learning_rate": 4.959236326109392e-05,
      "loss": 14.4309,
      "step": 10369
    },
    {
      "epoch": 10.38,
      "grad_norm": 29956.44140625,
      "learning_rate": 4.9587203302373584e-05,
      "loss": 14.8214,
      "step": 10370
    },
    {
      "epoch": 10.38,
      "grad_norm": 2948.698974609375,
      "learning_rate": 4.958204334365325e-05,
      "loss": 16.1168,
      "step": 10371
    },
    {
      "epoch": 10.38,
      "grad_norm": 4243.76611328125,
      "learning_rate": 4.957688338493292e-05,
      "loss": 26.8549,
      "step": 10372
    },
    {
      "epoch": 10.38,
      "grad_norm": 3821.164794921875,
      "learning_rate": 4.957172342621259e-05,
      "loss": 12.6735,
      "step": 10373
    },
    {
      "epoch": 10.38,
      "grad_norm": 4420.525390625,
      "learning_rate": 4.956656346749226e-05,
      "loss": 13.4428,
      "step": 10374
    },
    {
      "epoch": 10.39,
      "grad_norm": 19148.916015625,
      "learning_rate": 4.956140350877193e-05,
      "loss": 15.8409,
      "step": 10375
    },
    {
      "epoch": 10.39,
      "grad_norm": 42901.828125,
      "learning_rate": 4.95562435500516e-05,
      "loss": 20.7738,
      "step": 10376
    },
    {
      "epoch": 10.39,
      "grad_norm": 1538.7437744140625,
      "learning_rate": 4.955108359133127e-05,
      "loss": 17.3853,
      "step": 10377
    },
    {
      "epoch": 10.39,
      "grad_norm": 3087.854736328125,
      "learning_rate": 4.954592363261094e-05,
      "loss": 17.1568,
      "step": 10378
    },
    {
      "epoch": 10.39,
      "grad_norm": 23395.111328125,
      "learning_rate": 4.9540763673890614e-05,
      "loss": 16.7401,
      "step": 10379
    },
    {
      "epoch": 10.39,
      "grad_norm": 1818.8211669921875,
      "learning_rate": 4.953560371517028e-05,
      "loss": 14.0267,
      "step": 10380
    },
    {
      "epoch": 10.39,
      "grad_norm": 5245.58154296875,
      "learning_rate": 4.953044375644995e-05,
      "loss": 16.4779,
      "step": 10381
    },
    {
      "epoch": 10.39,
      "grad_norm": 14631.66015625,
      "learning_rate": 4.9525283797729624e-05,
      "loss": 17.2076,
      "step": 10382
    },
    {
      "epoch": 10.39,
      "grad_norm": 7367.86474609375,
      "learning_rate": 4.952012383900929e-05,
      "loss": 15.6156,
      "step": 10383
    },
    {
      "epoch": 10.39,
      "grad_norm": 17650.291015625,
      "learning_rate": 4.951496388028896e-05,
      "loss": 20.8569,
      "step": 10384
    },
    {
      "epoch": 10.4,
      "grad_norm": 1877.8021240234375,
      "learning_rate": 4.9509803921568634e-05,
      "loss": 14.373,
      "step": 10385
    },
    {
      "epoch": 10.4,
      "grad_norm": 3396.770751953125,
      "learning_rate": 4.95046439628483e-05,
      "loss": 17.5515,
      "step": 10386
    },
    {
      "epoch": 10.4,
      "grad_norm": 27456.69921875,
      "learning_rate": 4.949948400412797e-05,
      "loss": 16.1979,
      "step": 10387
    },
    {
      "epoch": 10.4,
      "grad_norm": 18978.064453125,
      "learning_rate": 4.9494324045407644e-05,
      "loss": 17.7965,
      "step": 10388
    },
    {
      "epoch": 10.4,
      "grad_norm": 17647.259765625,
      "learning_rate": 4.9489164086687305e-05,
      "loss": 17.0675,
      "step": 10389
    },
    {
      "epoch": 10.4,
      "grad_norm": 1989.518798828125,
      "learning_rate": 4.948400412796697e-05,
      "loss": 21.5954,
      "step": 10390
    },
    {
      "epoch": 10.4,
      "grad_norm": 3717.78515625,
      "learning_rate": 4.947884416924665e-05,
      "loss": 15.8624,
      "step": 10391
    },
    {
      "epoch": 10.4,
      "grad_norm": 4231.9296875,
      "learning_rate": 4.9473684210526315e-05,
      "loss": 13.1607,
      "step": 10392
    },
    {
      "epoch": 10.4,
      "grad_norm": 6850.91650390625,
      "learning_rate": 4.946852425180599e-05,
      "loss": 19.9857,
      "step": 10393
    },
    {
      "epoch": 10.4,
      "grad_norm": 2728.3486328125,
      "learning_rate": 4.946336429308566e-05,
      "loss": 17.4804,
      "step": 10394
    },
    {
      "epoch": 10.41,
      "grad_norm": 6375.65185546875,
      "learning_rate": 4.9458204334365325e-05,
      "loss": 15.8494,
      "step": 10395
    },
    {
      "epoch": 10.41,
      "grad_norm": 16161.0712890625,
      "learning_rate": 4.9453044375645e-05,
      "loss": 13.4086,
      "step": 10396
    },
    {
      "epoch": 10.41,
      "grad_norm": 7017.32275390625,
      "learning_rate": 4.944788441692467e-05,
      "loss": 15.0039,
      "step": 10397
    },
    {
      "epoch": 10.41,
      "grad_norm": 8497.564453125,
      "learning_rate": 4.9442724458204335e-05,
      "loss": 15.9332,
      "step": 10398
    },
    {
      "epoch": 10.41,
      "grad_norm": 11890.380859375,
      "learning_rate": 4.943756449948401e-05,
      "loss": 17.835,
      "step": 10399
    },
    {
      "epoch": 10.41,
      "grad_norm": 7544.705078125,
      "learning_rate": 4.943240454076368e-05,
      "loss": 14.9532,
      "step": 10400
    },
    {
      "epoch": 10.41,
      "grad_norm": 2501.019287109375,
      "learning_rate": 4.9427244582043345e-05,
      "loss": 15.0913,
      "step": 10401
    },
    {
      "epoch": 10.41,
      "grad_norm": 7443.1474609375,
      "learning_rate": 4.942208462332302e-05,
      "loss": 13.9015,
      "step": 10402
    },
    {
      "epoch": 10.41,
      "grad_norm": 6875.814453125,
      "learning_rate": 4.941692466460269e-05,
      "loss": 17.5114,
      "step": 10403
    },
    {
      "epoch": 10.41,
      "grad_norm": 3572.162841796875,
      "learning_rate": 4.9411764705882355e-05,
      "loss": 16.4725,
      "step": 10404
    },
    {
      "epoch": 10.42,
      "grad_norm": 595.1626586914062,
      "learning_rate": 4.940660474716203e-05,
      "loss": 15.9696,
      "step": 10405
    },
    {
      "epoch": 10.42,
      "grad_norm": 1167.1202392578125,
      "learning_rate": 4.94014447884417e-05,
      "loss": 15.3097,
      "step": 10406
    },
    {
      "epoch": 10.42,
      "grad_norm": 776.29296875,
      "learning_rate": 4.9396284829721365e-05,
      "loss": 15.9043,
      "step": 10407
    },
    {
      "epoch": 10.42,
      "grad_norm": 15655.3154296875,
      "learning_rate": 4.939112487100103e-05,
      "loss": 15.3297,
      "step": 10408
    },
    {
      "epoch": 10.42,
      "grad_norm": 3568.78515625,
      "learning_rate": 4.93859649122807e-05,
      "loss": 14.253,
      "step": 10409
    },
    {
      "epoch": 10.42,
      "grad_norm": 11609.5634765625,
      "learning_rate": 4.9380804953560375e-05,
      "loss": 19.2429,
      "step": 10410
    },
    {
      "epoch": 10.42,
      "grad_norm": 20263.46875,
      "learning_rate": 4.937564499484004e-05,
      "loss": 18.3722,
      "step": 10411
    },
    {
      "epoch": 10.42,
      "grad_norm": 70075.28125,
      "learning_rate": 4.937048503611971e-05,
      "loss": 17.4558,
      "step": 10412
    },
    {
      "epoch": 10.42,
      "grad_norm": 16880.560546875,
      "learning_rate": 4.9365325077399385e-05,
      "loss": 14.234,
      "step": 10413
    },
    {
      "epoch": 10.42,
      "grad_norm": 5213.5615234375,
      "learning_rate": 4.936016511867905e-05,
      "loss": 18.8908,
      "step": 10414
    },
    {
      "epoch": 10.43,
      "grad_norm": 4720.91064453125,
      "learning_rate": 4.935500515995872e-05,
      "loss": 14.8046,
      "step": 10415
    },
    {
      "epoch": 10.43,
      "grad_norm": 1785.2664794921875,
      "learning_rate": 4.9349845201238395e-05,
      "loss": 17.7078,
      "step": 10416
    },
    {
      "epoch": 10.43,
      "grad_norm": 30566.0859375,
      "learning_rate": 4.934468524251806e-05,
      "loss": 21.9479,
      "step": 10417
    },
    {
      "epoch": 10.43,
      "grad_norm": 4996.3154296875,
      "learning_rate": 4.933952528379773e-05,
      "loss": 15.9578,
      "step": 10418
    },
    {
      "epoch": 10.43,
      "grad_norm": 18103.65625,
      "learning_rate": 4.9334365325077405e-05,
      "loss": 20.3624,
      "step": 10419
    },
    {
      "epoch": 10.43,
      "grad_norm": 2076.37890625,
      "learning_rate": 4.932920536635707e-05,
      "loss": 13.2959,
      "step": 10420
    },
    {
      "epoch": 10.43,
      "grad_norm": 9789.4306640625,
      "learning_rate": 4.932404540763674e-05,
      "loss": 19.9932,
      "step": 10421
    },
    {
      "epoch": 10.43,
      "grad_norm": 9704.8916015625,
      "learning_rate": 4.9318885448916415e-05,
      "loss": 16.6782,
      "step": 10422
    },
    {
      "epoch": 10.43,
      "grad_norm": 8320.291015625,
      "learning_rate": 4.931372549019608e-05,
      "loss": 17.45,
      "step": 10423
    },
    {
      "epoch": 10.43,
      "grad_norm": 2086.2607421875,
      "learning_rate": 4.930856553147575e-05,
      "loss": 16.3487,
      "step": 10424
    },
    {
      "epoch": 10.44,
      "grad_norm": 4067.546875,
      "learning_rate": 4.930340557275542e-05,
      "loss": 17.4825,
      "step": 10425
    },
    {
      "epoch": 10.44,
      "grad_norm": 15038.1845703125,
      "learning_rate": 4.9298245614035086e-05,
      "loss": 16.9528,
      "step": 10426
    },
    {
      "epoch": 10.44,
      "grad_norm": 5833.51611328125,
      "learning_rate": 4.929308565531476e-05,
      "loss": 17.2303,
      "step": 10427
    },
    {
      "epoch": 10.44,
      "grad_norm": 14943.4013671875,
      "learning_rate": 4.928792569659443e-05,
      "loss": 20.1027,
      "step": 10428
    },
    {
      "epoch": 10.44,
      "grad_norm": 193020.5,
      "learning_rate": 4.9282765737874096e-05,
      "loss": 17.7191,
      "step": 10429
    },
    {
      "epoch": 10.44,
      "grad_norm": 4910.77490234375,
      "learning_rate": 4.927760577915377e-05,
      "loss": 19.1778,
      "step": 10430
    },
    {
      "epoch": 10.44,
      "grad_norm": 28693.54296875,
      "learning_rate": 4.927244582043344e-05,
      "loss": 16.0241,
      "step": 10431
    },
    {
      "epoch": 10.44,
      "grad_norm": 2034.0111083984375,
      "learning_rate": 4.9267285861713106e-05,
      "loss": 18.2589,
      "step": 10432
    },
    {
      "epoch": 10.44,
      "grad_norm": 12175.3193359375,
      "learning_rate": 4.926212590299278e-05,
      "loss": 18.8186,
      "step": 10433
    },
    {
      "epoch": 10.44,
      "grad_norm": 14462.796875,
      "learning_rate": 4.925696594427245e-05,
      "loss": 18.4067,
      "step": 10434
    },
    {
      "epoch": 10.45,
      "grad_norm": 7153.5849609375,
      "learning_rate": 4.9251805985552116e-05,
      "loss": 15.9925,
      "step": 10435
    },
    {
      "epoch": 10.45,
      "grad_norm": 10792.619140625,
      "learning_rate": 4.924664602683179e-05,
      "loss": 13.9875,
      "step": 10436
    },
    {
      "epoch": 10.45,
      "grad_norm": 4402.07763671875,
      "learning_rate": 4.924148606811146e-05,
      "loss": 16.3284,
      "step": 10437
    },
    {
      "epoch": 10.45,
      "grad_norm": 9168.2666015625,
      "learning_rate": 4.9236326109391126e-05,
      "loss": 14.3468,
      "step": 10438
    },
    {
      "epoch": 10.45,
      "grad_norm": 3092.64794921875,
      "learning_rate": 4.92311661506708e-05,
      "loss": 20.2689,
      "step": 10439
    },
    {
      "epoch": 10.45,
      "grad_norm": 9826.3271484375,
      "learning_rate": 4.922600619195047e-05,
      "loss": 14.5417,
      "step": 10440
    },
    {
      "epoch": 10.45,
      "grad_norm": 14093.4521484375,
      "learning_rate": 4.9220846233230136e-05,
      "loss": 19.4041,
      "step": 10441
    },
    {
      "epoch": 10.45,
      "grad_norm": 6622.8212890625,
      "learning_rate": 4.9215686274509804e-05,
      "loss": 15.1608,
      "step": 10442
    },
    {
      "epoch": 10.45,
      "grad_norm": 15898.74609375,
      "learning_rate": 4.921052631578947e-05,
      "loss": 17.8443,
      "step": 10443
    },
    {
      "epoch": 10.45,
      "grad_norm": 21849.537109375,
      "learning_rate": 4.9205366357069146e-05,
      "loss": 18.1122,
      "step": 10444
    },
    {
      "epoch": 10.46,
      "grad_norm": 4741.87646484375,
      "learning_rate": 4.9200206398348814e-05,
      "loss": 14.8455,
      "step": 10445
    },
    {
      "epoch": 10.46,
      "grad_norm": 63676.1328125,
      "learning_rate": 4.919504643962848e-05,
      "loss": 18.1647,
      "step": 10446
    },
    {
      "epoch": 10.46,
      "grad_norm": 9652.9404296875,
      "learning_rate": 4.9189886480908156e-05,
      "loss": 26.6426,
      "step": 10447
    },
    {
      "epoch": 10.46,
      "grad_norm": 162883.65625,
      "learning_rate": 4.9184726522187824e-05,
      "loss": 14.9539,
      "step": 10448
    },
    {
      "epoch": 10.46,
      "grad_norm": 30713.66015625,
      "learning_rate": 4.917956656346749e-05,
      "loss": 18.7019,
      "step": 10449
    },
    {
      "epoch": 10.46,
      "grad_norm": 48239.01171875,
      "learning_rate": 4.9174406604747166e-05,
      "loss": 15.4143,
      "step": 10450
    },
    {
      "epoch": 10.46,
      "grad_norm": 11403.1376953125,
      "learning_rate": 4.9169246646026834e-05,
      "loss": 14.2454,
      "step": 10451
    },
    {
      "epoch": 10.46,
      "grad_norm": 43991.74609375,
      "learning_rate": 4.91640866873065e-05,
      "loss": 25.7927,
      "step": 10452
    },
    {
      "epoch": 10.46,
      "grad_norm": 5040.40771484375,
      "learning_rate": 4.9158926728586176e-05,
      "loss": 14.2965,
      "step": 10453
    },
    {
      "epoch": 10.46,
      "grad_norm": 13226.943359375,
      "learning_rate": 4.9153766769865844e-05,
      "loss": 21.9688,
      "step": 10454
    },
    {
      "epoch": 10.47,
      "grad_norm": 8896.341796875,
      "learning_rate": 4.914860681114551e-05,
      "loss": 21.9044,
      "step": 10455
    },
    {
      "epoch": 10.47,
      "grad_norm": 3916.724853515625,
      "learning_rate": 4.9143446852425186e-05,
      "loss": 16.3447,
      "step": 10456
    },
    {
      "epoch": 10.47,
      "grad_norm": 7261.82568359375,
      "learning_rate": 4.9138286893704854e-05,
      "loss": 17.3801,
      "step": 10457
    },
    {
      "epoch": 10.47,
      "grad_norm": 7388.3203125,
      "learning_rate": 4.913312693498452e-05,
      "loss": 16.6727,
      "step": 10458
    },
    {
      "epoch": 10.47,
      "grad_norm": 12038.12109375,
      "learning_rate": 4.9127966976264196e-05,
      "loss": 15.7906,
      "step": 10459
    },
    {
      "epoch": 10.47,
      "grad_norm": 6997.5615234375,
      "learning_rate": 4.912280701754386e-05,
      "loss": 16.0912,
      "step": 10460
    },
    {
      "epoch": 10.47,
      "grad_norm": 26810.36328125,
      "learning_rate": 4.911764705882353e-05,
      "loss": 15.5887,
      "step": 10461
    },
    {
      "epoch": 10.47,
      "grad_norm": 4213.130859375,
      "learning_rate": 4.91124871001032e-05,
      "loss": 14.3666,
      "step": 10462
    },
    {
      "epoch": 10.47,
      "grad_norm": 9987.763671875,
      "learning_rate": 4.910732714138287e-05,
      "loss": 16.0258,
      "step": 10463
    },
    {
      "epoch": 10.47,
      "grad_norm": 4198.76171875,
      "learning_rate": 4.910216718266254e-05,
      "loss": 13.0604,
      "step": 10464
    },
    {
      "epoch": 10.48,
      "grad_norm": 1541.46826171875,
      "learning_rate": 4.909700722394221e-05,
      "loss": 15.0876,
      "step": 10465
    },
    {
      "epoch": 10.48,
      "grad_norm": 2407.468017578125,
      "learning_rate": 4.909184726522188e-05,
      "loss": 20.1779,
      "step": 10466
    },
    {
      "epoch": 10.48,
      "grad_norm": 90111.421875,
      "learning_rate": 4.908668730650155e-05,
      "loss": 17.6375,
      "step": 10467
    },
    {
      "epoch": 10.48,
      "grad_norm": 26447.556640625,
      "learning_rate": 4.908152734778122e-05,
      "loss": 17.1487,
      "step": 10468
    },
    {
      "epoch": 10.48,
      "grad_norm": 7784.30078125,
      "learning_rate": 4.907636738906089e-05,
      "loss": 13.0745,
      "step": 10469
    },
    {
      "epoch": 10.48,
      "grad_norm": 14562.5283203125,
      "learning_rate": 4.907120743034056e-05,
      "loss": 15.3986,
      "step": 10470
    },
    {
      "epoch": 10.48,
      "grad_norm": 16001.9462890625,
      "learning_rate": 4.906604747162023e-05,
      "loss": 13.7615,
      "step": 10471
    },
    {
      "epoch": 10.48,
      "grad_norm": 8930.693359375,
      "learning_rate": 4.90608875128999e-05,
      "loss": 23.0899,
      "step": 10472
    },
    {
      "epoch": 10.48,
      "grad_norm": 6963.01708984375,
      "learning_rate": 4.905572755417957e-05,
      "loss": 18.3779,
      "step": 10473
    },
    {
      "epoch": 10.48,
      "grad_norm": 7140.39892578125,
      "learning_rate": 4.905056759545924e-05,
      "loss": 17.9685,
      "step": 10474
    },
    {
      "epoch": 10.49,
      "grad_norm": 14256.4765625,
      "learning_rate": 4.904540763673891e-05,
      "loss": 16.8735,
      "step": 10475
    },
    {
      "epoch": 10.49,
      "grad_norm": 7130.0087890625,
      "learning_rate": 4.904024767801858e-05,
      "loss": 12.2411,
      "step": 10476
    },
    {
      "epoch": 10.49,
      "grad_norm": 24121.224609375,
      "learning_rate": 4.903508771929825e-05,
      "loss": 15.8686,
      "step": 10477
    },
    {
      "epoch": 10.49,
      "grad_norm": 33883.03125,
      "learning_rate": 4.902992776057792e-05,
      "loss": 14.5552,
      "step": 10478
    },
    {
      "epoch": 10.49,
      "grad_norm": 4921.4501953125,
      "learning_rate": 4.9024767801857585e-05,
      "loss": 18.2851,
      "step": 10479
    },
    {
      "epoch": 10.49,
      "grad_norm": 8599.427734375,
      "learning_rate": 4.901960784313725e-05,
      "loss": 17.6824,
      "step": 10480
    },
    {
      "epoch": 10.49,
      "grad_norm": 10419.6953125,
      "learning_rate": 4.901444788441693e-05,
      "loss": 19.5533,
      "step": 10481
    },
    {
      "epoch": 10.49,
      "grad_norm": 10856.22265625,
      "learning_rate": 4.9009287925696595e-05,
      "loss": 14.617,
      "step": 10482
    },
    {
      "epoch": 10.49,
      "grad_norm": 1025.1282958984375,
      "learning_rate": 4.900412796697626e-05,
      "loss": 17.5973,
      "step": 10483
    },
    {
      "epoch": 10.49,
      "grad_norm": 13826.93359375,
      "learning_rate": 4.899896800825594e-05,
      "loss": 18.388,
      "step": 10484
    },
    {
      "epoch": 10.5,
      "grad_norm": 5474.52490234375,
      "learning_rate": 4.8993808049535605e-05,
      "loss": 14.8446,
      "step": 10485
    },
    {
      "epoch": 10.5,
      "grad_norm": 5229.06982421875,
      "learning_rate": 4.898864809081527e-05,
      "loss": 17.624,
      "step": 10486
    },
    {
      "epoch": 10.5,
      "grad_norm": 2841.6103515625,
      "learning_rate": 4.898348813209495e-05,
      "loss": 17.386,
      "step": 10487
    },
    {
      "epoch": 10.5,
      "grad_norm": 66237.6875,
      "learning_rate": 4.8978328173374615e-05,
      "loss": 20.3474,
      "step": 10488
    },
    {
      "epoch": 10.5,
      "grad_norm": 11469.4716796875,
      "learning_rate": 4.897316821465428e-05,
      "loss": 20.3635,
      "step": 10489
    },
    {
      "epoch": 10.5,
      "grad_norm": 3071.2373046875,
      "learning_rate": 4.896800825593396e-05,
      "loss": 15.1049,
      "step": 10490
    },
    {
      "epoch": 10.5,
      "grad_norm": 170965.1875,
      "learning_rate": 4.8962848297213625e-05,
      "loss": 23.4512,
      "step": 10491
    },
    {
      "epoch": 10.5,
      "grad_norm": 13355.45703125,
      "learning_rate": 4.895768833849329e-05,
      "loss": 17.68,
      "step": 10492
    },
    {
      "epoch": 10.5,
      "grad_norm": 72208.4921875,
      "learning_rate": 4.895252837977297e-05,
      "loss": 17.0319,
      "step": 10493
    },
    {
      "epoch": 10.5,
      "grad_norm": 5826.048828125,
      "learning_rate": 4.8947368421052635e-05,
      "loss": 18.981,
      "step": 10494
    },
    {
      "epoch": 10.51,
      "grad_norm": 7466.65087890625,
      "learning_rate": 4.894220846233231e-05,
      "loss": 14.6772,
      "step": 10495
    },
    {
      "epoch": 10.51,
      "grad_norm": 43991.9765625,
      "learning_rate": 4.893704850361197e-05,
      "loss": 16.9459,
      "step": 10496
    },
    {
      "epoch": 10.51,
      "grad_norm": 8827.2451171875,
      "learning_rate": 4.893188854489164e-05,
      "loss": 16.8281,
      "step": 10497
    },
    {
      "epoch": 10.51,
      "grad_norm": 3536.59326171875,
      "learning_rate": 4.892672858617131e-05,
      "loss": 17.8258,
      "step": 10498
    },
    {
      "epoch": 10.51,
      "grad_norm": 4626.44287109375,
      "learning_rate": 4.892156862745098e-05,
      "loss": 17.515,
      "step": 10499
    },
    {
      "epoch": 10.51,
      "grad_norm": 20136.208984375,
      "learning_rate": 4.891640866873065e-05,
      "loss": 15.674,
      "step": 10500
    },
    {
      "epoch": 10.51,
      "grad_norm": 3714.040771484375,
      "learning_rate": 4.891124871001032e-05,
      "loss": 21.3643,
      "step": 10501
    },
    {
      "epoch": 10.51,
      "grad_norm": 3359.37109375,
      "learning_rate": 4.890608875128999e-05,
      "loss": 15.579,
      "step": 10502
    },
    {
      "epoch": 10.51,
      "grad_norm": 4882.27001953125,
      "learning_rate": 4.890092879256966e-05,
      "loss": 11.8067,
      "step": 10503
    },
    {
      "epoch": 10.51,
      "grad_norm": 19557.404296875,
      "learning_rate": 4.889576883384933e-05,
      "loss": 18.675,
      "step": 10504
    },
    {
      "epoch": 10.52,
      "grad_norm": 11892.556640625,
      "learning_rate": 4.8890608875129e-05,
      "loss": 15.5237,
      "step": 10505
    },
    {
      "epoch": 10.52,
      "grad_norm": 13012.470703125,
      "learning_rate": 4.888544891640867e-05,
      "loss": 30.6833,
      "step": 10506
    },
    {
      "epoch": 10.52,
      "grad_norm": 43323.6640625,
      "learning_rate": 4.888028895768834e-05,
      "loss": 21.8408,
      "step": 10507
    },
    {
      "epoch": 10.52,
      "grad_norm": 3672.775634765625,
      "learning_rate": 4.887512899896801e-05,
      "loss": 14.4468,
      "step": 10508
    },
    {
      "epoch": 10.52,
      "grad_norm": 14819.2041015625,
      "learning_rate": 4.886996904024768e-05,
      "loss": 15.4829,
      "step": 10509
    },
    {
      "epoch": 10.52,
      "grad_norm": 7872.1162109375,
      "learning_rate": 4.886480908152735e-05,
      "loss": 14.388,
      "step": 10510
    },
    {
      "epoch": 10.52,
      "grad_norm": 2880.102783203125,
      "learning_rate": 4.885964912280702e-05,
      "loss": 16.6622,
      "step": 10511
    },
    {
      "epoch": 10.52,
      "grad_norm": 11426.3173828125,
      "learning_rate": 4.8854489164086695e-05,
      "loss": 13.8197,
      "step": 10512
    },
    {
      "epoch": 10.52,
      "grad_norm": 5338.435546875,
      "learning_rate": 4.884932920536636e-05,
      "loss": 16.2873,
      "step": 10513
    },
    {
      "epoch": 10.52,
      "grad_norm": 8658.0751953125,
      "learning_rate": 4.8844169246646024e-05,
      "loss": 16.2345,
      "step": 10514
    },
    {
      "epoch": 10.53,
      "grad_norm": 3444.70458984375,
      "learning_rate": 4.88390092879257e-05,
      "loss": 17.3619,
      "step": 10515
    },
    {
      "epoch": 10.53,
      "grad_norm": 6944.94287109375,
      "learning_rate": 4.8833849329205366e-05,
      "loss": 22.6439,
      "step": 10516
    },
    {
      "epoch": 10.53,
      "grad_norm": 23533.21875,
      "learning_rate": 4.8828689370485034e-05,
      "loss": 15.9117,
      "step": 10517
    },
    {
      "epoch": 10.53,
      "grad_norm": 5140.4892578125,
      "learning_rate": 4.882352941176471e-05,
      "loss": 19.3199,
      "step": 10518
    },
    {
      "epoch": 10.53,
      "grad_norm": 25009.544921875,
      "learning_rate": 4.8818369453044376e-05,
      "loss": 13.7471,
      "step": 10519
    },
    {
      "epoch": 10.53,
      "grad_norm": 3270.1220703125,
      "learning_rate": 4.8813209494324044e-05,
      "loss": 21.4703,
      "step": 10520
    },
    {
      "epoch": 10.53,
      "grad_norm": 1151.3052978515625,
      "learning_rate": 4.880804953560372e-05,
      "loss": 15.5049,
      "step": 10521
    },
    {
      "epoch": 10.53,
      "grad_norm": 72236.5390625,
      "learning_rate": 4.8802889576883386e-05,
      "loss": 25.691,
      "step": 10522
    },
    {
      "epoch": 10.53,
      "grad_norm": 25305.501953125,
      "learning_rate": 4.8797729618163054e-05,
      "loss": 23.1966,
      "step": 10523
    },
    {
      "epoch": 10.53,
      "grad_norm": 3208.361328125,
      "learning_rate": 4.879256965944273e-05,
      "loss": 19.2748,
      "step": 10524
    },
    {
      "epoch": 10.54,
      "grad_norm": 2473.94580078125,
      "learning_rate": 4.8787409700722396e-05,
      "loss": 15.3869,
      "step": 10525
    },
    {
      "epoch": 10.54,
      "grad_norm": 10375.9912109375,
      "learning_rate": 4.878224974200207e-05,
      "loss": 19.9439,
      "step": 10526
    },
    {
      "epoch": 10.54,
      "grad_norm": 3997.97021484375,
      "learning_rate": 4.877708978328174e-05,
      "loss": 15.5568,
      "step": 10527
    },
    {
      "epoch": 10.54,
      "grad_norm": 11157.197265625,
      "learning_rate": 4.8771929824561406e-05,
      "loss": 19.5558,
      "step": 10528
    },
    {
      "epoch": 10.54,
      "grad_norm": 9320.21875,
      "learning_rate": 4.876676986584108e-05,
      "loss": 24.6214,
      "step": 10529
    },
    {
      "epoch": 10.54,
      "grad_norm": 66925.1796875,
      "learning_rate": 4.876160990712075e-05,
      "loss": 16.8631,
      "step": 10530
    },
    {
      "epoch": 10.54,
      "grad_norm": 1779.2945556640625,
      "learning_rate": 4.8756449948400416e-05,
      "loss": 13.4867,
      "step": 10531
    },
    {
      "epoch": 10.54,
      "grad_norm": 9571.06640625,
      "learning_rate": 4.8751289989680084e-05,
      "loss": 17.7336,
      "step": 10532
    },
    {
      "epoch": 10.54,
      "grad_norm": 15645.904296875,
      "learning_rate": 4.874613003095975e-05,
      "loss": 24.2332,
      "step": 10533
    },
    {
      "epoch": 10.54,
      "grad_norm": 102417.4296875,
      "learning_rate": 4.874097007223942e-05,
      "loss": 13.2738,
      "step": 10534
    },
    {
      "epoch": 10.55,
      "grad_norm": 3426.278564453125,
      "learning_rate": 4.8735810113519094e-05,
      "loss": 17.6675,
      "step": 10535
    },
    {
      "epoch": 10.55,
      "grad_norm": 26349.28515625,
      "learning_rate": 4.873065015479876e-05,
      "loss": 21.1356,
      "step": 10536
    },
    {
      "epoch": 10.55,
      "grad_norm": 27959.974609375,
      "learning_rate": 4.872549019607843e-05,
      "loss": 17.2759,
      "step": 10537
    },
    {
      "epoch": 10.55,
      "grad_norm": 26537.203125,
      "learning_rate": 4.8720330237358104e-05,
      "loss": 16.4603,
      "step": 10538
    },
    {
      "epoch": 10.55,
      "grad_norm": 112605.7109375,
      "learning_rate": 4.871517027863777e-05,
      "loss": 18.8457,
      "step": 10539
    },
    {
      "epoch": 10.55,
      "grad_norm": 9231.9609375,
      "learning_rate": 4.8710010319917446e-05,
      "loss": 14.9028,
      "step": 10540
    },
    {
      "epoch": 10.55,
      "grad_norm": 8747.2421875,
      "learning_rate": 4.8704850361197114e-05,
      "loss": 16.791,
      "step": 10541
    },
    {
      "epoch": 10.55,
      "grad_norm": 22031.00390625,
      "learning_rate": 4.869969040247678e-05,
      "loss": 20.5987,
      "step": 10542
    },
    {
      "epoch": 10.55,
      "grad_norm": 14027.9599609375,
      "learning_rate": 4.8694530443756456e-05,
      "loss": 19.9387,
      "step": 10543
    },
    {
      "epoch": 10.55,
      "grad_norm": 4933.09326171875,
      "learning_rate": 4.8689370485036124e-05,
      "loss": 18.9079,
      "step": 10544
    },
    {
      "epoch": 10.56,
      "grad_norm": 68183.765625,
      "learning_rate": 4.868421052631579e-05,
      "loss": 30.188,
      "step": 10545
    },
    {
      "epoch": 10.56,
      "grad_norm": 14335.1259765625,
      "learning_rate": 4.8679050567595466e-05,
      "loss": 14.6445,
      "step": 10546
    },
    {
      "epoch": 10.56,
      "grad_norm": 1317.21142578125,
      "learning_rate": 4.8673890608875134e-05,
      "loss": 15.2613,
      "step": 10547
    },
    {
      "epoch": 10.56,
      "grad_norm": 22050.49609375,
      "learning_rate": 4.86687306501548e-05,
      "loss": 11.7698,
      "step": 10548
    },
    {
      "epoch": 10.56,
      "grad_norm": 8355.69140625,
      "learning_rate": 4.8663570691434476e-05,
      "loss": 14.4488,
      "step": 10549
    },
    {
      "epoch": 10.56,
      "grad_norm": 41760.4296875,
      "learning_rate": 4.865841073271414e-05,
      "loss": 13.9558,
      "step": 10550
    },
    {
      "epoch": 10.56,
      "grad_norm": 3153.36669921875,
      "learning_rate": 4.8653250773993805e-05,
      "loss": 14.5869,
      "step": 10551
    },
    {
      "epoch": 10.56,
      "grad_norm": 9193.1455078125,
      "learning_rate": 4.864809081527348e-05,
      "loss": 17.2067,
      "step": 10552
    },
    {
      "epoch": 10.56,
      "grad_norm": 1421.4215087890625,
      "learning_rate": 4.864293085655315e-05,
      "loss": 19.0442,
      "step": 10553
    },
    {
      "epoch": 10.56,
      "grad_norm": 7843.1484375,
      "learning_rate": 4.863777089783282e-05,
      "loss": 17.5245,
      "step": 10554
    },
    {
      "epoch": 10.57,
      "grad_norm": 14820.513671875,
      "learning_rate": 4.863261093911249e-05,
      "loss": 23.9063,
      "step": 10555
    },
    {
      "epoch": 10.57,
      "grad_norm": 1305.9296875,
      "learning_rate": 4.862745098039216e-05,
      "loss": 15.6459,
      "step": 10556
    },
    {
      "epoch": 10.57,
      "grad_norm": 1527.1435546875,
      "learning_rate": 4.862229102167183e-05,
      "loss": 14.0937,
      "step": 10557
    },
    {
      "epoch": 10.57,
      "grad_norm": 13956.982421875,
      "learning_rate": 4.86171310629515e-05,
      "loss": 17.5521,
      "step": 10558
    },
    {
      "epoch": 10.57,
      "grad_norm": 5677.36279296875,
      "learning_rate": 4.861197110423117e-05,
      "loss": 20.8232,
      "step": 10559
    },
    {
      "epoch": 10.57,
      "grad_norm": 2778.2607421875,
      "learning_rate": 4.860681114551084e-05,
      "loss": 11.8524,
      "step": 10560
    },
    {
      "epoch": 10.57,
      "grad_norm": 4416.44921875,
      "learning_rate": 4.860165118679051e-05,
      "loss": 17.6347,
      "step": 10561
    },
    {
      "epoch": 10.57,
      "grad_norm": 3740.135009765625,
      "learning_rate": 4.859649122807018e-05,
      "loss": 18.3255,
      "step": 10562
    },
    {
      "epoch": 10.57,
      "grad_norm": 15347.4892578125,
      "learning_rate": 4.859133126934985e-05,
      "loss": 18.185,
      "step": 10563
    },
    {
      "epoch": 10.57,
      "grad_norm": 3996.785400390625,
      "learning_rate": 4.858617131062952e-05,
      "loss": 16.8846,
      "step": 10564
    },
    {
      "epoch": 10.58,
      "grad_norm": 2064.240234375,
      "learning_rate": 4.858101135190919e-05,
      "loss": 16.0928,
      "step": 10565
    },
    {
      "epoch": 10.58,
      "grad_norm": 11992.5986328125,
      "learning_rate": 4.857585139318886e-05,
      "loss": 16.7344,
      "step": 10566
    },
    {
      "epoch": 10.58,
      "grad_norm": 6149.0419921875,
      "learning_rate": 4.857069143446853e-05,
      "loss": 19.0691,
      "step": 10567
    },
    {
      "epoch": 10.58,
      "grad_norm": 13749.0869140625,
      "learning_rate": 4.85655314757482e-05,
      "loss": 20.3513,
      "step": 10568
    },
    {
      "epoch": 10.58,
      "grad_norm": 12542.318359375,
      "learning_rate": 4.8560371517027865e-05,
      "loss": 16.5651,
      "step": 10569
    },
    {
      "epoch": 10.58,
      "grad_norm": 4661.2421875,
      "learning_rate": 4.855521155830753e-05,
      "loss": 18.8938,
      "step": 10570
    },
    {
      "epoch": 10.58,
      "grad_norm": 18910.041015625,
      "learning_rate": 4.855005159958721e-05,
      "loss": 18.8823,
      "step": 10571
    },
    {
      "epoch": 10.58,
      "grad_norm": 3783.176025390625,
      "learning_rate": 4.8544891640866875e-05,
      "loss": 19.6696,
      "step": 10572
    },
    {
      "epoch": 10.58,
      "grad_norm": 9363.4033203125,
      "learning_rate": 4.853973168214654e-05,
      "loss": 23.3041,
      "step": 10573
    },
    {
      "epoch": 10.58,
      "grad_norm": 9941.1953125,
      "learning_rate": 4.853457172342622e-05,
      "loss": 15.3626,
      "step": 10574
    },
    {
      "epoch": 10.59,
      "grad_norm": 3459.3759765625,
      "learning_rate": 4.8529411764705885e-05,
      "loss": 12.4166,
      "step": 10575
    },
    {
      "epoch": 10.59,
      "grad_norm": 2879.608154296875,
      "learning_rate": 4.852425180598555e-05,
      "loss": 15.8172,
      "step": 10576
    },
    {
      "epoch": 10.59,
      "grad_norm": 5326.5859375,
      "learning_rate": 4.851909184726523e-05,
      "loss": 12.2482,
      "step": 10577
    },
    {
      "epoch": 10.59,
      "grad_norm": 11252.2890625,
      "learning_rate": 4.8513931888544895e-05,
      "loss": 19.4367,
      "step": 10578
    },
    {
      "epoch": 10.59,
      "grad_norm": 53794.765625,
      "learning_rate": 4.850877192982456e-05,
      "loss": 16.9185,
      "step": 10579
    },
    {
      "epoch": 10.59,
      "grad_norm": 6564.666015625,
      "learning_rate": 4.850361197110424e-05,
      "loss": 19.4609,
      "step": 10580
    },
    {
      "epoch": 10.59,
      "grad_norm": 6326.1328125,
      "learning_rate": 4.8498452012383905e-05,
      "loss": 22.6312,
      "step": 10581
    },
    {
      "epoch": 10.59,
      "grad_norm": 2317.747802734375,
      "learning_rate": 4.849329205366357e-05,
      "loss": 18.8522,
      "step": 10582
    },
    {
      "epoch": 10.59,
      "grad_norm": 33668.89453125,
      "learning_rate": 4.848813209494325e-05,
      "loss": 19.4323,
      "step": 10583
    },
    {
      "epoch": 10.59,
      "grad_norm": 5119.23876953125,
      "learning_rate": 4.8482972136222915e-05,
      "loss": 16.7067,
      "step": 10584
    },
    {
      "epoch": 10.6,
      "grad_norm": 61620.56640625,
      "learning_rate": 4.847781217750258e-05,
      "loss": 18.3259,
      "step": 10585
    },
    {
      "epoch": 10.6,
      "grad_norm": 6558.22314453125,
      "learning_rate": 4.847265221878225e-05,
      "loss": 17.1221,
      "step": 10586
    },
    {
      "epoch": 10.6,
      "grad_norm": 6147.46240234375,
      "learning_rate": 4.846749226006192e-05,
      "loss": 18.487,
      "step": 10587
    },
    {
      "epoch": 10.6,
      "grad_norm": 26217.146484375,
      "learning_rate": 4.846233230134159e-05,
      "loss": 17.7683,
      "step": 10588
    },
    {
      "epoch": 10.6,
      "grad_norm": 17414.16015625,
      "learning_rate": 4.845717234262126e-05,
      "loss": 12.1626,
      "step": 10589
    },
    {
      "epoch": 10.6,
      "grad_norm": 3800.4169921875,
      "learning_rate": 4.845201238390093e-05,
      "loss": 18.8672,
      "step": 10590
    },
    {
      "epoch": 10.6,
      "grad_norm": 2250.293212890625,
      "learning_rate": 4.84468524251806e-05,
      "loss": 15.2515,
      "step": 10591
    },
    {
      "epoch": 10.6,
      "grad_norm": 3135.459228515625,
      "learning_rate": 4.844169246646027e-05,
      "loss": 18.7555,
      "step": 10592
    },
    {
      "epoch": 10.6,
      "grad_norm": 35660.01953125,
      "learning_rate": 4.843653250773994e-05,
      "loss": 22.3863,
      "step": 10593
    },
    {
      "epoch": 10.6,
      "grad_norm": 4953.6083984375,
      "learning_rate": 4.843137254901961e-05,
      "loss": 19.0423,
      "step": 10594
    },
    {
      "epoch": 10.61,
      "grad_norm": 4123.90283203125,
      "learning_rate": 4.842621259029928e-05,
      "loss": 13.23,
      "step": 10595
    },
    {
      "epoch": 10.61,
      "grad_norm": 9193.8720703125,
      "learning_rate": 4.842105263157895e-05,
      "loss": 16.5094,
      "step": 10596
    },
    {
      "epoch": 10.61,
      "grad_norm": 21749.2890625,
      "learning_rate": 4.841589267285862e-05,
      "loss": 20.7,
      "step": 10597
    },
    {
      "epoch": 10.61,
      "grad_norm": 24654.876953125,
      "learning_rate": 4.841073271413829e-05,
      "loss": 18.5601,
      "step": 10598
    },
    {
      "epoch": 10.61,
      "grad_norm": 4449.5634765625,
      "learning_rate": 4.840557275541796e-05,
      "loss": 16.3795,
      "step": 10599
    },
    {
      "epoch": 10.61,
      "grad_norm": 2151.73974609375,
      "learning_rate": 4.840041279669763e-05,
      "loss": 16.9194,
      "step": 10600
    },
    {
      "epoch": 10.61,
      "grad_norm": 496919.125,
      "learning_rate": 4.83952528379773e-05,
      "loss": 21.7641,
      "step": 10601
    },
    {
      "epoch": 10.61,
      "grad_norm": 8217.755859375,
      "learning_rate": 4.839009287925697e-05,
      "loss": 17.7696,
      "step": 10602
    },
    {
      "epoch": 10.61,
      "grad_norm": 14782.6572265625,
      "learning_rate": 4.8384932920536636e-05,
      "loss": 18.2629,
      "step": 10603
    },
    {
      "epoch": 10.61,
      "grad_norm": 2080.306640625,
      "learning_rate": 4.8379772961816304e-05,
      "loss": 16.282,
      "step": 10604
    },
    {
      "epoch": 10.62,
      "grad_norm": 12284.7783203125,
      "learning_rate": 4.837461300309598e-05,
      "loss": 20.597,
      "step": 10605
    },
    {
      "epoch": 10.62,
      "grad_norm": 10222.6845703125,
      "learning_rate": 4.8369453044375646e-05,
      "loss": 15.763,
      "step": 10606
    },
    {
      "epoch": 10.62,
      "grad_norm": 17184.1875,
      "learning_rate": 4.8364293085655314e-05,
      "loss": 17.122,
      "step": 10607
    },
    {
      "epoch": 10.62,
      "grad_norm": 3506.14208984375,
      "learning_rate": 4.835913312693499e-05,
      "loss": 17.6917,
      "step": 10608
    },
    {
      "epoch": 10.62,
      "grad_norm": 9169.3115234375,
      "learning_rate": 4.8353973168214656e-05,
      "loss": 19.3067,
      "step": 10609
    },
    {
      "epoch": 10.62,
      "grad_norm": 6340.46826171875,
      "learning_rate": 4.8348813209494324e-05,
      "loss": 17.726,
      "step": 10610
    },
    {
      "epoch": 10.62,
      "grad_norm": 9639.681640625,
      "learning_rate": 4.8343653250774e-05,
      "loss": 19.2241,
      "step": 10611
    },
    {
      "epoch": 10.62,
      "grad_norm": 4219.404296875,
      "learning_rate": 4.8338493292053666e-05,
      "loss": 17.7549,
      "step": 10612
    },
    {
      "epoch": 10.62,
      "grad_norm": 10465.5185546875,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 18.8849,
      "step": 10613
    },
    {
      "epoch": 10.62,
      "grad_norm": 8466.70703125,
      "learning_rate": 4.832817337461301e-05,
      "loss": 18.0063,
      "step": 10614
    },
    {
      "epoch": 10.63,
      "grad_norm": 5624.0439453125,
      "learning_rate": 4.8323013415892676e-05,
      "loss": 15.0965,
      "step": 10615
    },
    {
      "epoch": 10.63,
      "grad_norm": 20402.23046875,
      "learning_rate": 4.8317853457172344e-05,
      "loss": 15.6226,
      "step": 10616
    },
    {
      "epoch": 10.63,
      "grad_norm": 8989.9609375,
      "learning_rate": 4.831269349845202e-05,
      "loss": 18.9162,
      "step": 10617
    },
    {
      "epoch": 10.63,
      "grad_norm": 5397.52099609375,
      "learning_rate": 4.8307533539731686e-05,
      "loss": 13.4747,
      "step": 10618
    },
    {
      "epoch": 10.63,
      "grad_norm": 2986.541748046875,
      "learning_rate": 4.8302373581011354e-05,
      "loss": 17.9715,
      "step": 10619
    },
    {
      "epoch": 10.63,
      "grad_norm": 6885.6513671875,
      "learning_rate": 4.829721362229103e-05,
      "loss": 14.952,
      "step": 10620
    },
    {
      "epoch": 10.63,
      "grad_norm": 6482.42041015625,
      "learning_rate": 4.829205366357069e-05,
      "loss": 16.6834,
      "step": 10621
    },
    {
      "epoch": 10.63,
      "grad_norm": 772.1069946289062,
      "learning_rate": 4.8286893704850364e-05,
      "loss": 16.9816,
      "step": 10622
    },
    {
      "epoch": 10.63,
      "grad_norm": 7216.99169921875,
      "learning_rate": 4.828173374613003e-05,
      "loss": 17.2284,
      "step": 10623
    },
    {
      "epoch": 10.63,
      "grad_norm": 16627.6796875,
      "learning_rate": 4.82765737874097e-05,
      "loss": 20.695,
      "step": 10624
    },
    {
      "epoch": 10.64,
      "grad_norm": 10753.9541015625,
      "learning_rate": 4.8271413828689374e-05,
      "loss": 15.6245,
      "step": 10625
    },
    {
      "epoch": 10.64,
      "grad_norm": 54260.46875,
      "learning_rate": 4.826625386996904e-05,
      "loss": 19.6559,
      "step": 10626
    },
    {
      "epoch": 10.64,
      "grad_norm": 4021.31103515625,
      "learning_rate": 4.826109391124871e-05,
      "loss": 17.6605,
      "step": 10627
    },
    {
      "epoch": 10.64,
      "grad_norm": 11492.8779296875,
      "learning_rate": 4.8255933952528384e-05,
      "loss": 20.3584,
      "step": 10628
    },
    {
      "epoch": 10.64,
      "grad_norm": 9603.060546875,
      "learning_rate": 4.825077399380805e-05,
      "loss": 15.9487,
      "step": 10629
    },
    {
      "epoch": 10.64,
      "grad_norm": 36383.3671875,
      "learning_rate": 4.824561403508772e-05,
      "loss": 22.3528,
      "step": 10630
    },
    {
      "epoch": 10.64,
      "grad_norm": 4297.77490234375,
      "learning_rate": 4.8240454076367394e-05,
      "loss": 13.2401,
      "step": 10631
    },
    {
      "epoch": 10.64,
      "grad_norm": 6032.3876953125,
      "learning_rate": 4.823529411764706e-05,
      "loss": 16.3962,
      "step": 10632
    },
    {
      "epoch": 10.64,
      "grad_norm": 36281.76171875,
      "learning_rate": 4.823013415892673e-05,
      "loss": 21.369,
      "step": 10633
    },
    {
      "epoch": 10.64,
      "grad_norm": 62133.19921875,
      "learning_rate": 4.8224974200206404e-05,
      "loss": 18.8775,
      "step": 10634
    },
    {
      "epoch": 10.65,
      "grad_norm": 11810.5205078125,
      "learning_rate": 4.821981424148607e-05,
      "loss": 24.0628,
      "step": 10635
    },
    {
      "epoch": 10.65,
      "grad_norm": 3861.56298828125,
      "learning_rate": 4.821465428276574e-05,
      "loss": 16.3261,
      "step": 10636
    },
    {
      "epoch": 10.65,
      "grad_norm": 851.967529296875,
      "learning_rate": 4.8209494324045414e-05,
      "loss": 19.9862,
      "step": 10637
    },
    {
      "epoch": 10.65,
      "grad_norm": 136265.703125,
      "learning_rate": 4.820433436532508e-05,
      "loss": 21.657,
      "step": 10638
    },
    {
      "epoch": 10.65,
      "grad_norm": 46393.046875,
      "learning_rate": 4.819917440660475e-05,
      "loss": 20.2191,
      "step": 10639
    },
    {
      "epoch": 10.65,
      "grad_norm": 16518.849609375,
      "learning_rate": 4.819401444788442e-05,
      "loss": 19.0102,
      "step": 10640
    },
    {
      "epoch": 10.65,
      "grad_norm": 22880.365234375,
      "learning_rate": 4.8188854489164085e-05,
      "loss": 17.1988,
      "step": 10641
    },
    {
      "epoch": 10.65,
      "grad_norm": 4052.94482421875,
      "learning_rate": 4.818369453044376e-05,
      "loss": 23.9159,
      "step": 10642
    },
    {
      "epoch": 10.65,
      "grad_norm": 15144.138671875,
      "learning_rate": 4.817853457172343e-05,
      "loss": 21.6441,
      "step": 10643
    },
    {
      "epoch": 10.65,
      "grad_norm": 11192.55078125,
      "learning_rate": 4.8173374613003095e-05,
      "loss": 14.8437,
      "step": 10644
    },
    {
      "epoch": 10.66,
      "grad_norm": 25008.880859375,
      "learning_rate": 4.816821465428277e-05,
      "loss": 15.2157,
      "step": 10645
    },
    {
      "epoch": 10.66,
      "grad_norm": 4046.418701171875,
      "learning_rate": 4.816305469556244e-05,
      "loss": 17.8919,
      "step": 10646
    },
    {
      "epoch": 10.66,
      "grad_norm": 7139.36865234375,
      "learning_rate": 4.8157894736842105e-05,
      "loss": 17.6358,
      "step": 10647
    },
    {
      "epoch": 10.66,
      "grad_norm": 3267.7451171875,
      "learning_rate": 4.815273477812178e-05,
      "loss": 14.7312,
      "step": 10648
    },
    {
      "epoch": 10.66,
      "grad_norm": 46946.03515625,
      "learning_rate": 4.814757481940145e-05,
      "loss": 17.3503,
      "step": 10649
    },
    {
      "epoch": 10.66,
      "grad_norm": 43803.9921875,
      "learning_rate": 4.8142414860681115e-05,
      "loss": 19.6087,
      "step": 10650
    },
    {
      "epoch": 10.66,
      "grad_norm": 7239.07763671875,
      "learning_rate": 4.813725490196079e-05,
      "loss": 17.3796,
      "step": 10651
    },
    {
      "epoch": 10.66,
      "grad_norm": 4184.7333984375,
      "learning_rate": 4.813209494324046e-05,
      "loss": 16.0781,
      "step": 10652
    },
    {
      "epoch": 10.66,
      "grad_norm": 14524.2275390625,
      "learning_rate": 4.8126934984520125e-05,
      "loss": 16.9558,
      "step": 10653
    },
    {
      "epoch": 10.66,
      "grad_norm": 183949.453125,
      "learning_rate": 4.81217750257998e-05,
      "loss": 20.3211,
      "step": 10654
    },
    {
      "epoch": 10.67,
      "grad_norm": 64233.734375,
      "learning_rate": 4.811661506707947e-05,
      "loss": 18.6471,
      "step": 10655
    },
    {
      "epoch": 10.67,
      "grad_norm": 23680.6328125,
      "learning_rate": 4.811145510835914e-05,
      "loss": 20.6391,
      "step": 10656
    },
    {
      "epoch": 10.67,
      "grad_norm": 2574.287109375,
      "learning_rate": 4.81062951496388e-05,
      "loss": 15.8004,
      "step": 10657
    },
    {
      "epoch": 10.67,
      "grad_norm": 5204.67578125,
      "learning_rate": 4.810113519091847e-05,
      "loss": 16.9535,
      "step": 10658
    },
    {
      "epoch": 10.67,
      "grad_norm": 12652.62890625,
      "learning_rate": 4.8095975232198145e-05,
      "loss": 20.1341,
      "step": 10659
    },
    {
      "epoch": 10.67,
      "grad_norm": 2693.822021484375,
      "learning_rate": 4.809081527347781e-05,
      "loss": 17.092,
      "step": 10660
    },
    {
      "epoch": 10.67,
      "grad_norm": 3226.826171875,
      "learning_rate": 4.808565531475748e-05,
      "loss": 16.1113,
      "step": 10661
    },
    {
      "epoch": 10.67,
      "grad_norm": 4554.3173828125,
      "learning_rate": 4.8080495356037155e-05,
      "loss": 14.8611,
      "step": 10662
    },
    {
      "epoch": 10.67,
      "grad_norm": 26418.6796875,
      "learning_rate": 4.807533539731682e-05,
      "loss": 14.6194,
      "step": 10663
    },
    {
      "epoch": 10.67,
      "grad_norm": 2577.4951171875,
      "learning_rate": 4.807017543859649e-05,
      "loss": 17.8014,
      "step": 10664
    },
    {
      "epoch": 10.68,
      "grad_norm": 4608.0908203125,
      "learning_rate": 4.8065015479876165e-05,
      "loss": 21.9402,
      "step": 10665
    },
    {
      "epoch": 10.68,
      "grad_norm": 11438.4423828125,
      "learning_rate": 4.805985552115583e-05,
      "loss": 16.9563,
      "step": 10666
    },
    {
      "epoch": 10.68,
      "grad_norm": 28058.943359375,
      "learning_rate": 4.80546955624355e-05,
      "loss": 18.9145,
      "step": 10667
    },
    {
      "epoch": 10.68,
      "grad_norm": 1731.4666748046875,
      "learning_rate": 4.8049535603715175e-05,
      "loss": 19.2205,
      "step": 10668
    },
    {
      "epoch": 10.68,
      "grad_norm": 1526.2568359375,
      "learning_rate": 4.804437564499484e-05,
      "loss": 18.0637,
      "step": 10669
    },
    {
      "epoch": 10.68,
      "grad_norm": 3271.48486328125,
      "learning_rate": 4.803921568627452e-05,
      "loss": 19.4042,
      "step": 10670
    },
    {
      "epoch": 10.68,
      "grad_norm": 11692.0869140625,
      "learning_rate": 4.8034055727554185e-05,
      "loss": 20.7793,
      "step": 10671
    },
    {
      "epoch": 10.68,
      "grad_norm": 2718.1015625,
      "learning_rate": 4.802889576883385e-05,
      "loss": 16.3241,
      "step": 10672
    },
    {
      "epoch": 10.68,
      "grad_norm": 11900.2294921875,
      "learning_rate": 4.802373581011353e-05,
      "loss": 13.5876,
      "step": 10673
    },
    {
      "epoch": 10.68,
      "grad_norm": 40302.734375,
      "learning_rate": 4.8018575851393195e-05,
      "loss": 15.5082,
      "step": 10674
    },
    {
      "epoch": 10.69,
      "grad_norm": 82709.328125,
      "learning_rate": 4.8013415892672856e-05,
      "loss": 17.7114,
      "step": 10675
    },
    {
      "epoch": 10.69,
      "grad_norm": 15335.2041015625,
      "learning_rate": 4.800825593395253e-05,
      "loss": 34.4407,
      "step": 10676
    },
    {
      "epoch": 10.69,
      "grad_norm": 12672.7373046875,
      "learning_rate": 4.80030959752322e-05,
      "loss": 17.6067,
      "step": 10677
    },
    {
      "epoch": 10.69,
      "grad_norm": 7237.50244140625,
      "learning_rate": 4.7997936016511866e-05,
      "loss": 15.417,
      "step": 10678
    },
    {
      "epoch": 10.69,
      "grad_norm": 5604.36767578125,
      "learning_rate": 4.799277605779154e-05,
      "loss": 16.9654,
      "step": 10679
    },
    {
      "epoch": 10.69,
      "grad_norm": 2453.6416015625,
      "learning_rate": 4.798761609907121e-05,
      "loss": 15.8585,
      "step": 10680
    },
    {
      "epoch": 10.69,
      "grad_norm": 3857.623046875,
      "learning_rate": 4.7982456140350876e-05,
      "loss": 17.0221,
      "step": 10681
    },
    {
      "epoch": 10.69,
      "grad_norm": 2712.842041015625,
      "learning_rate": 4.797729618163055e-05,
      "loss": 17.5593,
      "step": 10682
    },
    {
      "epoch": 10.69,
      "grad_norm": 12583.818359375,
      "learning_rate": 4.797213622291022e-05,
      "loss": 16.342,
      "step": 10683
    },
    {
      "epoch": 10.69,
      "grad_norm": 2361.7314453125,
      "learning_rate": 4.7966976264189886e-05,
      "loss": 15.9485,
      "step": 10684
    },
    {
      "epoch": 10.7,
      "grad_norm": 7050.19873046875,
      "learning_rate": 4.796181630546956e-05,
      "loss": 18.8962,
      "step": 10685
    },
    {
      "epoch": 10.7,
      "grad_norm": 4334.9619140625,
      "learning_rate": 4.795665634674923e-05,
      "loss": 19.69,
      "step": 10686
    },
    {
      "epoch": 10.7,
      "grad_norm": 27210.724609375,
      "learning_rate": 4.79514963880289e-05,
      "loss": 19.4781,
      "step": 10687
    },
    {
      "epoch": 10.7,
      "grad_norm": 24736.294921875,
      "learning_rate": 4.794633642930857e-05,
      "loss": 22.6528,
      "step": 10688
    },
    {
      "epoch": 10.7,
      "grad_norm": 3299.396240234375,
      "learning_rate": 4.794117647058824e-05,
      "loss": 17.1317,
      "step": 10689
    },
    {
      "epoch": 10.7,
      "grad_norm": 9849.98828125,
      "learning_rate": 4.793601651186791e-05,
      "loss": 15.8198,
      "step": 10690
    },
    {
      "epoch": 10.7,
      "grad_norm": 3096.5400390625,
      "learning_rate": 4.793085655314758e-05,
      "loss": 18.9279,
      "step": 10691
    },
    {
      "epoch": 10.7,
      "grad_norm": 3424.0380859375,
      "learning_rate": 4.792569659442725e-05,
      "loss": 21.9438,
      "step": 10692
    },
    {
      "epoch": 10.7,
      "grad_norm": 10787.251953125,
      "learning_rate": 4.7920536635706916e-05,
      "loss": 17.5799,
      "step": 10693
    },
    {
      "epoch": 10.7,
      "grad_norm": 10750.580078125,
      "learning_rate": 4.791537667698658e-05,
      "loss": 18.2544,
      "step": 10694
    },
    {
      "epoch": 10.71,
      "grad_norm": 3332.716796875,
      "learning_rate": 4.791021671826625e-05,
      "loss": 16.4197,
      "step": 10695
    },
    {
      "epoch": 10.71,
      "grad_norm": 12279.3720703125,
      "learning_rate": 4.7905056759545926e-05,
      "loss": 20.3573,
      "step": 10696
    },
    {
      "epoch": 10.71,
      "grad_norm": 6860.78857421875,
      "learning_rate": 4.789989680082559e-05,
      "loss": 17.3591,
      "step": 10697
    },
    {
      "epoch": 10.71,
      "grad_norm": 28545.0546875,
      "learning_rate": 4.789473684210526e-05,
      "loss": 17.8458,
      "step": 10698
    },
    {
      "epoch": 10.71,
      "grad_norm": 19119.787109375,
      "learning_rate": 4.7889576883384936e-05,
      "loss": 19.4734,
      "step": 10699
    },
    {
      "epoch": 10.71,
      "grad_norm": 2665.516845703125,
      "learning_rate": 4.78844169246646e-05,
      "loss": 15.7382,
      "step": 10700
    },
    {
      "epoch": 10.71,
      "grad_norm": 1074.6531982421875,
      "learning_rate": 4.787925696594428e-05,
      "loss": 17.4757,
      "step": 10701
    },
    {
      "epoch": 10.71,
      "grad_norm": 15911.0625,
      "learning_rate": 4.7874097007223946e-05,
      "loss": 16.0401,
      "step": 10702
    },
    {
      "epoch": 10.71,
      "grad_norm": 13682.943359375,
      "learning_rate": 4.786893704850361e-05,
      "loss": 18.5089,
      "step": 10703
    },
    {
      "epoch": 10.71,
      "grad_norm": 4313.04541015625,
      "learning_rate": 4.786377708978329e-05,
      "loss": 18.1194,
      "step": 10704
    },
    {
      "epoch": 10.72,
      "grad_norm": 82506.5859375,
      "learning_rate": 4.7858617131062956e-05,
      "loss": 14.7643,
      "step": 10705
    },
    {
      "epoch": 10.72,
      "grad_norm": 4863.46044921875,
      "learning_rate": 4.785345717234262e-05,
      "loss": 17.5709,
      "step": 10706
    },
    {
      "epoch": 10.72,
      "grad_norm": 3227.8857421875,
      "learning_rate": 4.78482972136223e-05,
      "loss": 14.0651,
      "step": 10707
    },
    {
      "epoch": 10.72,
      "grad_norm": 1735.2646484375,
      "learning_rate": 4.7843137254901966e-05,
      "loss": 15.5038,
      "step": 10708
    },
    {
      "epoch": 10.72,
      "grad_norm": 4615.14501953125,
      "learning_rate": 4.783797729618163e-05,
      "loss": 15.0855,
      "step": 10709
    },
    {
      "epoch": 10.72,
      "grad_norm": 3936.531494140625,
      "learning_rate": 4.783281733746131e-05,
      "loss": 18.672,
      "step": 10710
    },
    {
      "epoch": 10.72,
      "grad_norm": 75719.90625,
      "learning_rate": 4.782765737874097e-05,
      "loss": 18.2034,
      "step": 10711
    },
    {
      "epoch": 10.72,
      "grad_norm": 13004.7783203125,
      "learning_rate": 4.7822497420020637e-05,
      "loss": 17.8036,
      "step": 10712
    },
    {
      "epoch": 10.72,
      "grad_norm": 7754.0322265625,
      "learning_rate": 4.781733746130031e-05,
      "loss": 16.544,
      "step": 10713
    },
    {
      "epoch": 10.72,
      "grad_norm": 2433.095458984375,
      "learning_rate": 4.781217750257998e-05,
      "loss": 13.0821,
      "step": 10714
    },
    {
      "epoch": 10.73,
      "grad_norm": 34265.11328125,
      "learning_rate": 4.780701754385965e-05,
      "loss": 16.9662,
      "step": 10715
    },
    {
      "epoch": 10.73,
      "grad_norm": 2162.774658203125,
      "learning_rate": 4.780185758513932e-05,
      "loss": 18.5334,
      "step": 10716
    },
    {
      "epoch": 10.73,
      "grad_norm": 11123.279296875,
      "learning_rate": 4.779669762641899e-05,
      "loss": 18.3017,
      "step": 10717
    },
    {
      "epoch": 10.73,
      "grad_norm": 6953.7216796875,
      "learning_rate": 4.779153766769866e-05,
      "loss": 13.9638,
      "step": 10718
    },
    {
      "epoch": 10.73,
      "grad_norm": 7530.42236328125,
      "learning_rate": 4.778637770897833e-05,
      "loss": 21.2596,
      "step": 10719
    },
    {
      "epoch": 10.73,
      "grad_norm": 2536.08203125,
      "learning_rate": 4.7781217750258e-05,
      "loss": 14.1311,
      "step": 10720
    },
    {
      "epoch": 10.73,
      "grad_norm": 29023.19921875,
      "learning_rate": 4.777605779153767e-05,
      "loss": 22.5266,
      "step": 10721
    },
    {
      "epoch": 10.73,
      "grad_norm": 5005.36865234375,
      "learning_rate": 4.777089783281734e-05,
      "loss": 22.1867,
      "step": 10722
    },
    {
      "epoch": 10.73,
      "grad_norm": 3281.30224609375,
      "learning_rate": 4.776573787409701e-05,
      "loss": 16.4388,
      "step": 10723
    },
    {
      "epoch": 10.73,
      "grad_norm": 17410.224609375,
      "learning_rate": 4.776057791537668e-05,
      "loss": 15.9025,
      "step": 10724
    },
    {
      "epoch": 10.74,
      "grad_norm": 15597.39453125,
      "learning_rate": 4.775541795665635e-05,
      "loss": 20.0775,
      "step": 10725
    },
    {
      "epoch": 10.74,
      "grad_norm": 3189.048828125,
      "learning_rate": 4.775025799793602e-05,
      "loss": 16.4087,
      "step": 10726
    },
    {
      "epoch": 10.74,
      "grad_norm": 4462.36865234375,
      "learning_rate": 4.774509803921569e-05,
      "loss": 17.6447,
      "step": 10727
    },
    {
      "epoch": 10.74,
      "grad_norm": 16828.623046875,
      "learning_rate": 4.773993808049536e-05,
      "loss": 17.7805,
      "step": 10728
    },
    {
      "epoch": 10.74,
      "grad_norm": 16647.646484375,
      "learning_rate": 4.773477812177503e-05,
      "loss": 18.4673,
      "step": 10729
    },
    {
      "epoch": 10.74,
      "grad_norm": 17953.845703125,
      "learning_rate": 4.77296181630547e-05,
      "loss": 18.0497,
      "step": 10730
    },
    {
      "epoch": 10.74,
      "grad_norm": 16387.107421875,
      "learning_rate": 4.7724458204334364e-05,
      "loss": 19.8646,
      "step": 10731
    },
    {
      "epoch": 10.74,
      "grad_norm": 12517.111328125,
      "learning_rate": 4.771929824561404e-05,
      "loss": 16.6414,
      "step": 10732
    },
    {
      "epoch": 10.74,
      "grad_norm": 15324.275390625,
      "learning_rate": 4.771413828689371e-05,
      "loss": 17.4901,
      "step": 10733
    },
    {
      "epoch": 10.74,
      "grad_norm": 6846.75390625,
      "learning_rate": 4.7708978328173374e-05,
      "loss": 19.0458,
      "step": 10734
    },
    {
      "epoch": 10.75,
      "grad_norm": 3217.91943359375,
      "learning_rate": 4.770381836945305e-05,
      "loss": 14.8552,
      "step": 10735
    },
    {
      "epoch": 10.75,
      "grad_norm": 47205.51171875,
      "learning_rate": 4.769865841073272e-05,
      "loss": 18.8367,
      "step": 10736
    },
    {
      "epoch": 10.75,
      "grad_norm": 3710.77001953125,
      "learning_rate": 4.7693498452012384e-05,
      "loss": 16.4009,
      "step": 10737
    },
    {
      "epoch": 10.75,
      "grad_norm": 4555.43017578125,
      "learning_rate": 4.768833849329206e-05,
      "loss": 18.4738,
      "step": 10738
    },
    {
      "epoch": 10.75,
      "grad_norm": 1828.1146240234375,
      "learning_rate": 4.768317853457173e-05,
      "loss": 17.6258,
      "step": 10739
    },
    {
      "epoch": 10.75,
      "grad_norm": 20634.53515625,
      "learning_rate": 4.7678018575851394e-05,
      "loss": 19.831,
      "step": 10740
    },
    {
      "epoch": 10.75,
      "grad_norm": 9404.2392578125,
      "learning_rate": 4.767285861713107e-05,
      "loss": 15.3401,
      "step": 10741
    },
    {
      "epoch": 10.75,
      "grad_norm": 5585.61962890625,
      "learning_rate": 4.766769865841074e-05,
      "loss": 19.6996,
      "step": 10742
    },
    {
      "epoch": 10.75,
      "grad_norm": 14252.5986328125,
      "learning_rate": 4.7662538699690404e-05,
      "loss": 18.3989,
      "step": 10743
    },
    {
      "epoch": 10.75,
      "grad_norm": 3481.96240234375,
      "learning_rate": 4.765737874097008e-05,
      "loss": 20.1497,
      "step": 10744
    },
    {
      "epoch": 10.76,
      "grad_norm": 12038.3515625,
      "learning_rate": 4.765221878224975e-05,
      "loss": 17.0022,
      "step": 10745
    },
    {
      "epoch": 10.76,
      "grad_norm": 8913.654296875,
      "learning_rate": 4.7647058823529414e-05,
      "loss": 15.345,
      "step": 10746
    },
    {
      "epoch": 10.76,
      "grad_norm": 10822.701171875,
      "learning_rate": 4.764189886480908e-05,
      "loss": 20.0457,
      "step": 10747
    },
    {
      "epoch": 10.76,
      "grad_norm": 20175.814453125,
      "learning_rate": 4.763673890608875e-05,
      "loss": 13.1007,
      "step": 10748
    },
    {
      "epoch": 10.76,
      "grad_norm": 9459.3359375,
      "learning_rate": 4.7631578947368424e-05,
      "loss": 16.8513,
      "step": 10749
    },
    {
      "epoch": 10.76,
      "grad_norm": 6482.50146484375,
      "learning_rate": 4.762641898864809e-05,
      "loss": 14.8579,
      "step": 10750
    },
    {
      "epoch": 10.76,
      "grad_norm": 5562.30322265625,
      "learning_rate": 4.762125902992776e-05,
      "loss": 23.6558,
      "step": 10751
    },
    {
      "epoch": 10.76,
      "grad_norm": 25328.9609375,
      "learning_rate": 4.7616099071207434e-05,
      "loss": 15.1217,
      "step": 10752
    },
    {
      "epoch": 10.76,
      "grad_norm": 17765.44140625,
      "learning_rate": 4.76109391124871e-05,
      "loss": 18.146,
      "step": 10753
    },
    {
      "epoch": 10.76,
      "grad_norm": 2728.4814453125,
      "learning_rate": 4.760577915376677e-05,
      "loss": 12.4486,
      "step": 10754
    },
    {
      "epoch": 10.77,
      "grad_norm": 3559.00634765625,
      "learning_rate": 4.7600619195046444e-05,
      "loss": 18.6167,
      "step": 10755
    },
    {
      "epoch": 10.77,
      "grad_norm": 14615.462890625,
      "learning_rate": 4.759545923632611e-05,
      "loss": 19.2713,
      "step": 10756
    },
    {
      "epoch": 10.77,
      "grad_norm": 34144.80078125,
      "learning_rate": 4.759029927760578e-05,
      "loss": 24.0007,
      "step": 10757
    },
    {
      "epoch": 10.77,
      "grad_norm": 5173.0654296875,
      "learning_rate": 4.7585139318885454e-05,
      "loss": 24.3972,
      "step": 10758
    },
    {
      "epoch": 10.77,
      "grad_norm": 3275.816650390625,
      "learning_rate": 4.757997936016512e-05,
      "loss": 16.5013,
      "step": 10759
    },
    {
      "epoch": 10.77,
      "grad_norm": 6306.75146484375,
      "learning_rate": 4.757481940144479e-05,
      "loss": 20.4891,
      "step": 10760
    },
    {
      "epoch": 10.77,
      "grad_norm": 58449.34375,
      "learning_rate": 4.7569659442724464e-05,
      "loss": 15.2711,
      "step": 10761
    },
    {
      "epoch": 10.77,
      "grad_norm": 4991.1591796875,
      "learning_rate": 4.756449948400413e-05,
      "loss": 18.8308,
      "step": 10762
    },
    {
      "epoch": 10.77,
      "grad_norm": 3901.558837890625,
      "learning_rate": 4.75593395252838e-05,
      "loss": 17.923,
      "step": 10763
    },
    {
      "epoch": 10.77,
      "grad_norm": 20696.83203125,
      "learning_rate": 4.755417956656347e-05,
      "loss": 15.694,
      "step": 10764
    },
    {
      "epoch": 10.78,
      "grad_norm": 9780.76953125,
      "learning_rate": 4.7549019607843135e-05,
      "loss": 21.7125,
      "step": 10765
    },
    {
      "epoch": 10.78,
      "grad_norm": 59937.18359375,
      "learning_rate": 4.754385964912281e-05,
      "loss": 26.731,
      "step": 10766
    },
    {
      "epoch": 10.78,
      "grad_norm": 8099.15625,
      "learning_rate": 4.753869969040248e-05,
      "loss": 22.3221,
      "step": 10767
    },
    {
      "epoch": 10.78,
      "grad_norm": 8583.5341796875,
      "learning_rate": 4.7533539731682145e-05,
      "loss": 18.9182,
      "step": 10768
    },
    {
      "epoch": 10.78,
      "grad_norm": 15216.287109375,
      "learning_rate": 4.752837977296182e-05,
      "loss": 19.8582,
      "step": 10769
    },
    {
      "epoch": 10.78,
      "grad_norm": 2866.855224609375,
      "learning_rate": 4.752321981424149e-05,
      "loss": 19.9644,
      "step": 10770
    },
    {
      "epoch": 10.78,
      "grad_norm": 252444.171875,
      "learning_rate": 4.7518059855521155e-05,
      "loss": 20.7234,
      "step": 10771
    },
    {
      "epoch": 10.78,
      "grad_norm": 15727.4443359375,
      "learning_rate": 4.751289989680083e-05,
      "loss": 18.7293,
      "step": 10772
    },
    {
      "epoch": 10.78,
      "grad_norm": 3855.530029296875,
      "learning_rate": 4.75077399380805e-05,
      "loss": 19.6227,
      "step": 10773
    },
    {
      "epoch": 10.78,
      "grad_norm": 3848.380859375,
      "learning_rate": 4.7502579979360165e-05,
      "loss": 18.2944,
      "step": 10774
    },
    {
      "epoch": 10.79,
      "grad_norm": 4294.43408203125,
      "learning_rate": 4.749742002063984e-05,
      "loss": 16.6398,
      "step": 10775
    },
    {
      "epoch": 10.79,
      "grad_norm": 11208.4189453125,
      "learning_rate": 4.749226006191951e-05,
      "loss": 15.2484,
      "step": 10776
    },
    {
      "epoch": 10.79,
      "grad_norm": 10795.0712890625,
      "learning_rate": 4.7487100103199175e-05,
      "loss": 12.8437,
      "step": 10777
    },
    {
      "epoch": 10.79,
      "grad_norm": 3800.05712890625,
      "learning_rate": 4.748194014447885e-05,
      "loss": 15.2273,
      "step": 10778
    },
    {
      "epoch": 10.79,
      "grad_norm": 13690.5224609375,
      "learning_rate": 4.747678018575852e-05,
      "loss": 20.1054,
      "step": 10779
    },
    {
      "epoch": 10.79,
      "grad_norm": 4189.13525390625,
      "learning_rate": 4.7471620227038185e-05,
      "loss": 19.0888,
      "step": 10780
    },
    {
      "epoch": 10.79,
      "grad_norm": 164322.15625,
      "learning_rate": 4.746646026831786e-05,
      "loss": 26.3551,
      "step": 10781
    },
    {
      "epoch": 10.79,
      "grad_norm": 4893.3466796875,
      "learning_rate": 4.746130030959752e-05,
      "loss": 27.8564,
      "step": 10782
    },
    {
      "epoch": 10.79,
      "grad_norm": 8208.4599609375,
      "learning_rate": 4.7456140350877195e-05,
      "loss": 17.6208,
      "step": 10783
    },
    {
      "epoch": 10.79,
      "grad_norm": 927.724609375,
      "learning_rate": 4.745098039215686e-05,
      "loss": 18.6327,
      "step": 10784
    },
    {
      "epoch": 10.8,
      "grad_norm": 11403.1103515625,
      "learning_rate": 4.744582043343653e-05,
      "loss": 19.2588,
      "step": 10785
    },
    {
      "epoch": 10.8,
      "grad_norm": 6178.99267578125,
      "learning_rate": 4.7440660474716205e-05,
      "loss": 20.3188,
      "step": 10786
    },
    {
      "epoch": 10.8,
      "grad_norm": 163098.046875,
      "learning_rate": 4.743550051599587e-05,
      "loss": 17.9998,
      "step": 10787
    },
    {
      "epoch": 10.8,
      "grad_norm": 26240.001953125,
      "learning_rate": 4.743034055727554e-05,
      "loss": 21.6257,
      "step": 10788
    },
    {
      "epoch": 10.8,
      "grad_norm": 5623.66552734375,
      "learning_rate": 4.7425180598555215e-05,
      "loss": 16.1121,
      "step": 10789
    },
    {
      "epoch": 10.8,
      "grad_norm": 27471.568359375,
      "learning_rate": 4.742002063983488e-05,
      "loss": 23.1475,
      "step": 10790
    },
    {
      "epoch": 10.8,
      "grad_norm": 9511.921875,
      "learning_rate": 4.741486068111455e-05,
      "loss": 17.6374,
      "step": 10791
    },
    {
      "epoch": 10.8,
      "grad_norm": 3392.943115234375,
      "learning_rate": 4.7409700722394225e-05,
      "loss": 17.2564,
      "step": 10792
    },
    {
      "epoch": 10.8,
      "grad_norm": 1231.0662841796875,
      "learning_rate": 4.740454076367389e-05,
      "loss": 19.5557,
      "step": 10793
    },
    {
      "epoch": 10.8,
      "grad_norm": 50622.41796875,
      "learning_rate": 4.739938080495356e-05,
      "loss": 18.891,
      "step": 10794
    },
    {
      "epoch": 10.81,
      "grad_norm": 27026.302734375,
      "learning_rate": 4.7394220846233235e-05,
      "loss": 21.342,
      "step": 10795
    },
    {
      "epoch": 10.81,
      "grad_norm": 57104.26171875,
      "learning_rate": 4.73890608875129e-05,
      "loss": 16.8964,
      "step": 10796
    },
    {
      "epoch": 10.81,
      "grad_norm": 4647.8603515625,
      "learning_rate": 4.738390092879257e-05,
      "loss": 17.6585,
      "step": 10797
    },
    {
      "epoch": 10.81,
      "grad_norm": 2455.723876953125,
      "learning_rate": 4.7378740970072245e-05,
      "loss": 17.9884,
      "step": 10798
    },
    {
      "epoch": 10.81,
      "grad_norm": 17820.876953125,
      "learning_rate": 4.737358101135191e-05,
      "loss": 15.3818,
      "step": 10799
    },
    {
      "epoch": 10.81,
      "grad_norm": 25693.775390625,
      "learning_rate": 4.736842105263158e-05,
      "loss": 23.9909,
      "step": 10800
    },
    {
      "epoch": 10.81,
      "grad_norm": 36786.70703125,
      "learning_rate": 4.736326109391125e-05,
      "loss": 17.8583,
      "step": 10801
    },
    {
      "epoch": 10.81,
      "grad_norm": 5756.2548828125,
      "learning_rate": 4.7358101135190916e-05,
      "loss": 16.4172,
      "step": 10802
    },
    {
      "epoch": 10.81,
      "grad_norm": 27702.703125,
      "learning_rate": 4.735294117647059e-05,
      "loss": 18.635,
      "step": 10803
    },
    {
      "epoch": 10.81,
      "grad_norm": 14473.2822265625,
      "learning_rate": 4.734778121775026e-05,
      "loss": 17.466,
      "step": 10804
    },
    {
      "epoch": 10.82,
      "grad_norm": 26154.07421875,
      "learning_rate": 4.7342621259029926e-05,
      "loss": 20.7417,
      "step": 10805
    },
    {
      "epoch": 10.82,
      "grad_norm": 12724.9150390625,
      "learning_rate": 4.73374613003096e-05,
      "loss": 21.304,
      "step": 10806
    },
    {
      "epoch": 10.82,
      "grad_norm": 2238.830078125,
      "learning_rate": 4.733230134158927e-05,
      "loss": 18.6132,
      "step": 10807
    },
    {
      "epoch": 10.82,
      "grad_norm": 14252.2646484375,
      "learning_rate": 4.7327141382868936e-05,
      "loss": 21.8144,
      "step": 10808
    },
    {
      "epoch": 10.82,
      "grad_norm": 6309.5283203125,
      "learning_rate": 4.732198142414861e-05,
      "loss": 22.9627,
      "step": 10809
    },
    {
      "epoch": 10.82,
      "grad_norm": 13072.87890625,
      "learning_rate": 4.731682146542828e-05,
      "loss": 22.0401,
      "step": 10810
    },
    {
      "epoch": 10.82,
      "grad_norm": 4626.92236328125,
      "learning_rate": 4.7311661506707946e-05,
      "loss": 14.1024,
      "step": 10811
    },
    {
      "epoch": 10.82,
      "grad_norm": 3109.1279296875,
      "learning_rate": 4.730650154798762e-05,
      "loss": 15.1742,
      "step": 10812
    },
    {
      "epoch": 10.82,
      "grad_norm": 3400.611328125,
      "learning_rate": 4.730134158926729e-05,
      "loss": 16.7948,
      "step": 10813
    },
    {
      "epoch": 10.82,
      "grad_norm": 11483.2529296875,
      "learning_rate": 4.7296181630546956e-05,
      "loss": 18.9201,
      "step": 10814
    },
    {
      "epoch": 10.83,
      "grad_norm": 19854.98046875,
      "learning_rate": 4.729102167182663e-05,
      "loss": 20.5602,
      "step": 10815
    },
    {
      "epoch": 10.83,
      "grad_norm": 30709.068359375,
      "learning_rate": 4.72858617131063e-05,
      "loss": 19.1291,
      "step": 10816
    },
    {
      "epoch": 10.83,
      "grad_norm": 1820.477294921875,
      "learning_rate": 4.728070175438597e-05,
      "loss": 14.2349,
      "step": 10817
    },
    {
      "epoch": 10.83,
      "grad_norm": 3116.1103515625,
      "learning_rate": 4.7275541795665634e-05,
      "loss": 16.0177,
      "step": 10818
    },
    {
      "epoch": 10.83,
      "grad_norm": 3934.462646484375,
      "learning_rate": 4.72703818369453e-05,
      "loss": 18.226,
      "step": 10819
    },
    {
      "epoch": 10.83,
      "grad_norm": 1110.9014892578125,
      "learning_rate": 4.7265221878224976e-05,
      "loss": 15.133,
      "step": 10820
    },
    {
      "epoch": 10.83,
      "grad_norm": 1502.7744140625,
      "learning_rate": 4.7260061919504644e-05,
      "loss": 15.5236,
      "step": 10821
    },
    {
      "epoch": 10.83,
      "grad_norm": 2452.076171875,
      "learning_rate": 4.725490196078431e-05,
      "loss": 17.0941,
      "step": 10822
    },
    {
      "epoch": 10.83,
      "grad_norm": 4437.646484375,
      "learning_rate": 4.7249742002063986e-05,
      "loss": 18.6569,
      "step": 10823
    },
    {
      "epoch": 10.83,
      "grad_norm": 2029.5230712890625,
      "learning_rate": 4.7244582043343654e-05,
      "loss": 17.4531,
      "step": 10824
    },
    {
      "epoch": 10.84,
      "grad_norm": 1538.0648193359375,
      "learning_rate": 4.723942208462332e-05,
      "loss": 19.751,
      "step": 10825
    },
    {
      "epoch": 10.84,
      "grad_norm": 9879.2724609375,
      "learning_rate": 4.7234262125902996e-05,
      "loss": 17.6023,
      "step": 10826
    },
    {
      "epoch": 10.84,
      "grad_norm": 16231.51953125,
      "learning_rate": 4.7229102167182664e-05,
      "loss": 23.0359,
      "step": 10827
    },
    {
      "epoch": 10.84,
      "grad_norm": 13244.0361328125,
      "learning_rate": 4.722394220846233e-05,
      "loss": 22.4629,
      "step": 10828
    },
    {
      "epoch": 10.84,
      "grad_norm": 14693.5400390625,
      "learning_rate": 4.7218782249742006e-05,
      "loss": 26.6931,
      "step": 10829
    },
    {
      "epoch": 10.84,
      "grad_norm": 9791.7666015625,
      "learning_rate": 4.7213622291021674e-05,
      "loss": 24.1181,
      "step": 10830
    },
    {
      "epoch": 10.84,
      "grad_norm": 2575.967041015625,
      "learning_rate": 4.720846233230135e-05,
      "loss": 19.3781,
      "step": 10831
    },
    {
      "epoch": 10.84,
      "grad_norm": 19656.421875,
      "learning_rate": 4.7203302373581016e-05,
      "loss": 16.5508,
      "step": 10832
    },
    {
      "epoch": 10.84,
      "grad_norm": 3527.625732421875,
      "learning_rate": 4.7198142414860684e-05,
      "loss": 24.0879,
      "step": 10833
    },
    {
      "epoch": 10.84,
      "grad_norm": 7376.7744140625,
      "learning_rate": 4.719298245614036e-05,
      "loss": 15.4887,
      "step": 10834
    },
    {
      "epoch": 10.85,
      "grad_norm": 1900.05078125,
      "learning_rate": 4.7187822497420026e-05,
      "loss": 16.2832,
      "step": 10835
    },
    {
      "epoch": 10.85,
      "grad_norm": 23810.9375,
      "learning_rate": 4.718266253869969e-05,
      "loss": 20.3439,
      "step": 10836
    },
    {
      "epoch": 10.85,
      "grad_norm": 4864.93115234375,
      "learning_rate": 4.717750257997936e-05,
      "loss": 15.7976,
      "step": 10837
    },
    {
      "epoch": 10.85,
      "grad_norm": 6689.6796875,
      "learning_rate": 4.717234262125903e-05,
      "loss": 21.0953,
      "step": 10838
    },
    {
      "epoch": 10.85,
      "grad_norm": 5825.76708984375,
      "learning_rate": 4.71671826625387e-05,
      "loss": 16.8658,
      "step": 10839
    },
    {
      "epoch": 10.85,
      "grad_norm": 227144.28125,
      "learning_rate": 4.716202270381837e-05,
      "loss": 13.9853,
      "step": 10840
    },
    {
      "epoch": 10.85,
      "grad_norm": 12996.8515625,
      "learning_rate": 4.715686274509804e-05,
      "loss": 17.0056,
      "step": 10841
    },
    {
      "epoch": 10.85,
      "grad_norm": 5202.2490234375,
      "learning_rate": 4.715170278637771e-05,
      "loss": 16.7586,
      "step": 10842
    },
    {
      "epoch": 10.85,
      "grad_norm": 11118.2646484375,
      "learning_rate": 4.714654282765738e-05,
      "loss": 13.9405,
      "step": 10843
    },
    {
      "epoch": 10.85,
      "grad_norm": 3551.92138671875,
      "learning_rate": 4.714138286893705e-05,
      "loss": 14.612,
      "step": 10844
    },
    {
      "epoch": 10.86,
      "grad_norm": 6600.8876953125,
      "learning_rate": 4.7136222910216724e-05,
      "loss": 19.7221,
      "step": 10845
    },
    {
      "epoch": 10.86,
      "grad_norm": 7976.744140625,
      "learning_rate": 4.713106295149639e-05,
      "loss": 17.4691,
      "step": 10846
    },
    {
      "epoch": 10.86,
      "grad_norm": 31290.48828125,
      "learning_rate": 4.712590299277606e-05,
      "loss": 23.7044,
      "step": 10847
    },
    {
      "epoch": 10.86,
      "grad_norm": 1829.2496337890625,
      "learning_rate": 4.7120743034055734e-05,
      "loss": 18.5807,
      "step": 10848
    },
    {
      "epoch": 10.86,
      "grad_norm": 4570.37841796875,
      "learning_rate": 4.71155830753354e-05,
      "loss": 18.1596,
      "step": 10849
    },
    {
      "epoch": 10.86,
      "grad_norm": 603975.0625,
      "learning_rate": 4.711042311661507e-05,
      "loss": 17.1491,
      "step": 10850
    },
    {
      "epoch": 10.86,
      "grad_norm": 8436.9248046875,
      "learning_rate": 4.7105263157894744e-05,
      "loss": 19.493,
      "step": 10851
    },
    {
      "epoch": 10.86,
      "grad_norm": 5559.208984375,
      "learning_rate": 4.710010319917441e-05,
      "loss": 18.0563,
      "step": 10852
    },
    {
      "epoch": 10.86,
      "grad_norm": 5667.921875,
      "learning_rate": 4.709494324045408e-05,
      "loss": 18.6497,
      "step": 10853
    },
    {
      "epoch": 10.86,
      "grad_norm": 33175.44921875,
      "learning_rate": 4.708978328173375e-05,
      "loss": 19.5208,
      "step": 10854
    },
    {
      "epoch": 10.87,
      "grad_norm": 139633.46875,
      "learning_rate": 4.7084623323013415e-05,
      "loss": 16.8059,
      "step": 10855
    },
    {
      "epoch": 10.87,
      "grad_norm": 7272.07763671875,
      "learning_rate": 4.707946336429308e-05,
      "loss": 17.5099,
      "step": 10856
    },
    {
      "epoch": 10.87,
      "grad_norm": 13070.603515625,
      "learning_rate": 4.707430340557276e-05,
      "loss": 14.291,
      "step": 10857
    },
    {
      "epoch": 10.87,
      "grad_norm": 11138.572265625,
      "learning_rate": 4.7069143446852425e-05,
      "loss": 14.899,
      "step": 10858
    },
    {
      "epoch": 10.87,
      "grad_norm": 21282.8125,
      "learning_rate": 4.70639834881321e-05,
      "loss": 15.0133,
      "step": 10859
    },
    {
      "epoch": 10.87,
      "grad_norm": 46511.48828125,
      "learning_rate": 4.705882352941177e-05,
      "loss": 18.1713,
      "step": 10860
    },
    {
      "epoch": 10.87,
      "grad_norm": 1656.5577392578125,
      "learning_rate": 4.7053663570691435e-05,
      "loss": 13.663,
      "step": 10861
    },
    {
      "epoch": 10.87,
      "grad_norm": 5846.22265625,
      "learning_rate": 4.704850361197111e-05,
      "loss": 25.1415,
      "step": 10862
    },
    {
      "epoch": 10.87,
      "grad_norm": 15042.1689453125,
      "learning_rate": 4.704334365325078e-05,
      "loss": 16.4325,
      "step": 10863
    },
    {
      "epoch": 10.87,
      "grad_norm": 6814.86572265625,
      "learning_rate": 4.7038183694530445e-05,
      "loss": 13.4576,
      "step": 10864
    },
    {
      "epoch": 10.88,
      "grad_norm": 2088.374267578125,
      "learning_rate": 4.703302373581012e-05,
      "loss": 18.1095,
      "step": 10865
    },
    {
      "epoch": 10.88,
      "grad_norm": 9775.38671875,
      "learning_rate": 4.702786377708979e-05,
      "loss": 22.3021,
      "step": 10866
    },
    {
      "epoch": 10.88,
      "grad_norm": 7163.10888671875,
      "learning_rate": 4.7022703818369455e-05,
      "loss": 15.7595,
      "step": 10867
    },
    {
      "epoch": 10.88,
      "grad_norm": 1209.6287841796875,
      "learning_rate": 4.701754385964913e-05,
      "loss": 15.7559,
      "step": 10868
    },
    {
      "epoch": 10.88,
      "grad_norm": 5622.30029296875,
      "learning_rate": 4.70123839009288e-05,
      "loss": 20.3804,
      "step": 10869
    },
    {
      "epoch": 10.88,
      "grad_norm": 63337.875,
      "learning_rate": 4.7007223942208465e-05,
      "loss": 21.8053,
      "step": 10870
    },
    {
      "epoch": 10.88,
      "grad_norm": 8724.6513671875,
      "learning_rate": 4.700206398348814e-05,
      "loss": 17.1951,
      "step": 10871
    },
    {
      "epoch": 10.88,
      "grad_norm": 3180.283447265625,
      "learning_rate": 4.69969040247678e-05,
      "loss": 20.1845,
      "step": 10872
    },
    {
      "epoch": 10.88,
      "grad_norm": 16165.3408203125,
      "learning_rate": 4.699174406604747e-05,
      "loss": 19.1429,
      "step": 10873
    },
    {
      "epoch": 10.88,
      "grad_norm": 2307.916748046875,
      "learning_rate": 4.698658410732714e-05,
      "loss": 22.5143,
      "step": 10874
    },
    {
      "epoch": 10.89,
      "grad_norm": 6935.68603515625,
      "learning_rate": 4.698142414860681e-05,
      "loss": 23.3378,
      "step": 10875
    },
    {
      "epoch": 10.89,
      "grad_norm": 1920.357666015625,
      "learning_rate": 4.6976264189886485e-05,
      "loss": 14.377,
      "step": 10876
    },
    {
      "epoch": 10.89,
      "grad_norm": 8149.79736328125,
      "learning_rate": 4.697110423116615e-05,
      "loss": 17.302,
      "step": 10877
    },
    {
      "epoch": 10.89,
      "grad_norm": 2985.244140625,
      "learning_rate": 4.696594427244582e-05,
      "loss": 24.6053,
      "step": 10878
    },
    {
      "epoch": 10.89,
      "grad_norm": 2863.736328125,
      "learning_rate": 4.6960784313725495e-05,
      "loss": 17.7178,
      "step": 10879
    },
    {
      "epoch": 10.89,
      "grad_norm": 1998.6710205078125,
      "learning_rate": 4.695562435500516e-05,
      "loss": 22.268,
      "step": 10880
    },
    {
      "epoch": 10.89,
      "grad_norm": 4148.71044921875,
      "learning_rate": 4.695046439628483e-05,
      "loss": 20.6354,
      "step": 10881
    },
    {
      "epoch": 10.89,
      "grad_norm": 30462.96484375,
      "learning_rate": 4.6945304437564505e-05,
      "loss": 20.4964,
      "step": 10882
    },
    {
      "epoch": 10.89,
      "grad_norm": 10529.1103515625,
      "learning_rate": 4.694014447884417e-05,
      "loss": 26.9238,
      "step": 10883
    },
    {
      "epoch": 10.89,
      "grad_norm": 9164.478515625,
      "learning_rate": 4.693498452012384e-05,
      "loss": 18.4953,
      "step": 10884
    },
    {
      "epoch": 10.9,
      "grad_norm": 4126.1650390625,
      "learning_rate": 4.6929824561403515e-05,
      "loss": 15.8715,
      "step": 10885
    },
    {
      "epoch": 10.9,
      "grad_norm": 25477.578125,
      "learning_rate": 4.692466460268318e-05,
      "loss": 17.4124,
      "step": 10886
    },
    {
      "epoch": 10.9,
      "grad_norm": 3464.518798828125,
      "learning_rate": 4.691950464396285e-05,
      "loss": 14.1487,
      "step": 10887
    },
    {
      "epoch": 10.9,
      "grad_norm": 10504.3828125,
      "learning_rate": 4.6914344685242525e-05,
      "loss": 17.1606,
      "step": 10888
    },
    {
      "epoch": 10.9,
      "grad_norm": 19882.955078125,
      "learning_rate": 4.6909184726522186e-05,
      "loss": 18.9374,
      "step": 10889
    },
    {
      "epoch": 10.9,
      "grad_norm": 3046.996337890625,
      "learning_rate": 4.690402476780186e-05,
      "loss": 22.9438,
      "step": 10890
    },
    {
      "epoch": 10.9,
      "grad_norm": 10646.6298828125,
      "learning_rate": 4.689886480908153e-05,
      "loss": 20.4207,
      "step": 10891
    },
    {
      "epoch": 10.9,
      "grad_norm": 62847.41015625,
      "learning_rate": 4.6893704850361196e-05,
      "loss": 26.7594,
      "step": 10892
    },
    {
      "epoch": 10.9,
      "grad_norm": 3239.7138671875,
      "learning_rate": 4.688854489164087e-05,
      "loss": 15.8229,
      "step": 10893
    },
    {
      "epoch": 10.9,
      "grad_norm": 9372.4638671875,
      "learning_rate": 4.688338493292054e-05,
      "loss": 18.7239,
      "step": 10894
    },
    {
      "epoch": 10.91,
      "grad_norm": 4575.85546875,
      "learning_rate": 4.6878224974200206e-05,
      "loss": 24.6951,
      "step": 10895
    },
    {
      "epoch": 10.91,
      "grad_norm": 7311.0654296875,
      "learning_rate": 4.687306501547988e-05,
      "loss": 18.8637,
      "step": 10896
    },
    {
      "epoch": 10.91,
      "grad_norm": 4608.51904296875,
      "learning_rate": 4.686790505675955e-05,
      "loss": 19.9372,
      "step": 10897
    },
    {
      "epoch": 10.91,
      "grad_norm": 2344.10546875,
      "learning_rate": 4.6862745098039216e-05,
      "loss": 11.4039,
      "step": 10898
    },
    {
      "epoch": 10.91,
      "grad_norm": 11684.8046875,
      "learning_rate": 4.685758513931889e-05,
      "loss": 22.3398,
      "step": 10899
    },
    {
      "epoch": 10.91,
      "grad_norm": 6791.34814453125,
      "learning_rate": 4.685242518059856e-05,
      "loss": 17.9417,
      "step": 10900
    },
    {
      "epoch": 10.91,
      "grad_norm": 94012.7578125,
      "learning_rate": 4.6847265221878226e-05,
      "loss": 19.8087,
      "step": 10901
    },
    {
      "epoch": 10.91,
      "grad_norm": 18667.51953125,
      "learning_rate": 4.68421052631579e-05,
      "loss": 24.2182,
      "step": 10902
    },
    {
      "epoch": 10.91,
      "grad_norm": 18550.021484375,
      "learning_rate": 4.683694530443757e-05,
      "loss": 15.9505,
      "step": 10903
    },
    {
      "epoch": 10.91,
      "grad_norm": 1734.1954345703125,
      "learning_rate": 4.6831785345717236e-05,
      "loss": 18.6872,
      "step": 10904
    },
    {
      "epoch": 10.92,
      "grad_norm": 4703.267578125,
      "learning_rate": 4.682662538699691e-05,
      "loss": 20.1173,
      "step": 10905
    },
    {
      "epoch": 10.92,
      "grad_norm": 124338.0859375,
      "learning_rate": 4.682146542827658e-05,
      "loss": 18.9653,
      "step": 10906
    },
    {
      "epoch": 10.92,
      "grad_norm": 24094.068359375,
      "learning_rate": 4.6816305469556246e-05,
      "loss": 18.3621,
      "step": 10907
    },
    {
      "epoch": 10.92,
      "grad_norm": 5308.0751953125,
      "learning_rate": 4.6811145510835914e-05,
      "loss": 14.3624,
      "step": 10908
    },
    {
      "epoch": 10.92,
      "grad_norm": 4129.2646484375,
      "learning_rate": 4.680598555211558e-05,
      "loss": 19.8776,
      "step": 10909
    },
    {
      "epoch": 10.92,
      "grad_norm": 6531.0478515625,
      "learning_rate": 4.6800825593395256e-05,
      "loss": 18.4109,
      "step": 10910
    },
    {
      "epoch": 10.92,
      "grad_norm": 27115.2109375,
      "learning_rate": 4.6795665634674924e-05,
      "loss": 18.7304,
      "step": 10911
    },
    {
      "epoch": 10.92,
      "grad_norm": 8312.90234375,
      "learning_rate": 4.679050567595459e-05,
      "loss": 16.2233,
      "step": 10912
    },
    {
      "epoch": 10.92,
      "grad_norm": 18055.333984375,
      "learning_rate": 4.6785345717234266e-05,
      "loss": 20.3041,
      "step": 10913
    },
    {
      "epoch": 10.92,
      "grad_norm": 16634.54296875,
      "learning_rate": 4.6780185758513934e-05,
      "loss": 21.6302,
      "step": 10914
    },
    {
      "epoch": 10.93,
      "grad_norm": 3077.67236328125,
      "learning_rate": 4.67750257997936e-05,
      "loss": 20.7437,
      "step": 10915
    },
    {
      "epoch": 10.93,
      "grad_norm": 22966.279296875,
      "learning_rate": 4.6769865841073276e-05,
      "loss": 19.0477,
      "step": 10916
    },
    {
      "epoch": 10.93,
      "grad_norm": 5611.7431640625,
      "learning_rate": 4.6764705882352944e-05,
      "loss": 15.762,
      "step": 10917
    },
    {
      "epoch": 10.93,
      "grad_norm": 9525.78125,
      "learning_rate": 4.675954592363261e-05,
      "loss": 25.5933,
      "step": 10918
    },
    {
      "epoch": 10.93,
      "grad_norm": 2770.767578125,
      "learning_rate": 4.6754385964912286e-05,
      "loss": 13.5021,
      "step": 10919
    },
    {
      "epoch": 10.93,
      "grad_norm": 9224.751953125,
      "learning_rate": 4.6749226006191954e-05,
      "loss": 19.1512,
      "step": 10920
    },
    {
      "epoch": 10.93,
      "grad_norm": 2182.03466796875,
      "learning_rate": 4.674406604747162e-05,
      "loss": 17.0941,
      "step": 10921
    },
    {
      "epoch": 10.93,
      "grad_norm": 10901.75,
      "learning_rate": 4.6738906088751296e-05,
      "loss": 20.9449,
      "step": 10922
    },
    {
      "epoch": 10.93,
      "grad_norm": 722.8499755859375,
      "learning_rate": 4.6733746130030964e-05,
      "loss": 20.4952,
      "step": 10923
    },
    {
      "epoch": 10.93,
      "grad_norm": 6858.43115234375,
      "learning_rate": 4.672858617131063e-05,
      "loss": 20.1371,
      "step": 10924
    },
    {
      "epoch": 10.94,
      "grad_norm": 4173.83837890625,
      "learning_rate": 4.67234262125903e-05,
      "loss": 20.9222,
      "step": 10925
    },
    {
      "epoch": 10.94,
      "grad_norm": 5084.56884765625,
      "learning_rate": 4.671826625386997e-05,
      "loss": 17.3075,
      "step": 10926
    },
    {
      "epoch": 10.94,
      "grad_norm": 1594.899658203125,
      "learning_rate": 4.671310629514964e-05,
      "loss": 16.7552,
      "step": 10927
    },
    {
      "epoch": 10.94,
      "grad_norm": 5204.994140625,
      "learning_rate": 4.670794633642931e-05,
      "loss": 17.9075,
      "step": 10928
    },
    {
      "epoch": 10.94,
      "grad_norm": 12470.7890625,
      "learning_rate": 4.670278637770898e-05,
      "loss": 15.0628,
      "step": 10929
    },
    {
      "epoch": 10.94,
      "grad_norm": 4944.01123046875,
      "learning_rate": 4.669762641898865e-05,
      "loss": 19.3199,
      "step": 10930
    },
    {
      "epoch": 10.94,
      "grad_norm": 13936.4404296875,
      "learning_rate": 4.669246646026832e-05,
      "loss": 16.7465,
      "step": 10931
    },
    {
      "epoch": 10.94,
      "grad_norm": 22934.759765625,
      "learning_rate": 4.668730650154799e-05,
      "loss": 16.0333,
      "step": 10932
    },
    {
      "epoch": 10.94,
      "grad_norm": 19487.029296875,
      "learning_rate": 4.668214654282766e-05,
      "loss": 19.0824,
      "step": 10933
    },
    {
      "epoch": 10.94,
      "grad_norm": 5611.1796875,
      "learning_rate": 4.667698658410733e-05,
      "loss": 17.793,
      "step": 10934
    },
    {
      "epoch": 10.95,
      "grad_norm": 3177.294189453125,
      "learning_rate": 4.6671826625387e-05,
      "loss": 24.8451,
      "step": 10935
    },
    {
      "epoch": 10.95,
      "grad_norm": 26821.240234375,
      "learning_rate": 4.666666666666667e-05,
      "loss": 16.4056,
      "step": 10936
    },
    {
      "epoch": 10.95,
      "grad_norm": 11497.1943359375,
      "learning_rate": 4.666150670794634e-05,
      "loss": 19.8342,
      "step": 10937
    },
    {
      "epoch": 10.95,
      "grad_norm": 5862.80322265625,
      "learning_rate": 4.665634674922601e-05,
      "loss": 21.2217,
      "step": 10938
    },
    {
      "epoch": 10.95,
      "grad_norm": 26542.423828125,
      "learning_rate": 4.665118679050568e-05,
      "loss": 20.6417,
      "step": 10939
    },
    {
      "epoch": 10.95,
      "grad_norm": 13366.8388671875,
      "learning_rate": 4.664602683178535e-05,
      "loss": 25.0404,
      "step": 10940
    },
    {
      "epoch": 10.95,
      "grad_norm": 3635.273193359375,
      "learning_rate": 4.664086687306502e-05,
      "loss": 16.9104,
      "step": 10941
    },
    {
      "epoch": 10.95,
      "grad_norm": 21476.98046875,
      "learning_rate": 4.663570691434469e-05,
      "loss": 22.7339,
      "step": 10942
    },
    {
      "epoch": 10.95,
      "grad_norm": 98562.296875,
      "learning_rate": 4.663054695562435e-05,
      "loss": 16.6849,
      "step": 10943
    },
    {
      "epoch": 10.95,
      "grad_norm": 38417.99609375,
      "learning_rate": 4.662538699690403e-05,
      "loss": 20.8975,
      "step": 10944
    },
    {
      "epoch": 10.96,
      "grad_norm": 3244.259033203125,
      "learning_rate": 4.6620227038183695e-05,
      "loss": 24.3768,
      "step": 10945
    },
    {
      "epoch": 10.96,
      "grad_norm": 10060.154296875,
      "learning_rate": 4.661506707946336e-05,
      "loss": 14.6244,
      "step": 10946
    },
    {
      "epoch": 10.96,
      "grad_norm": 1460.7618408203125,
      "learning_rate": 4.660990712074304e-05,
      "loss": 18.64,
      "step": 10947
    },
    {
      "epoch": 10.96,
      "grad_norm": 9986.8671875,
      "learning_rate": 4.6604747162022705e-05,
      "loss": 22.5704,
      "step": 10948
    },
    {
      "epoch": 10.96,
      "grad_norm": 18226.310546875,
      "learning_rate": 4.659958720330237e-05,
      "loss": 16.5632,
      "step": 10949
    },
    {
      "epoch": 10.96,
      "grad_norm": 12597.8271484375,
      "learning_rate": 4.659442724458205e-05,
      "loss": 17.9625,
      "step": 10950
    },
    {
      "epoch": 10.96,
      "grad_norm": 3207.782470703125,
      "learning_rate": 4.6589267285861715e-05,
      "loss": 16.1156,
      "step": 10951
    },
    {
      "epoch": 10.96,
      "grad_norm": 5616.95654296875,
      "learning_rate": 4.658410732714138e-05,
      "loss": 20.4336,
      "step": 10952
    },
    {
      "epoch": 10.96,
      "grad_norm": 8690.3486328125,
      "learning_rate": 4.657894736842106e-05,
      "loss": 14.0065,
      "step": 10953
    },
    {
      "epoch": 10.96,
      "grad_norm": 12484.6689453125,
      "learning_rate": 4.6573787409700725e-05,
      "loss": 14.5538,
      "step": 10954
    },
    {
      "epoch": 10.97,
      "grad_norm": 5147.744140625,
      "learning_rate": 4.656862745098039e-05,
      "loss": 15.9016,
      "step": 10955
    },
    {
      "epoch": 10.97,
      "grad_norm": 9289.9794921875,
      "learning_rate": 4.656346749226007e-05,
      "loss": 18.8902,
      "step": 10956
    },
    {
      "epoch": 10.97,
      "grad_norm": 15982.658203125,
      "learning_rate": 4.6558307533539735e-05,
      "loss": 20.264,
      "step": 10957
    },
    {
      "epoch": 10.97,
      "grad_norm": 8203.3486328125,
      "learning_rate": 4.65531475748194e-05,
      "loss": 25.8482,
      "step": 10958
    },
    {
      "epoch": 10.97,
      "grad_norm": 19326.24609375,
      "learning_rate": 4.654798761609908e-05,
      "loss": 24.9236,
      "step": 10959
    },
    {
      "epoch": 10.97,
      "grad_norm": 2358.0576171875,
      "learning_rate": 4.6542827657378745e-05,
      "loss": 18.3373,
      "step": 10960
    },
    {
      "epoch": 10.97,
      "grad_norm": 8263.06640625,
      "learning_rate": 4.653766769865841e-05,
      "loss": 19.8769,
      "step": 10961
    },
    {
      "epoch": 10.97,
      "grad_norm": 3812.876708984375,
      "learning_rate": 4.653250773993808e-05,
      "loss": 21.5832,
      "step": 10962
    },
    {
      "epoch": 10.97,
      "grad_norm": 6504.50634765625,
      "learning_rate": 4.652734778121775e-05,
      "loss": 18.223,
      "step": 10963
    },
    {
      "epoch": 10.97,
      "grad_norm": 2340.54150390625,
      "learning_rate": 4.652218782249742e-05,
      "loss": 15.6338,
      "step": 10964
    },
    {
      "epoch": 10.98,
      "grad_norm": 15270.2177734375,
      "learning_rate": 4.651702786377709e-05,
      "loss": 18.2201,
      "step": 10965
    },
    {
      "epoch": 10.98,
      "grad_norm": 25188.62109375,
      "learning_rate": 4.651186790505676e-05,
      "loss": 13.8806,
      "step": 10966
    },
    {
      "epoch": 10.98,
      "grad_norm": 6398.24951171875,
      "learning_rate": 4.650670794633643e-05,
      "loss": 17.094,
      "step": 10967
    },
    {
      "epoch": 10.98,
      "grad_norm": 25674.32421875,
      "learning_rate": 4.65015479876161e-05,
      "loss": 23.1145,
      "step": 10968
    },
    {
      "epoch": 10.98,
      "grad_norm": 66431.1640625,
      "learning_rate": 4.649638802889577e-05,
      "loss": 14.6344,
      "step": 10969
    },
    {
      "epoch": 10.98,
      "grad_norm": 1220.17529296875,
      "learning_rate": 4.649122807017544e-05,
      "loss": 16.4174,
      "step": 10970
    },
    {
      "epoch": 10.98,
      "grad_norm": 10753.30078125,
      "learning_rate": 4.648606811145511e-05,
      "loss": 22.6877,
      "step": 10971
    },
    {
      "epoch": 10.98,
      "grad_norm": 6758.80224609375,
      "learning_rate": 4.648090815273478e-05,
      "loss": 22.7718,
      "step": 10972
    },
    {
      "epoch": 10.98,
      "grad_norm": 10477.9404296875,
      "learning_rate": 4.647574819401445e-05,
      "loss": 23.3918,
      "step": 10973
    },
    {
      "epoch": 10.98,
      "grad_norm": 5241.3671875,
      "learning_rate": 4.647058823529412e-05,
      "loss": 15.4178,
      "step": 10974
    },
    {
      "epoch": 10.99,
      "grad_norm": 65927.09375,
      "learning_rate": 4.646542827657379e-05,
      "loss": 20.762,
      "step": 10975
    },
    {
      "epoch": 10.99,
      "grad_norm": 3270.538330078125,
      "learning_rate": 4.646026831785346e-05,
      "loss": 19.7628,
      "step": 10976
    },
    {
      "epoch": 10.99,
      "grad_norm": 25489.73046875,
      "learning_rate": 4.645510835913313e-05,
      "loss": 22.598,
      "step": 10977
    },
    {
      "epoch": 10.99,
      "grad_norm": 9483.4150390625,
      "learning_rate": 4.6449948400412805e-05,
      "loss": 16.3107,
      "step": 10978
    },
    {
      "epoch": 10.99,
      "grad_norm": 3680.870361328125,
      "learning_rate": 4.6444788441692466e-05,
      "loss": 19.5791,
      "step": 10979
    },
    {
      "epoch": 10.99,
      "grad_norm": 56473.05078125,
      "learning_rate": 4.6439628482972134e-05,
      "loss": 19.6132,
      "step": 10980
    },
    {
      "epoch": 10.99,
      "grad_norm": 2095.17431640625,
      "learning_rate": 4.643446852425181e-05,
      "loss": 18.6991,
      "step": 10981
    },
    {
      "epoch": 10.99,
      "grad_norm": 1507.7381591796875,
      "learning_rate": 4.6429308565531476e-05,
      "loss": 16.9964,
      "step": 10982
    },
    {
      "epoch": 10.99,
      "grad_norm": 2911.265869140625,
      "learning_rate": 4.6424148606811144e-05,
      "loss": 17.3418,
      "step": 10983
    },
    {
      "epoch": 10.99,
      "grad_norm": 9075.109375,
      "learning_rate": 4.641898864809082e-05,
      "loss": 24.8454,
      "step": 10984
    },
    {
      "epoch": 11.0,
      "grad_norm": 3513.04150390625,
      "learning_rate": 4.6413828689370486e-05,
      "loss": 16.8229,
      "step": 10985
    },
    {
      "epoch": 11.0,
      "grad_norm": 56524.8984375,
      "learning_rate": 4.6408668730650154e-05,
      "loss": 18.8061,
      "step": 10986
    },
    {
      "epoch": 11.0,
      "grad_norm": 1342.0224609375,
      "learning_rate": 4.640350877192983e-05,
      "loss": 14.0588,
      "step": 10987
    },
    {
      "epoch": 11.0,
      "grad_norm": 3187.15966796875,
      "learning_rate": 4.6398348813209496e-05,
      "loss": 26.5748,
      "step": 10988
    },
    {
      "epoch": 11.0,
      "grad_norm": 28189.470703125,
      "learning_rate": 4.6393188854489164e-05,
      "loss": 13.7699,
      "step": 10989
    },
    {
      "epoch": 11.0,
      "grad_norm": 170350.703125,
      "learning_rate": 4.638802889576884e-05,
      "loss": 24.4681,
      "step": 10990
    },
    {
      "epoch": 11.0,
      "grad_norm": 8616.765625,
      "learning_rate": 4.6382868937048506e-05,
      "loss": 22.7793,
      "step": 10991
    },
    {
      "epoch": 11.0,
      "grad_norm": 6942.466796875,
      "learning_rate": 4.637770897832818e-05,
      "loss": 18.4458,
      "step": 10992
    },
    {
      "epoch": 11.0,
      "grad_norm": 12858.1806640625,
      "learning_rate": 4.637254901960785e-05,
      "loss": 19.9637,
      "step": 10993
    },
    {
      "epoch": 11.01,
      "grad_norm": 7679.20068359375,
      "learning_rate": 4.6367389060887516e-05,
      "loss": 13.9565,
      "step": 10994
    },
    {
      "epoch": 11.01,
      "grad_norm": 45836.6796875,
      "learning_rate": 4.636222910216719e-05,
      "loss": 18.0431,
      "step": 10995
    },
    {
      "epoch": 11.01,
      "grad_norm": 11884.759765625,
      "learning_rate": 4.635706914344686e-05,
      "loss": 15.3615,
      "step": 10996
    },
    {
      "epoch": 11.01,
      "grad_norm": 2634.123779296875,
      "learning_rate": 4.635190918472652e-05,
      "loss": 15.2036,
      "step": 10997
    },
    {
      "epoch": 11.01,
      "grad_norm": 7932.51904296875,
      "learning_rate": 4.6346749226006194e-05,
      "loss": 15.2136,
      "step": 10998
    },
    {
      "epoch": 11.01,
      "grad_norm": 5188.990234375,
      "learning_rate": 4.634158926728586e-05,
      "loss": 18.9634,
      "step": 10999
    },
    {
      "epoch": 11.01,
      "grad_norm": 5523.7529296875,
      "learning_rate": 4.633642930856553e-05,
      "loss": 20.003,
      "step": 11000
    },
    {
      "epoch": 11.01,
      "grad_norm": 2075.922607421875,
      "learning_rate": 4.6331269349845204e-05,
      "loss": 16.3623,
      "step": 11001
    },
    {
      "epoch": 11.01,
      "grad_norm": 12258.5576171875,
      "learning_rate": 4.632610939112487e-05,
      "loss": 19.6164,
      "step": 11002
    },
    {
      "epoch": 11.01,
      "grad_norm": 12755.64453125,
      "learning_rate": 4.632094943240454e-05,
      "loss": 17.1653,
      "step": 11003
    },
    {
      "epoch": 11.02,
      "grad_norm": 945209.25,
      "learning_rate": 4.6315789473684214e-05,
      "loss": 26.6145,
      "step": 11004
    },
    {
      "epoch": 11.02,
      "grad_norm": 3932.93359375,
      "learning_rate": 4.631062951496388e-05,
      "loss": 17.4811,
      "step": 11005
    },
    {
      "epoch": 11.02,
      "grad_norm": 34571.94921875,
      "learning_rate": 4.6305469556243556e-05,
      "loss": 20.5161,
      "step": 11006
    },
    {
      "epoch": 11.02,
      "grad_norm": 8129.98828125,
      "learning_rate": 4.6300309597523224e-05,
      "loss": 21.9451,
      "step": 11007
    },
    {
      "epoch": 11.02,
      "grad_norm": 37423.87890625,
      "learning_rate": 4.629514963880289e-05,
      "loss": 15.7104,
      "step": 11008
    },
    {
      "epoch": 11.02,
      "grad_norm": 5572.91455078125,
      "learning_rate": 4.6289989680082566e-05,
      "loss": 14.7119,
      "step": 11009
    },
    {
      "epoch": 11.02,
      "grad_norm": 5559.14453125,
      "learning_rate": 4.6284829721362234e-05,
      "loss": 21.9395,
      "step": 11010
    },
    {
      "epoch": 11.02,
      "grad_norm": 6557.04833984375,
      "learning_rate": 4.62796697626419e-05,
      "loss": 21.8288,
      "step": 11011
    },
    {
      "epoch": 11.02,
      "grad_norm": 17096.953125,
      "learning_rate": 4.6274509803921576e-05,
      "loss": 15.2018,
      "step": 11012
    },
    {
      "epoch": 11.02,
      "grad_norm": 6070.236328125,
      "learning_rate": 4.6269349845201244e-05,
      "loss": 23.196,
      "step": 11013
    },
    {
      "epoch": 11.03,
      "grad_norm": 6339.27392578125,
      "learning_rate": 4.626418988648091e-05,
      "loss": 17.6554,
      "step": 11014
    },
    {
      "epoch": 11.03,
      "grad_norm": 8211.5712890625,
      "learning_rate": 4.625902992776058e-05,
      "loss": 16.9624,
      "step": 11015
    },
    {
      "epoch": 11.03,
      "grad_norm": 3761.25048828125,
      "learning_rate": 4.625386996904025e-05,
      "loss": 25.4927,
      "step": 11016
    },
    {
      "epoch": 11.03,
      "grad_norm": 21155.140625,
      "learning_rate": 4.6248710010319915e-05,
      "loss": 16.9059,
      "step": 11017
    },
    {
      "epoch": 11.03,
      "grad_norm": 1334.3040771484375,
      "learning_rate": 4.624355005159959e-05,
      "loss": 17.6777,
      "step": 11018
    },
    {
      "epoch": 11.03,
      "grad_norm": 12983.6591796875,
      "learning_rate": 4.623839009287926e-05,
      "loss": 18.6775,
      "step": 11019
    },
    {
      "epoch": 11.03,
      "grad_norm": 9958.13671875,
      "learning_rate": 4.623323013415893e-05,
      "loss": 18.8801,
      "step": 11020
    },
    {
      "epoch": 11.03,
      "grad_norm": 14420.314453125,
      "learning_rate": 4.62280701754386e-05,
      "loss": 18.7283,
      "step": 11021
    },
    {
      "epoch": 11.03,
      "grad_norm": 4355.72265625,
      "learning_rate": 4.622291021671827e-05,
      "loss": 15.1671,
      "step": 11022
    },
    {
      "epoch": 11.03,
      "grad_norm": 12889.8623046875,
      "learning_rate": 4.621775025799794e-05,
      "loss": 22.3104,
      "step": 11023
    },
    {
      "epoch": 11.04,
      "grad_norm": 3676.227294921875,
      "learning_rate": 4.621259029927761e-05,
      "loss": 18.032,
      "step": 11024
    },
    {
      "epoch": 11.04,
      "grad_norm": 24454.98828125,
      "learning_rate": 4.620743034055728e-05,
      "loss": 21.4029,
      "step": 11025
    },
    {
      "epoch": 11.04,
      "grad_norm": 3300.22265625,
      "learning_rate": 4.620227038183695e-05,
      "loss": 19.0865,
      "step": 11026
    },
    {
      "epoch": 11.04,
      "grad_norm": 21307.78515625,
      "learning_rate": 4.619711042311662e-05,
      "loss": 20.1929,
      "step": 11027
    },
    {
      "epoch": 11.04,
      "grad_norm": 10033.6337890625,
      "learning_rate": 4.619195046439629e-05,
      "loss": 17.2972,
      "step": 11028
    },
    {
      "epoch": 11.04,
      "grad_norm": 5233.5361328125,
      "learning_rate": 4.618679050567596e-05,
      "loss": 19.0697,
      "step": 11029
    },
    {
      "epoch": 11.04,
      "grad_norm": 7338.5126953125,
      "learning_rate": 4.618163054695563e-05,
      "loss": 16.7754,
      "step": 11030
    },
    {
      "epoch": 11.04,
      "grad_norm": 5545.54345703125,
      "learning_rate": 4.61764705882353e-05,
      "loss": 12.408,
      "step": 11031
    },
    {
      "epoch": 11.04,
      "grad_norm": 3275.00927734375,
      "learning_rate": 4.6171310629514965e-05,
      "loss": 18.6949,
      "step": 11032
    },
    {
      "epoch": 11.04,
      "grad_norm": 6019.89404296875,
      "learning_rate": 4.616615067079463e-05,
      "loss": 21.2768,
      "step": 11033
    },
    {
      "epoch": 11.05,
      "grad_norm": 21693.646484375,
      "learning_rate": 4.616099071207431e-05,
      "loss": 23.5392,
      "step": 11034
    },
    {
      "epoch": 11.05,
      "grad_norm": 5991.3525390625,
      "learning_rate": 4.6155830753353975e-05,
      "loss": 20.9353,
      "step": 11035
    },
    {
      "epoch": 11.05,
      "grad_norm": 18708.2109375,
      "learning_rate": 4.615067079463364e-05,
      "loss": 18.8359,
      "step": 11036
    },
    {
      "epoch": 11.05,
      "grad_norm": 8247.9345703125,
      "learning_rate": 4.614551083591332e-05,
      "loss": 16.8875,
      "step": 11037
    },
    {
      "epoch": 11.05,
      "grad_norm": 6916.8466796875,
      "learning_rate": 4.6140350877192985e-05,
      "loss": 14.4227,
      "step": 11038
    },
    {
      "epoch": 11.05,
      "grad_norm": 7115.34619140625,
      "learning_rate": 4.613519091847265e-05,
      "loss": 23.3131,
      "step": 11039
    },
    {
      "epoch": 11.05,
      "grad_norm": 3816.689208984375,
      "learning_rate": 4.613003095975233e-05,
      "loss": 18.7709,
      "step": 11040
    },
    {
      "epoch": 11.05,
      "grad_norm": 8758.521484375,
      "learning_rate": 4.6124871001031995e-05,
      "loss": 19.2354,
      "step": 11041
    },
    {
      "epoch": 11.05,
      "grad_norm": 2912.312255859375,
      "learning_rate": 4.611971104231166e-05,
      "loss": 20.6931,
      "step": 11042
    },
    {
      "epoch": 11.05,
      "grad_norm": 1443.2874755859375,
      "learning_rate": 4.611455108359134e-05,
      "loss": 18.6664,
      "step": 11043
    },
    {
      "epoch": 11.06,
      "grad_norm": 2315.50244140625,
      "learning_rate": 4.6109391124871005e-05,
      "loss": 15.1018,
      "step": 11044
    },
    {
      "epoch": 11.06,
      "grad_norm": 34602.95703125,
      "learning_rate": 4.610423116615067e-05,
      "loss": 19.4969,
      "step": 11045
    },
    {
      "epoch": 11.06,
      "grad_norm": 29948.32421875,
      "learning_rate": 4.609907120743035e-05,
      "loss": 19.4078,
      "step": 11046
    },
    {
      "epoch": 11.06,
      "grad_norm": 6219.3251953125,
      "learning_rate": 4.6093911248710015e-05,
      "loss": 16.7408,
      "step": 11047
    },
    {
      "epoch": 11.06,
      "grad_norm": 9378.9052734375,
      "learning_rate": 4.608875128998968e-05,
      "loss": 24.8532,
      "step": 11048
    },
    {
      "epoch": 11.06,
      "grad_norm": 31576.806640625,
      "learning_rate": 4.608359133126936e-05,
      "loss": 20.9691,
      "step": 11049
    },
    {
      "epoch": 11.06,
      "grad_norm": 47010.7265625,
      "learning_rate": 4.607843137254902e-05,
      "loss": 24.2081,
      "step": 11050
    },
    {
      "epoch": 11.06,
      "grad_norm": 161074.125,
      "learning_rate": 4.607327141382869e-05,
      "loss": 20.6481,
      "step": 11051
    },
    {
      "epoch": 11.06,
      "grad_norm": 15148.7744140625,
      "learning_rate": 4.606811145510836e-05,
      "loss": 24.6307,
      "step": 11052
    },
    {
      "epoch": 11.06,
      "grad_norm": 14710.96484375,
      "learning_rate": 4.606295149638803e-05,
      "loss": 23.5505,
      "step": 11053
    },
    {
      "epoch": 11.07,
      "grad_norm": 8303.3837890625,
      "learning_rate": 4.60577915376677e-05,
      "loss": 16.228,
      "step": 11054
    },
    {
      "epoch": 11.07,
      "grad_norm": 3380.177001953125,
      "learning_rate": 4.605263157894737e-05,
      "loss": 24.0777,
      "step": 11055
    },
    {
      "epoch": 11.07,
      "grad_norm": 12690.994140625,
      "learning_rate": 4.604747162022704e-05,
      "loss": 19.7434,
      "step": 11056
    },
    {
      "epoch": 11.07,
      "grad_norm": 12716.0966796875,
      "learning_rate": 4.604231166150671e-05,
      "loss": 21.5833,
      "step": 11057
    },
    {
      "epoch": 11.07,
      "grad_norm": 45281.5,
      "learning_rate": 4.603715170278638e-05,
      "loss": 19.8772,
      "step": 11058
    },
    {
      "epoch": 11.07,
      "grad_norm": 2869.6064453125,
      "learning_rate": 4.603199174406605e-05,
      "loss": 16.9893,
      "step": 11059
    },
    {
      "epoch": 11.07,
      "grad_norm": 9355.435546875,
      "learning_rate": 4.602683178534572e-05,
      "loss": 16.8666,
      "step": 11060
    },
    {
      "epoch": 11.07,
      "grad_norm": 37225.83984375,
      "learning_rate": 4.602167182662539e-05,
      "loss": 19.6771,
      "step": 11061
    },
    {
      "epoch": 11.07,
      "grad_norm": 4463.9306640625,
      "learning_rate": 4.601651186790506e-05,
      "loss": 20.8043,
      "step": 11062
    },
    {
      "epoch": 11.07,
      "grad_norm": 5537.21533203125,
      "learning_rate": 4.601135190918473e-05,
      "loss": 14.4403,
      "step": 11063
    },
    {
      "epoch": 11.08,
      "grad_norm": 28759.96484375,
      "learning_rate": 4.60061919504644e-05,
      "loss": 15.6251,
      "step": 11064
    },
    {
      "epoch": 11.08,
      "grad_norm": 2507.38818359375,
      "learning_rate": 4.600103199174407e-05,
      "loss": 24.7363,
      "step": 11065
    },
    {
      "epoch": 11.08,
      "grad_norm": 21025.390625,
      "learning_rate": 4.599587203302374e-05,
      "loss": 17.772,
      "step": 11066
    },
    {
      "epoch": 11.08,
      "grad_norm": 9036.4638671875,
      "learning_rate": 4.599071207430341e-05,
      "loss": 23.9217,
      "step": 11067
    },
    {
      "epoch": 11.08,
      "grad_norm": 24935.568359375,
      "learning_rate": 4.598555211558308e-05,
      "loss": 22.0811,
      "step": 11068
    },
    {
      "epoch": 11.08,
      "grad_norm": 2845.496337890625,
      "learning_rate": 4.5980392156862746e-05,
      "loss": 18.5888,
      "step": 11069
    },
    {
      "epoch": 11.08,
      "grad_norm": 50607.57421875,
      "learning_rate": 4.597523219814241e-05,
      "loss": 22.4721,
      "step": 11070
    },
    {
      "epoch": 11.08,
      "grad_norm": 9746.3798828125,
      "learning_rate": 4.597007223942209e-05,
      "loss": 23.3483,
      "step": 11071
    },
    {
      "epoch": 11.08,
      "grad_norm": 24852.7265625,
      "learning_rate": 4.5964912280701756e-05,
      "loss": 19.3729,
      "step": 11072
    },
    {
      "epoch": 11.08,
      "grad_norm": 764.8999633789062,
      "learning_rate": 4.595975232198142e-05,
      "loss": 18.3354,
      "step": 11073
    },
    {
      "epoch": 11.09,
      "grad_norm": 11839.0703125,
      "learning_rate": 4.59545923632611e-05,
      "loss": 21.9024,
      "step": 11074
    },
    {
      "epoch": 11.09,
      "grad_norm": 21790.099609375,
      "learning_rate": 4.5949432404540766e-05,
      "loss": 18.5291,
      "step": 11075
    },
    {
      "epoch": 11.09,
      "grad_norm": 5690.8818359375,
      "learning_rate": 4.594427244582043e-05,
      "loss": 22.0207,
      "step": 11076
    },
    {
      "epoch": 11.09,
      "grad_norm": 2896.75341796875,
      "learning_rate": 4.593911248710011e-05,
      "loss": 19.0916,
      "step": 11077
    },
    {
      "epoch": 11.09,
      "grad_norm": 9547.265625,
      "learning_rate": 4.5933952528379776e-05,
      "loss": 20.829,
      "step": 11078
    },
    {
      "epoch": 11.09,
      "grad_norm": 5606.77880859375,
      "learning_rate": 4.592879256965944e-05,
      "loss": 23.1229,
      "step": 11079
    },
    {
      "epoch": 11.09,
      "grad_norm": 6243.740234375,
      "learning_rate": 4.592363261093912e-05,
      "loss": 23.6338,
      "step": 11080
    },
    {
      "epoch": 11.09,
      "grad_norm": 584.3989868164062,
      "learning_rate": 4.5918472652218786e-05,
      "loss": 14.6899,
      "step": 11081
    },
    {
      "epoch": 11.09,
      "grad_norm": 5853.53857421875,
      "learning_rate": 4.591331269349845e-05,
      "loss": 18.7227,
      "step": 11082
    },
    {
      "epoch": 11.09,
      "grad_norm": 31126.345703125,
      "learning_rate": 4.590815273477813e-05,
      "loss": 19.2705,
      "step": 11083
    },
    {
      "epoch": 11.1,
      "grad_norm": 68730.2890625,
      "learning_rate": 4.5902992776057796e-05,
      "loss": 13.9672,
      "step": 11084
    },
    {
      "epoch": 11.1,
      "grad_norm": 4249.8271484375,
      "learning_rate": 4.589783281733746e-05,
      "loss": 13.3309,
      "step": 11085
    },
    {
      "epoch": 11.1,
      "grad_norm": 3677.095947265625,
      "learning_rate": 4.589267285861713e-05,
      "loss": 21.1548,
      "step": 11086
    },
    {
      "epoch": 11.1,
      "grad_norm": 13603.0224609375,
      "learning_rate": 4.58875128998968e-05,
      "loss": 16.556,
      "step": 11087
    },
    {
      "epoch": 11.1,
      "grad_norm": 38782.3125,
      "learning_rate": 4.588235294117647e-05,
      "loss": 22.8683,
      "step": 11088
    },
    {
      "epoch": 11.1,
      "grad_norm": 15440.587890625,
      "learning_rate": 4.587719298245614e-05,
      "loss": 19.2967,
      "step": 11089
    },
    {
      "epoch": 11.1,
      "grad_norm": 1265.98388671875,
      "learning_rate": 4.587203302373581e-05,
      "loss": 19.9961,
      "step": 11090
    },
    {
      "epoch": 11.1,
      "grad_norm": 19425.029296875,
      "learning_rate": 4.586687306501548e-05,
      "loss": 24.8531,
      "step": 11091
    },
    {
      "epoch": 11.1,
      "grad_norm": 14456.109375,
      "learning_rate": 4.586171310629515e-05,
      "loss": 36.6648,
      "step": 11092
    },
    {
      "epoch": 11.1,
      "grad_norm": 3315.957275390625,
      "learning_rate": 4.585655314757482e-05,
      "loss": 25.3476,
      "step": 11093
    },
    {
      "epoch": 11.11,
      "grad_norm": 2392.39599609375,
      "learning_rate": 4.585139318885449e-05,
      "loss": 14.984,
      "step": 11094
    },
    {
      "epoch": 11.11,
      "grad_norm": 3309.68017578125,
      "learning_rate": 4.584623323013416e-05,
      "loss": 21.3065,
      "step": 11095
    },
    {
      "epoch": 11.11,
      "grad_norm": 7892.7353515625,
      "learning_rate": 4.584107327141383e-05,
      "loss": 15.8377,
      "step": 11096
    },
    {
      "epoch": 11.11,
      "grad_norm": 23085.232421875,
      "learning_rate": 4.58359133126935e-05,
      "loss": 31.4196,
      "step": 11097
    },
    {
      "epoch": 11.11,
      "grad_norm": 11441.009765625,
      "learning_rate": 4.583075335397317e-05,
      "loss": 16.7926,
      "step": 11098
    },
    {
      "epoch": 11.11,
      "grad_norm": 1565.193603515625,
      "learning_rate": 4.582559339525284e-05,
      "loss": 17.5278,
      "step": 11099
    },
    {
      "epoch": 11.11,
      "grad_norm": 8408.2353515625,
      "learning_rate": 4.582043343653251e-05,
      "loss": 24.664,
      "step": 11100
    },
    {
      "epoch": 11.11,
      "grad_norm": 7399.0576171875,
      "learning_rate": 4.581527347781218e-05,
      "loss": 12.5045,
      "step": 11101
    },
    {
      "epoch": 11.11,
      "grad_norm": 3306.105712890625,
      "learning_rate": 4.581011351909185e-05,
      "loss": 21.9279,
      "step": 11102
    },
    {
      "epoch": 11.11,
      "grad_norm": 19091.19921875,
      "learning_rate": 4.580495356037152e-05,
      "loss": 17.6156,
      "step": 11103
    },
    {
      "epoch": 11.12,
      "grad_norm": 2283.79736328125,
      "learning_rate": 4.5799793601651184e-05,
      "loss": 16.884,
      "step": 11104
    },
    {
      "epoch": 11.12,
      "grad_norm": 5674.53076171875,
      "learning_rate": 4.579463364293086e-05,
      "loss": 17.636,
      "step": 11105
    },
    {
      "epoch": 11.12,
      "grad_norm": 7086.8994140625,
      "learning_rate": 4.5789473684210527e-05,
      "loss": 21.1632,
      "step": 11106
    },
    {
      "epoch": 11.12,
      "grad_norm": 2497.89990234375,
      "learning_rate": 4.5784313725490194e-05,
      "loss": 17.2035,
      "step": 11107
    },
    {
      "epoch": 11.12,
      "grad_norm": 5440.28173828125,
      "learning_rate": 4.577915376676987e-05,
      "loss": 20.6869,
      "step": 11108
    },
    {
      "epoch": 11.12,
      "grad_norm": 4679.7412109375,
      "learning_rate": 4.5773993808049537e-05,
      "loss": 16.0326,
      "step": 11109
    },
    {
      "epoch": 11.12,
      "grad_norm": 2343.68701171875,
      "learning_rate": 4.5768833849329204e-05,
      "loss": 23.3935,
      "step": 11110
    },
    {
      "epoch": 11.12,
      "grad_norm": 23321.669921875,
      "learning_rate": 4.576367389060888e-05,
      "loss": 20.4944,
      "step": 11111
    },
    {
      "epoch": 11.12,
      "grad_norm": 3404.34521484375,
      "learning_rate": 4.5758513931888547e-05,
      "loss": 36.0453,
      "step": 11112
    },
    {
      "epoch": 11.12,
      "grad_norm": 8751.1337890625,
      "learning_rate": 4.5753353973168214e-05,
      "loss": 21.2064,
      "step": 11113
    },
    {
      "epoch": 11.13,
      "grad_norm": 11629.009765625,
      "learning_rate": 4.574819401444789e-05,
      "loss": 20.9519,
      "step": 11114
    },
    {
      "epoch": 11.13,
      "grad_norm": 2728.9716796875,
      "learning_rate": 4.5743034055727557e-05,
      "loss": 21.1139,
      "step": 11115
    },
    {
      "epoch": 11.13,
      "grad_norm": 5120.83642578125,
      "learning_rate": 4.5737874097007224e-05,
      "loss": 18.7827,
      "step": 11116
    },
    {
      "epoch": 11.13,
      "grad_norm": 22118.34765625,
      "learning_rate": 4.57327141382869e-05,
      "loss": 22.1512,
      "step": 11117
    },
    {
      "epoch": 11.13,
      "grad_norm": 43616.08984375,
      "learning_rate": 4.5727554179566567e-05,
      "loss": 17.8931,
      "step": 11118
    },
    {
      "epoch": 11.13,
      "grad_norm": 7261.71533203125,
      "learning_rate": 4.5722394220846234e-05,
      "loss": 16.635,
      "step": 11119
    },
    {
      "epoch": 11.13,
      "grad_norm": 56926.0625,
      "learning_rate": 4.571723426212591e-05,
      "loss": 21.4896,
      "step": 11120
    },
    {
      "epoch": 11.13,
      "grad_norm": 4653.83447265625,
      "learning_rate": 4.5712074303405577e-05,
      "loss": 21.8181,
      "step": 11121
    },
    {
      "epoch": 11.13,
      "grad_norm": 4789.82080078125,
      "learning_rate": 4.5706914344685244e-05,
      "loss": 21.3684,
      "step": 11122
    },
    {
      "epoch": 11.13,
      "grad_norm": 11438.6943359375,
      "learning_rate": 4.570175438596491e-05,
      "loss": 21.971,
      "step": 11123
    },
    {
      "epoch": 11.14,
      "grad_norm": 14729.595703125,
      "learning_rate": 4.569659442724458e-05,
      "loss": 18.8366,
      "step": 11124
    },
    {
      "epoch": 11.14,
      "grad_norm": 200367.359375,
      "learning_rate": 4.5691434468524254e-05,
      "loss": 19.6221,
      "step": 11125
    },
    {
      "epoch": 11.14,
      "grad_norm": 1178.7635498046875,
      "learning_rate": 4.568627450980392e-05,
      "loss": 19.7818,
      "step": 11126
    },
    {
      "epoch": 11.14,
      "grad_norm": 11708.49609375,
      "learning_rate": 4.568111455108359e-05,
      "loss": 23.583,
      "step": 11127
    },
    {
      "epoch": 11.14,
      "grad_norm": 3047.186767578125,
      "learning_rate": 4.5675954592363264e-05,
      "loss": 14.9322,
      "step": 11128
    },
    {
      "epoch": 11.14,
      "grad_norm": 10927.8056640625,
      "learning_rate": 4.567079463364293e-05,
      "loss": 21.8634,
      "step": 11129
    },
    {
      "epoch": 11.14,
      "grad_norm": 18785.0,
      "learning_rate": 4.56656346749226e-05,
      "loss": 20.591,
      "step": 11130
    },
    {
      "epoch": 11.14,
      "grad_norm": 9669.4306640625,
      "learning_rate": 4.5660474716202274e-05,
      "loss": 17.9838,
      "step": 11131
    },
    {
      "epoch": 11.14,
      "grad_norm": 7120.06640625,
      "learning_rate": 4.565531475748194e-05,
      "loss": 20.1434,
      "step": 11132
    },
    {
      "epoch": 11.14,
      "grad_norm": 5597.78759765625,
      "learning_rate": 4.565015479876161e-05,
      "loss": 12.4792,
      "step": 11133
    },
    {
      "epoch": 11.15,
      "grad_norm": 3090.5009765625,
      "learning_rate": 4.5644994840041284e-05,
      "loss": 14.8695,
      "step": 11134
    },
    {
      "epoch": 11.15,
      "grad_norm": 5855.31201171875,
      "learning_rate": 4.563983488132095e-05,
      "loss": 13.644,
      "step": 11135
    },
    {
      "epoch": 11.15,
      "grad_norm": 9625.833984375,
      "learning_rate": 4.563467492260062e-05,
      "loss": 19.8836,
      "step": 11136
    },
    {
      "epoch": 11.15,
      "grad_norm": 9483.00390625,
      "learning_rate": 4.5629514963880294e-05,
      "loss": 21.9077,
      "step": 11137
    },
    {
      "epoch": 11.15,
      "grad_norm": 24678.943359375,
      "learning_rate": 4.562435500515996e-05,
      "loss": 21.6459,
      "step": 11138
    },
    {
      "epoch": 11.15,
      "grad_norm": 7847.21044921875,
      "learning_rate": 4.561919504643964e-05,
      "loss": 18.2443,
      "step": 11139
    },
    {
      "epoch": 11.15,
      "grad_norm": 3499.87939453125,
      "learning_rate": 4.56140350877193e-05,
      "loss": 13.0287,
      "step": 11140
    },
    {
      "epoch": 11.15,
      "grad_norm": 7834.92236328125,
      "learning_rate": 4.5608875128998965e-05,
      "loss": 30.195,
      "step": 11141
    },
    {
      "epoch": 11.15,
      "grad_norm": 11768.5927734375,
      "learning_rate": 4.560371517027864e-05,
      "loss": 15.7403,
      "step": 11142
    },
    {
      "epoch": 11.15,
      "grad_norm": 34171.1328125,
      "learning_rate": 4.559855521155831e-05,
      "loss": 18.193,
      "step": 11143
    },
    {
      "epoch": 11.16,
      "grad_norm": 14538.693359375,
      "learning_rate": 4.5593395252837975e-05,
      "loss": 16.7272,
      "step": 11144
    },
    {
      "epoch": 11.16,
      "grad_norm": 6055.43310546875,
      "learning_rate": 4.558823529411765e-05,
      "loss": 17.684,
      "step": 11145
    },
    {
      "epoch": 11.16,
      "grad_norm": 7462.61181640625,
      "learning_rate": 4.558307533539732e-05,
      "loss": 25.2631,
      "step": 11146
    },
    {
      "epoch": 11.16,
      "grad_norm": 13667.2587890625,
      "learning_rate": 4.5577915376676985e-05,
      "loss": 30.7486,
      "step": 11147
    },
    {
      "epoch": 11.16,
      "grad_norm": 31083.2421875,
      "learning_rate": 4.557275541795666e-05,
      "loss": 23.4954,
      "step": 11148
    },
    {
      "epoch": 11.16,
      "grad_norm": 2066.553955078125,
      "learning_rate": 4.556759545923633e-05,
      "loss": 15.3547,
      "step": 11149
    },
    {
      "epoch": 11.16,
      "grad_norm": 78253.0859375,
      "learning_rate": 4.5562435500515995e-05,
      "loss": 21.592,
      "step": 11150
    },
    {
      "epoch": 11.16,
      "grad_norm": 53916.51171875,
      "learning_rate": 4.555727554179567e-05,
      "loss": 18.7688,
      "step": 11151
    },
    {
      "epoch": 11.16,
      "grad_norm": 13236.5205078125,
      "learning_rate": 4.555211558307534e-05,
      "loss": 16.7984,
      "step": 11152
    },
    {
      "epoch": 11.16,
      "grad_norm": 1568.0762939453125,
      "learning_rate": 4.554695562435501e-05,
      "loss": 24.5281,
      "step": 11153
    },
    {
      "epoch": 11.17,
      "grad_norm": 21008.51953125,
      "learning_rate": 4.554179566563468e-05,
      "loss": 15.318,
      "step": 11154
    },
    {
      "epoch": 11.17,
      "grad_norm": 2038.6883544921875,
      "learning_rate": 4.553663570691435e-05,
      "loss": 16.7371,
      "step": 11155
    },
    {
      "epoch": 11.17,
      "grad_norm": 127267.75,
      "learning_rate": 4.553147574819402e-05,
      "loss": 16.2645,
      "step": 11156
    },
    {
      "epoch": 11.17,
      "grad_norm": 4015.809326171875,
      "learning_rate": 4.552631578947369e-05,
      "loss": 27.9099,
      "step": 11157
    },
    {
      "epoch": 11.17,
      "grad_norm": 8526.0576171875,
      "learning_rate": 4.552115583075335e-05,
      "loss": 19.7433,
      "step": 11158
    },
    {
      "epoch": 11.17,
      "grad_norm": 2201.827392578125,
      "learning_rate": 4.5515995872033025e-05,
      "loss": 19.1163,
      "step": 11159
    },
    {
      "epoch": 11.17,
      "grad_norm": 7064.52001953125,
      "learning_rate": 4.551083591331269e-05,
      "loss": 22.0763,
      "step": 11160
    },
    {
      "epoch": 11.17,
      "grad_norm": 35197.1953125,
      "learning_rate": 4.550567595459236e-05,
      "loss": 18.3748,
      "step": 11161
    },
    {
      "epoch": 11.17,
      "grad_norm": 36217.5703125,
      "learning_rate": 4.5500515995872035e-05,
      "loss": 15.317,
      "step": 11162
    },
    {
      "epoch": 11.17,
      "grad_norm": 5009.20068359375,
      "learning_rate": 4.54953560371517e-05,
      "loss": 18.1382,
      "step": 11163
    },
    {
      "epoch": 11.18,
      "grad_norm": 3991.663818359375,
      "learning_rate": 4.549019607843137e-05,
      "loss": 20.0453,
      "step": 11164
    },
    {
      "epoch": 11.18,
      "grad_norm": 40909.71875,
      "learning_rate": 4.5485036119711045e-05,
      "loss": 14.5072,
      "step": 11165
    },
    {
      "epoch": 11.18,
      "grad_norm": 3806.25732421875,
      "learning_rate": 4.547987616099071e-05,
      "loss": 17.8529,
      "step": 11166
    },
    {
      "epoch": 11.18,
      "grad_norm": 121264.71875,
      "learning_rate": 4.547471620227039e-05,
      "loss": 14.1184,
      "step": 11167
    },
    {
      "epoch": 11.18,
      "grad_norm": 26360.005859375,
      "learning_rate": 4.5469556243550055e-05,
      "loss": 14.0058,
      "step": 11168
    },
    {
      "epoch": 11.18,
      "grad_norm": 62387.8984375,
      "learning_rate": 4.546439628482972e-05,
      "loss": 16.8226,
      "step": 11169
    },
    {
      "epoch": 11.18,
      "grad_norm": 8345.1787109375,
      "learning_rate": 4.54592363261094e-05,
      "loss": 17.2091,
      "step": 11170
    },
    {
      "epoch": 11.18,
      "grad_norm": 19103.859375,
      "learning_rate": 4.5454076367389065e-05,
      "loss": 18.199,
      "step": 11171
    },
    {
      "epoch": 11.18,
      "grad_norm": 4456.10791015625,
      "learning_rate": 4.544891640866873e-05,
      "loss": 14.8075,
      "step": 11172
    },
    {
      "epoch": 11.18,
      "grad_norm": 6278.599609375,
      "learning_rate": 4.544375644994841e-05,
      "loss": 24.5603,
      "step": 11173
    },
    {
      "epoch": 11.19,
      "grad_norm": 6259.5263671875,
      "learning_rate": 4.5438596491228075e-05,
      "loss": 19.3776,
      "step": 11174
    },
    {
      "epoch": 11.19,
      "grad_norm": 4404.041015625,
      "learning_rate": 4.5433436532507736e-05,
      "loss": 18.9606,
      "step": 11175
    },
    {
      "epoch": 11.19,
      "grad_norm": 2003.505615234375,
      "learning_rate": 4.542827657378741e-05,
      "loss": 18.4764,
      "step": 11176
    },
    {
      "epoch": 11.19,
      "grad_norm": 67769.5234375,
      "learning_rate": 4.542311661506708e-05,
      "loss": 19.9133,
      "step": 11177
    },
    {
      "epoch": 11.19,
      "grad_norm": 153507.234375,
      "learning_rate": 4.5417956656346746e-05,
      "loss": 23.1529,
      "step": 11178
    },
    {
      "epoch": 11.19,
      "grad_norm": 4567.3642578125,
      "learning_rate": 4.541279669762642e-05,
      "loss": 21.7368,
      "step": 11179
    },
    {
      "epoch": 11.19,
      "grad_norm": 13697.1787109375,
      "learning_rate": 4.540763673890609e-05,
      "loss": 18.9132,
      "step": 11180
    },
    {
      "epoch": 11.19,
      "grad_norm": 10425.9248046875,
      "learning_rate": 4.540247678018576e-05,
      "loss": 20.4954,
      "step": 11181
    },
    {
      "epoch": 11.19,
      "grad_norm": 5550.9609375,
      "learning_rate": 4.539731682146543e-05,
      "loss": 16.691,
      "step": 11182
    },
    {
      "epoch": 11.19,
      "grad_norm": 20549.810546875,
      "learning_rate": 4.53921568627451e-05,
      "loss": 17.9542,
      "step": 11183
    },
    {
      "epoch": 11.2,
      "grad_norm": 19354.203125,
      "learning_rate": 4.538699690402477e-05,
      "loss": 31.187,
      "step": 11184
    },
    {
      "epoch": 11.2,
      "grad_norm": 8913.8701171875,
      "learning_rate": 4.538183694530444e-05,
      "loss": 19.3532,
      "step": 11185
    },
    {
      "epoch": 11.2,
      "grad_norm": 4302.84423828125,
      "learning_rate": 4.537667698658411e-05,
      "loss": 14.8698,
      "step": 11186
    },
    {
      "epoch": 11.2,
      "grad_norm": 54073.29296875,
      "learning_rate": 4.537151702786378e-05,
      "loss": 20.5666,
      "step": 11187
    },
    {
      "epoch": 11.2,
      "grad_norm": 13294.8828125,
      "learning_rate": 4.536635706914345e-05,
      "loss": 20.5316,
      "step": 11188
    },
    {
      "epoch": 11.2,
      "grad_norm": 71983.921875,
      "learning_rate": 4.536119711042312e-05,
      "loss": 12.2804,
      "step": 11189
    },
    {
      "epoch": 11.2,
      "grad_norm": 2339.872802734375,
      "learning_rate": 4.535603715170279e-05,
      "loss": 20.8013,
      "step": 11190
    },
    {
      "epoch": 11.2,
      "grad_norm": 2532.13330078125,
      "learning_rate": 4.535087719298246e-05,
      "loss": 26.631,
      "step": 11191
    },
    {
      "epoch": 11.2,
      "grad_norm": 1552.2835693359375,
      "learning_rate": 4.534571723426213e-05,
      "loss": 19.2211,
      "step": 11192
    },
    {
      "epoch": 11.2,
      "grad_norm": 78731.21875,
      "learning_rate": 4.5340557275541796e-05,
      "loss": 19.367,
      "step": 11193
    },
    {
      "epoch": 11.21,
      "grad_norm": 21230.7734375,
      "learning_rate": 4.5335397316821464e-05,
      "loss": 24.6014,
      "step": 11194
    },
    {
      "epoch": 11.21,
      "grad_norm": 34185.85546875,
      "learning_rate": 4.533023735810114e-05,
      "loss": 27.3909,
      "step": 11195
    },
    {
      "epoch": 11.21,
      "grad_norm": 7349.42333984375,
      "learning_rate": 4.5325077399380806e-05,
      "loss": 21.4843,
      "step": 11196
    },
    {
      "epoch": 11.21,
      "grad_norm": 13613.33984375,
      "learning_rate": 4.5319917440660474e-05,
      "loss": 18.2456,
      "step": 11197
    },
    {
      "epoch": 11.21,
      "grad_norm": 4306.787109375,
      "learning_rate": 4.531475748194015e-05,
      "loss": 16.5684,
      "step": 11198
    },
    {
      "epoch": 11.21,
      "grad_norm": 1201.5841064453125,
      "learning_rate": 4.5309597523219816e-05,
      "loss": 21.8956,
      "step": 11199
    },
    {
      "epoch": 11.21,
      "grad_norm": 7159.2421875,
      "learning_rate": 4.5304437564499484e-05,
      "loss": 15.8112,
      "step": 11200
    },
    {
      "epoch": 11.21,
      "grad_norm": 2682.793212890625,
      "learning_rate": 4.529927760577916e-05,
      "loss": 21.0458,
      "step": 11201
    },
    {
      "epoch": 11.21,
      "grad_norm": 6470.01904296875,
      "learning_rate": 4.5294117647058826e-05,
      "loss": 16.4124,
      "step": 11202
    },
    {
      "epoch": 11.21,
      "grad_norm": 1507.34619140625,
      "learning_rate": 4.5288957688338494e-05,
      "loss": 15.9292,
      "step": 11203
    },
    {
      "epoch": 11.22,
      "grad_norm": 7736.98095703125,
      "learning_rate": 4.528379772961817e-05,
      "loss": 31.0891,
      "step": 11204
    },
    {
      "epoch": 11.22,
      "grad_norm": 22061.6015625,
      "learning_rate": 4.5278637770897836e-05,
      "loss": 22.8286,
      "step": 11205
    },
    {
      "epoch": 11.22,
      "grad_norm": 6859.6279296875,
      "learning_rate": 4.5273477812177504e-05,
      "loss": 21.9656,
      "step": 11206
    },
    {
      "epoch": 11.22,
      "grad_norm": 3316.433349609375,
      "learning_rate": 4.526831785345718e-05,
      "loss": 19.9132,
      "step": 11207
    },
    {
      "epoch": 11.22,
      "grad_norm": 5291.30029296875,
      "learning_rate": 4.5263157894736846e-05,
      "loss": 25.3662,
      "step": 11208
    },
    {
      "epoch": 11.22,
      "grad_norm": 6747.20166015625,
      "learning_rate": 4.5257997936016514e-05,
      "loss": 20.0873,
      "step": 11209
    },
    {
      "epoch": 11.22,
      "grad_norm": 28542.00390625,
      "learning_rate": 4.525283797729619e-05,
      "loss": 19.4594,
      "step": 11210
    },
    {
      "epoch": 11.22,
      "grad_norm": 3122.14404296875,
      "learning_rate": 4.524767801857585e-05,
      "loss": 29.6194,
      "step": 11211
    },
    {
      "epoch": 11.22,
      "grad_norm": 17664.6875,
      "learning_rate": 4.5242518059855524e-05,
      "loss": 22.0895,
      "step": 11212
    },
    {
      "epoch": 11.22,
      "grad_norm": 11839.1474609375,
      "learning_rate": 4.523735810113519e-05,
      "loss": 22.7395,
      "step": 11213
    },
    {
      "epoch": 11.23,
      "grad_norm": 14836.5634765625,
      "learning_rate": 4.523219814241486e-05,
      "loss": 22.1477,
      "step": 11214
    },
    {
      "epoch": 11.23,
      "grad_norm": 4255.7578125,
      "learning_rate": 4.5227038183694534e-05,
      "loss": 21.9166,
      "step": 11215
    },
    {
      "epoch": 11.23,
      "grad_norm": 103773.71875,
      "learning_rate": 4.52218782249742e-05,
      "loss": 26.4949,
      "step": 11216
    },
    {
      "epoch": 11.23,
      "grad_norm": 27874.220703125,
      "learning_rate": 4.521671826625387e-05,
      "loss": 21.9024,
      "step": 11217
    },
    {
      "epoch": 11.23,
      "grad_norm": 8159.63671875,
      "learning_rate": 4.5211558307533544e-05,
      "loss": 19.798,
      "step": 11218
    },
    {
      "epoch": 11.23,
      "grad_norm": 5682.7861328125,
      "learning_rate": 4.520639834881321e-05,
      "loss": 21.1957,
      "step": 11219
    },
    {
      "epoch": 11.23,
      "grad_norm": 25018.814453125,
      "learning_rate": 4.520123839009288e-05,
      "loss": 24.2664,
      "step": 11220
    },
    {
      "epoch": 11.23,
      "grad_norm": 10369.9638671875,
      "learning_rate": 4.5196078431372554e-05,
      "loss": 23.2618,
      "step": 11221
    },
    {
      "epoch": 11.23,
      "grad_norm": 20187.55859375,
      "learning_rate": 4.519091847265222e-05,
      "loss": 17.294,
      "step": 11222
    },
    {
      "epoch": 11.23,
      "grad_norm": 8792.7109375,
      "learning_rate": 4.518575851393189e-05,
      "loss": 24.377,
      "step": 11223
    },
    {
      "epoch": 11.24,
      "grad_norm": 3939.490966796875,
      "learning_rate": 4.5180598555211564e-05,
      "loss": 18.7796,
      "step": 11224
    },
    {
      "epoch": 11.24,
      "grad_norm": 13799.6650390625,
      "learning_rate": 4.517543859649123e-05,
      "loss": 21.1457,
      "step": 11225
    },
    {
      "epoch": 11.24,
      "grad_norm": 14065.7529296875,
      "learning_rate": 4.51702786377709e-05,
      "loss": 15.8072,
      "step": 11226
    },
    {
      "epoch": 11.24,
      "grad_norm": 7333.92626953125,
      "learning_rate": 4.5165118679050574e-05,
      "loss": 17.6399,
      "step": 11227
    },
    {
      "epoch": 11.24,
      "grad_norm": 29059.974609375,
      "learning_rate": 4.515995872033024e-05,
      "loss": 21.2271,
      "step": 11228
    },
    {
      "epoch": 11.24,
      "grad_norm": 3709.2490234375,
      "learning_rate": 4.515479876160991e-05,
      "loss": 15.6637,
      "step": 11229
    },
    {
      "epoch": 11.24,
      "grad_norm": 80088.7265625,
      "learning_rate": 4.514963880288958e-05,
      "loss": 17.6235,
      "step": 11230
    },
    {
      "epoch": 11.24,
      "grad_norm": 17571.884765625,
      "learning_rate": 4.5144478844169245e-05,
      "loss": 24.0275,
      "step": 11231
    },
    {
      "epoch": 11.24,
      "grad_norm": 9867.7685546875,
      "learning_rate": 4.513931888544892e-05,
      "loss": 23.4245,
      "step": 11232
    },
    {
      "epoch": 11.24,
      "grad_norm": 38154.02734375,
      "learning_rate": 4.513415892672859e-05,
      "loss": 30.9099,
      "step": 11233
    },
    {
      "epoch": 11.25,
      "grad_norm": 4936.5048828125,
      "learning_rate": 4.5128998968008255e-05,
      "loss": 21.4234,
      "step": 11234
    },
    {
      "epoch": 11.25,
      "grad_norm": 13021.9541015625,
      "learning_rate": 4.512383900928793e-05,
      "loss": 27.8931,
      "step": 11235
    },
    {
      "epoch": 11.25,
      "grad_norm": 5624.30517578125,
      "learning_rate": 4.51186790505676e-05,
      "loss": 17.1446,
      "step": 11236
    },
    {
      "epoch": 11.25,
      "grad_norm": 1479.1463623046875,
      "learning_rate": 4.5113519091847265e-05,
      "loss": 20.6989,
      "step": 11237
    },
    {
      "epoch": 11.25,
      "grad_norm": 4199.99267578125,
      "learning_rate": 4.510835913312694e-05,
      "loss": 17.5316,
      "step": 11238
    },
    {
      "epoch": 11.25,
      "grad_norm": 2128.15185546875,
      "learning_rate": 4.510319917440661e-05,
      "loss": 22.5032,
      "step": 11239
    },
    {
      "epoch": 11.25,
      "grad_norm": 13559.8017578125,
      "learning_rate": 4.5098039215686275e-05,
      "loss": 19.246,
      "step": 11240
    },
    {
      "epoch": 11.25,
      "grad_norm": 2844.7724609375,
      "learning_rate": 4.509287925696595e-05,
      "loss": 21.3547,
      "step": 11241
    },
    {
      "epoch": 11.25,
      "grad_norm": 20614.099609375,
      "learning_rate": 4.508771929824562e-05,
      "loss": 17.625,
      "step": 11242
    },
    {
      "epoch": 11.25,
      "grad_norm": 4488.60205078125,
      "learning_rate": 4.5082559339525285e-05,
      "loss": 20.1763,
      "step": 11243
    },
    {
      "epoch": 11.26,
      "grad_norm": 8396.62109375,
      "learning_rate": 4.507739938080496e-05,
      "loss": 23.1558,
      "step": 11244
    },
    {
      "epoch": 11.26,
      "grad_norm": 7334.859375,
      "learning_rate": 4.507223942208463e-05,
      "loss": 20.3062,
      "step": 11245
    },
    {
      "epoch": 11.26,
      "grad_norm": 7709.98583984375,
      "learning_rate": 4.5067079463364295e-05,
      "loss": 20.4701,
      "step": 11246
    },
    {
      "epoch": 11.26,
      "grad_norm": 13705.7177734375,
      "learning_rate": 4.506191950464396e-05,
      "loss": 18.3843,
      "step": 11247
    },
    {
      "epoch": 11.26,
      "grad_norm": 14376.0302734375,
      "learning_rate": 4.505675954592363e-05,
      "loss": 18.2381,
      "step": 11248
    },
    {
      "epoch": 11.26,
      "grad_norm": 6862.142578125,
      "learning_rate": 4.5051599587203305e-05,
      "loss": 27.6004,
      "step": 11249
    },
    {
      "epoch": 11.26,
      "grad_norm": 68189.4453125,
      "learning_rate": 4.504643962848297e-05,
      "loss": 25.5013,
      "step": 11250
    },
    {
      "epoch": 11.26,
      "grad_norm": 12159.14453125,
      "learning_rate": 4.504127966976264e-05,
      "loss": 19.1443,
      "step": 11251
    },
    {
      "epoch": 11.26,
      "grad_norm": 1460.4124755859375,
      "learning_rate": 4.5036119711042315e-05,
      "loss": 13.9695,
      "step": 11252
    },
    {
      "epoch": 11.26,
      "grad_norm": 2385.2587890625,
      "learning_rate": 4.503095975232198e-05,
      "loss": 18.5819,
      "step": 11253
    },
    {
      "epoch": 11.27,
      "grad_norm": 6935.68701171875,
      "learning_rate": 4.502579979360165e-05,
      "loss": 38.1339,
      "step": 11254
    },
    {
      "epoch": 11.27,
      "grad_norm": 17339.015625,
      "learning_rate": 4.5020639834881325e-05,
      "loss": 19.3623,
      "step": 11255
    },
    {
      "epoch": 11.27,
      "grad_norm": 21028.82421875,
      "learning_rate": 4.501547987616099e-05,
      "loss": 13.2062,
      "step": 11256
    },
    {
      "epoch": 11.27,
      "grad_norm": 7698.31396484375,
      "learning_rate": 4.501031991744066e-05,
      "loss": 20.088,
      "step": 11257
    },
    {
      "epoch": 11.27,
      "grad_norm": 7035.63232421875,
      "learning_rate": 4.5005159958720335e-05,
      "loss": 19.9939,
      "step": 11258
    },
    {
      "epoch": 11.27,
      "grad_norm": 9564.072265625,
      "learning_rate": 4.5e-05,
      "loss": 17.3316,
      "step": 11259
    },
    {
      "epoch": 11.27,
      "grad_norm": 2487.296630859375,
      "learning_rate": 4.499484004127967e-05,
      "loss": 19.9933,
      "step": 11260
    },
    {
      "epoch": 11.27,
      "grad_norm": 18398.212890625,
      "learning_rate": 4.4989680082559345e-05,
      "loss": 22.7358,
      "step": 11261
    },
    {
      "epoch": 11.27,
      "grad_norm": 8372.1337890625,
      "learning_rate": 4.498452012383901e-05,
      "loss": 18.4609,
      "step": 11262
    },
    {
      "epoch": 11.27,
      "grad_norm": 3739.29443359375,
      "learning_rate": 4.497936016511868e-05,
      "loss": 15.2247,
      "step": 11263
    },
    {
      "epoch": 11.28,
      "grad_norm": 1897.0560302734375,
      "learning_rate": 4.4974200206398355e-05,
      "loss": 21.8482,
      "step": 11264
    },
    {
      "epoch": 11.28,
      "grad_norm": 4329.89599609375,
      "learning_rate": 4.4969040247678016e-05,
      "loss": 20.676,
      "step": 11265
    },
    {
      "epoch": 11.28,
      "grad_norm": 14633.2373046875,
      "learning_rate": 4.496388028895769e-05,
      "loss": 16.7513,
      "step": 11266
    },
    {
      "epoch": 11.28,
      "grad_norm": 13602.9892578125,
      "learning_rate": 4.495872033023736e-05,
      "loss": 20.7165,
      "step": 11267
    },
    {
      "epoch": 11.28,
      "grad_norm": 144030.515625,
      "learning_rate": 4.4953560371517026e-05,
      "loss": 24.9442,
      "step": 11268
    },
    {
      "epoch": 11.28,
      "grad_norm": 3022.30078125,
      "learning_rate": 4.49484004127967e-05,
      "loss": 16.358,
      "step": 11269
    },
    {
      "epoch": 11.28,
      "grad_norm": 14962.1513671875,
      "learning_rate": 4.494324045407637e-05,
      "loss": 15.7223,
      "step": 11270
    },
    {
      "epoch": 11.28,
      "grad_norm": 1484.5279541015625,
      "learning_rate": 4.4938080495356036e-05,
      "loss": 23.4732,
      "step": 11271
    },
    {
      "epoch": 11.28,
      "grad_norm": 12602.5810546875,
      "learning_rate": 4.493292053663571e-05,
      "loss": 23.0145,
      "step": 11272
    },
    {
      "epoch": 11.28,
      "grad_norm": 9845.2080078125,
      "learning_rate": 4.492776057791538e-05,
      "loss": 17.8477,
      "step": 11273
    },
    {
      "epoch": 11.29,
      "grad_norm": 5967.3720703125,
      "learning_rate": 4.4922600619195046e-05,
      "loss": 13.296,
      "step": 11274
    },
    {
      "epoch": 11.29,
      "grad_norm": 355418.78125,
      "learning_rate": 4.491744066047472e-05,
      "loss": 27.4123,
      "step": 11275
    },
    {
      "epoch": 11.29,
      "grad_norm": 36748.37109375,
      "learning_rate": 4.491228070175439e-05,
      "loss": 22.0379,
      "step": 11276
    },
    {
      "epoch": 11.29,
      "grad_norm": 6034.60498046875,
      "learning_rate": 4.4907120743034056e-05,
      "loss": 26.0744,
      "step": 11277
    },
    {
      "epoch": 11.29,
      "grad_norm": 11685.826171875,
      "learning_rate": 4.490196078431373e-05,
      "loss": 24.168,
      "step": 11278
    },
    {
      "epoch": 11.29,
      "grad_norm": 17427.9765625,
      "learning_rate": 4.48968008255934e-05,
      "loss": 27.2853,
      "step": 11279
    },
    {
      "epoch": 11.29,
      "grad_norm": 3837.724853515625,
      "learning_rate": 4.4891640866873066e-05,
      "loss": 13.8053,
      "step": 11280
    },
    {
      "epoch": 11.29,
      "grad_norm": 4529.513671875,
      "learning_rate": 4.488648090815274e-05,
      "loss": 20.7877,
      "step": 11281
    },
    {
      "epoch": 11.29,
      "grad_norm": 2482.12646484375,
      "learning_rate": 4.488132094943241e-05,
      "loss": 22.3865,
      "step": 11282
    },
    {
      "epoch": 11.29,
      "grad_norm": 2044.7315673828125,
      "learning_rate": 4.4876160990712076e-05,
      "loss": 20.0917,
      "step": 11283
    },
    {
      "epoch": 11.3,
      "grad_norm": 5656.08935546875,
      "learning_rate": 4.4871001031991744e-05,
      "loss": 21.4627,
      "step": 11284
    },
    {
      "epoch": 11.3,
      "grad_norm": 7885.771484375,
      "learning_rate": 4.486584107327141e-05,
      "loss": 18.8893,
      "step": 11285
    },
    {
      "epoch": 11.3,
      "grad_norm": 11689.4794921875,
      "learning_rate": 4.4860681114551086e-05,
      "loss": 21.1688,
      "step": 11286
    },
    {
      "epoch": 11.3,
      "grad_norm": 25027.306640625,
      "learning_rate": 4.4855521155830754e-05,
      "loss": 23.1953,
      "step": 11287
    },
    {
      "epoch": 11.3,
      "grad_norm": 61788.48046875,
      "learning_rate": 4.485036119711042e-05,
      "loss": 21.9605,
      "step": 11288
    },
    {
      "epoch": 11.3,
      "grad_norm": 30376.89453125,
      "learning_rate": 4.4845201238390096e-05,
      "loss": 25.9638,
      "step": 11289
    },
    {
      "epoch": 11.3,
      "grad_norm": 12344.2685546875,
      "learning_rate": 4.4840041279669764e-05,
      "loss": 20.528,
      "step": 11290
    },
    {
      "epoch": 11.3,
      "grad_norm": 13997.7744140625,
      "learning_rate": 4.483488132094943e-05,
      "loss": 19.5178,
      "step": 11291
    },
    {
      "epoch": 11.3,
      "grad_norm": 20197.2109375,
      "learning_rate": 4.4829721362229106e-05,
      "loss": 24.724,
      "step": 11292
    },
    {
      "epoch": 11.3,
      "grad_norm": 4300.337890625,
      "learning_rate": 4.4824561403508774e-05,
      "loss": 18.7467,
      "step": 11293
    },
    {
      "epoch": 11.31,
      "grad_norm": 10873.357421875,
      "learning_rate": 4.481940144478844e-05,
      "loss": 20.3432,
      "step": 11294
    },
    {
      "epoch": 11.31,
      "grad_norm": 2236.702880859375,
      "learning_rate": 4.4814241486068116e-05,
      "loss": 16.9295,
      "step": 11295
    },
    {
      "epoch": 11.31,
      "grad_norm": 17495.40234375,
      "learning_rate": 4.4809081527347784e-05,
      "loss": 22.0566,
      "step": 11296
    },
    {
      "epoch": 11.31,
      "grad_norm": 2436.58984375,
      "learning_rate": 4.480392156862745e-05,
      "loss": 23.0439,
      "step": 11297
    },
    {
      "epoch": 11.31,
      "grad_norm": 10749.8759765625,
      "learning_rate": 4.4798761609907126e-05,
      "loss": 20.8935,
      "step": 11298
    },
    {
      "epoch": 11.31,
      "grad_norm": 17092.611328125,
      "learning_rate": 4.4793601651186794e-05,
      "loss": 22.4575,
      "step": 11299
    },
    {
      "epoch": 11.31,
      "grad_norm": 5656.85888671875,
      "learning_rate": 4.478844169246647e-05,
      "loss": 22.241,
      "step": 11300
    },
    {
      "epoch": 11.31,
      "grad_norm": 2128.19873046875,
      "learning_rate": 4.478328173374613e-05,
      "loss": 21.0232,
      "step": 11301
    },
    {
      "epoch": 11.31,
      "grad_norm": 1965.7469482421875,
      "learning_rate": 4.47781217750258e-05,
      "loss": 23.0559,
      "step": 11302
    },
    {
      "epoch": 11.31,
      "grad_norm": 11902.2900390625,
      "learning_rate": 4.477296181630547e-05,
      "loss": 32.5904,
      "step": 11303
    },
    {
      "epoch": 11.32,
      "grad_norm": 3648.0537109375,
      "learning_rate": 4.476780185758514e-05,
      "loss": 18.9663,
      "step": 11304
    },
    {
      "epoch": 11.32,
      "grad_norm": 5059.71826171875,
      "learning_rate": 4.476264189886481e-05,
      "loss": 17.9958,
      "step": 11305
    },
    {
      "epoch": 11.32,
      "grad_norm": 10141.44921875,
      "learning_rate": 4.475748194014448e-05,
      "loss": 23.2507,
      "step": 11306
    },
    {
      "epoch": 11.32,
      "grad_norm": 44885.76953125,
      "learning_rate": 4.475232198142415e-05,
      "loss": 22.906,
      "step": 11307
    },
    {
      "epoch": 11.32,
      "grad_norm": 13999.6767578125,
      "learning_rate": 4.474716202270382e-05,
      "loss": 23.3663,
      "step": 11308
    },
    {
      "epoch": 11.32,
      "grad_norm": 13131.5810546875,
      "learning_rate": 4.474200206398349e-05,
      "loss": 23.5749,
      "step": 11309
    },
    {
      "epoch": 11.32,
      "grad_norm": 1875.7076416015625,
      "learning_rate": 4.473684210526316e-05,
      "loss": 15.8287,
      "step": 11310
    },
    {
      "epoch": 11.32,
      "grad_norm": 6175.8330078125,
      "learning_rate": 4.473168214654283e-05,
      "loss": 29.2886,
      "step": 11311
    },
    {
      "epoch": 11.32,
      "grad_norm": 32875.15625,
      "learning_rate": 4.47265221878225e-05,
      "loss": 25.2711,
      "step": 11312
    },
    {
      "epoch": 11.32,
      "grad_norm": 28587.580078125,
      "learning_rate": 4.472136222910217e-05,
      "loss": 19.8258,
      "step": 11313
    },
    {
      "epoch": 11.33,
      "grad_norm": 3036.490966796875,
      "learning_rate": 4.4716202270381844e-05,
      "loss": 18.4201,
      "step": 11314
    },
    {
      "epoch": 11.33,
      "grad_norm": 7125.47998046875,
      "learning_rate": 4.471104231166151e-05,
      "loss": 17.3944,
      "step": 11315
    },
    {
      "epoch": 11.33,
      "grad_norm": 10268.548828125,
      "learning_rate": 4.470588235294118e-05,
      "loss": 16.2393,
      "step": 11316
    },
    {
      "epoch": 11.33,
      "grad_norm": 3467.077392578125,
      "learning_rate": 4.4700722394220854e-05,
      "loss": 23.2938,
      "step": 11317
    },
    {
      "epoch": 11.33,
      "grad_norm": 4014.88525390625,
      "learning_rate": 4.4695562435500515e-05,
      "loss": 13.6476,
      "step": 11318
    },
    {
      "epoch": 11.33,
      "grad_norm": 4168.400390625,
      "learning_rate": 4.469040247678018e-05,
      "loss": 18.4238,
      "step": 11319
    },
    {
      "epoch": 11.33,
      "grad_norm": 3296.115234375,
      "learning_rate": 4.468524251805986e-05,
      "loss": 28.2936,
      "step": 11320
    },
    {
      "epoch": 11.33,
      "grad_norm": 210791.328125,
      "learning_rate": 4.4680082559339525e-05,
      "loss": 12.8932,
      "step": 11321
    },
    {
      "epoch": 11.33,
      "grad_norm": 13314.9248046875,
      "learning_rate": 4.467492260061919e-05,
      "loss": 23.2502,
      "step": 11322
    },
    {
      "epoch": 11.33,
      "grad_norm": 19965.27734375,
      "learning_rate": 4.466976264189887e-05,
      "loss": 20.818,
      "step": 11323
    },
    {
      "epoch": 11.34,
      "grad_norm": 17822.923828125,
      "learning_rate": 4.4664602683178535e-05,
      "loss": 33.9578,
      "step": 11324
    },
    {
      "epoch": 11.34,
      "grad_norm": 8231.861328125,
      "learning_rate": 4.46594427244582e-05,
      "loss": 22.4559,
      "step": 11325
    },
    {
      "epoch": 11.34,
      "grad_norm": 15973.8623046875,
      "learning_rate": 4.465428276573788e-05,
      "loss": 22.7388,
      "step": 11326
    },
    {
      "epoch": 11.34,
      "grad_norm": 5709.36181640625,
      "learning_rate": 4.4649122807017545e-05,
      "loss": 21.8926,
      "step": 11327
    },
    {
      "epoch": 11.34,
      "grad_norm": 5461.08203125,
      "learning_rate": 4.464396284829722e-05,
      "loss": 22.5208,
      "step": 11328
    },
    {
      "epoch": 11.34,
      "grad_norm": 13959.3076171875,
      "learning_rate": 4.463880288957689e-05,
      "loss": 20.2676,
      "step": 11329
    },
    {
      "epoch": 11.34,
      "grad_norm": 12737.744140625,
      "learning_rate": 4.4633642930856555e-05,
      "loss": 13.9883,
      "step": 11330
    },
    {
      "epoch": 11.34,
      "grad_norm": 3781.13623046875,
      "learning_rate": 4.462848297213623e-05,
      "loss": 28.0372,
      "step": 11331
    },
    {
      "epoch": 11.34,
      "grad_norm": 32245.076171875,
      "learning_rate": 4.46233230134159e-05,
      "loss": 35.4478,
      "step": 11332
    },
    {
      "epoch": 11.34,
      "grad_norm": 11061.32421875,
      "learning_rate": 4.4618163054695565e-05,
      "loss": 13.6489,
      "step": 11333
    },
    {
      "epoch": 11.35,
      "grad_norm": 27303.099609375,
      "learning_rate": 4.461300309597524e-05,
      "loss": 17.9816,
      "step": 11334
    },
    {
      "epoch": 11.35,
      "grad_norm": 9385.916015625,
      "learning_rate": 4.460784313725491e-05,
      "loss": 24.2669,
      "step": 11335
    },
    {
      "epoch": 11.35,
      "grad_norm": 11822.701171875,
      "learning_rate": 4.460268317853457e-05,
      "loss": 25.4722,
      "step": 11336
    },
    {
      "epoch": 11.35,
      "grad_norm": 11562.8759765625,
      "learning_rate": 4.459752321981424e-05,
      "loss": 22.2768,
      "step": 11337
    },
    {
      "epoch": 11.35,
      "grad_norm": 19667.6328125,
      "learning_rate": 4.459236326109391e-05,
      "loss": 18.2219,
      "step": 11338
    },
    {
      "epoch": 11.35,
      "grad_norm": 12097.951171875,
      "learning_rate": 4.458720330237358e-05,
      "loss": 17.5666,
      "step": 11339
    },
    {
      "epoch": 11.35,
      "grad_norm": 12619.1708984375,
      "learning_rate": 4.458204334365325e-05,
      "loss": 24.0364,
      "step": 11340
    },
    {
      "epoch": 11.35,
      "grad_norm": 6089.99462890625,
      "learning_rate": 4.457688338493292e-05,
      "loss": 21.2831,
      "step": 11341
    },
    {
      "epoch": 11.35,
      "grad_norm": 5860.861328125,
      "learning_rate": 4.4571723426212595e-05,
      "loss": 22.2128,
      "step": 11342
    },
    {
      "epoch": 11.35,
      "grad_norm": 4646.53369140625,
      "learning_rate": 4.456656346749226e-05,
      "loss": 26.2922,
      "step": 11343
    },
    {
      "epoch": 11.36,
      "grad_norm": 12222.5263671875,
      "learning_rate": 4.456140350877193e-05,
      "loss": 15.7144,
      "step": 11344
    },
    {
      "epoch": 11.36,
      "grad_norm": 1404.91259765625,
      "learning_rate": 4.4556243550051605e-05,
      "loss": 23.7785,
      "step": 11345
    },
    {
      "epoch": 11.36,
      "grad_norm": 2476.458984375,
      "learning_rate": 4.455108359133127e-05,
      "loss": 22.3448,
      "step": 11346
    },
    {
      "epoch": 11.36,
      "grad_norm": 72081.8046875,
      "learning_rate": 4.454592363261094e-05,
      "loss": 35.4211,
      "step": 11347
    },
    {
      "epoch": 11.36,
      "grad_norm": 8927.947265625,
      "learning_rate": 4.4540763673890615e-05,
      "loss": 27.5623,
      "step": 11348
    },
    {
      "epoch": 11.36,
      "grad_norm": 4142.18798828125,
      "learning_rate": 4.453560371517028e-05,
      "loss": 19.8571,
      "step": 11349
    },
    {
      "epoch": 11.36,
      "grad_norm": 7576.35009765625,
      "learning_rate": 4.453044375644995e-05,
      "loss": 16.2979,
      "step": 11350
    },
    {
      "epoch": 11.36,
      "grad_norm": 26949.0859375,
      "learning_rate": 4.4525283797729625e-05,
      "loss": 17.4458,
      "step": 11351
    },
    {
      "epoch": 11.36,
      "grad_norm": 9472.681640625,
      "learning_rate": 4.452012383900929e-05,
      "loss": 34.6406,
      "step": 11352
    },
    {
      "epoch": 11.36,
      "grad_norm": 11575.6455078125,
      "learning_rate": 4.451496388028896e-05,
      "loss": 20.2912,
      "step": 11353
    },
    {
      "epoch": 11.37,
      "grad_norm": 15617.8837890625,
      "learning_rate": 4.450980392156863e-05,
      "loss": 24.739,
      "step": 11354
    },
    {
      "epoch": 11.37,
      "grad_norm": 2680.14794921875,
      "learning_rate": 4.4504643962848296e-05,
      "loss": 23.8934,
      "step": 11355
    },
    {
      "epoch": 11.37,
      "grad_norm": 55057.984375,
      "learning_rate": 4.449948400412797e-05,
      "loss": 24.0051,
      "step": 11356
    },
    {
      "epoch": 11.37,
      "grad_norm": 12113.5615234375,
      "learning_rate": 4.449432404540764e-05,
      "loss": 15.675,
      "step": 11357
    },
    {
      "epoch": 11.37,
      "grad_norm": 16192.431640625,
      "learning_rate": 4.4489164086687306e-05,
      "loss": 25.5654,
      "step": 11358
    },
    {
      "epoch": 11.37,
      "grad_norm": 3265.702392578125,
      "learning_rate": 4.448400412796698e-05,
      "loss": 26.9413,
      "step": 11359
    },
    {
      "epoch": 11.37,
      "grad_norm": 12724.076171875,
      "learning_rate": 4.447884416924665e-05,
      "loss": 27.9033,
      "step": 11360
    },
    {
      "epoch": 11.37,
      "grad_norm": 9299.62109375,
      "learning_rate": 4.4473684210526316e-05,
      "loss": 20.4482,
      "step": 11361
    },
    {
      "epoch": 11.37,
      "grad_norm": 17143.91796875,
      "learning_rate": 4.446852425180599e-05,
      "loss": 25.3498,
      "step": 11362
    },
    {
      "epoch": 11.37,
      "grad_norm": 6429.4375,
      "learning_rate": 4.446336429308566e-05,
      "loss": 23.6835,
      "step": 11363
    },
    {
      "epoch": 11.38,
      "grad_norm": 8631.1162109375,
      "learning_rate": 4.4458204334365326e-05,
      "loss": 18.19,
      "step": 11364
    },
    {
      "epoch": 11.38,
      "grad_norm": 5323.1923828125,
      "learning_rate": 4.4453044375645e-05,
      "loss": 25.0399,
      "step": 11365
    },
    {
      "epoch": 11.38,
      "grad_norm": 1267.85888671875,
      "learning_rate": 4.444788441692467e-05,
      "loss": 26.7392,
      "step": 11366
    },
    {
      "epoch": 11.38,
      "grad_norm": 2936.904052734375,
      "learning_rate": 4.4442724458204336e-05,
      "loss": 22.5787,
      "step": 11367
    },
    {
      "epoch": 11.38,
      "grad_norm": 4969.3154296875,
      "learning_rate": 4.443756449948401e-05,
      "loss": 23.0923,
      "step": 11368
    },
    {
      "epoch": 11.38,
      "grad_norm": 1612.3240966796875,
      "learning_rate": 4.443240454076368e-05,
      "loss": 13.8816,
      "step": 11369
    },
    {
      "epoch": 11.38,
      "grad_norm": 17009.8203125,
      "learning_rate": 4.4427244582043346e-05,
      "loss": 20.7339,
      "step": 11370
    },
    {
      "epoch": 11.38,
      "grad_norm": 4424.0361328125,
      "learning_rate": 4.442208462332302e-05,
      "loss": 24.5946,
      "step": 11371
    },
    {
      "epoch": 11.38,
      "grad_norm": 11855.2607421875,
      "learning_rate": 4.441692466460268e-05,
      "loss": 21.5834,
      "step": 11372
    },
    {
      "epoch": 11.38,
      "grad_norm": 11904.7685546875,
      "learning_rate": 4.4411764705882356e-05,
      "loss": 20.0031,
      "step": 11373
    },
    {
      "epoch": 11.39,
      "grad_norm": 26323.173828125,
      "learning_rate": 4.4406604747162024e-05,
      "loss": 13.077,
      "step": 11374
    },
    {
      "epoch": 11.39,
      "grad_norm": 50699.63671875,
      "learning_rate": 4.440144478844169e-05,
      "loss": 19.2731,
      "step": 11375
    },
    {
      "epoch": 11.39,
      "grad_norm": 13990.8759765625,
      "learning_rate": 4.4396284829721366e-05,
      "loss": 28.5102,
      "step": 11376
    },
    {
      "epoch": 11.39,
      "grad_norm": 205500.796875,
      "learning_rate": 4.4391124871001034e-05,
      "loss": 32.882,
      "step": 11377
    },
    {
      "epoch": 11.39,
      "grad_norm": 3469.966796875,
      "learning_rate": 4.43859649122807e-05,
      "loss": 20.365,
      "step": 11378
    },
    {
      "epoch": 11.39,
      "grad_norm": 3303.58251953125,
      "learning_rate": 4.4380804953560376e-05,
      "loss": 20.0191,
      "step": 11379
    },
    {
      "epoch": 11.39,
      "grad_norm": 11263.0166015625,
      "learning_rate": 4.4375644994840044e-05,
      "loss": 16.7069,
      "step": 11380
    },
    {
      "epoch": 11.39,
      "grad_norm": 9958.08984375,
      "learning_rate": 4.437048503611971e-05,
      "loss": 23.9539,
      "step": 11381
    },
    {
      "epoch": 11.39,
      "grad_norm": 1760.854248046875,
      "learning_rate": 4.4365325077399386e-05,
      "loss": 15.7292,
      "step": 11382
    },
    {
      "epoch": 11.39,
      "grad_norm": 2869.282958984375,
      "learning_rate": 4.4360165118679054e-05,
      "loss": 15.7622,
      "step": 11383
    },
    {
      "epoch": 11.4,
      "grad_norm": 12657.50390625,
      "learning_rate": 4.435500515995872e-05,
      "loss": 23.8801,
      "step": 11384
    },
    {
      "epoch": 11.4,
      "grad_norm": 144315.296875,
      "learning_rate": 4.4349845201238396e-05,
      "loss": 21.7308,
      "step": 11385
    },
    {
      "epoch": 11.4,
      "grad_norm": 29086.16015625,
      "learning_rate": 4.4344685242518064e-05,
      "loss": 21.6024,
      "step": 11386
    },
    {
      "epoch": 11.4,
      "grad_norm": 16028.9853515625,
      "learning_rate": 4.433952528379773e-05,
      "loss": 23.1921,
      "step": 11387
    },
    {
      "epoch": 11.4,
      "grad_norm": 25225.09765625,
      "learning_rate": 4.4334365325077406e-05,
      "loss": 21.7119,
      "step": 11388
    },
    {
      "epoch": 11.4,
      "grad_norm": 53020.76953125,
      "learning_rate": 4.4329205366357074e-05,
      "loss": 19.8795,
      "step": 11389
    },
    {
      "epoch": 11.4,
      "grad_norm": 5846.98828125,
      "learning_rate": 4.432404540763674e-05,
      "loss": 23.457,
      "step": 11390
    },
    {
      "epoch": 11.4,
      "grad_norm": 70757.90625,
      "learning_rate": 4.431888544891641e-05,
      "loss": 29.1299,
      "step": 11391
    },
    {
      "epoch": 11.4,
      "grad_norm": 12134.13671875,
      "learning_rate": 4.431372549019608e-05,
      "loss": 21.4971,
      "step": 11392
    },
    {
      "epoch": 11.4,
      "grad_norm": 19620.814453125,
      "learning_rate": 4.430856553147575e-05,
      "loss": 25.333,
      "step": 11393
    },
    {
      "epoch": 11.41,
      "grad_norm": 1618.7265625,
      "learning_rate": 4.430340557275542e-05,
      "loss": 22.5812,
      "step": 11394
    },
    {
      "epoch": 11.41,
      "grad_norm": 13199.55078125,
      "learning_rate": 4.429824561403509e-05,
      "loss": 25.2279,
      "step": 11395
    },
    {
      "epoch": 11.41,
      "grad_norm": 2191.84326171875,
      "learning_rate": 4.429308565531476e-05,
      "loss": 21.9955,
      "step": 11396
    },
    {
      "epoch": 11.41,
      "grad_norm": 3054.91650390625,
      "learning_rate": 4.428792569659443e-05,
      "loss": 24.3589,
      "step": 11397
    },
    {
      "epoch": 11.41,
      "grad_norm": 37546.09375,
      "learning_rate": 4.42827657378741e-05,
      "loss": 28.2666,
      "step": 11398
    },
    {
      "epoch": 11.41,
      "grad_norm": 12638.87890625,
      "learning_rate": 4.427760577915377e-05,
      "loss": 21.9044,
      "step": 11399
    },
    {
      "epoch": 11.41,
      "grad_norm": 10407.109375,
      "learning_rate": 4.427244582043344e-05,
      "loss": 27.2759,
      "step": 11400
    },
    {
      "epoch": 11.41,
      "grad_norm": 1986.626708984375,
      "learning_rate": 4.426728586171311e-05,
      "loss": 28.6169,
      "step": 11401
    },
    {
      "epoch": 11.41,
      "grad_norm": 41032.22265625,
      "learning_rate": 4.426212590299278e-05,
      "loss": 28.684,
      "step": 11402
    },
    {
      "epoch": 11.41,
      "grad_norm": 7339.59619140625,
      "learning_rate": 4.425696594427245e-05,
      "loss": 21.3966,
      "step": 11403
    },
    {
      "epoch": 11.42,
      "grad_norm": 14248.888671875,
      "learning_rate": 4.425180598555212e-05,
      "loss": 32.7726,
      "step": 11404
    },
    {
      "epoch": 11.42,
      "grad_norm": 5438.6767578125,
      "learning_rate": 4.424664602683179e-05,
      "loss": 28.7444,
      "step": 11405
    },
    {
      "epoch": 11.42,
      "grad_norm": 312440.46875,
      "learning_rate": 4.424148606811146e-05,
      "loss": 21.5293,
      "step": 11406
    },
    {
      "epoch": 11.42,
      "grad_norm": 72543.8828125,
      "learning_rate": 4.423632610939113e-05,
      "loss": 23.0554,
      "step": 11407
    },
    {
      "epoch": 11.42,
      "grad_norm": 5311.6953125,
      "learning_rate": 4.4231166150670795e-05,
      "loss": 23.7278,
      "step": 11408
    },
    {
      "epoch": 11.42,
      "grad_norm": 174635.296875,
      "learning_rate": 4.422600619195046e-05,
      "loss": 31.5323,
      "step": 11409
    },
    {
      "epoch": 11.42,
      "grad_norm": 7168.998046875,
      "learning_rate": 4.422084623323014e-05,
      "loss": 17.7594,
      "step": 11410
    },
    {
      "epoch": 11.42,
      "grad_norm": 159443.46875,
      "learning_rate": 4.4215686274509805e-05,
      "loss": 24.0372,
      "step": 11411
    },
    {
      "epoch": 11.42,
      "grad_norm": 5050.41162109375,
      "learning_rate": 4.421052631578947e-05,
      "loss": 29.4888,
      "step": 11412
    },
    {
      "epoch": 11.42,
      "grad_norm": 7017.77392578125,
      "learning_rate": 4.420536635706915e-05,
      "loss": 15.2687,
      "step": 11413
    },
    {
      "epoch": 11.43,
      "grad_norm": 5143.9306640625,
      "learning_rate": 4.4200206398348815e-05,
      "loss": 21.602,
      "step": 11414
    },
    {
      "epoch": 11.43,
      "grad_norm": 3714.370849609375,
      "learning_rate": 4.419504643962848e-05,
      "loss": 25.6631,
      "step": 11415
    },
    {
      "epoch": 11.43,
      "grad_norm": 8880.6337890625,
      "learning_rate": 4.418988648090816e-05,
      "loss": 24.8327,
      "step": 11416
    },
    {
      "epoch": 11.43,
      "grad_norm": 46673.12109375,
      "learning_rate": 4.4184726522187825e-05,
      "loss": 31.3272,
      "step": 11417
    },
    {
      "epoch": 11.43,
      "grad_norm": 38162.5390625,
      "learning_rate": 4.417956656346749e-05,
      "loss": 27.923,
      "step": 11418
    },
    {
      "epoch": 11.43,
      "grad_norm": 7337.39697265625,
      "learning_rate": 4.417440660474717e-05,
      "loss": 25.6828,
      "step": 11419
    },
    {
      "epoch": 11.43,
      "grad_norm": 6992.69091796875,
      "learning_rate": 4.4169246646026835e-05,
      "loss": 22.8597,
      "step": 11420
    },
    {
      "epoch": 11.43,
      "grad_norm": 29936.873046875,
      "learning_rate": 4.41640866873065e-05,
      "loss": 28.3699,
      "step": 11421
    },
    {
      "epoch": 11.43,
      "grad_norm": 4810.62060546875,
      "learning_rate": 4.415892672858618e-05,
      "loss": 15.1055,
      "step": 11422
    },
    {
      "epoch": 11.43,
      "grad_norm": 9712.4521484375,
      "learning_rate": 4.4153766769865845e-05,
      "loss": 24.37,
      "step": 11423
    },
    {
      "epoch": 11.44,
      "grad_norm": 6568.39013671875,
      "learning_rate": 4.414860681114551e-05,
      "loss": 37.4477,
      "step": 11424
    },
    {
      "epoch": 11.44,
      "grad_norm": 2349.341552734375,
      "learning_rate": 4.414344685242519e-05,
      "loss": 16.3145,
      "step": 11425
    },
    {
      "epoch": 11.44,
      "grad_norm": 7359.935546875,
      "learning_rate": 4.413828689370485e-05,
      "loss": 17.9985,
      "step": 11426
    },
    {
      "epoch": 11.44,
      "grad_norm": 11593.845703125,
      "learning_rate": 4.413312693498452e-05,
      "loss": 27.4681,
      "step": 11427
    },
    {
      "epoch": 11.44,
      "grad_norm": 5257.642578125,
      "learning_rate": 4.412796697626419e-05,
      "loss": 25.2377,
      "step": 11428
    },
    {
      "epoch": 11.44,
      "grad_norm": 2366.998291015625,
      "learning_rate": 4.412280701754386e-05,
      "loss": 24.613,
      "step": 11429
    },
    {
      "epoch": 11.44,
      "grad_norm": 21806.34765625,
      "learning_rate": 4.411764705882353e-05,
      "loss": 21.3532,
      "step": 11430
    },
    {
      "epoch": 11.44,
      "grad_norm": 45119.125,
      "learning_rate": 4.41124871001032e-05,
      "loss": 22.9831,
      "step": 11431
    },
    {
      "epoch": 11.44,
      "grad_norm": 16711.115234375,
      "learning_rate": 4.410732714138287e-05,
      "loss": 21.1518,
      "step": 11432
    },
    {
      "epoch": 11.44,
      "grad_norm": 5629.119140625,
      "learning_rate": 4.410216718266254e-05,
      "loss": 24.4077,
      "step": 11433
    },
    {
      "epoch": 11.45,
      "grad_norm": 11619.2275390625,
      "learning_rate": 4.409700722394221e-05,
      "loss": 20.087,
      "step": 11434
    },
    {
      "epoch": 11.45,
      "grad_norm": 18785.861328125,
      "learning_rate": 4.409184726522188e-05,
      "loss": 18.2861,
      "step": 11435
    },
    {
      "epoch": 11.45,
      "grad_norm": 21937.365234375,
      "learning_rate": 4.408668730650155e-05,
      "loss": 15.5119,
      "step": 11436
    },
    {
      "epoch": 11.45,
      "grad_norm": 23860.279296875,
      "learning_rate": 4.408152734778122e-05,
      "loss": 41.5117,
      "step": 11437
    },
    {
      "epoch": 11.45,
      "grad_norm": 4019.06689453125,
      "learning_rate": 4.407636738906089e-05,
      "loss": 21.5663,
      "step": 11438
    },
    {
      "epoch": 11.45,
      "grad_norm": 10542.7080078125,
      "learning_rate": 4.407120743034056e-05,
      "loss": 21.1922,
      "step": 11439
    },
    {
      "epoch": 11.45,
      "grad_norm": 3591.737548828125,
      "learning_rate": 4.406604747162023e-05,
      "loss": 16.0821,
      "step": 11440
    },
    {
      "epoch": 11.45,
      "grad_norm": 25977.740234375,
      "learning_rate": 4.40608875128999e-05,
      "loss": 24.4746,
      "step": 11441
    },
    {
      "epoch": 11.45,
      "grad_norm": 134695.078125,
      "learning_rate": 4.405572755417957e-05,
      "loss": 28.1838,
      "step": 11442
    },
    {
      "epoch": 11.45,
      "grad_norm": 3856.72705078125,
      "learning_rate": 4.405056759545924e-05,
      "loss": 24.6011,
      "step": 11443
    },
    {
      "epoch": 11.46,
      "grad_norm": 3996.530029296875,
      "learning_rate": 4.404540763673891e-05,
      "loss": 21.6585,
      "step": 11444
    },
    {
      "epoch": 11.46,
      "grad_norm": 4701.4375,
      "learning_rate": 4.4040247678018576e-05,
      "loss": 24.1088,
      "step": 11445
    },
    {
      "epoch": 11.46,
      "grad_norm": 8806.2548828125,
      "learning_rate": 4.403508771929824e-05,
      "loss": 26.322,
      "step": 11446
    },
    {
      "epoch": 11.46,
      "grad_norm": 30877.263671875,
      "learning_rate": 4.402992776057792e-05,
      "loss": 29.5499,
      "step": 11447
    },
    {
      "epoch": 11.46,
      "grad_norm": 27289.923828125,
      "learning_rate": 4.4024767801857586e-05,
      "loss": 20.4968,
      "step": 11448
    },
    {
      "epoch": 11.46,
      "grad_norm": 4551.8037109375,
      "learning_rate": 4.401960784313725e-05,
      "loss": 13.9751,
      "step": 11449
    },
    {
      "epoch": 11.46,
      "grad_norm": 4965.6357421875,
      "learning_rate": 4.401444788441693e-05,
      "loss": 24.2789,
      "step": 11450
    },
    {
      "epoch": 11.46,
      "grad_norm": 6602.00927734375,
      "learning_rate": 4.4009287925696596e-05,
      "loss": 17.9091,
      "step": 11451
    },
    {
      "epoch": 11.46,
      "grad_norm": 10646.8759765625,
      "learning_rate": 4.400412796697626e-05,
      "loss": 24.3528,
      "step": 11452
    },
    {
      "epoch": 11.46,
      "grad_norm": 12158.7685546875,
      "learning_rate": 4.399896800825594e-05,
      "loss": 24.5376,
      "step": 11453
    },
    {
      "epoch": 11.47,
      "grad_norm": 3516.9755859375,
      "learning_rate": 4.3993808049535606e-05,
      "loss": 23.6628,
      "step": 11454
    },
    {
      "epoch": 11.47,
      "grad_norm": 3702.434814453125,
      "learning_rate": 4.398864809081527e-05,
      "loss": 20.3826,
      "step": 11455
    },
    {
      "epoch": 11.47,
      "grad_norm": 12137.7099609375,
      "learning_rate": 4.398348813209495e-05,
      "loss": 32.8903,
      "step": 11456
    },
    {
      "epoch": 11.47,
      "grad_norm": 4156.794921875,
      "learning_rate": 4.3978328173374616e-05,
      "loss": 21.7464,
      "step": 11457
    },
    {
      "epoch": 11.47,
      "grad_norm": 8725.7548828125,
      "learning_rate": 4.397316821465429e-05,
      "loss": 17.7285,
      "step": 11458
    },
    {
      "epoch": 11.47,
      "grad_norm": 6526.6865234375,
      "learning_rate": 4.396800825593396e-05,
      "loss": 19.0943,
      "step": 11459
    },
    {
      "epoch": 11.47,
      "grad_norm": 4873.94482421875,
      "learning_rate": 4.3962848297213626e-05,
      "loss": 16.8278,
      "step": 11460
    },
    {
      "epoch": 11.47,
      "grad_norm": 30389.478515625,
      "learning_rate": 4.39576883384933e-05,
      "loss": 15.4686,
      "step": 11461
    },
    {
      "epoch": 11.47,
      "grad_norm": 2351.570068359375,
      "learning_rate": 4.395252837977296e-05,
      "loss": 21.243,
      "step": 11462
    },
    {
      "epoch": 11.47,
      "grad_norm": 25559.685546875,
      "learning_rate": 4.394736842105263e-05,
      "loss": 22.4045,
      "step": 11463
    },
    {
      "epoch": 11.48,
      "grad_norm": 5199.2353515625,
      "learning_rate": 4.39422084623323e-05,
      "loss": 26.2947,
      "step": 11464
    },
    {
      "epoch": 11.48,
      "grad_norm": 9140.3486328125,
      "learning_rate": 4.393704850361197e-05,
      "loss": 22.252,
      "step": 11465
    },
    {
      "epoch": 11.48,
      "grad_norm": 10752.2607421875,
      "learning_rate": 4.393188854489164e-05,
      "loss": 22.9929,
      "step": 11466
    },
    {
      "epoch": 11.48,
      "grad_norm": 64936.75,
      "learning_rate": 4.392672858617131e-05,
      "loss": 26.6261,
      "step": 11467
    },
    {
      "epoch": 11.48,
      "grad_norm": 5309.064453125,
      "learning_rate": 4.392156862745098e-05,
      "loss": 31.6821,
      "step": 11468
    },
    {
      "epoch": 11.48,
      "grad_norm": 6465.64013671875,
      "learning_rate": 4.391640866873065e-05,
      "loss": 19.1742,
      "step": 11469
    },
    {
      "epoch": 11.48,
      "grad_norm": 12936.3564453125,
      "learning_rate": 4.391124871001032e-05,
      "loss": 20.5852,
      "step": 11470
    },
    {
      "epoch": 11.48,
      "grad_norm": 18766.595703125,
      "learning_rate": 4.390608875128999e-05,
      "loss": 20.9777,
      "step": 11471
    },
    {
      "epoch": 11.48,
      "grad_norm": 23421.755859375,
      "learning_rate": 4.390092879256966e-05,
      "loss": 16.471,
      "step": 11472
    },
    {
      "epoch": 11.48,
      "grad_norm": 7156.4677734375,
      "learning_rate": 4.389576883384933e-05,
      "loss": 20.3173,
      "step": 11473
    },
    {
      "epoch": 11.49,
      "grad_norm": 15193.7001953125,
      "learning_rate": 4.3890608875129e-05,
      "loss": 27.1496,
      "step": 11474
    },
    {
      "epoch": 11.49,
      "grad_norm": 6967.8486328125,
      "learning_rate": 4.3885448916408676e-05,
      "loss": 29.509,
      "step": 11475
    },
    {
      "epoch": 11.49,
      "grad_norm": 7424.12890625,
      "learning_rate": 4.388028895768834e-05,
      "loss": 21.4495,
      "step": 11476
    },
    {
      "epoch": 11.49,
      "grad_norm": 33454.23046875,
      "learning_rate": 4.387512899896801e-05,
      "loss": 16.7257,
      "step": 11477
    },
    {
      "epoch": 11.49,
      "grad_norm": 2917.95361328125,
      "learning_rate": 4.3869969040247686e-05,
      "loss": 14.8307,
      "step": 11478
    },
    {
      "epoch": 11.49,
      "grad_norm": 22028.126953125,
      "learning_rate": 4.3864809081527347e-05,
      "loss": 39.6792,
      "step": 11479
    },
    {
      "epoch": 11.49,
      "grad_norm": 6095.0458984375,
      "learning_rate": 4.3859649122807014e-05,
      "loss": 23.3219,
      "step": 11480
    },
    {
      "epoch": 11.49,
      "grad_norm": 11592.8720703125,
      "learning_rate": 4.385448916408669e-05,
      "loss": 33.8088,
      "step": 11481
    },
    {
      "epoch": 11.49,
      "grad_norm": 5100.9150390625,
      "learning_rate": 4.3849329205366357e-05,
      "loss": 23.2918,
      "step": 11482
    },
    {
      "epoch": 11.49,
      "grad_norm": 2598.37451171875,
      "learning_rate": 4.3844169246646024e-05,
      "loss": 21.0008,
      "step": 11483
    },
    {
      "epoch": 11.5,
      "grad_norm": 4363.4794921875,
      "learning_rate": 4.38390092879257e-05,
      "loss": 21.6056,
      "step": 11484
    },
    {
      "epoch": 11.5,
      "grad_norm": 10668.515625,
      "learning_rate": 4.3833849329205367e-05,
      "loss": 26.6024,
      "step": 11485
    },
    {
      "epoch": 11.5,
      "grad_norm": 5462.890625,
      "learning_rate": 4.3828689370485034e-05,
      "loss": 24.2709,
      "step": 11486
    },
    {
      "epoch": 11.5,
      "grad_norm": 5599.4765625,
      "learning_rate": 4.382352941176471e-05,
      "loss": 26.4121,
      "step": 11487
    },
    {
      "epoch": 11.5,
      "grad_norm": 16375.94140625,
      "learning_rate": 4.3818369453044377e-05,
      "loss": 19.6196,
      "step": 11488
    },
    {
      "epoch": 11.5,
      "grad_norm": 21136.45703125,
      "learning_rate": 4.381320949432405e-05,
      "loss": 36.4597,
      "step": 11489
    },
    {
      "epoch": 11.5,
      "grad_norm": 4853.16943359375,
      "learning_rate": 4.380804953560372e-05,
      "loss": 20.2005,
      "step": 11490
    },
    {
      "epoch": 11.5,
      "grad_norm": 9029.90234375,
      "learning_rate": 4.3802889576883387e-05,
      "loss": 17.8069,
      "step": 11491
    },
    {
      "epoch": 11.5,
      "grad_norm": 25690.646484375,
      "learning_rate": 4.379772961816306e-05,
      "loss": 27.3303,
      "step": 11492
    },
    {
      "epoch": 11.5,
      "grad_norm": 8825.9580078125,
      "learning_rate": 4.379256965944273e-05,
      "loss": 26.1592,
      "step": 11493
    },
    {
      "epoch": 11.51,
      "grad_norm": 7254.7109375,
      "learning_rate": 4.3787409700722397e-05,
      "loss": 31.0365,
      "step": 11494
    },
    {
      "epoch": 11.51,
      "grad_norm": 17554.6640625,
      "learning_rate": 4.378224974200207e-05,
      "loss": 31.1473,
      "step": 11495
    },
    {
      "epoch": 11.51,
      "grad_norm": 10910.9384765625,
      "learning_rate": 4.377708978328174e-05,
      "loss": 24.7414,
      "step": 11496
    },
    {
      "epoch": 11.51,
      "grad_norm": 2645.316162109375,
      "learning_rate": 4.37719298245614e-05,
      "loss": 20.9244,
      "step": 11497
    },
    {
      "epoch": 11.51,
      "grad_norm": 2821.515380859375,
      "learning_rate": 4.3766769865841074e-05,
      "loss": 29.2113,
      "step": 11498
    },
    {
      "epoch": 11.51,
      "grad_norm": 8452.5322265625,
      "learning_rate": 4.376160990712074e-05,
      "loss": 27.6651,
      "step": 11499
    },
    {
      "epoch": 11.51,
      "grad_norm": 6163.47998046875,
      "learning_rate": 4.375644994840041e-05,
      "loss": 22.3599,
      "step": 11500
    },
    {
      "epoch": 11.51,
      "grad_norm": 12329.091796875,
      "learning_rate": 4.3751289989680084e-05,
      "loss": 23.9267,
      "step": 11501
    },
    {
      "epoch": 11.51,
      "grad_norm": 709306.25,
      "learning_rate": 4.374613003095975e-05,
      "loss": 23.3026,
      "step": 11502
    },
    {
      "epoch": 11.51,
      "grad_norm": 65501.19140625,
      "learning_rate": 4.3740970072239427e-05,
      "loss": 37.6852,
      "step": 11503
    },
    {
      "epoch": 11.52,
      "grad_norm": 40320.04296875,
      "learning_rate": 4.3735810113519094e-05,
      "loss": 34.4852,
      "step": 11504
    },
    {
      "epoch": 11.52,
      "grad_norm": 5035.82177734375,
      "learning_rate": 4.373065015479876e-05,
      "loss": 24.0338,
      "step": 11505
    },
    {
      "epoch": 11.52,
      "grad_norm": 6514.8408203125,
      "learning_rate": 4.3725490196078437e-05,
      "loss": 26.1466,
      "step": 11506
    },
    {
      "epoch": 11.52,
      "grad_norm": 26642.83203125,
      "learning_rate": 4.3720330237358104e-05,
      "loss": 17.0742,
      "step": 11507
    },
    {
      "epoch": 11.52,
      "grad_norm": 5816.51416015625,
      "learning_rate": 4.371517027863777e-05,
      "loss": 23.9648,
      "step": 11508
    },
    {
      "epoch": 11.52,
      "grad_norm": 5492.36474609375,
      "learning_rate": 4.3710010319917447e-05,
      "loss": 23.4328,
      "step": 11509
    },
    {
      "epoch": 11.52,
      "grad_norm": 12810.53515625,
      "learning_rate": 4.3704850361197114e-05,
      "loss": 24.8312,
      "step": 11510
    },
    {
      "epoch": 11.52,
      "grad_norm": 14567.419921875,
      "learning_rate": 4.369969040247678e-05,
      "loss": 19.8609,
      "step": 11511
    },
    {
      "epoch": 11.52,
      "grad_norm": 21400.7890625,
      "learning_rate": 4.3694530443756457e-05,
      "loss": 18.4818,
      "step": 11512
    },
    {
      "epoch": 11.52,
      "grad_norm": 23311.869140625,
      "learning_rate": 4.3689370485036124e-05,
      "loss": 24.7407,
      "step": 11513
    },
    {
      "epoch": 11.53,
      "grad_norm": 12856.142578125,
      "learning_rate": 4.368421052631579e-05,
      "loss": 22.4673,
      "step": 11514
    },
    {
      "epoch": 11.53,
      "grad_norm": 4418.3115234375,
      "learning_rate": 4.367905056759546e-05,
      "loss": 27.6971,
      "step": 11515
    },
    {
      "epoch": 11.53,
      "grad_norm": 57413.9375,
      "learning_rate": 4.367389060887513e-05,
      "loss": 22.8739,
      "step": 11516
    },
    {
      "epoch": 11.53,
      "grad_norm": 10161.900390625,
      "learning_rate": 4.36687306501548e-05,
      "loss": 15.6432,
      "step": 11517
    },
    {
      "epoch": 11.53,
      "grad_norm": 12616.943359375,
      "learning_rate": 4.366357069143447e-05,
      "loss": 22.5824,
      "step": 11518
    },
    {
      "epoch": 11.53,
      "grad_norm": 23708.763671875,
      "learning_rate": 4.365841073271414e-05,
      "loss": 18.1676,
      "step": 11519
    },
    {
      "epoch": 11.53,
      "grad_norm": 5999.169921875,
      "learning_rate": 4.365325077399381e-05,
      "loss": 28.0072,
      "step": 11520
    },
    {
      "epoch": 11.53,
      "grad_norm": 2008.76708984375,
      "learning_rate": 4.364809081527348e-05,
      "loss": 24.6931,
      "step": 11521
    },
    {
      "epoch": 11.53,
      "grad_norm": 13329.279296875,
      "learning_rate": 4.364293085655315e-05,
      "loss": 17.1753,
      "step": 11522
    },
    {
      "epoch": 11.53,
      "grad_norm": 13315.57421875,
      "learning_rate": 4.363777089783282e-05,
      "loss": 24.3793,
      "step": 11523
    },
    {
      "epoch": 11.54,
      "grad_norm": 9324.833984375,
      "learning_rate": 4.363261093911249e-05,
      "loss": 21.282,
      "step": 11524
    },
    {
      "epoch": 11.54,
      "grad_norm": 8255.18359375,
      "learning_rate": 4.362745098039216e-05,
      "loss": 17.6059,
      "step": 11525
    },
    {
      "epoch": 11.54,
      "grad_norm": 5811.791015625,
      "learning_rate": 4.362229102167183e-05,
      "loss": 26.586,
      "step": 11526
    },
    {
      "epoch": 11.54,
      "grad_norm": 26521.75,
      "learning_rate": 4.36171310629515e-05,
      "loss": 30.3864,
      "step": 11527
    },
    {
      "epoch": 11.54,
      "grad_norm": 3226.0478515625,
      "learning_rate": 4.361197110423117e-05,
      "loss": 24.8554,
      "step": 11528
    },
    {
      "epoch": 11.54,
      "grad_norm": 4200.26416015625,
      "learning_rate": 4.360681114551084e-05,
      "loss": 18.4459,
      "step": 11529
    },
    {
      "epoch": 11.54,
      "grad_norm": 13122.5791015625,
      "learning_rate": 4.360165118679051e-05,
      "loss": 25.6617,
      "step": 11530
    },
    {
      "epoch": 11.54,
      "grad_norm": 14282.3408203125,
      "learning_rate": 4.359649122807018e-05,
      "loss": 19.8417,
      "step": 11531
    },
    {
      "epoch": 11.54,
      "grad_norm": 4063.70654296875,
      "learning_rate": 4.359133126934985e-05,
      "loss": 25.5983,
      "step": 11532
    },
    {
      "epoch": 11.54,
      "grad_norm": 21003.40234375,
      "learning_rate": 4.358617131062951e-05,
      "loss": 23.9811,
      "step": 11533
    },
    {
      "epoch": 11.55,
      "grad_norm": 105571.5546875,
      "learning_rate": 4.358101135190919e-05,
      "loss": 30.0126,
      "step": 11534
    },
    {
      "epoch": 11.55,
      "grad_norm": 23014.06640625,
      "learning_rate": 4.3575851393188855e-05,
      "loss": 20.689,
      "step": 11535
    },
    {
      "epoch": 11.55,
      "grad_norm": 9339.0126953125,
      "learning_rate": 4.357069143446852e-05,
      "loss": 14.6829,
      "step": 11536
    },
    {
      "epoch": 11.55,
      "grad_norm": 4267.2158203125,
      "learning_rate": 4.35655314757482e-05,
      "loss": 22.9875,
      "step": 11537
    },
    {
      "epoch": 11.55,
      "grad_norm": 20328.4296875,
      "learning_rate": 4.3560371517027865e-05,
      "loss": 19.5949,
      "step": 11538
    },
    {
      "epoch": 11.55,
      "grad_norm": 1456.05029296875,
      "learning_rate": 4.355521155830753e-05,
      "loss": 22.2051,
      "step": 11539
    },
    {
      "epoch": 11.55,
      "grad_norm": 23906.0234375,
      "learning_rate": 4.355005159958721e-05,
      "loss": 33.7131,
      "step": 11540
    },
    {
      "epoch": 11.55,
      "grad_norm": 13618.6728515625,
      "learning_rate": 4.3544891640866875e-05,
      "loss": 18.9004,
      "step": 11541
    },
    {
      "epoch": 11.55,
      "grad_norm": 10941.5302734375,
      "learning_rate": 4.353973168214654e-05,
      "loss": 18.9226,
      "step": 11542
    },
    {
      "epoch": 11.55,
      "grad_norm": 3194.66455078125,
      "learning_rate": 4.353457172342622e-05,
      "loss": 24.6205,
      "step": 11543
    },
    {
      "epoch": 11.56,
      "grad_norm": 33955.50390625,
      "learning_rate": 4.3529411764705885e-05,
      "loss": 28.5721,
      "step": 11544
    },
    {
      "epoch": 11.56,
      "grad_norm": 5086.95068359375,
      "learning_rate": 4.352425180598555e-05,
      "loss": 27.0199,
      "step": 11545
    },
    {
      "epoch": 11.56,
      "grad_norm": 17437.755859375,
      "learning_rate": 4.351909184726523e-05,
      "loss": 20.43,
      "step": 11546
    },
    {
      "epoch": 11.56,
      "grad_norm": 13791.4404296875,
      "learning_rate": 4.3513931888544895e-05,
      "loss": 20.1799,
      "step": 11547
    },
    {
      "epoch": 11.56,
      "grad_norm": 46315.10546875,
      "learning_rate": 4.350877192982456e-05,
      "loss": 30.5132,
      "step": 11548
    },
    {
      "epoch": 11.56,
      "grad_norm": 3356.04541015625,
      "learning_rate": 4.350361197110424e-05,
      "loss": 17.0637,
      "step": 11549
    },
    {
      "epoch": 11.56,
      "grad_norm": 21691.15234375,
      "learning_rate": 4.3498452012383905e-05,
      "loss": 26.8541,
      "step": 11550
    },
    {
      "epoch": 11.56,
      "grad_norm": 10482.0009765625,
      "learning_rate": 4.349329205366357e-05,
      "loss": 19.0565,
      "step": 11551
    },
    {
      "epoch": 11.56,
      "grad_norm": 5624.30712890625,
      "learning_rate": 4.348813209494324e-05,
      "loss": 16.0094,
      "step": 11552
    },
    {
      "epoch": 11.56,
      "grad_norm": 33243.484375,
      "learning_rate": 4.348297213622291e-05,
      "loss": 24.0141,
      "step": 11553
    },
    {
      "epoch": 11.57,
      "grad_norm": 11545.4306640625,
      "learning_rate": 4.347781217750258e-05,
      "loss": 20.6726,
      "step": 11554
    },
    {
      "epoch": 11.57,
      "grad_norm": 7146.861328125,
      "learning_rate": 4.347265221878225e-05,
      "loss": 37.0375,
      "step": 11555
    },
    {
      "epoch": 11.57,
      "grad_norm": 4272.42578125,
      "learning_rate": 4.346749226006192e-05,
      "loss": 19.8495,
      "step": 11556
    },
    {
      "epoch": 11.57,
      "grad_norm": 31020.568359375,
      "learning_rate": 4.346233230134159e-05,
      "loss": 19.746,
      "step": 11557
    },
    {
      "epoch": 11.57,
      "grad_norm": 72256.5078125,
      "learning_rate": 4.345717234262126e-05,
      "loss": 22.0615,
      "step": 11558
    },
    {
      "epoch": 11.57,
      "grad_norm": 5181.609375,
      "learning_rate": 4.345201238390093e-05,
      "loss": 30.5447,
      "step": 11559
    },
    {
      "epoch": 11.57,
      "grad_norm": 52342.1640625,
      "learning_rate": 4.34468524251806e-05,
      "loss": 23.05,
      "step": 11560
    },
    {
      "epoch": 11.57,
      "grad_norm": 14853.3515625,
      "learning_rate": 4.344169246646027e-05,
      "loss": 32.9492,
      "step": 11561
    },
    {
      "epoch": 11.57,
      "grad_norm": 17316.0234375,
      "learning_rate": 4.343653250773994e-05,
      "loss": 27.5894,
      "step": 11562
    },
    {
      "epoch": 11.57,
      "grad_norm": 7139.7373046875,
      "learning_rate": 4.343137254901961e-05,
      "loss": 16.9127,
      "step": 11563
    },
    {
      "epoch": 11.58,
      "grad_norm": 5291.880859375,
      "learning_rate": 4.342621259029928e-05,
      "loss": 23.5067,
      "step": 11564
    },
    {
      "epoch": 11.58,
      "grad_norm": 55124.46875,
      "learning_rate": 4.342105263157895e-05,
      "loss": 30.5267,
      "step": 11565
    },
    {
      "epoch": 11.58,
      "grad_norm": 7512.12158203125,
      "learning_rate": 4.341589267285862e-05,
      "loss": 24.8699,
      "step": 11566
    },
    {
      "epoch": 11.58,
      "grad_norm": 2987.272216796875,
      "learning_rate": 4.341073271413829e-05,
      "loss": 29.0951,
      "step": 11567
    },
    {
      "epoch": 11.58,
      "grad_norm": 10134.3369140625,
      "learning_rate": 4.340557275541796e-05,
      "loss": 38.7325,
      "step": 11568
    },
    {
      "epoch": 11.58,
      "grad_norm": 24617.693359375,
      "learning_rate": 4.3400412796697626e-05,
      "loss": 21.4909,
      "step": 11569
    },
    {
      "epoch": 11.58,
      "grad_norm": 5584.35400390625,
      "learning_rate": 4.3395252837977294e-05,
      "loss": 18.5167,
      "step": 11570
    },
    {
      "epoch": 11.58,
      "grad_norm": 28234.9609375,
      "learning_rate": 4.339009287925697e-05,
      "loss": 25.8556,
      "step": 11571
    },
    {
      "epoch": 11.58,
      "grad_norm": 7999.43310546875,
      "learning_rate": 4.3384932920536636e-05,
      "loss": 16.9298,
      "step": 11572
    },
    {
      "epoch": 11.58,
      "grad_norm": 24239.54296875,
      "learning_rate": 4.3379772961816304e-05,
      "loss": 20.6539,
      "step": 11573
    },
    {
      "epoch": 11.59,
      "grad_norm": 8874.37890625,
      "learning_rate": 4.337461300309598e-05,
      "loss": 30.0684,
      "step": 11574
    },
    {
      "epoch": 11.59,
      "grad_norm": 141472.875,
      "learning_rate": 4.3369453044375646e-05,
      "loss": 28.4072,
      "step": 11575
    },
    {
      "epoch": 11.59,
      "grad_norm": 6013.0361328125,
      "learning_rate": 4.3364293085655314e-05,
      "loss": 45.5002,
      "step": 11576
    },
    {
      "epoch": 11.59,
      "grad_norm": 23024.70703125,
      "learning_rate": 4.335913312693499e-05,
      "loss": 20.3511,
      "step": 11577
    },
    {
      "epoch": 11.59,
      "grad_norm": 122048.3046875,
      "learning_rate": 4.3353973168214656e-05,
      "loss": 16.9362,
      "step": 11578
    },
    {
      "epoch": 11.59,
      "grad_norm": 5332.94482421875,
      "learning_rate": 4.3348813209494324e-05,
      "loss": 14.2571,
      "step": 11579
    },
    {
      "epoch": 11.59,
      "grad_norm": 19233.796875,
      "learning_rate": 4.3343653250774e-05,
      "loss": 22.7188,
      "step": 11580
    },
    {
      "epoch": 11.59,
      "grad_norm": 6454.49365234375,
      "learning_rate": 4.3338493292053666e-05,
      "loss": 20.7971,
      "step": 11581
    },
    {
      "epoch": 11.59,
      "grad_norm": 22667.09765625,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 28.0818,
      "step": 11582
    },
    {
      "epoch": 11.59,
      "grad_norm": 12282.56640625,
      "learning_rate": 4.332817337461301e-05,
      "loss": 29.8195,
      "step": 11583
    },
    {
      "epoch": 11.6,
      "grad_norm": 29140.275390625,
      "learning_rate": 4.3323013415892676e-05,
      "loss": 19.2676,
      "step": 11584
    },
    {
      "epoch": 11.6,
      "grad_norm": 11134.1826171875,
      "learning_rate": 4.3317853457172344e-05,
      "loss": 31.2205,
      "step": 11585
    },
    {
      "epoch": 11.6,
      "grad_norm": 1758.9068603515625,
      "learning_rate": 4.331269349845202e-05,
      "loss": 17.531,
      "step": 11586
    },
    {
      "epoch": 11.6,
      "grad_norm": 3467.010498046875,
      "learning_rate": 4.330753353973168e-05,
      "loss": 24.0003,
      "step": 11587
    },
    {
      "epoch": 11.6,
      "grad_norm": 4950.546875,
      "learning_rate": 4.3302373581011354e-05,
      "loss": 15.8745,
      "step": 11588
    },
    {
      "epoch": 11.6,
      "grad_norm": 15190.466796875,
      "learning_rate": 4.329721362229102e-05,
      "loss": 20.6163,
      "step": 11589
    },
    {
      "epoch": 11.6,
      "grad_norm": 11684.71875,
      "learning_rate": 4.329205366357069e-05,
      "loss": 28.341,
      "step": 11590
    },
    {
      "epoch": 11.6,
      "grad_norm": 4844.2802734375,
      "learning_rate": 4.3286893704850364e-05,
      "loss": 33.4173,
      "step": 11591
    },
    {
      "epoch": 11.6,
      "grad_norm": 19429.162109375,
      "learning_rate": 4.328173374613003e-05,
      "loss": 18.2823,
      "step": 11592
    },
    {
      "epoch": 11.6,
      "grad_norm": 9584.56640625,
      "learning_rate": 4.32765737874097e-05,
      "loss": 19.1558,
      "step": 11593
    },
    {
      "epoch": 11.61,
      "grad_norm": 3502.337890625,
      "learning_rate": 4.3271413828689374e-05,
      "loss": 28.0068,
      "step": 11594
    },
    {
      "epoch": 11.61,
      "grad_norm": 6104.3408203125,
      "learning_rate": 4.326625386996904e-05,
      "loss": 34.4864,
      "step": 11595
    },
    {
      "epoch": 11.61,
      "grad_norm": 3529.207275390625,
      "learning_rate": 4.326109391124871e-05,
      "loss": 28.5067,
      "step": 11596
    },
    {
      "epoch": 11.61,
      "grad_norm": 23407.916015625,
      "learning_rate": 4.3255933952528384e-05,
      "loss": 19.3377,
      "step": 11597
    },
    {
      "epoch": 11.61,
      "grad_norm": 9540.4833984375,
      "learning_rate": 4.325077399380805e-05,
      "loss": 19.3087,
      "step": 11598
    },
    {
      "epoch": 11.61,
      "grad_norm": 920.6373291015625,
      "learning_rate": 4.324561403508772e-05,
      "loss": 19.5886,
      "step": 11599
    },
    {
      "epoch": 11.61,
      "grad_norm": 10961.7685546875,
      "learning_rate": 4.3240454076367394e-05,
      "loss": 24.5928,
      "step": 11600
    },
    {
      "epoch": 11.61,
      "grad_norm": 7842.083984375,
      "learning_rate": 4.323529411764706e-05,
      "loss": 24.4659,
      "step": 11601
    },
    {
      "epoch": 11.61,
      "grad_norm": 5532.53076171875,
      "learning_rate": 4.323013415892673e-05,
      "loss": 33.9822,
      "step": 11602
    },
    {
      "epoch": 11.61,
      "grad_norm": 5954.84423828125,
      "learning_rate": 4.3224974200206404e-05,
      "loss": 29.2781,
      "step": 11603
    },
    {
      "epoch": 11.62,
      "grad_norm": 7899.4677734375,
      "learning_rate": 4.321981424148607e-05,
      "loss": 20.4847,
      "step": 11604
    },
    {
      "epoch": 11.62,
      "grad_norm": 11687.6064453125,
      "learning_rate": 4.321465428276574e-05,
      "loss": 28.9747,
      "step": 11605
    },
    {
      "epoch": 11.62,
      "grad_norm": 4053.079833984375,
      "learning_rate": 4.320949432404541e-05,
      "loss": 21.5997,
      "step": 11606
    },
    {
      "epoch": 11.62,
      "grad_norm": 9047.4580078125,
      "learning_rate": 4.3204334365325075e-05,
      "loss": 26.2714,
      "step": 11607
    },
    {
      "epoch": 11.62,
      "grad_norm": 4788.083984375,
      "learning_rate": 4.319917440660475e-05,
      "loss": 25.2869,
      "step": 11608
    },
    {
      "epoch": 11.62,
      "grad_norm": 7325.50048828125,
      "learning_rate": 4.319401444788442e-05,
      "loss": 34.8577,
      "step": 11609
    },
    {
      "epoch": 11.62,
      "grad_norm": 7116.4873046875,
      "learning_rate": 4.3188854489164085e-05,
      "loss": 18.4899,
      "step": 11610
    },
    {
      "epoch": 11.62,
      "grad_norm": 10684.6904296875,
      "learning_rate": 4.318369453044376e-05,
      "loss": 23.423,
      "step": 11611
    },
    {
      "epoch": 11.62,
      "grad_norm": 5889.91162109375,
      "learning_rate": 4.317853457172343e-05,
      "loss": 23.3519,
      "step": 11612
    },
    {
      "epoch": 11.62,
      "grad_norm": 3703.849853515625,
      "learning_rate": 4.3173374613003095e-05,
      "loss": 27.5145,
      "step": 11613
    },
    {
      "epoch": 11.63,
      "grad_norm": 4104.2421875,
      "learning_rate": 4.316821465428277e-05,
      "loss": 26.4435,
      "step": 11614
    },
    {
      "epoch": 11.63,
      "grad_norm": 28354.087890625,
      "learning_rate": 4.316305469556244e-05,
      "loss": 31.376,
      "step": 11615
    },
    {
      "epoch": 11.63,
      "grad_norm": 96768.4375,
      "learning_rate": 4.3157894736842105e-05,
      "loss": 18.3931,
      "step": 11616
    },
    {
      "epoch": 11.63,
      "grad_norm": 2262.678955078125,
      "learning_rate": 4.315273477812178e-05,
      "loss": 25.7262,
      "step": 11617
    },
    {
      "epoch": 11.63,
      "grad_norm": 22209.734375,
      "learning_rate": 4.314757481940145e-05,
      "loss": 38.4102,
      "step": 11618
    },
    {
      "epoch": 11.63,
      "grad_norm": 8434.482421875,
      "learning_rate": 4.314241486068112e-05,
      "loss": 31.5132,
      "step": 11619
    },
    {
      "epoch": 11.63,
      "grad_norm": 5546.28466796875,
      "learning_rate": 4.313725490196079e-05,
      "loss": 19.7907,
      "step": 11620
    },
    {
      "epoch": 11.63,
      "grad_norm": 7800.6201171875,
      "learning_rate": 4.313209494324046e-05,
      "loss": 24.9948,
      "step": 11621
    },
    {
      "epoch": 11.63,
      "grad_norm": 70841.9296875,
      "learning_rate": 4.3126934984520125e-05,
      "loss": 22.0601,
      "step": 11622
    },
    {
      "epoch": 11.63,
      "grad_norm": 13318.662109375,
      "learning_rate": 4.312177502579979e-05,
      "loss": 28.9111,
      "step": 11623
    },
    {
      "epoch": 11.64,
      "grad_norm": 10933.5751953125,
      "learning_rate": 4.311661506707946e-05,
      "loss": 23.8346,
      "step": 11624
    },
    {
      "epoch": 11.64,
      "grad_norm": 8785.9599609375,
      "learning_rate": 4.3111455108359135e-05,
      "loss": 19.9994,
      "step": 11625
    },
    {
      "epoch": 11.64,
      "grad_norm": 7496.69287109375,
      "learning_rate": 4.31062951496388e-05,
      "loss": 29.5363,
      "step": 11626
    },
    {
      "epoch": 11.64,
      "grad_norm": 18097.439453125,
      "learning_rate": 4.310113519091847e-05,
      "loss": 16.9881,
      "step": 11627
    },
    {
      "epoch": 11.64,
      "grad_norm": 7280.0595703125,
      "learning_rate": 4.3095975232198145e-05,
      "loss": 37.0296,
      "step": 11628
    },
    {
      "epoch": 11.64,
      "grad_norm": 7210.85498046875,
      "learning_rate": 4.309081527347781e-05,
      "loss": 15.7728,
      "step": 11629
    },
    {
      "epoch": 11.64,
      "grad_norm": 3281.3447265625,
      "learning_rate": 4.308565531475748e-05,
      "loss": 38.9899,
      "step": 11630
    },
    {
      "epoch": 11.64,
      "grad_norm": 13718.91015625,
      "learning_rate": 4.3080495356037155e-05,
      "loss": 28.1356,
      "step": 11631
    },
    {
      "epoch": 11.64,
      "grad_norm": 20150.89453125,
      "learning_rate": 4.307533539731682e-05,
      "loss": 16.503,
      "step": 11632
    },
    {
      "epoch": 11.64,
      "grad_norm": 6490.3037109375,
      "learning_rate": 4.30701754385965e-05,
      "loss": 29.064,
      "step": 11633
    },
    {
      "epoch": 11.65,
      "grad_norm": 16215.36328125,
      "learning_rate": 4.3065015479876165e-05,
      "loss": 23.214,
      "step": 11634
    },
    {
      "epoch": 11.65,
      "grad_norm": 17729.109375,
      "learning_rate": 4.305985552115583e-05,
      "loss": 36.8826,
      "step": 11635
    },
    {
      "epoch": 11.65,
      "grad_norm": 3329.495849609375,
      "learning_rate": 4.305469556243551e-05,
      "loss": 16.168,
      "step": 11636
    },
    {
      "epoch": 11.65,
      "grad_norm": 6687.353515625,
      "learning_rate": 4.3049535603715175e-05,
      "loss": 30.6161,
      "step": 11637
    },
    {
      "epoch": 11.65,
      "grad_norm": 4191.6279296875,
      "learning_rate": 4.304437564499484e-05,
      "loss": 21.7737,
      "step": 11638
    },
    {
      "epoch": 11.65,
      "grad_norm": 66765.1796875,
      "learning_rate": 4.303921568627452e-05,
      "loss": 33.7717,
      "step": 11639
    },
    {
      "epoch": 11.65,
      "grad_norm": 5614.85986328125,
      "learning_rate": 4.303405572755418e-05,
      "loss": 24.038,
      "step": 11640
    },
    {
      "epoch": 11.65,
      "grad_norm": 23146.78125,
      "learning_rate": 4.3028895768833846e-05,
      "loss": 30.4889,
      "step": 11641
    },
    {
      "epoch": 11.65,
      "grad_norm": 5743.81689453125,
      "learning_rate": 4.302373581011352e-05,
      "loss": 25.6595,
      "step": 11642
    },
    {
      "epoch": 11.65,
      "grad_norm": 5175.888671875,
      "learning_rate": 4.301857585139319e-05,
      "loss": 37.2114,
      "step": 11643
    },
    {
      "epoch": 11.66,
      "grad_norm": 15929.0859375,
      "learning_rate": 4.3013415892672856e-05,
      "loss": 24.601,
      "step": 11644
    },
    {
      "epoch": 11.66,
      "grad_norm": 14607.224609375,
      "learning_rate": 4.300825593395253e-05,
      "loss": 27.948,
      "step": 11645
    },
    {
      "epoch": 11.66,
      "grad_norm": 10365.9130859375,
      "learning_rate": 4.30030959752322e-05,
      "loss": 15.0938,
      "step": 11646
    },
    {
      "epoch": 11.66,
      "grad_norm": 11028.744140625,
      "learning_rate": 4.2997936016511866e-05,
      "loss": 34.2623,
      "step": 11647
    },
    {
      "epoch": 11.66,
      "grad_norm": 5610.22119140625,
      "learning_rate": 4.299277605779154e-05,
      "loss": 26.3824,
      "step": 11648
    },
    {
      "epoch": 11.66,
      "grad_norm": 9651.8095703125,
      "learning_rate": 4.298761609907121e-05,
      "loss": 30.0544,
      "step": 11649
    },
    {
      "epoch": 11.66,
      "grad_norm": 12599.5439453125,
      "learning_rate": 4.298245614035088e-05,
      "loss": 29.5256,
      "step": 11650
    },
    {
      "epoch": 11.66,
      "grad_norm": 1244.5445556640625,
      "learning_rate": 4.297729618163055e-05,
      "loss": 27.9072,
      "step": 11651
    },
    {
      "epoch": 11.66,
      "grad_norm": 4138.8330078125,
      "learning_rate": 4.297213622291022e-05,
      "loss": 24.731,
      "step": 11652
    },
    {
      "epoch": 11.66,
      "grad_norm": 17973.96875,
      "learning_rate": 4.296697626418989e-05,
      "loss": 28.0519,
      "step": 11653
    },
    {
      "epoch": 11.67,
      "grad_norm": 37479.7265625,
      "learning_rate": 4.296181630546956e-05,
      "loss": 26.363,
      "step": 11654
    },
    {
      "epoch": 11.67,
      "grad_norm": 16697.60546875,
      "learning_rate": 4.295665634674923e-05,
      "loss": 21.5291,
      "step": 11655
    },
    {
      "epoch": 11.67,
      "grad_norm": 11762.7509765625,
      "learning_rate": 4.29514963880289e-05,
      "loss": 29.695,
      "step": 11656
    },
    {
      "epoch": 11.67,
      "grad_norm": 8315.4169921875,
      "learning_rate": 4.294633642930857e-05,
      "loss": 21.1102,
      "step": 11657
    },
    {
      "epoch": 11.67,
      "grad_norm": 5845.154296875,
      "learning_rate": 4.294117647058823e-05,
      "loss": 25.9647,
      "step": 11658
    },
    {
      "epoch": 11.67,
      "grad_norm": 13825.646484375,
      "learning_rate": 4.2936016511867906e-05,
      "loss": 26.0246,
      "step": 11659
    },
    {
      "epoch": 11.67,
      "grad_norm": 1252.1883544921875,
      "learning_rate": 4.2930856553147574e-05,
      "loss": 22.6773,
      "step": 11660
    },
    {
      "epoch": 11.67,
      "grad_norm": 78296.1640625,
      "learning_rate": 4.292569659442724e-05,
      "loss": 35.3152,
      "step": 11661
    },
    {
      "epoch": 11.67,
      "grad_norm": 4036.277587890625,
      "learning_rate": 4.2920536635706916e-05,
      "loss": 23.4047,
      "step": 11662
    },
    {
      "epoch": 11.67,
      "grad_norm": 1437.536865234375,
      "learning_rate": 4.2915376676986584e-05,
      "loss": 32.9466,
      "step": 11663
    },
    {
      "epoch": 11.68,
      "grad_norm": 17309.052734375,
      "learning_rate": 4.291021671826626e-05,
      "loss": 27.6881,
      "step": 11664
    },
    {
      "epoch": 11.68,
      "grad_norm": 104543.6640625,
      "learning_rate": 4.2905056759545926e-05,
      "loss": 30.5513,
      "step": 11665
    },
    {
      "epoch": 11.68,
      "grad_norm": 5422.0966796875,
      "learning_rate": 4.2899896800825594e-05,
      "loss": 24.9203,
      "step": 11666
    },
    {
      "epoch": 11.68,
      "grad_norm": 6523.2744140625,
      "learning_rate": 4.289473684210527e-05,
      "loss": 32.0567,
      "step": 11667
    },
    {
      "epoch": 11.68,
      "grad_norm": 1192.0587158203125,
      "learning_rate": 4.2889576883384936e-05,
      "loss": 27.7914,
      "step": 11668
    },
    {
      "epoch": 11.68,
      "grad_norm": 21669.703125,
      "learning_rate": 4.2884416924664604e-05,
      "loss": 19.7611,
      "step": 11669
    },
    {
      "epoch": 11.68,
      "grad_norm": 19504.146484375,
      "learning_rate": 4.287925696594428e-05,
      "loss": 25.8631,
      "step": 11670
    },
    {
      "epoch": 11.68,
      "grad_norm": 153005.828125,
      "learning_rate": 4.2874097007223946e-05,
      "loss": 31.1692,
      "step": 11671
    },
    {
      "epoch": 11.68,
      "grad_norm": 16152.1279296875,
      "learning_rate": 4.2868937048503614e-05,
      "loss": 35.7834,
      "step": 11672
    },
    {
      "epoch": 11.68,
      "grad_norm": 5105.5712890625,
      "learning_rate": 4.286377708978329e-05,
      "loss": 20.9686,
      "step": 11673
    },
    {
      "epoch": 11.69,
      "grad_norm": 18953.033203125,
      "learning_rate": 4.2858617131062956e-05,
      "loss": 33.918,
      "step": 11674
    },
    {
      "epoch": 11.69,
      "grad_norm": 19591.306640625,
      "learning_rate": 4.2853457172342624e-05,
      "loss": 20.889,
      "step": 11675
    },
    {
      "epoch": 11.69,
      "grad_norm": 14533.5927734375,
      "learning_rate": 4.284829721362229e-05,
      "loss": 31.6779,
      "step": 11676
    },
    {
      "epoch": 11.69,
      "grad_norm": 6250.27685546875,
      "learning_rate": 4.284313725490196e-05,
      "loss": 32.5069,
      "step": 11677
    },
    {
      "epoch": 11.69,
      "grad_norm": 10591.9189453125,
      "learning_rate": 4.2837977296181634e-05,
      "loss": 16.9799,
      "step": 11678
    },
    {
      "epoch": 11.69,
      "grad_norm": 6941.2666015625,
      "learning_rate": 4.28328173374613e-05,
      "loss": 21.2053,
      "step": 11679
    },
    {
      "epoch": 11.69,
      "grad_norm": 19581.35546875,
      "learning_rate": 4.282765737874097e-05,
      "loss": 29.4652,
      "step": 11680
    },
    {
      "epoch": 11.69,
      "grad_norm": 7972.11474609375,
      "learning_rate": 4.2822497420020644e-05,
      "loss": 26.5602,
      "step": 11681
    },
    {
      "epoch": 11.69,
      "grad_norm": 11007.3447265625,
      "learning_rate": 4.281733746130031e-05,
      "loss": 26.9525,
      "step": 11682
    },
    {
      "epoch": 11.69,
      "grad_norm": 4828.84716796875,
      "learning_rate": 4.281217750257998e-05,
      "loss": 41.7752,
      "step": 11683
    },
    {
      "epoch": 11.7,
      "grad_norm": 7461.984375,
      "learning_rate": 4.2807017543859654e-05,
      "loss": 36.965,
      "step": 11684
    },
    {
      "epoch": 11.7,
      "grad_norm": 8660.689453125,
      "learning_rate": 4.280185758513932e-05,
      "loss": 23.4989,
      "step": 11685
    },
    {
      "epoch": 11.7,
      "grad_norm": 5103.4931640625,
      "learning_rate": 4.279669762641899e-05,
      "loss": 26.9369,
      "step": 11686
    },
    {
      "epoch": 11.7,
      "grad_norm": 4487.27783203125,
      "learning_rate": 4.2791537667698664e-05,
      "loss": 21.8481,
      "step": 11687
    },
    {
      "epoch": 11.7,
      "grad_norm": 44387.2890625,
      "learning_rate": 4.278637770897833e-05,
      "loss": 20.8352,
      "step": 11688
    },
    {
      "epoch": 11.7,
      "grad_norm": 7745.03759765625,
      "learning_rate": 4.2781217750258e-05,
      "loss": 25.567,
      "step": 11689
    },
    {
      "epoch": 11.7,
      "grad_norm": 32460.673828125,
      "learning_rate": 4.2776057791537674e-05,
      "loss": 35.999,
      "step": 11690
    },
    {
      "epoch": 11.7,
      "grad_norm": 40574.71875,
      "learning_rate": 4.277089783281734e-05,
      "loss": 24.1297,
      "step": 11691
    },
    {
      "epoch": 11.7,
      "grad_norm": 2111.067138671875,
      "learning_rate": 4.276573787409701e-05,
      "loss": 33.0747,
      "step": 11692
    },
    {
      "epoch": 11.7,
      "grad_norm": 23512.078125,
      "learning_rate": 4.2760577915376684e-05,
      "loss": 34.6335,
      "step": 11693
    },
    {
      "epoch": 11.71,
      "grad_norm": 8459.5166015625,
      "learning_rate": 4.2755417956656345e-05,
      "loss": 33.0208,
      "step": 11694
    },
    {
      "epoch": 11.71,
      "grad_norm": 4765.53515625,
      "learning_rate": 4.275025799793602e-05,
      "loss": 32.5274,
      "step": 11695
    },
    {
      "epoch": 11.71,
      "grad_norm": 15030.4072265625,
      "learning_rate": 4.274509803921569e-05,
      "loss": 29.3445,
      "step": 11696
    },
    {
      "epoch": 11.71,
      "grad_norm": 27202.197265625,
      "learning_rate": 4.2739938080495355e-05,
      "loss": 27.9108,
      "step": 11697
    },
    {
      "epoch": 11.71,
      "grad_norm": 17750.708984375,
      "learning_rate": 4.273477812177503e-05,
      "loss": 29.2699,
      "step": 11698
    },
    {
      "epoch": 11.71,
      "grad_norm": 3211.255615234375,
      "learning_rate": 4.27296181630547e-05,
      "loss": 25.1716,
      "step": 11699
    },
    {
      "epoch": 11.71,
      "grad_norm": 14734.1015625,
      "learning_rate": 4.2724458204334365e-05,
      "loss": 22.9276,
      "step": 11700
    },
    {
      "epoch": 11.71,
      "grad_norm": 30687.712890625,
      "learning_rate": 4.271929824561404e-05,
      "loss": 37.1936,
      "step": 11701
    },
    {
      "epoch": 11.71,
      "grad_norm": 23584.06640625,
      "learning_rate": 4.271413828689371e-05,
      "loss": 23.4934,
      "step": 11702
    },
    {
      "epoch": 11.71,
      "grad_norm": 12084.76171875,
      "learning_rate": 4.2708978328173375e-05,
      "loss": 21.9236,
      "step": 11703
    },
    {
      "epoch": 11.72,
      "grad_norm": 14117.94921875,
      "learning_rate": 4.270381836945305e-05,
      "loss": 21.8749,
      "step": 11704
    },
    {
      "epoch": 11.72,
      "grad_norm": 68197.203125,
      "learning_rate": 4.269865841073272e-05,
      "loss": 26.4853,
      "step": 11705
    },
    {
      "epoch": 11.72,
      "grad_norm": 13876.2060546875,
      "learning_rate": 4.2693498452012385e-05,
      "loss": 27.593,
      "step": 11706
    },
    {
      "epoch": 11.72,
      "grad_norm": 32762.396484375,
      "learning_rate": 4.268833849329206e-05,
      "loss": 22.6193,
      "step": 11707
    },
    {
      "epoch": 11.72,
      "grad_norm": 10982.4873046875,
      "learning_rate": 4.268317853457173e-05,
      "loss": 13.106,
      "step": 11708
    },
    {
      "epoch": 11.72,
      "grad_norm": 2751.196533203125,
      "learning_rate": 4.2678018575851395e-05,
      "loss": 30.3817,
      "step": 11709
    },
    {
      "epoch": 11.72,
      "grad_norm": 59285.90234375,
      "learning_rate": 4.267285861713107e-05,
      "loss": 40.8838,
      "step": 11710
    },
    {
      "epoch": 11.72,
      "grad_norm": 20808.546875,
      "learning_rate": 4.266769865841074e-05,
      "loss": 19.6971,
      "step": 11711
    },
    {
      "epoch": 11.72,
      "grad_norm": 15302.6083984375,
      "learning_rate": 4.2662538699690405e-05,
      "loss": 35.2557,
      "step": 11712
    },
    {
      "epoch": 11.72,
      "grad_norm": 11398.671875,
      "learning_rate": 4.265737874097007e-05,
      "loss": 24.5431,
      "step": 11713
    },
    {
      "epoch": 11.73,
      "grad_norm": 31717.595703125,
      "learning_rate": 4.265221878224974e-05,
      "loss": 26.8601,
      "step": 11714
    },
    {
      "epoch": 11.73,
      "grad_norm": 5581.44775390625,
      "learning_rate": 4.2647058823529415e-05,
      "loss": 24.5092,
      "step": 11715
    },
    {
      "epoch": 11.73,
      "grad_norm": 5986.4755859375,
      "learning_rate": 4.264189886480908e-05,
      "loss": 23.7857,
      "step": 11716
    },
    {
      "epoch": 11.73,
      "grad_norm": 10146.962890625,
      "learning_rate": 4.263673890608875e-05,
      "loss": 20.5784,
      "step": 11717
    },
    {
      "epoch": 11.73,
      "grad_norm": 5565.8369140625,
      "learning_rate": 4.2631578947368425e-05,
      "loss": 20.1925,
      "step": 11718
    },
    {
      "epoch": 11.73,
      "grad_norm": 15251.8759765625,
      "learning_rate": 4.262641898864809e-05,
      "loss": 25.6317,
      "step": 11719
    },
    {
      "epoch": 11.73,
      "grad_norm": 4884.73681640625,
      "learning_rate": 4.262125902992776e-05,
      "loss": 22.1564,
      "step": 11720
    },
    {
      "epoch": 11.73,
      "grad_norm": 31551.046875,
      "learning_rate": 4.2616099071207435e-05,
      "loss": 15.9257,
      "step": 11721
    },
    {
      "epoch": 11.73,
      "grad_norm": 5291.5498046875,
      "learning_rate": 4.26109391124871e-05,
      "loss": 32.5557,
      "step": 11722
    },
    {
      "epoch": 11.73,
      "grad_norm": 25819.041015625,
      "learning_rate": 4.260577915376677e-05,
      "loss": 38.6022,
      "step": 11723
    },
    {
      "epoch": 11.74,
      "grad_norm": 4450.7626953125,
      "learning_rate": 4.2600619195046445e-05,
      "loss": 27.3104,
      "step": 11724
    },
    {
      "epoch": 11.74,
      "grad_norm": 43015.38671875,
      "learning_rate": 4.259545923632611e-05,
      "loss": 26.2082,
      "step": 11725
    },
    {
      "epoch": 11.74,
      "grad_norm": 7058.44873046875,
      "learning_rate": 4.259029927760578e-05,
      "loss": 36.098,
      "step": 11726
    },
    {
      "epoch": 11.74,
      "grad_norm": 5277.54296875,
      "learning_rate": 4.2585139318885455e-05,
      "loss": 27.3011,
      "step": 11727
    },
    {
      "epoch": 11.74,
      "grad_norm": 7893.9423828125,
      "learning_rate": 4.257997936016512e-05,
      "loss": 31.0278,
      "step": 11728
    },
    {
      "epoch": 11.74,
      "grad_norm": 7759.9619140625,
      "learning_rate": 4.257481940144479e-05,
      "loss": 26.3204,
      "step": 11729
    },
    {
      "epoch": 11.74,
      "grad_norm": 11555.7646484375,
      "learning_rate": 4.256965944272446e-05,
      "loss": 32.5171,
      "step": 11730
    },
    {
      "epoch": 11.74,
      "grad_norm": 5951.49560546875,
      "learning_rate": 4.2564499484004126e-05,
      "loss": 32.2977,
      "step": 11731
    },
    {
      "epoch": 11.74,
      "grad_norm": 17123.640625,
      "learning_rate": 4.25593395252838e-05,
      "loss": 27.1682,
      "step": 11732
    },
    {
      "epoch": 11.74,
      "grad_norm": 636.8623046875,
      "learning_rate": 4.255417956656347e-05,
      "loss": 19.7781,
      "step": 11733
    },
    {
      "epoch": 11.75,
      "grad_norm": 35479.375,
      "learning_rate": 4.2549019607843136e-05,
      "loss": 37.1363,
      "step": 11734
    },
    {
      "epoch": 11.75,
      "grad_norm": 43555.94921875,
      "learning_rate": 4.254385964912281e-05,
      "loss": 41.1385,
      "step": 11735
    },
    {
      "epoch": 11.75,
      "grad_norm": 5598.91015625,
      "learning_rate": 4.253869969040248e-05,
      "loss": 35.5613,
      "step": 11736
    },
    {
      "epoch": 11.75,
      "grad_norm": 78617.4609375,
      "learning_rate": 4.2533539731682146e-05,
      "loss": 14.3682,
      "step": 11737
    },
    {
      "epoch": 11.75,
      "grad_norm": 21110.5546875,
      "learning_rate": 4.252837977296182e-05,
      "loss": 26.4291,
      "step": 11738
    },
    {
      "epoch": 11.75,
      "grad_norm": 170310.40625,
      "learning_rate": 4.252321981424149e-05,
      "loss": 18.834,
      "step": 11739
    },
    {
      "epoch": 11.75,
      "grad_norm": 1384.66455078125,
      "learning_rate": 4.2518059855521156e-05,
      "loss": 28.4842,
      "step": 11740
    },
    {
      "epoch": 11.75,
      "grad_norm": 15329.6015625,
      "learning_rate": 4.251289989680083e-05,
      "loss": 29.1173,
      "step": 11741
    },
    {
      "epoch": 11.75,
      "grad_norm": 5663.19091796875,
      "learning_rate": 4.25077399380805e-05,
      "loss": 24.7406,
      "step": 11742
    },
    {
      "epoch": 11.75,
      "grad_norm": 5585.89453125,
      "learning_rate": 4.2502579979360166e-05,
      "loss": 17.5708,
      "step": 11743
    },
    {
      "epoch": 11.76,
      "grad_norm": 7659.9267578125,
      "learning_rate": 4.249742002063984e-05,
      "loss": 28.8183,
      "step": 11744
    },
    {
      "epoch": 11.76,
      "grad_norm": 3304.791015625,
      "learning_rate": 4.249226006191951e-05,
      "loss": 14.1753,
      "step": 11745
    },
    {
      "epoch": 11.76,
      "grad_norm": 8562.0556640625,
      "learning_rate": 4.2487100103199176e-05,
      "loss": 24.777,
      "step": 11746
    },
    {
      "epoch": 11.76,
      "grad_norm": 13811.6689453125,
      "learning_rate": 4.248194014447885e-05,
      "loss": 22.9491,
      "step": 11747
    },
    {
      "epoch": 11.76,
      "grad_norm": 13659.7021484375,
      "learning_rate": 4.247678018575851e-05,
      "loss": 34.1917,
      "step": 11748
    },
    {
      "epoch": 11.76,
      "grad_norm": 13466.82421875,
      "learning_rate": 4.2471620227038186e-05,
      "loss": 25.7433,
      "step": 11749
    },
    {
      "epoch": 11.76,
      "grad_norm": 22112.681640625,
      "learning_rate": 4.2466460268317854e-05,
      "loss": 34.0086,
      "step": 11750
    },
    {
      "epoch": 11.76,
      "grad_norm": 25023.791015625,
      "learning_rate": 4.246130030959752e-05,
      "loss": 35.6868,
      "step": 11751
    },
    {
      "epoch": 11.76,
      "grad_norm": 5825.451171875,
      "learning_rate": 4.2456140350877196e-05,
      "loss": 28.6606,
      "step": 11752
    },
    {
      "epoch": 11.76,
      "grad_norm": 2808.49609375,
      "learning_rate": 4.2450980392156864e-05,
      "loss": 31.7315,
      "step": 11753
    },
    {
      "epoch": 11.77,
      "grad_norm": 2631.874267578125,
      "learning_rate": 4.244582043343653e-05,
      "loss": 31.7956,
      "step": 11754
    },
    {
      "epoch": 11.77,
      "grad_norm": 6860.154296875,
      "learning_rate": 4.2440660474716206e-05,
      "loss": 27.4378,
      "step": 11755
    },
    {
      "epoch": 11.77,
      "grad_norm": 15825.9912109375,
      "learning_rate": 4.2435500515995874e-05,
      "loss": 33.3199,
      "step": 11756
    },
    {
      "epoch": 11.77,
      "grad_norm": 36065.78125,
      "learning_rate": 4.243034055727554e-05,
      "loss": 31.6933,
      "step": 11757
    },
    {
      "epoch": 11.77,
      "grad_norm": 17255.595703125,
      "learning_rate": 4.2425180598555216e-05,
      "loss": 36.3871,
      "step": 11758
    },
    {
      "epoch": 11.77,
      "grad_norm": 12195.7705078125,
      "learning_rate": 4.2420020639834884e-05,
      "loss": 31.3423,
      "step": 11759
    },
    {
      "epoch": 11.77,
      "grad_norm": 21406.82421875,
      "learning_rate": 4.241486068111455e-05,
      "loss": 13.3866,
      "step": 11760
    },
    {
      "epoch": 11.77,
      "grad_norm": 4136.6611328125,
      "learning_rate": 4.2409700722394226e-05,
      "loss": 34.7555,
      "step": 11761
    },
    {
      "epoch": 11.77,
      "grad_norm": 10210.6103515625,
      "learning_rate": 4.2404540763673894e-05,
      "loss": 40.1773,
      "step": 11762
    },
    {
      "epoch": 11.77,
      "grad_norm": 26906.8671875,
      "learning_rate": 4.239938080495356e-05,
      "loss": 17.0326,
      "step": 11763
    },
    {
      "epoch": 11.78,
      "grad_norm": 5344.705078125,
      "learning_rate": 4.2394220846233236e-05,
      "loss": 33.3948,
      "step": 11764
    },
    {
      "epoch": 11.78,
      "grad_norm": 119401.8984375,
      "learning_rate": 4.23890608875129e-05,
      "loss": 31.8361,
      "step": 11765
    },
    {
      "epoch": 11.78,
      "grad_norm": 13261.359375,
      "learning_rate": 4.238390092879257e-05,
      "loss": 25.1001,
      "step": 11766
    },
    {
      "epoch": 11.78,
      "grad_norm": 6513.94189453125,
      "learning_rate": 4.237874097007224e-05,
      "loss": 27.5455,
      "step": 11767
    },
    {
      "epoch": 11.78,
      "grad_norm": 5471.04931640625,
      "learning_rate": 4.237358101135191e-05,
      "loss": 25.7996,
      "step": 11768
    },
    {
      "epoch": 11.78,
      "grad_norm": 24614.197265625,
      "learning_rate": 4.236842105263158e-05,
      "loss": 36.1131,
      "step": 11769
    },
    {
      "epoch": 11.78,
      "grad_norm": 24136.36328125,
      "learning_rate": 4.236326109391125e-05,
      "loss": 35.9138,
      "step": 11770
    },
    {
      "epoch": 11.78,
      "grad_norm": 8193.1201171875,
      "learning_rate": 4.235810113519092e-05,
      "loss": 39.6538,
      "step": 11771
    },
    {
      "epoch": 11.78,
      "grad_norm": 18558.076171875,
      "learning_rate": 4.235294117647059e-05,
      "loss": 27.2082,
      "step": 11772
    },
    {
      "epoch": 11.78,
      "grad_norm": 69846.03125,
      "learning_rate": 4.234778121775026e-05,
      "loss": 36.8872,
      "step": 11773
    },
    {
      "epoch": 11.79,
      "grad_norm": 8376.0966796875,
      "learning_rate": 4.234262125902993e-05,
      "loss": 37.5475,
      "step": 11774
    },
    {
      "epoch": 11.79,
      "grad_norm": 3092.176025390625,
      "learning_rate": 4.23374613003096e-05,
      "loss": 26.6662,
      "step": 11775
    },
    {
      "epoch": 11.79,
      "grad_norm": 45664.4765625,
      "learning_rate": 4.233230134158927e-05,
      "loss": 26.8729,
      "step": 11776
    },
    {
      "epoch": 11.79,
      "grad_norm": 32871.0859375,
      "learning_rate": 4.232714138286894e-05,
      "loss": 29.7285,
      "step": 11777
    },
    {
      "epoch": 11.79,
      "grad_norm": 14441.70703125,
      "learning_rate": 4.232198142414861e-05,
      "loss": 25.9202,
      "step": 11778
    },
    {
      "epoch": 11.79,
      "grad_norm": 5748.2548828125,
      "learning_rate": 4.231682146542828e-05,
      "loss": 28.202,
      "step": 11779
    },
    {
      "epoch": 11.79,
      "grad_norm": 10351.2412109375,
      "learning_rate": 4.2311661506707954e-05,
      "loss": 23.3658,
      "step": 11780
    },
    {
      "epoch": 11.79,
      "grad_norm": 12236.2900390625,
      "learning_rate": 4.230650154798762e-05,
      "loss": 35.6375,
      "step": 11781
    },
    {
      "epoch": 11.79,
      "grad_norm": 9387.9365234375,
      "learning_rate": 4.230134158926729e-05,
      "loss": 23.7489,
      "step": 11782
    },
    {
      "epoch": 11.79,
      "grad_norm": 2755.23681640625,
      "learning_rate": 4.229618163054696e-05,
      "loss": 25.7644,
      "step": 11783
    },
    {
      "epoch": 11.8,
      "grad_norm": 231919.96875,
      "learning_rate": 4.2291021671826625e-05,
      "loss": 25.6144,
      "step": 11784
    },
    {
      "epoch": 11.8,
      "grad_norm": 8405.9814453125,
      "learning_rate": 4.228586171310629e-05,
      "loss": 26.0026,
      "step": 11785
    },
    {
      "epoch": 11.8,
      "grad_norm": 37750.15625,
      "learning_rate": 4.228070175438597e-05,
      "loss": 24.5919,
      "step": 11786
    },
    {
      "epoch": 11.8,
      "grad_norm": 22815.533203125,
      "learning_rate": 4.2275541795665635e-05,
      "loss": 26.6453,
      "step": 11787
    },
    {
      "epoch": 11.8,
      "grad_norm": 88400.0390625,
      "learning_rate": 4.22703818369453e-05,
      "loss": 34.8643,
      "step": 11788
    },
    {
      "epoch": 11.8,
      "grad_norm": 30969.455078125,
      "learning_rate": 4.226522187822498e-05,
      "loss": 34.5217,
      "step": 11789
    },
    {
      "epoch": 11.8,
      "grad_norm": 18659.09375,
      "learning_rate": 4.2260061919504645e-05,
      "loss": 29.0357,
      "step": 11790
    },
    {
      "epoch": 11.8,
      "grad_norm": 273425.625,
      "learning_rate": 4.225490196078431e-05,
      "loss": 31.1047,
      "step": 11791
    },
    {
      "epoch": 11.8,
      "grad_norm": 27011.6640625,
      "learning_rate": 4.224974200206399e-05,
      "loss": 29.2729,
      "step": 11792
    },
    {
      "epoch": 11.8,
      "grad_norm": 6921.45751953125,
      "learning_rate": 4.2244582043343655e-05,
      "loss": 29.3874,
      "step": 11793
    },
    {
      "epoch": 11.81,
      "grad_norm": 8063.84375,
      "learning_rate": 4.223942208462333e-05,
      "loss": 36.562,
      "step": 11794
    },
    {
      "epoch": 11.81,
      "grad_norm": 39846.6875,
      "learning_rate": 4.2234262125903e-05,
      "loss": 28.2928,
      "step": 11795
    },
    {
      "epoch": 11.81,
      "grad_norm": 16517.05859375,
      "learning_rate": 4.2229102167182665e-05,
      "loss": 15.6592,
      "step": 11796
    },
    {
      "epoch": 11.81,
      "grad_norm": 119588.46875,
      "learning_rate": 4.222394220846234e-05,
      "loss": 21.9774,
      "step": 11797
    },
    {
      "epoch": 11.81,
      "grad_norm": 7721.939453125,
      "learning_rate": 4.221878224974201e-05,
      "loss": 24.7295,
      "step": 11798
    },
    {
      "epoch": 11.81,
      "grad_norm": 16095.4443359375,
      "learning_rate": 4.2213622291021675e-05,
      "loss": 37.1721,
      "step": 11799
    },
    {
      "epoch": 11.81,
      "grad_norm": 4255.50927734375,
      "learning_rate": 4.220846233230135e-05,
      "loss": 35.5747,
      "step": 11800
    },
    {
      "epoch": 11.81,
      "grad_norm": 13153.9130859375,
      "learning_rate": 4.220330237358101e-05,
      "loss": 23.3835,
      "step": 11801
    },
    {
      "epoch": 11.81,
      "grad_norm": 19568.140625,
      "learning_rate": 4.219814241486068e-05,
      "loss": 29.9611,
      "step": 11802
    },
    {
      "epoch": 11.81,
      "grad_norm": 35254.8671875,
      "learning_rate": 4.219298245614035e-05,
      "loss": 35.4042,
      "step": 11803
    },
    {
      "epoch": 11.82,
      "grad_norm": 15506.26171875,
      "learning_rate": 4.218782249742002e-05,
      "loss": 30.9972,
      "step": 11804
    },
    {
      "epoch": 11.82,
      "grad_norm": 10234.1162109375,
      "learning_rate": 4.218266253869969e-05,
      "loss": 29.6983,
      "step": 11805
    },
    {
      "epoch": 11.82,
      "grad_norm": 3291.49560546875,
      "learning_rate": 4.217750257997936e-05,
      "loss": 15.9934,
      "step": 11806
    },
    {
      "epoch": 11.82,
      "grad_norm": 1962.7337646484375,
      "learning_rate": 4.217234262125903e-05,
      "loss": 41.1144,
      "step": 11807
    },
    {
      "epoch": 11.82,
      "grad_norm": 26413.03515625,
      "learning_rate": 4.2167182662538705e-05,
      "loss": 18.471,
      "step": 11808
    },
    {
      "epoch": 11.82,
      "grad_norm": 3969.165771484375,
      "learning_rate": 4.216202270381837e-05,
      "loss": 19.8003,
      "step": 11809
    },
    {
      "epoch": 11.82,
      "grad_norm": 4994.71044921875,
      "learning_rate": 4.215686274509804e-05,
      "loss": 20.2305,
      "step": 11810
    },
    {
      "epoch": 11.82,
      "grad_norm": 13253.904296875,
      "learning_rate": 4.2151702786377715e-05,
      "loss": 21.2192,
      "step": 11811
    },
    {
      "epoch": 11.82,
      "grad_norm": 9820.8466796875,
      "learning_rate": 4.214654282765738e-05,
      "loss": 24.715,
      "step": 11812
    },
    {
      "epoch": 11.82,
      "grad_norm": 28746.455078125,
      "learning_rate": 4.214138286893705e-05,
      "loss": 26.4186,
      "step": 11813
    },
    {
      "epoch": 11.83,
      "grad_norm": 6701.2294921875,
      "learning_rate": 4.2136222910216725e-05,
      "loss": 17.8345,
      "step": 11814
    },
    {
      "epoch": 11.83,
      "grad_norm": 17118.056640625,
      "learning_rate": 4.213106295149639e-05,
      "loss": 19.325,
      "step": 11815
    },
    {
      "epoch": 11.83,
      "grad_norm": 16475.875,
      "learning_rate": 4.212590299277606e-05,
      "loss": 26.3009,
      "step": 11816
    },
    {
      "epoch": 11.83,
      "grad_norm": 3892.552490234375,
      "learning_rate": 4.2120743034055735e-05,
      "loss": 21.098,
      "step": 11817
    },
    {
      "epoch": 11.83,
      "grad_norm": 3128.98681640625,
      "learning_rate": 4.21155830753354e-05,
      "loss": 27.0822,
      "step": 11818
    },
    {
      "epoch": 11.83,
      "grad_norm": 173586.515625,
      "learning_rate": 4.211042311661506e-05,
      "loss": 35.7233,
      "step": 11819
    },
    {
      "epoch": 11.83,
      "grad_norm": 4854.55810546875,
      "learning_rate": 4.210526315789474e-05,
      "loss": 30.2857,
      "step": 11820
    },
    {
      "epoch": 11.83,
      "grad_norm": 5329.89892578125,
      "learning_rate": 4.2100103199174406e-05,
      "loss": 29.8317,
      "step": 11821
    },
    {
      "epoch": 11.83,
      "grad_norm": 5643.064453125,
      "learning_rate": 4.209494324045408e-05,
      "loss": 18.6395,
      "step": 11822
    },
    {
      "epoch": 11.83,
      "grad_norm": 3338.0908203125,
      "learning_rate": 4.208978328173375e-05,
      "loss": 27.692,
      "step": 11823
    },
    {
      "epoch": 11.84,
      "grad_norm": 8074.43896484375,
      "learning_rate": 4.2084623323013416e-05,
      "loss": 38.5729,
      "step": 11824
    },
    {
      "epoch": 11.84,
      "grad_norm": 45613.43359375,
      "learning_rate": 4.207946336429309e-05,
      "loss": 33.0152,
      "step": 11825
    },
    {
      "epoch": 11.84,
      "grad_norm": 12489.203125,
      "learning_rate": 4.207430340557276e-05,
      "loss": 21.8708,
      "step": 11826
    },
    {
      "epoch": 11.84,
      "grad_norm": 5940.2490234375,
      "learning_rate": 4.2069143446852426e-05,
      "loss": 31.309,
      "step": 11827
    },
    {
      "epoch": 11.84,
      "grad_norm": 4225.3505859375,
      "learning_rate": 4.20639834881321e-05,
      "loss": 42.5537,
      "step": 11828
    },
    {
      "epoch": 11.84,
      "grad_norm": 24536.5078125,
      "learning_rate": 4.205882352941177e-05,
      "loss": 27.5808,
      "step": 11829
    },
    {
      "epoch": 11.84,
      "grad_norm": 79682.765625,
      "learning_rate": 4.2053663570691436e-05,
      "loss": 19.6824,
      "step": 11830
    },
    {
      "epoch": 11.84,
      "grad_norm": 4019.555419921875,
      "learning_rate": 4.204850361197111e-05,
      "loss": 22.267,
      "step": 11831
    },
    {
      "epoch": 11.84,
      "grad_norm": 4414.37255859375,
      "learning_rate": 4.204334365325078e-05,
      "loss": 15.9049,
      "step": 11832
    },
    {
      "epoch": 11.84,
      "grad_norm": 31978.271484375,
      "learning_rate": 4.2038183694530446e-05,
      "loss": 29.4489,
      "step": 11833
    },
    {
      "epoch": 11.85,
      "grad_norm": 38094.06640625,
      "learning_rate": 4.203302373581012e-05,
      "loss": 18.2395,
      "step": 11834
    },
    {
      "epoch": 11.85,
      "grad_norm": 18696.693359375,
      "learning_rate": 4.202786377708979e-05,
      "loss": 31.7885,
      "step": 11835
    },
    {
      "epoch": 11.85,
      "grad_norm": 9356.875,
      "learning_rate": 4.2022703818369456e-05,
      "loss": 25.301,
      "step": 11836
    },
    {
      "epoch": 11.85,
      "grad_norm": 43554.90234375,
      "learning_rate": 4.201754385964912e-05,
      "loss": 22.0158,
      "step": 11837
    },
    {
      "epoch": 11.85,
      "grad_norm": 14520.5751953125,
      "learning_rate": 4.201238390092879e-05,
      "loss": 25.0224,
      "step": 11838
    },
    {
      "epoch": 11.85,
      "grad_norm": 8070.23779296875,
      "learning_rate": 4.2007223942208466e-05,
      "loss": 33.4101,
      "step": 11839
    },
    {
      "epoch": 11.85,
      "grad_norm": 9893.8388671875,
      "learning_rate": 4.200206398348813e-05,
      "loss": 21.5677,
      "step": 11840
    },
    {
      "epoch": 11.85,
      "grad_norm": 10126.69921875,
      "learning_rate": 4.19969040247678e-05,
      "loss": 27.9188,
      "step": 11841
    },
    {
      "epoch": 11.85,
      "grad_norm": 16238.8466796875,
      "learning_rate": 4.1991744066047476e-05,
      "loss": 28.5292,
      "step": 11842
    },
    {
      "epoch": 11.85,
      "grad_norm": 14308.6376953125,
      "learning_rate": 4.198658410732714e-05,
      "loss": 22.5973,
      "step": 11843
    },
    {
      "epoch": 11.86,
      "grad_norm": 10530.0029296875,
      "learning_rate": 4.198142414860681e-05,
      "loss": 39.1541,
      "step": 11844
    },
    {
      "epoch": 11.86,
      "grad_norm": 11960.8544921875,
      "learning_rate": 4.1976264189886486e-05,
      "loss": 54.6418,
      "step": 11845
    },
    {
      "epoch": 11.86,
      "grad_norm": 16159.2060546875,
      "learning_rate": 4.197110423116615e-05,
      "loss": 25.778,
      "step": 11846
    },
    {
      "epoch": 11.86,
      "grad_norm": 36542.2265625,
      "learning_rate": 4.196594427244582e-05,
      "loss": 27.008,
      "step": 11847
    },
    {
      "epoch": 11.86,
      "grad_norm": 9566.408203125,
      "learning_rate": 4.1960784313725496e-05,
      "loss": 19.9358,
      "step": 11848
    },
    {
      "epoch": 11.86,
      "grad_norm": 6728.09326171875,
      "learning_rate": 4.195562435500516e-05,
      "loss": 23.5612,
      "step": 11849
    },
    {
      "epoch": 11.86,
      "grad_norm": 3546.22802734375,
      "learning_rate": 4.195046439628483e-05,
      "loss": 31.5656,
      "step": 11850
    },
    {
      "epoch": 11.86,
      "grad_norm": 86451.34375,
      "learning_rate": 4.1945304437564506e-05,
      "loss": 37.0999,
      "step": 11851
    },
    {
      "epoch": 11.86,
      "grad_norm": 32494.72265625,
      "learning_rate": 4.194014447884417e-05,
      "loss": 25.6742,
      "step": 11852
    },
    {
      "epoch": 11.86,
      "grad_norm": 13624.306640625,
      "learning_rate": 4.193498452012384e-05,
      "loss": 30.5396,
      "step": 11853
    },
    {
      "epoch": 11.87,
      "grad_norm": 20117.439453125,
      "learning_rate": 4.1929824561403516e-05,
      "loss": 26.743,
      "step": 11854
    },
    {
      "epoch": 11.87,
      "grad_norm": 6960.8095703125,
      "learning_rate": 4.1924664602683177e-05,
      "loss": 43.9708,
      "step": 11855
    },
    {
      "epoch": 11.87,
      "grad_norm": 2716.33642578125,
      "learning_rate": 4.191950464396285e-05,
      "loss": 33.3857,
      "step": 11856
    },
    {
      "epoch": 11.87,
      "grad_norm": 5796.2119140625,
      "learning_rate": 4.191434468524252e-05,
      "loss": 21.8635,
      "step": 11857
    },
    {
      "epoch": 11.87,
      "grad_norm": 21013.9609375,
      "learning_rate": 4.1909184726522187e-05,
      "loss": 27.3699,
      "step": 11858
    },
    {
      "epoch": 11.87,
      "grad_norm": 4820.72314453125,
      "learning_rate": 4.190402476780186e-05,
      "loss": 18.8908,
      "step": 11859
    },
    {
      "epoch": 11.87,
      "grad_norm": 2168.230712890625,
      "learning_rate": 4.189886480908153e-05,
      "loss": 25.2952,
      "step": 11860
    },
    {
      "epoch": 11.87,
      "grad_norm": 1840600.125,
      "learning_rate": 4.1893704850361197e-05,
      "loss": 18.489,
      "step": 11861
    },
    {
      "epoch": 11.87,
      "grad_norm": 12521.6455078125,
      "learning_rate": 4.188854489164087e-05,
      "loss": 43.5328,
      "step": 11862
    },
    {
      "epoch": 11.87,
      "grad_norm": 5617.22900390625,
      "learning_rate": 4.188338493292054e-05,
      "loss": 30.63,
      "step": 11863
    },
    {
      "epoch": 11.88,
      "grad_norm": 10751.521484375,
      "learning_rate": 4.1878224974200207e-05,
      "loss": 27.4846,
      "step": 11864
    },
    {
      "epoch": 11.88,
      "grad_norm": 16789.26953125,
      "learning_rate": 4.187306501547988e-05,
      "loss": 24.0609,
      "step": 11865
    },
    {
      "epoch": 11.88,
      "grad_norm": 3346.7919921875,
      "learning_rate": 4.186790505675955e-05,
      "loss": 29.2907,
      "step": 11866
    },
    {
      "epoch": 11.88,
      "grad_norm": 33454.859375,
      "learning_rate": 4.1862745098039217e-05,
      "loss": 24.2632,
      "step": 11867
    },
    {
      "epoch": 11.88,
      "grad_norm": 22533.36328125,
      "learning_rate": 4.185758513931889e-05,
      "loss": 29.4564,
      "step": 11868
    },
    {
      "epoch": 11.88,
      "grad_norm": 20967.689453125,
      "learning_rate": 4.185242518059856e-05,
      "loss": 28.1086,
      "step": 11869
    },
    {
      "epoch": 11.88,
      "grad_norm": 7578.296875,
      "learning_rate": 4.1847265221878227e-05,
      "loss": 32.9191,
      "step": 11870
    },
    {
      "epoch": 11.88,
      "grad_norm": 6270.279296875,
      "learning_rate": 4.18421052631579e-05,
      "loss": 25.3711,
      "step": 11871
    },
    {
      "epoch": 11.88,
      "grad_norm": 94781.9140625,
      "learning_rate": 4.183694530443757e-05,
      "loss": 44.5179,
      "step": 11872
    },
    {
      "epoch": 11.88,
      "grad_norm": 8294.1826171875,
      "learning_rate": 4.1831785345717237e-05,
      "loss": 22.5012,
      "step": 11873
    },
    {
      "epoch": 11.89,
      "grad_norm": 8520.630859375,
      "learning_rate": 4.1826625386996904e-05,
      "loss": 29.7152,
      "step": 11874
    },
    {
      "epoch": 11.89,
      "grad_norm": 8245.021484375,
      "learning_rate": 4.182146542827657e-05,
      "loss": 33.542,
      "step": 11875
    },
    {
      "epoch": 11.89,
      "grad_norm": 6371.02490234375,
      "learning_rate": 4.1816305469556247e-05,
      "loss": 33.7338,
      "step": 11876
    },
    {
      "epoch": 11.89,
      "grad_norm": 4779.83544921875,
      "learning_rate": 4.1811145510835914e-05,
      "loss": 27.1377,
      "step": 11877
    },
    {
      "epoch": 11.89,
      "grad_norm": 12672.5185546875,
      "learning_rate": 4.180598555211558e-05,
      "loss": 26.8208,
      "step": 11878
    },
    {
      "epoch": 11.89,
      "grad_norm": 3054.20849609375,
      "learning_rate": 4.1800825593395257e-05,
      "loss": 24.9667,
      "step": 11879
    },
    {
      "epoch": 11.89,
      "grad_norm": 7579.67431640625,
      "learning_rate": 4.1795665634674924e-05,
      "loss": 22.8568,
      "step": 11880
    },
    {
      "epoch": 11.89,
      "grad_norm": 8267.3466796875,
      "learning_rate": 4.179050567595459e-05,
      "loss": 26.5513,
      "step": 11881
    },
    {
      "epoch": 11.89,
      "grad_norm": 11453.912109375,
      "learning_rate": 4.1785345717234267e-05,
      "loss": 22.8844,
      "step": 11882
    },
    {
      "epoch": 11.89,
      "grad_norm": 47082.05078125,
      "learning_rate": 4.1780185758513934e-05,
      "loss": 21.1788,
      "step": 11883
    },
    {
      "epoch": 11.9,
      "grad_norm": 9946.83984375,
      "learning_rate": 4.17750257997936e-05,
      "loss": 32.6864,
      "step": 11884
    },
    {
      "epoch": 11.9,
      "grad_norm": 10824.13671875,
      "learning_rate": 4.1769865841073277e-05,
      "loss": 28.9855,
      "step": 11885
    },
    {
      "epoch": 11.9,
      "grad_norm": 12616.1826171875,
      "learning_rate": 4.1764705882352944e-05,
      "loss": 28.7984,
      "step": 11886
    },
    {
      "epoch": 11.9,
      "grad_norm": 6075.75,
      "learning_rate": 4.175954592363261e-05,
      "loss": 35.151,
      "step": 11887
    },
    {
      "epoch": 11.9,
      "grad_norm": 47174.32421875,
      "learning_rate": 4.1754385964912287e-05,
      "loss": 23.9768,
      "step": 11888
    },
    {
      "epoch": 11.9,
      "grad_norm": 20942.677734375,
      "learning_rate": 4.1749226006191954e-05,
      "loss": 36.5241,
      "step": 11889
    },
    {
      "epoch": 11.9,
      "grad_norm": 15391.6455078125,
      "learning_rate": 4.174406604747162e-05,
      "loss": 33.2663,
      "step": 11890
    },
    {
      "epoch": 11.9,
      "grad_norm": 105906.734375,
      "learning_rate": 4.173890608875129e-05,
      "loss": 23.7695,
      "step": 11891
    },
    {
      "epoch": 11.9,
      "grad_norm": 5563.38330078125,
      "learning_rate": 4.173374613003096e-05,
      "loss": 31.2263,
      "step": 11892
    },
    {
      "epoch": 11.9,
      "grad_norm": 14024.91015625,
      "learning_rate": 4.172858617131063e-05,
      "loss": 23.8067,
      "step": 11893
    },
    {
      "epoch": 11.91,
      "grad_norm": 2244.25,
      "learning_rate": 4.17234262125903e-05,
      "loss": 36.7599,
      "step": 11894
    },
    {
      "epoch": 11.91,
      "grad_norm": 27948.546875,
      "learning_rate": 4.171826625386997e-05,
      "loss": 21.2115,
      "step": 11895
    },
    {
      "epoch": 11.91,
      "grad_norm": 31785.326171875,
      "learning_rate": 4.171310629514964e-05,
      "loss": 16.3566,
      "step": 11896
    },
    {
      "epoch": 11.91,
      "grad_norm": 17692.76953125,
      "learning_rate": 4.170794633642931e-05,
      "loss": 39.6925,
      "step": 11897
    },
    {
      "epoch": 11.91,
      "grad_norm": 14202.9267578125,
      "learning_rate": 4.170278637770898e-05,
      "loss": 17.5556,
      "step": 11898
    },
    {
      "epoch": 11.91,
      "grad_norm": 5596.6416015625,
      "learning_rate": 4.169762641898865e-05,
      "loss": 26.0287,
      "step": 11899
    },
    {
      "epoch": 11.91,
      "grad_norm": 55902.9375,
      "learning_rate": 4.169246646026832e-05,
      "loss": 32.1266,
      "step": 11900
    },
    {
      "epoch": 11.91,
      "grad_norm": 25359.06640625,
      "learning_rate": 4.168730650154799e-05,
      "loss": 39.3064,
      "step": 11901
    },
    {
      "epoch": 11.91,
      "grad_norm": 3393.486572265625,
      "learning_rate": 4.168214654282766e-05,
      "loss": 23.2244,
      "step": 11902
    },
    {
      "epoch": 11.91,
      "grad_norm": 1679.9940185546875,
      "learning_rate": 4.167698658410733e-05,
      "loss": 22.0954,
      "step": 11903
    },
    {
      "epoch": 11.92,
      "grad_norm": 3099.15673828125,
      "learning_rate": 4.1671826625387e-05,
      "loss": 24.6348,
      "step": 11904
    },
    {
      "epoch": 11.92,
      "grad_norm": 53995.23046875,
      "learning_rate": 4.166666666666667e-05,
      "loss": 30.2104,
      "step": 11905
    },
    {
      "epoch": 11.92,
      "grad_norm": 4027.40966796875,
      "learning_rate": 4.166150670794634e-05,
      "loss": 18.9921,
      "step": 11906
    },
    {
      "epoch": 11.92,
      "grad_norm": 2497.419189453125,
      "learning_rate": 4.165634674922601e-05,
      "loss": 24.5764,
      "step": 11907
    },
    {
      "epoch": 11.92,
      "grad_norm": 37374.43359375,
      "learning_rate": 4.1651186790505675e-05,
      "loss": 21.4056,
      "step": 11908
    },
    {
      "epoch": 11.92,
      "grad_norm": 21183.564453125,
      "learning_rate": 4.164602683178534e-05,
      "loss": 23.0661,
      "step": 11909
    },
    {
      "epoch": 11.92,
      "grad_norm": 64243.59375,
      "learning_rate": 4.164086687306502e-05,
      "loss": 32.9373,
      "step": 11910
    },
    {
      "epoch": 11.92,
      "grad_norm": 30601.45703125,
      "learning_rate": 4.1635706914344685e-05,
      "loss": 34.2148,
      "step": 11911
    },
    {
      "epoch": 11.92,
      "grad_norm": 8720.5830078125,
      "learning_rate": 4.163054695562435e-05,
      "loss": 22.7903,
      "step": 11912
    },
    {
      "epoch": 11.92,
      "grad_norm": 12330.7890625,
      "learning_rate": 4.162538699690403e-05,
      "loss": 38.5164,
      "step": 11913
    },
    {
      "epoch": 11.93,
      "grad_norm": 8742.12109375,
      "learning_rate": 4.1620227038183695e-05,
      "loss": 28.2593,
      "step": 11914
    },
    {
      "epoch": 11.93,
      "grad_norm": 6675.75244140625,
      "learning_rate": 4.161506707946336e-05,
      "loss": 43.9602,
      "step": 11915
    },
    {
      "epoch": 11.93,
      "grad_norm": 34107.85546875,
      "learning_rate": 4.160990712074304e-05,
      "loss": 22.8926,
      "step": 11916
    },
    {
      "epoch": 11.93,
      "grad_norm": 17495.8125,
      "learning_rate": 4.1604747162022705e-05,
      "loss": 32.4106,
      "step": 11917
    },
    {
      "epoch": 11.93,
      "grad_norm": 8019.79931640625,
      "learning_rate": 4.159958720330237e-05,
      "loss": 38.8113,
      "step": 11918
    },
    {
      "epoch": 11.93,
      "grad_norm": 6041.62890625,
      "learning_rate": 4.159442724458205e-05,
      "loss": 40.1169,
      "step": 11919
    },
    {
      "epoch": 11.93,
      "grad_norm": 11620.7568359375,
      "learning_rate": 4.1589267285861715e-05,
      "loss": 27.9771,
      "step": 11920
    },
    {
      "epoch": 11.93,
      "grad_norm": 10235.8759765625,
      "learning_rate": 4.158410732714138e-05,
      "loss": 24.4411,
      "step": 11921
    },
    {
      "epoch": 11.93,
      "grad_norm": 8088.388671875,
      "learning_rate": 4.157894736842106e-05,
      "loss": 18.2518,
      "step": 11922
    },
    {
      "epoch": 11.93,
      "grad_norm": 14381.7197265625,
      "learning_rate": 4.1573787409700725e-05,
      "loss": 22.3014,
      "step": 11923
    },
    {
      "epoch": 11.94,
      "grad_norm": 10419.5810546875,
      "learning_rate": 4.156862745098039e-05,
      "loss": 39.4536,
      "step": 11924
    },
    {
      "epoch": 11.94,
      "grad_norm": 21507.427734375,
      "learning_rate": 4.156346749226007e-05,
      "loss": 34.0978,
      "step": 11925
    },
    {
      "epoch": 11.94,
      "grad_norm": 93630.546875,
      "learning_rate": 4.155830753353973e-05,
      "loss": 25.8973,
      "step": 11926
    },
    {
      "epoch": 11.94,
      "grad_norm": 10918.65234375,
      "learning_rate": 4.15531475748194e-05,
      "loss": 31.0213,
      "step": 11927
    },
    {
      "epoch": 11.94,
      "grad_norm": 43975.99609375,
      "learning_rate": 4.154798761609907e-05,
      "loss": 28.0116,
      "step": 11928
    },
    {
      "epoch": 11.94,
      "grad_norm": 24188.603515625,
      "learning_rate": 4.154282765737874e-05,
      "loss": 19.6479,
      "step": 11929
    },
    {
      "epoch": 11.94,
      "grad_norm": 4408.1279296875,
      "learning_rate": 4.153766769865841e-05,
      "loss": 27.1388,
      "step": 11930
    },
    {
      "epoch": 11.94,
      "grad_norm": 4928.73388671875,
      "learning_rate": 4.153250773993808e-05,
      "loss": 34.5597,
      "step": 11931
    },
    {
      "epoch": 11.94,
      "grad_norm": 4173.123046875,
      "learning_rate": 4.152734778121775e-05,
      "loss": 26.636,
      "step": 11932
    },
    {
      "epoch": 11.94,
      "grad_norm": 4460.32470703125,
      "learning_rate": 4.152218782249742e-05,
      "loss": 22.699,
      "step": 11933
    },
    {
      "epoch": 11.95,
      "grad_norm": 6013.43212890625,
      "learning_rate": 4.151702786377709e-05,
      "loss": 24.5474,
      "step": 11934
    },
    {
      "epoch": 11.95,
      "grad_norm": 52408.4140625,
      "learning_rate": 4.151186790505676e-05,
      "loss": 33.8474,
      "step": 11935
    },
    {
      "epoch": 11.95,
      "grad_norm": 40041.22265625,
      "learning_rate": 4.150670794633643e-05,
      "loss": 24.6384,
      "step": 11936
    },
    {
      "epoch": 11.95,
      "grad_norm": 6289.609375,
      "learning_rate": 4.15015479876161e-05,
      "loss": 29.9384,
      "step": 11937
    },
    {
      "epoch": 11.95,
      "grad_norm": 18394.541015625,
      "learning_rate": 4.149638802889577e-05,
      "loss": 22.1549,
      "step": 11938
    },
    {
      "epoch": 11.95,
      "grad_norm": 25012.416015625,
      "learning_rate": 4.149122807017544e-05,
      "loss": 24.4547,
      "step": 11939
    },
    {
      "epoch": 11.95,
      "grad_norm": 7119.6201171875,
      "learning_rate": 4.148606811145511e-05,
      "loss": 23.3445,
      "step": 11940
    },
    {
      "epoch": 11.95,
      "grad_norm": 4535.1533203125,
      "learning_rate": 4.1480908152734785e-05,
      "loss": 21.8288,
      "step": 11941
    },
    {
      "epoch": 11.95,
      "grad_norm": 1757.3892822265625,
      "learning_rate": 4.147574819401445e-05,
      "loss": 26.1811,
      "step": 11942
    },
    {
      "epoch": 11.95,
      "grad_norm": 16193.037109375,
      "learning_rate": 4.147058823529412e-05,
      "loss": 25.9102,
      "step": 11943
    },
    {
      "epoch": 11.96,
      "grad_norm": 10336.9794921875,
      "learning_rate": 4.146542827657379e-05,
      "loss": 29.3917,
      "step": 11944
    },
    {
      "epoch": 11.96,
      "grad_norm": 11952.0634765625,
      "learning_rate": 4.1460268317853456e-05,
      "loss": 30.5467,
      "step": 11945
    },
    {
      "epoch": 11.96,
      "grad_norm": 11407.337890625,
      "learning_rate": 4.1455108359133124e-05,
      "loss": 22.1176,
      "step": 11946
    },
    {
      "epoch": 11.96,
      "grad_norm": 5872.5478515625,
      "learning_rate": 4.14499484004128e-05,
      "loss": 26.1558,
      "step": 11947
    },
    {
      "epoch": 11.96,
      "grad_norm": 41213.40625,
      "learning_rate": 4.1444788441692466e-05,
      "loss": 26.1962,
      "step": 11948
    },
    {
      "epoch": 11.96,
      "grad_norm": 2595.896728515625,
      "learning_rate": 4.1439628482972134e-05,
      "loss": 18.4483,
      "step": 11949
    },
    {
      "epoch": 11.96,
      "grad_norm": 3397.26318359375,
      "learning_rate": 4.143446852425181e-05,
      "loss": 18.6552,
      "step": 11950
    },
    {
      "epoch": 11.96,
      "grad_norm": 53079.36328125,
      "learning_rate": 4.1429308565531476e-05,
      "loss": 21.0538,
      "step": 11951
    },
    {
      "epoch": 11.96,
      "grad_norm": 3268.625732421875,
      "learning_rate": 4.1424148606811144e-05,
      "loss": 19.5816,
      "step": 11952
    },
    {
      "epoch": 11.96,
      "grad_norm": 10811.48828125,
      "learning_rate": 4.141898864809082e-05,
      "loss": 16.0784,
      "step": 11953
    },
    {
      "epoch": 11.97,
      "grad_norm": 12828.431640625,
      "learning_rate": 4.1413828689370486e-05,
      "loss": 51.348,
      "step": 11954
    },
    {
      "epoch": 11.97,
      "grad_norm": 12037.68359375,
      "learning_rate": 4.140866873065016e-05,
      "loss": 29.2279,
      "step": 11955
    },
    {
      "epoch": 11.97,
      "grad_norm": 11140.6318359375,
      "learning_rate": 4.140350877192983e-05,
      "loss": 32.716,
      "step": 11956
    },
    {
      "epoch": 11.97,
      "grad_norm": 9981.2509765625,
      "learning_rate": 4.1398348813209496e-05,
      "loss": 21.348,
      "step": 11957
    },
    {
      "epoch": 11.97,
      "grad_norm": 14674.5078125,
      "learning_rate": 4.139318885448917e-05,
      "loss": 28.7014,
      "step": 11958
    },
    {
      "epoch": 11.97,
      "grad_norm": 17201.462890625,
      "learning_rate": 4.138802889576884e-05,
      "loss": 22.2369,
      "step": 11959
    },
    {
      "epoch": 11.97,
      "grad_norm": 63716.40234375,
      "learning_rate": 4.1382868937048506e-05,
      "loss": 28.7902,
      "step": 11960
    },
    {
      "epoch": 11.97,
      "grad_norm": 11059.0263671875,
      "learning_rate": 4.137770897832818e-05,
      "loss": 23.8136,
      "step": 11961
    },
    {
      "epoch": 11.97,
      "grad_norm": 13225.5302734375,
      "learning_rate": 4.137254901960784e-05,
      "loss": 37.6715,
      "step": 11962
    },
    {
      "epoch": 11.97,
      "grad_norm": 424030.6875,
      "learning_rate": 4.136738906088751e-05,
      "loss": 43.2644,
      "step": 11963
    },
    {
      "epoch": 11.98,
      "grad_norm": 8674.7041015625,
      "learning_rate": 4.1362229102167184e-05,
      "loss": 14.7127,
      "step": 11964
    },
    {
      "epoch": 11.98,
      "grad_norm": 13043.9560546875,
      "learning_rate": 4.135706914344685e-05,
      "loss": 34.2471,
      "step": 11965
    },
    {
      "epoch": 11.98,
      "grad_norm": 7986.638671875,
      "learning_rate": 4.135190918472652e-05,
      "loss": 25.214,
      "step": 11966
    },
    {
      "epoch": 11.98,
      "grad_norm": 9292.126953125,
      "learning_rate": 4.1346749226006194e-05,
      "loss": 19.7157,
      "step": 11967
    },
    {
      "epoch": 11.98,
      "grad_norm": 32174.427734375,
      "learning_rate": 4.134158926728586e-05,
      "loss": 32.3961,
      "step": 11968
    },
    {
      "epoch": 11.98,
      "grad_norm": 6909.73291015625,
      "learning_rate": 4.1336429308565536e-05,
      "loss": 25.882,
      "step": 11969
    },
    {
      "epoch": 11.98,
      "grad_norm": 8706.2060546875,
      "learning_rate": 4.1331269349845204e-05,
      "loss": 36.51,
      "step": 11970
    },
    {
      "epoch": 11.98,
      "grad_norm": 31775.515625,
      "learning_rate": 4.132610939112487e-05,
      "loss": 35.7646,
      "step": 11971
    },
    {
      "epoch": 11.98,
      "grad_norm": 24433.6875,
      "learning_rate": 4.1320949432404546e-05,
      "loss": 21.5538,
      "step": 11972
    },
    {
      "epoch": 11.98,
      "grad_norm": 22226.16015625,
      "learning_rate": 4.1315789473684214e-05,
      "loss": 43.7943,
      "step": 11973
    },
    {
      "epoch": 11.99,
      "grad_norm": 7454.37109375,
      "learning_rate": 4.131062951496388e-05,
      "loss": 29.8835,
      "step": 11974
    },
    {
      "epoch": 11.99,
      "grad_norm": 17053.20703125,
      "learning_rate": 4.1305469556243556e-05,
      "loss": 30.6863,
      "step": 11975
    },
    {
      "epoch": 11.99,
      "grad_norm": 1845.989013671875,
      "learning_rate": 4.1300309597523224e-05,
      "loss": 33.7856,
      "step": 11976
    },
    {
      "epoch": 11.99,
      "grad_norm": 5595.82666015625,
      "learning_rate": 4.129514963880289e-05,
      "loss": 28.4037,
      "step": 11977
    },
    {
      "epoch": 11.99,
      "grad_norm": 6244.5673828125,
      "learning_rate": 4.1289989680082566e-05,
      "loss": 34.4914,
      "step": 11978
    },
    {
      "epoch": 11.99,
      "grad_norm": 10257.6259765625,
      "learning_rate": 4.1284829721362234e-05,
      "loss": 32.7955,
      "step": 11979
    },
    {
      "epoch": 11.99,
      "grad_norm": 30841.873046875,
      "learning_rate": 4.1279669762641895e-05,
      "loss": 32.8811,
      "step": 11980
    },
    {
      "epoch": 11.99,
      "grad_norm": 14341.1640625,
      "learning_rate": 4.127450980392157e-05,
      "loss": 23.715,
      "step": 11981
    },
    {
      "epoch": 11.99,
      "grad_norm": 883571.6875,
      "learning_rate": 4.126934984520124e-05,
      "loss": 37.3363,
      "step": 11982
    },
    {
      "epoch": 11.99,
      "grad_norm": 18012.025390625,
      "learning_rate": 4.126418988648091e-05,
      "loss": 28.9387,
      "step": 11983
    },
    {
      "epoch": 12.0,
      "grad_norm": 4941.41162109375,
      "learning_rate": 4.125902992776058e-05,
      "loss": 20.1363,
      "step": 11984
    },
    {
      "epoch": 12.0,
      "grad_norm": 5582.94921875,
      "learning_rate": 4.125386996904025e-05,
      "loss": 17.8986,
      "step": 11985
    },
    {
      "epoch": 12.0,
      "grad_norm": 10224.814453125,
      "learning_rate": 4.124871001031992e-05,
      "loss": 34.829,
      "step": 11986
    },
    {
      "epoch": 12.0,
      "grad_norm": 14150.359375,
      "learning_rate": 4.124355005159959e-05,
      "loss": 27.6162,
      "step": 11987
    },
    {
      "epoch": 12.0,
      "grad_norm": 131858.65625,
      "learning_rate": 4.123839009287926e-05,
      "loss": 32.5983,
      "step": 11988
    },
    {
      "epoch": 12.0,
      "grad_norm": 15731.431640625,
      "learning_rate": 4.123323013415893e-05,
      "loss": 33.5359,
      "step": 11989
    },
    {
      "epoch": 12.0,
      "grad_norm": 16030.671875,
      "learning_rate": 4.12280701754386e-05,
      "loss": 23.7017,
      "step": 11990
    },
    {
      "epoch": 12.0,
      "grad_norm": 45591.3203125,
      "learning_rate": 4.122291021671827e-05,
      "loss": 31.0704,
      "step": 11991
    },
    {
      "epoch": 12.0,
      "grad_norm": 6280.34375,
      "learning_rate": 4.121775025799794e-05,
      "loss": 37.7278,
      "step": 11992
    },
    {
      "epoch": 12.01,
      "grad_norm": 6858.57958984375,
      "learning_rate": 4.121259029927761e-05,
      "loss": 29.0738,
      "step": 11993
    },
    {
      "epoch": 12.01,
      "grad_norm": 143375.671875,
      "learning_rate": 4.120743034055728e-05,
      "loss": 34.6136,
      "step": 11994
    },
    {
      "epoch": 12.01,
      "grad_norm": 45558.640625,
      "learning_rate": 4.120227038183695e-05,
      "loss": 45.0868,
      "step": 11995
    },
    {
      "epoch": 12.01,
      "grad_norm": 124070.5546875,
      "learning_rate": 4.119711042311662e-05,
      "loss": 35.847,
      "step": 11996
    },
    {
      "epoch": 12.01,
      "grad_norm": 33696.2578125,
      "learning_rate": 4.119195046439629e-05,
      "loss": 21.852,
      "step": 11997
    },
    {
      "epoch": 12.01,
      "grad_norm": 47884.26953125,
      "learning_rate": 4.1186790505675955e-05,
      "loss": 24.616,
      "step": 11998
    },
    {
      "epoch": 12.01,
      "grad_norm": 21367.341796875,
      "learning_rate": 4.118163054695562e-05,
      "loss": 29.1873,
      "step": 11999
    },
    {
      "epoch": 12.01,
      "grad_norm": 63611.1875,
      "learning_rate": 4.11764705882353e-05,
      "loss": 40.4515,
      "step": 12000
    },
    {
      "epoch": 12.01,
      "grad_norm": 13912.98828125,
      "learning_rate": 4.1171310629514965e-05,
      "loss": 38.1245,
      "step": 12001
    },
    {
      "epoch": 12.01,
      "grad_norm": 6745.74755859375,
      "learning_rate": 4.116615067079463e-05,
      "loss": 27.4375,
      "step": 12002
    },
    {
      "epoch": 12.02,
      "grad_norm": 7435.19287109375,
      "learning_rate": 4.116099071207431e-05,
      "loss": 30.6925,
      "step": 12003
    },
    {
      "epoch": 12.02,
      "grad_norm": 61249.46875,
      "learning_rate": 4.1155830753353975e-05,
      "loss": 27.2789,
      "step": 12004
    },
    {
      "epoch": 12.02,
      "grad_norm": 63596.28125,
      "learning_rate": 4.115067079463364e-05,
      "loss": 29.4378,
      "step": 12005
    },
    {
      "epoch": 12.02,
      "grad_norm": 16164.2890625,
      "learning_rate": 4.114551083591332e-05,
      "loss": 36.184,
      "step": 12006
    },
    {
      "epoch": 12.02,
      "grad_norm": 13058.9423828125,
      "learning_rate": 4.1140350877192985e-05,
      "loss": 26.3969,
      "step": 12007
    },
    {
      "epoch": 12.02,
      "grad_norm": 17199.6171875,
      "learning_rate": 4.113519091847265e-05,
      "loss": 29.8891,
      "step": 12008
    },
    {
      "epoch": 12.02,
      "grad_norm": 5981.78955078125,
      "learning_rate": 4.113003095975233e-05,
      "loss": 18.749,
      "step": 12009
    },
    {
      "epoch": 12.02,
      "grad_norm": 35193.703125,
      "learning_rate": 4.1124871001031995e-05,
      "loss": 23.2299,
      "step": 12010
    },
    {
      "epoch": 12.02,
      "grad_norm": 70641.3828125,
      "learning_rate": 4.111971104231166e-05,
      "loss": 33.1035,
      "step": 12011
    },
    {
      "epoch": 12.02,
      "grad_norm": 3164.50537109375,
      "learning_rate": 4.111455108359134e-05,
      "loss": 26.3314,
      "step": 12012
    },
    {
      "epoch": 12.03,
      "grad_norm": 35564.99609375,
      "learning_rate": 4.1109391124871005e-05,
      "loss": 39.9962,
      "step": 12013
    },
    {
      "epoch": 12.03,
      "grad_norm": 418854.25,
      "learning_rate": 4.110423116615067e-05,
      "loss": 37.1863,
      "step": 12014
    },
    {
      "epoch": 12.03,
      "grad_norm": 10358.296875,
      "learning_rate": 4.109907120743035e-05,
      "loss": 35.3495,
      "step": 12015
    },
    {
      "epoch": 12.03,
      "grad_norm": 9367.9970703125,
      "learning_rate": 4.109391124871001e-05,
      "loss": 31.0282,
      "step": 12016
    },
    {
      "epoch": 12.03,
      "grad_norm": 5230.04833984375,
      "learning_rate": 4.108875128998968e-05,
      "loss": 31.1352,
      "step": 12017
    },
    {
      "epoch": 12.03,
      "grad_norm": 15737.7275390625,
      "learning_rate": 4.108359133126935e-05,
      "loss": 23.7077,
      "step": 12018
    },
    {
      "epoch": 12.03,
      "grad_norm": 32086.87890625,
      "learning_rate": 4.107843137254902e-05,
      "loss": 17.4834,
      "step": 12019
    },
    {
      "epoch": 12.03,
      "grad_norm": 28301.9765625,
      "learning_rate": 4.107327141382869e-05,
      "loss": 26.9974,
      "step": 12020
    },
    {
      "epoch": 12.03,
      "grad_norm": 69019.84375,
      "learning_rate": 4.106811145510836e-05,
      "loss": 30.5823,
      "step": 12021
    },
    {
      "epoch": 12.03,
      "grad_norm": 6254.2451171875,
      "learning_rate": 4.106295149638803e-05,
      "loss": 34.8965,
      "step": 12022
    },
    {
      "epoch": 12.04,
      "grad_norm": 36199.46875,
      "learning_rate": 4.10577915376677e-05,
      "loss": 31.7001,
      "step": 12023
    },
    {
      "epoch": 12.04,
      "grad_norm": 3696.690185546875,
      "learning_rate": 4.105263157894737e-05,
      "loss": 28.5683,
      "step": 12024
    },
    {
      "epoch": 12.04,
      "grad_norm": 1462.9981689453125,
      "learning_rate": 4.104747162022704e-05,
      "loss": 27.0563,
      "step": 12025
    },
    {
      "epoch": 12.04,
      "grad_norm": 20117.986328125,
      "learning_rate": 4.104231166150671e-05,
      "loss": 14.9273,
      "step": 12026
    },
    {
      "epoch": 12.04,
      "grad_norm": 10500.5380859375,
      "learning_rate": 4.103715170278638e-05,
      "loss": 24.2243,
      "step": 12027
    },
    {
      "epoch": 12.04,
      "grad_norm": 50798.19140625,
      "learning_rate": 4.103199174406605e-05,
      "loss": 30.4931,
      "step": 12028
    },
    {
      "epoch": 12.04,
      "grad_norm": 10485.5732421875,
      "learning_rate": 4.102683178534572e-05,
      "loss": 29.839,
      "step": 12029
    },
    {
      "epoch": 12.04,
      "grad_norm": 4374.07177734375,
      "learning_rate": 4.102167182662539e-05,
      "loss": 27.8897,
      "step": 12030
    },
    {
      "epoch": 12.04,
      "grad_norm": 28981.779296875,
      "learning_rate": 4.101651186790506e-05,
      "loss": 20.1699,
      "step": 12031
    },
    {
      "epoch": 12.04,
      "grad_norm": 21586.021484375,
      "learning_rate": 4.101135190918473e-05,
      "loss": 25.2594,
      "step": 12032
    },
    {
      "epoch": 12.05,
      "grad_norm": 2261.92578125,
      "learning_rate": 4.10061919504644e-05,
      "loss": 38.1373,
      "step": 12033
    },
    {
      "epoch": 12.05,
      "grad_norm": 11069.4345703125,
      "learning_rate": 4.100103199174407e-05,
      "loss": 20.3601,
      "step": 12034
    },
    {
      "epoch": 12.05,
      "grad_norm": 27026.505859375,
      "learning_rate": 4.0995872033023736e-05,
      "loss": 30.2789,
      "step": 12035
    },
    {
      "epoch": 12.05,
      "grad_norm": 8436.2265625,
      "learning_rate": 4.0990712074303404e-05,
      "loss": 29.8458,
      "step": 12036
    },
    {
      "epoch": 12.05,
      "grad_norm": 37522.56640625,
      "learning_rate": 4.098555211558308e-05,
      "loss": 23.1829,
      "step": 12037
    },
    {
      "epoch": 12.05,
      "grad_norm": 16051.9541015625,
      "learning_rate": 4.0980392156862746e-05,
      "loss": 27.1653,
      "step": 12038
    },
    {
      "epoch": 12.05,
      "grad_norm": 65000.90625,
      "learning_rate": 4.0975232198142414e-05,
      "loss": 40.3691,
      "step": 12039
    },
    {
      "epoch": 12.05,
      "grad_norm": 14753.263671875,
      "learning_rate": 4.097007223942209e-05,
      "loss": 23.427,
      "step": 12040
    },
    {
      "epoch": 12.05,
      "grad_norm": 50757.35546875,
      "learning_rate": 4.0964912280701756e-05,
      "loss": 19.9669,
      "step": 12041
    },
    {
      "epoch": 12.05,
      "grad_norm": 15902.705078125,
      "learning_rate": 4.0959752321981424e-05,
      "loss": 37.5859,
      "step": 12042
    },
    {
      "epoch": 12.06,
      "grad_norm": 6243.9462890625,
      "learning_rate": 4.09545923632611e-05,
      "loss": 17.7417,
      "step": 12043
    },
    {
      "epoch": 12.06,
      "grad_norm": 5152.85205078125,
      "learning_rate": 4.0949432404540766e-05,
      "loss": 18.9484,
      "step": 12044
    },
    {
      "epoch": 12.06,
      "grad_norm": 69723.890625,
      "learning_rate": 4.0944272445820434e-05,
      "loss": 26.3538,
      "step": 12045
    },
    {
      "epoch": 12.06,
      "grad_norm": 3452.906494140625,
      "learning_rate": 4.093911248710011e-05,
      "loss": 31.8053,
      "step": 12046
    },
    {
      "epoch": 12.06,
      "grad_norm": 6559.10888671875,
      "learning_rate": 4.0933952528379776e-05,
      "loss": 16.4378,
      "step": 12047
    },
    {
      "epoch": 12.06,
      "grad_norm": 11689.8583984375,
      "learning_rate": 4.0928792569659444e-05,
      "loss": 30.4908,
      "step": 12048
    },
    {
      "epoch": 12.06,
      "grad_norm": 9125.869140625,
      "learning_rate": 4.092363261093912e-05,
      "loss": 36.2971,
      "step": 12049
    },
    {
      "epoch": 12.06,
      "grad_norm": 3572.98681640625,
      "learning_rate": 4.0918472652218786e-05,
      "loss": 25.8386,
      "step": 12050
    },
    {
      "epoch": 12.06,
      "grad_norm": 17184.41796875,
      "learning_rate": 4.0913312693498454e-05,
      "loss": 47.0978,
      "step": 12051
    },
    {
      "epoch": 12.06,
      "grad_norm": 11191.296875,
      "learning_rate": 4.090815273477812e-05,
      "loss": 29.0202,
      "step": 12052
    },
    {
      "epoch": 12.07,
      "grad_norm": 9439.2734375,
      "learning_rate": 4.090299277605779e-05,
      "loss": 16.0834,
      "step": 12053
    },
    {
      "epoch": 12.07,
      "grad_norm": 1387.3331298828125,
      "learning_rate": 4.0897832817337464e-05,
      "loss": 34.1203,
      "step": 12054
    },
    {
      "epoch": 12.07,
      "grad_norm": 10964.3984375,
      "learning_rate": 4.089267285861713e-05,
      "loss": 28.2835,
      "step": 12055
    },
    {
      "epoch": 12.07,
      "grad_norm": 46971.91796875,
      "learning_rate": 4.08875128998968e-05,
      "loss": 31.1346,
      "step": 12056
    },
    {
      "epoch": 12.07,
      "grad_norm": 16479.97265625,
      "learning_rate": 4.0882352941176474e-05,
      "loss": 38.6509,
      "step": 12057
    },
    {
      "epoch": 12.07,
      "grad_norm": 8401.34765625,
      "learning_rate": 4.087719298245614e-05,
      "loss": 41.1947,
      "step": 12058
    },
    {
      "epoch": 12.07,
      "grad_norm": 9729.83984375,
      "learning_rate": 4.087203302373581e-05,
      "loss": 38.6162,
      "step": 12059
    },
    {
      "epoch": 12.07,
      "grad_norm": 22657.046875,
      "learning_rate": 4.0866873065015484e-05,
      "loss": 30.2968,
      "step": 12060
    },
    {
      "epoch": 12.07,
      "grad_norm": 159568.515625,
      "learning_rate": 4.086171310629515e-05,
      "loss": 34.8408,
      "step": 12061
    },
    {
      "epoch": 12.07,
      "grad_norm": 12534.94921875,
      "learning_rate": 4.085655314757482e-05,
      "loss": 30.3696,
      "step": 12062
    },
    {
      "epoch": 12.08,
      "grad_norm": 28488.587890625,
      "learning_rate": 4.0851393188854494e-05,
      "loss": 40.9274,
      "step": 12063
    },
    {
      "epoch": 12.08,
      "grad_norm": 39100.83203125,
      "learning_rate": 4.084623323013416e-05,
      "loss": 35.0193,
      "step": 12064
    },
    {
      "epoch": 12.08,
      "grad_norm": 8943.5634765625,
      "learning_rate": 4.084107327141383e-05,
      "loss": 16.6648,
      "step": 12065
    },
    {
      "epoch": 12.08,
      "grad_norm": 10581.22265625,
      "learning_rate": 4.0835913312693504e-05,
      "loss": 27.9769,
      "step": 12066
    },
    {
      "epoch": 12.08,
      "grad_norm": 23810.509765625,
      "learning_rate": 4.083075335397317e-05,
      "loss": 32.1327,
      "step": 12067
    },
    {
      "epoch": 12.08,
      "grad_norm": 14764.5244140625,
      "learning_rate": 4.082559339525284e-05,
      "loss": 27.0877,
      "step": 12068
    },
    {
      "epoch": 12.08,
      "grad_norm": 2882.580078125,
      "learning_rate": 4.082043343653251e-05,
      "loss": 14.7273,
      "step": 12069
    },
    {
      "epoch": 12.08,
      "grad_norm": 4893.53271484375,
      "learning_rate": 4.0815273477812175e-05,
      "loss": 31.3446,
      "step": 12070
    },
    {
      "epoch": 12.08,
      "grad_norm": 3307.233642578125,
      "learning_rate": 4.081011351909185e-05,
      "loss": 32.5253,
      "step": 12071
    },
    {
      "epoch": 12.08,
      "grad_norm": 4830.57080078125,
      "learning_rate": 4.080495356037152e-05,
      "loss": 22.6344,
      "step": 12072
    },
    {
      "epoch": 12.09,
      "grad_norm": 8031.52001953125,
      "learning_rate": 4.0799793601651185e-05,
      "loss": 22.4809,
      "step": 12073
    },
    {
      "epoch": 12.09,
      "grad_norm": 24514.857421875,
      "learning_rate": 4.079463364293086e-05,
      "loss": 30.3739,
      "step": 12074
    },
    {
      "epoch": 12.09,
      "grad_norm": 6791.30224609375,
      "learning_rate": 4.078947368421053e-05,
      "loss": 26.6326,
      "step": 12075
    },
    {
      "epoch": 12.09,
      "grad_norm": 16391.48046875,
      "learning_rate": 4.0784313725490195e-05,
      "loss": 31.2075,
      "step": 12076
    },
    {
      "epoch": 12.09,
      "grad_norm": 100043.5234375,
      "learning_rate": 4.077915376676987e-05,
      "loss": 33.6684,
      "step": 12077
    },
    {
      "epoch": 12.09,
      "grad_norm": 6397.88671875,
      "learning_rate": 4.077399380804954e-05,
      "loss": 29.8087,
      "step": 12078
    },
    {
      "epoch": 12.09,
      "grad_norm": 2028.973876953125,
      "learning_rate": 4.0768833849329205e-05,
      "loss": 33.9528,
      "step": 12079
    },
    {
      "epoch": 12.09,
      "grad_norm": 14128.2734375,
      "learning_rate": 4.076367389060888e-05,
      "loss": 41.3792,
      "step": 12080
    },
    {
      "epoch": 12.09,
      "grad_norm": 6500.6005859375,
      "learning_rate": 4.075851393188855e-05,
      "loss": 23.4976,
      "step": 12081
    },
    {
      "epoch": 12.09,
      "grad_norm": 24959.18359375,
      "learning_rate": 4.0753353973168215e-05,
      "loss": 25.3031,
      "step": 12082
    },
    {
      "epoch": 12.1,
      "grad_norm": 5826.0146484375,
      "learning_rate": 4.074819401444789e-05,
      "loss": 34.4369,
      "step": 12083
    },
    {
      "epoch": 12.1,
      "grad_norm": 22356.125,
      "learning_rate": 4.074303405572756e-05,
      "loss": 26.2383,
      "step": 12084
    },
    {
      "epoch": 12.1,
      "grad_norm": 33656.89453125,
      "learning_rate": 4.0737874097007225e-05,
      "loss": 43.6081,
      "step": 12085
    },
    {
      "epoch": 12.1,
      "grad_norm": 20185.0703125,
      "learning_rate": 4.07327141382869e-05,
      "loss": 35.3002,
      "step": 12086
    },
    {
      "epoch": 12.1,
      "grad_norm": 43311.15625,
      "learning_rate": 4.072755417956656e-05,
      "loss": 45.9663,
      "step": 12087
    },
    {
      "epoch": 12.1,
      "grad_norm": 9674.4111328125,
      "learning_rate": 4.0722394220846235e-05,
      "loss": 22.8435,
      "step": 12088
    },
    {
      "epoch": 12.1,
      "grad_norm": 18450.4609375,
      "learning_rate": 4.07172342621259e-05,
      "loss": 28.8022,
      "step": 12089
    },
    {
      "epoch": 12.1,
      "grad_norm": 56038.27734375,
      "learning_rate": 4.071207430340557e-05,
      "loss": 28.3019,
      "step": 12090
    },
    {
      "epoch": 12.1,
      "grad_norm": 26596.859375,
      "learning_rate": 4.0706914344685245e-05,
      "loss": 37.0115,
      "step": 12091
    },
    {
      "epoch": 12.1,
      "grad_norm": 10740.294921875,
      "learning_rate": 4.070175438596491e-05,
      "loss": 43.9791,
      "step": 12092
    },
    {
      "epoch": 12.11,
      "grad_norm": 9491.3349609375,
      "learning_rate": 4.069659442724458e-05,
      "loss": 37.2814,
      "step": 12093
    },
    {
      "epoch": 12.11,
      "grad_norm": 14883.48046875,
      "learning_rate": 4.0691434468524255e-05,
      "loss": 28.5494,
      "step": 12094
    },
    {
      "epoch": 12.11,
      "grad_norm": 8176.544921875,
      "learning_rate": 4.068627450980392e-05,
      "loss": 36.4453,
      "step": 12095
    },
    {
      "epoch": 12.11,
      "grad_norm": 63051.03125,
      "learning_rate": 4.068111455108359e-05,
      "loss": 38.2951,
      "step": 12096
    },
    {
      "epoch": 12.11,
      "grad_norm": 16908.603515625,
      "learning_rate": 4.0675954592363265e-05,
      "loss": 35.312,
      "step": 12097
    },
    {
      "epoch": 12.11,
      "grad_norm": 46107.19921875,
      "learning_rate": 4.067079463364293e-05,
      "loss": 39.607,
      "step": 12098
    },
    {
      "epoch": 12.11,
      "grad_norm": 7540.73046875,
      "learning_rate": 4.06656346749226e-05,
      "loss": 22.3317,
      "step": 12099
    },
    {
      "epoch": 12.11,
      "grad_norm": 19632.47265625,
      "learning_rate": 4.0660474716202275e-05,
      "loss": 33.3212,
      "step": 12100
    },
    {
      "epoch": 12.11,
      "grad_norm": 2697.3330078125,
      "learning_rate": 4.065531475748194e-05,
      "loss": 14.0022,
      "step": 12101
    },
    {
      "epoch": 12.11,
      "grad_norm": 65338.6328125,
      "learning_rate": 4.065015479876162e-05,
      "loss": 25.5674,
      "step": 12102
    },
    {
      "epoch": 12.12,
      "grad_norm": 35280.16015625,
      "learning_rate": 4.0644994840041285e-05,
      "loss": 37.3233,
      "step": 12103
    },
    {
      "epoch": 12.12,
      "grad_norm": 2695.949951171875,
      "learning_rate": 4.063983488132095e-05,
      "loss": 39.7711,
      "step": 12104
    },
    {
      "epoch": 12.12,
      "grad_norm": 21493.421875,
      "learning_rate": 4.063467492260062e-05,
      "loss": 41.908,
      "step": 12105
    },
    {
      "epoch": 12.12,
      "grad_norm": 9816.3828125,
      "learning_rate": 4.062951496388029e-05,
      "loss": 31.3789,
      "step": 12106
    },
    {
      "epoch": 12.12,
      "grad_norm": 63358.60546875,
      "learning_rate": 4.0624355005159956e-05,
      "loss": 14.5957,
      "step": 12107
    },
    {
      "epoch": 12.12,
      "grad_norm": 10228.1982421875,
      "learning_rate": 4.061919504643963e-05,
      "loss": 34.7542,
      "step": 12108
    },
    {
      "epoch": 12.12,
      "grad_norm": 27190.533203125,
      "learning_rate": 4.06140350877193e-05,
      "loss": 33.0221,
      "step": 12109
    },
    {
      "epoch": 12.12,
      "grad_norm": 2646.2294921875,
      "learning_rate": 4.0608875128998966e-05,
      "loss": 36.3995,
      "step": 12110
    },
    {
      "epoch": 12.12,
      "grad_norm": 4724.71240234375,
      "learning_rate": 4.060371517027864e-05,
      "loss": 32.6039,
      "step": 12111
    },
    {
      "epoch": 12.12,
      "grad_norm": 10093.8955078125,
      "learning_rate": 4.059855521155831e-05,
      "loss": 34.2236,
      "step": 12112
    },
    {
      "epoch": 12.13,
      "grad_norm": 30712.697265625,
      "learning_rate": 4.0593395252837976e-05,
      "loss": 37.9596,
      "step": 12113
    },
    {
      "epoch": 12.13,
      "grad_norm": 7131.5029296875,
      "learning_rate": 4.058823529411765e-05,
      "loss": 29.1163,
      "step": 12114
    },
    {
      "epoch": 12.13,
      "grad_norm": 8532.5087890625,
      "learning_rate": 4.058307533539732e-05,
      "loss": 29.0748,
      "step": 12115
    },
    {
      "epoch": 12.13,
      "grad_norm": 16086.2138671875,
      "learning_rate": 4.057791537667699e-05,
      "loss": 32.2945,
      "step": 12116
    },
    {
      "epoch": 12.13,
      "grad_norm": 10549.8818359375,
      "learning_rate": 4.057275541795666e-05,
      "loss": 24.2871,
      "step": 12117
    },
    {
      "epoch": 12.13,
      "grad_norm": 35940.19921875,
      "learning_rate": 4.056759545923633e-05,
      "loss": 37.5192,
      "step": 12118
    },
    {
      "epoch": 12.13,
      "grad_norm": 11568.7685546875,
      "learning_rate": 4.0562435500516e-05,
      "loss": 29.0288,
      "step": 12119
    },
    {
      "epoch": 12.13,
      "grad_norm": 10363.23828125,
      "learning_rate": 4.055727554179567e-05,
      "loss": 17.6252,
      "step": 12120
    },
    {
      "epoch": 12.13,
      "grad_norm": 8254.2490234375,
      "learning_rate": 4.055211558307534e-05,
      "loss": 23.5676,
      "step": 12121
    },
    {
      "epoch": 12.13,
      "grad_norm": 26554.796875,
      "learning_rate": 4.054695562435501e-05,
      "loss": 38.5531,
      "step": 12122
    },
    {
      "epoch": 12.14,
      "grad_norm": 9128.3427734375,
      "learning_rate": 4.0541795665634674e-05,
      "loss": 35.513,
      "step": 12123
    },
    {
      "epoch": 12.14,
      "grad_norm": 62898.68359375,
      "learning_rate": 4.053663570691434e-05,
      "loss": 33.6293,
      "step": 12124
    },
    {
      "epoch": 12.14,
      "grad_norm": 3301.194580078125,
      "learning_rate": 4.0531475748194016e-05,
      "loss": 12.8824,
      "step": 12125
    },
    {
      "epoch": 12.14,
      "grad_norm": 30622.28125,
      "learning_rate": 4.0526315789473684e-05,
      "loss": 39.5213,
      "step": 12126
    },
    {
      "epoch": 12.14,
      "grad_norm": 15700.998046875,
      "learning_rate": 4.052115583075335e-05,
      "loss": 24.8266,
      "step": 12127
    },
    {
      "epoch": 12.14,
      "grad_norm": 84906.5,
      "learning_rate": 4.0515995872033026e-05,
      "loss": 42.7794,
      "step": 12128
    },
    {
      "epoch": 12.14,
      "grad_norm": 359841.1875,
      "learning_rate": 4.0510835913312694e-05,
      "loss": 33.4681,
      "step": 12129
    },
    {
      "epoch": 12.14,
      "grad_norm": 4961.107421875,
      "learning_rate": 4.050567595459237e-05,
      "loss": 32.6059,
      "step": 12130
    },
    {
      "epoch": 12.14,
      "grad_norm": 25392.65625,
      "learning_rate": 4.0500515995872036e-05,
      "loss": 26.4122,
      "step": 12131
    },
    {
      "epoch": 12.14,
      "grad_norm": 11598.001953125,
      "learning_rate": 4.0495356037151704e-05,
      "loss": 29.9662,
      "step": 12132
    },
    {
      "epoch": 12.15,
      "grad_norm": 54448.3125,
      "learning_rate": 4.049019607843138e-05,
      "loss": 24.7289,
      "step": 12133
    },
    {
      "epoch": 12.15,
      "grad_norm": 174977.28125,
      "learning_rate": 4.0485036119711046e-05,
      "loss": 43.9023,
      "step": 12134
    },
    {
      "epoch": 12.15,
      "grad_norm": 9422.359375,
      "learning_rate": 4.0479876160990714e-05,
      "loss": 30.0556,
      "step": 12135
    },
    {
      "epoch": 12.15,
      "grad_norm": 3634.775634765625,
      "learning_rate": 4.047471620227039e-05,
      "loss": 23.95,
      "step": 12136
    },
    {
      "epoch": 12.15,
      "grad_norm": 13126.4140625,
      "learning_rate": 4.0469556243550056e-05,
      "loss": 29.1476,
      "step": 12137
    },
    {
      "epoch": 12.15,
      "grad_norm": 8886.333984375,
      "learning_rate": 4.0464396284829724e-05,
      "loss": 42.2141,
      "step": 12138
    },
    {
      "epoch": 12.15,
      "grad_norm": 5435.16455078125,
      "learning_rate": 4.04592363261094e-05,
      "loss": 41.4998,
      "step": 12139
    },
    {
      "epoch": 12.15,
      "grad_norm": 15898.7060546875,
      "learning_rate": 4.0454076367389066e-05,
      "loss": 33.2132,
      "step": 12140
    },
    {
      "epoch": 12.15,
      "grad_norm": 9708.7548828125,
      "learning_rate": 4.044891640866873e-05,
      "loss": 39.5056,
      "step": 12141
    },
    {
      "epoch": 12.15,
      "grad_norm": 4427.02978515625,
      "learning_rate": 4.04437564499484e-05,
      "loss": 30.412,
      "step": 12142
    },
    {
      "epoch": 12.16,
      "grad_norm": 32594.419921875,
      "learning_rate": 4.043859649122807e-05,
      "loss": 38.6499,
      "step": 12143
    },
    {
      "epoch": 12.16,
      "grad_norm": 7595.00244140625,
      "learning_rate": 4.0433436532507744e-05,
      "loss": 35.1581,
      "step": 12144
    },
    {
      "epoch": 12.16,
      "grad_norm": 5063.2333984375,
      "learning_rate": 4.042827657378741e-05,
      "loss": 16.4318,
      "step": 12145
    },
    {
      "epoch": 12.16,
      "grad_norm": 6722.59619140625,
      "learning_rate": 4.042311661506708e-05,
      "loss": 32.5567,
      "step": 12146
    },
    {
      "epoch": 12.16,
      "grad_norm": 6580.0068359375,
      "learning_rate": 4.0417956656346754e-05,
      "loss": 25.9046,
      "step": 12147
    },
    {
      "epoch": 12.16,
      "grad_norm": 26063.15234375,
      "learning_rate": 4.041279669762642e-05,
      "loss": 36.8352,
      "step": 12148
    },
    {
      "epoch": 12.16,
      "grad_norm": 14810.4892578125,
      "learning_rate": 4.040763673890609e-05,
      "loss": 25.9483,
      "step": 12149
    },
    {
      "epoch": 12.16,
      "grad_norm": 7248.611328125,
      "learning_rate": 4.0402476780185764e-05,
      "loss": 30.5795,
      "step": 12150
    },
    {
      "epoch": 12.16,
      "grad_norm": 9612.9951171875,
      "learning_rate": 4.039731682146543e-05,
      "loss": 29.5368,
      "step": 12151
    },
    {
      "epoch": 12.16,
      "grad_norm": 20298.115234375,
      "learning_rate": 4.03921568627451e-05,
      "loss": 38.6338,
      "step": 12152
    },
    {
      "epoch": 12.17,
      "grad_norm": 48532.859375,
      "learning_rate": 4.0386996904024774e-05,
      "loss": 39.0552,
      "step": 12153
    },
    {
      "epoch": 12.17,
      "grad_norm": 16188.04296875,
      "learning_rate": 4.038183694530444e-05,
      "loss": 25.8562,
      "step": 12154
    },
    {
      "epoch": 12.17,
      "grad_norm": 13208.3046875,
      "learning_rate": 4.037667698658411e-05,
      "loss": 32.7803,
      "step": 12155
    },
    {
      "epoch": 12.17,
      "grad_norm": 16066.8466796875,
      "learning_rate": 4.0371517027863784e-05,
      "loss": 28.7005,
      "step": 12156
    },
    {
      "epoch": 12.17,
      "grad_norm": 3195.9853515625,
      "learning_rate": 4.036635706914345e-05,
      "loss": 29.1129,
      "step": 12157
    },
    {
      "epoch": 12.17,
      "grad_norm": 53305.125,
      "learning_rate": 4.036119711042312e-05,
      "loss": 17.5919,
      "step": 12158
    },
    {
      "epoch": 12.17,
      "grad_norm": 6232.92724609375,
      "learning_rate": 4.035603715170279e-05,
      "loss": 35.63,
      "step": 12159
    },
    {
      "epoch": 12.17,
      "grad_norm": 16318.763671875,
      "learning_rate": 4.0350877192982455e-05,
      "loss": 39.6068,
      "step": 12160
    },
    {
      "epoch": 12.17,
      "grad_norm": 10898.9765625,
      "learning_rate": 4.034571723426213e-05,
      "loss": 35.2556,
      "step": 12161
    },
    {
      "epoch": 12.17,
      "grad_norm": 10600.6240234375,
      "learning_rate": 4.03405572755418e-05,
      "loss": 23.8471,
      "step": 12162
    },
    {
      "epoch": 12.18,
      "grad_norm": 6353.861328125,
      "learning_rate": 4.0335397316821465e-05,
      "loss": 26.7501,
      "step": 12163
    },
    {
      "epoch": 12.18,
      "grad_norm": 9718.9296875,
      "learning_rate": 4.033023735810114e-05,
      "loss": 35.1607,
      "step": 12164
    },
    {
      "epoch": 12.18,
      "grad_norm": 6235.455078125,
      "learning_rate": 4.032507739938081e-05,
      "loss": 33.7291,
      "step": 12165
    },
    {
      "epoch": 12.18,
      "grad_norm": 26843.576171875,
      "learning_rate": 4.0319917440660475e-05,
      "loss": 33.1439,
      "step": 12166
    },
    {
      "epoch": 12.18,
      "grad_norm": 6100.0595703125,
      "learning_rate": 4.031475748194015e-05,
      "loss": 34.7588,
      "step": 12167
    },
    {
      "epoch": 12.18,
      "grad_norm": 14608.5751953125,
      "learning_rate": 4.030959752321982e-05,
      "loss": 23.9107,
      "step": 12168
    },
    {
      "epoch": 12.18,
      "grad_norm": 67363.9453125,
      "learning_rate": 4.0304437564499485e-05,
      "loss": 27.1196,
      "step": 12169
    },
    {
      "epoch": 12.18,
      "grad_norm": 9577.6728515625,
      "learning_rate": 4.029927760577916e-05,
      "loss": 34.2426,
      "step": 12170
    },
    {
      "epoch": 12.18,
      "grad_norm": 20432.228515625,
      "learning_rate": 4.029411764705883e-05,
      "loss": 34.9241,
      "step": 12171
    },
    {
      "epoch": 12.18,
      "grad_norm": 63635.79296875,
      "learning_rate": 4.0288957688338495e-05,
      "loss": 24.9258,
      "step": 12172
    },
    {
      "epoch": 12.19,
      "grad_norm": 256912.71875,
      "learning_rate": 4.028379772961817e-05,
      "loss": 32.5287,
      "step": 12173
    },
    {
      "epoch": 12.19,
      "grad_norm": 16260.197265625,
      "learning_rate": 4.027863777089784e-05,
      "loss": 37.228,
      "step": 12174
    },
    {
      "epoch": 12.19,
      "grad_norm": 3034.11376953125,
      "learning_rate": 4.0273477812177505e-05,
      "loss": 33.0917,
      "step": 12175
    },
    {
      "epoch": 12.19,
      "grad_norm": 13142.7119140625,
      "learning_rate": 4.026831785345718e-05,
      "loss": 43.5196,
      "step": 12176
    },
    {
      "epoch": 12.19,
      "grad_norm": 7017.12841796875,
      "learning_rate": 4.026315789473684e-05,
      "loss": 21.262,
      "step": 12177
    },
    {
      "epoch": 12.19,
      "grad_norm": 10029.7978515625,
      "learning_rate": 4.0257997936016515e-05,
      "loss": 31.2159,
      "step": 12178
    },
    {
      "epoch": 12.19,
      "grad_norm": 5474.900390625,
      "learning_rate": 4.025283797729618e-05,
      "loss": 24.8616,
      "step": 12179
    },
    {
      "epoch": 12.19,
      "grad_norm": 20203.78515625,
      "learning_rate": 4.024767801857585e-05,
      "loss": 27.2192,
      "step": 12180
    },
    {
      "epoch": 12.19,
      "grad_norm": 11219.2587890625,
      "learning_rate": 4.0242518059855525e-05,
      "loss": 23.9997,
      "step": 12181
    },
    {
      "epoch": 12.19,
      "grad_norm": 53825.4609375,
      "learning_rate": 4.023735810113519e-05,
      "loss": 29.3724,
      "step": 12182
    },
    {
      "epoch": 12.2,
      "grad_norm": 10183.0576171875,
      "learning_rate": 4.023219814241486e-05,
      "loss": 33.4676,
      "step": 12183
    },
    {
      "epoch": 12.2,
      "grad_norm": 16558.19921875,
      "learning_rate": 4.0227038183694535e-05,
      "loss": 33.3273,
      "step": 12184
    },
    {
      "epoch": 12.2,
      "grad_norm": 34056.51953125,
      "learning_rate": 4.02218782249742e-05,
      "loss": 22.2337,
      "step": 12185
    },
    {
      "epoch": 12.2,
      "grad_norm": 10801.091796875,
      "learning_rate": 4.021671826625387e-05,
      "loss": 24.5202,
      "step": 12186
    },
    {
      "epoch": 12.2,
      "grad_norm": 26356.529296875,
      "learning_rate": 4.0211558307533545e-05,
      "loss": 22.7491,
      "step": 12187
    },
    {
      "epoch": 12.2,
      "grad_norm": 6664.294921875,
      "learning_rate": 4.020639834881321e-05,
      "loss": 30.3719,
      "step": 12188
    },
    {
      "epoch": 12.2,
      "grad_norm": 10898.568359375,
      "learning_rate": 4.020123839009288e-05,
      "loss": 26.9686,
      "step": 12189
    },
    {
      "epoch": 12.2,
      "grad_norm": 5120.76416015625,
      "learning_rate": 4.0196078431372555e-05,
      "loss": 35.3217,
      "step": 12190
    },
    {
      "epoch": 12.2,
      "grad_norm": 33017.70703125,
      "learning_rate": 4.019091847265222e-05,
      "loss": 19.0625,
      "step": 12191
    },
    {
      "epoch": 12.2,
      "grad_norm": 62490.26171875,
      "learning_rate": 4.018575851393189e-05,
      "loss": 32.1244,
      "step": 12192
    },
    {
      "epoch": 12.21,
      "grad_norm": 8694.90625,
      "learning_rate": 4.0180598555211565e-05,
      "loss": 34.6358,
      "step": 12193
    },
    {
      "epoch": 12.21,
      "grad_norm": 7315.75830078125,
      "learning_rate": 4.017543859649123e-05,
      "loss": 38.2226,
      "step": 12194
    },
    {
      "epoch": 12.21,
      "grad_norm": 2780.94482421875,
      "learning_rate": 4.01702786377709e-05,
      "loss": 32.9604,
      "step": 12195
    },
    {
      "epoch": 12.21,
      "grad_norm": 17648.94921875,
      "learning_rate": 4.016511867905057e-05,
      "loss": 34.6838,
      "step": 12196
    },
    {
      "epoch": 12.21,
      "grad_norm": 6995.65185546875,
      "learning_rate": 4.0159958720330236e-05,
      "loss": 27.998,
      "step": 12197
    },
    {
      "epoch": 12.21,
      "grad_norm": 5208.40869140625,
      "learning_rate": 4.015479876160991e-05,
      "loss": 22.7604,
      "step": 12198
    },
    {
      "epoch": 12.21,
      "grad_norm": 10163.09375,
      "learning_rate": 4.014963880288958e-05,
      "loss": 32.3042,
      "step": 12199
    },
    {
      "epoch": 12.21,
      "grad_norm": 8732.396484375,
      "learning_rate": 4.0144478844169246e-05,
      "loss": 24.804,
      "step": 12200
    },
    {
      "epoch": 12.21,
      "grad_norm": 39518.16015625,
      "learning_rate": 4.013931888544892e-05,
      "loss": 30.7122,
      "step": 12201
    },
    {
      "epoch": 12.21,
      "grad_norm": 30744.26953125,
      "learning_rate": 4.013415892672859e-05,
      "loss": 40.1977,
      "step": 12202
    },
    {
      "epoch": 12.22,
      "grad_norm": 3242.626708984375,
      "learning_rate": 4.0128998968008256e-05,
      "loss": 31.6381,
      "step": 12203
    },
    {
      "epoch": 12.22,
      "grad_norm": 29846.61328125,
      "learning_rate": 4.012383900928793e-05,
      "loss": 35.0174,
      "step": 12204
    },
    {
      "epoch": 12.22,
      "grad_norm": 11456.255859375,
      "learning_rate": 4.01186790505676e-05,
      "loss": 40.5975,
      "step": 12205
    },
    {
      "epoch": 12.22,
      "grad_norm": 6530.80029296875,
      "learning_rate": 4.0113519091847266e-05,
      "loss": 29.0953,
      "step": 12206
    },
    {
      "epoch": 12.22,
      "grad_norm": 3769.39208984375,
      "learning_rate": 4.010835913312694e-05,
      "loss": 28.0433,
      "step": 12207
    },
    {
      "epoch": 12.22,
      "grad_norm": 1101.4879150390625,
      "learning_rate": 4.010319917440661e-05,
      "loss": 34.2885,
      "step": 12208
    },
    {
      "epoch": 12.22,
      "grad_norm": 11419.6376953125,
      "learning_rate": 4.0098039215686276e-05,
      "loss": 20.8413,
      "step": 12209
    },
    {
      "epoch": 12.22,
      "grad_norm": 39192.38671875,
      "learning_rate": 4.009287925696595e-05,
      "loss": 29.4129,
      "step": 12210
    },
    {
      "epoch": 12.22,
      "grad_norm": 7461.7353515625,
      "learning_rate": 4.008771929824562e-05,
      "loss": 21.8627,
      "step": 12211
    },
    {
      "epoch": 12.22,
      "grad_norm": 89024.25,
      "learning_rate": 4.0082559339525286e-05,
      "loss": 26.0635,
      "step": 12212
    },
    {
      "epoch": 12.23,
      "grad_norm": 14688.076171875,
      "learning_rate": 4.007739938080495e-05,
      "loss": 32.7935,
      "step": 12213
    },
    {
      "epoch": 12.23,
      "grad_norm": 35792.40625,
      "learning_rate": 4.007223942208462e-05,
      "loss": 16.4819,
      "step": 12214
    },
    {
      "epoch": 12.23,
      "grad_norm": 4124.2568359375,
      "learning_rate": 4.0067079463364296e-05,
      "loss": 21.9794,
      "step": 12215
    },
    {
      "epoch": 12.23,
      "grad_norm": 5366.4560546875,
      "learning_rate": 4.006191950464396e-05,
      "loss": 29.0751,
      "step": 12216
    },
    {
      "epoch": 12.23,
      "grad_norm": 11138.029296875,
      "learning_rate": 4.005675954592363e-05,
      "loss": 18.0193,
      "step": 12217
    },
    {
      "epoch": 12.23,
      "grad_norm": 44272.74609375,
      "learning_rate": 4.0051599587203306e-05,
      "loss": 31.2653,
      "step": 12218
    },
    {
      "epoch": 12.23,
      "grad_norm": 8910.1669921875,
      "learning_rate": 4.004643962848297e-05,
      "loss": 27.8043,
      "step": 12219
    },
    {
      "epoch": 12.23,
      "grad_norm": 11794.080078125,
      "learning_rate": 4.004127966976264e-05,
      "loss": 36.0852,
      "step": 12220
    },
    {
      "epoch": 12.23,
      "grad_norm": 1803.131591796875,
      "learning_rate": 4.0036119711042316e-05,
      "loss": 36.253,
      "step": 12221
    },
    {
      "epoch": 12.23,
      "grad_norm": 16612.27734375,
      "learning_rate": 4.003095975232198e-05,
      "loss": 29.0111,
      "step": 12222
    },
    {
      "epoch": 12.24,
      "grad_norm": 7569.04931640625,
      "learning_rate": 4.002579979360165e-05,
      "loss": 41.6668,
      "step": 12223
    },
    {
      "epoch": 12.24,
      "grad_norm": 10017.3984375,
      "learning_rate": 4.0020639834881326e-05,
      "loss": 21.1459,
      "step": 12224
    },
    {
      "epoch": 12.24,
      "grad_norm": 33707.11328125,
      "learning_rate": 4.001547987616099e-05,
      "loss": 36.4178,
      "step": 12225
    },
    {
      "epoch": 12.24,
      "grad_norm": 51664.3203125,
      "learning_rate": 4.001031991744066e-05,
      "loss": 32.9236,
      "step": 12226
    },
    {
      "epoch": 12.24,
      "grad_norm": 8678.1767578125,
      "learning_rate": 4.0005159958720336e-05,
      "loss": 15.9239,
      "step": 12227
    },
    {
      "epoch": 12.24,
      "grad_norm": 35330.33203125,
      "learning_rate": 4e-05,
      "loss": 24.8108,
      "step": 12228
    },
    {
      "epoch": 12.24,
      "grad_norm": 8512.068359375,
      "learning_rate": 3.999484004127967e-05,
      "loss": 34.8248,
      "step": 12229
    },
    {
      "epoch": 12.24,
      "grad_norm": 22256.318359375,
      "learning_rate": 3.998968008255934e-05,
      "loss": 37.1207,
      "step": 12230
    },
    {
      "epoch": 12.24,
      "grad_norm": 7947.4873046875,
      "learning_rate": 3.9984520123839006e-05,
      "loss": 22.9945,
      "step": 12231
    },
    {
      "epoch": 12.24,
      "grad_norm": 18932.857421875,
      "learning_rate": 3.997936016511868e-05,
      "loss": 23.2923,
      "step": 12232
    },
    {
      "epoch": 12.25,
      "grad_norm": 19874.462890625,
      "learning_rate": 3.997420020639835e-05,
      "loss": 35.6578,
      "step": 12233
    },
    {
      "epoch": 12.25,
      "grad_norm": 25589.048828125,
      "learning_rate": 3.9969040247678017e-05,
      "loss": 25.5743,
      "step": 12234
    },
    {
      "epoch": 12.25,
      "grad_norm": 28306.830078125,
      "learning_rate": 3.996388028895769e-05,
      "loss": 25.7572,
      "step": 12235
    },
    {
      "epoch": 12.25,
      "grad_norm": 8935.419921875,
      "learning_rate": 3.995872033023736e-05,
      "loss": 24.3638,
      "step": 12236
    },
    {
      "epoch": 12.25,
      "grad_norm": 14339.6640625,
      "learning_rate": 3.9953560371517027e-05,
      "loss": 27.0332,
      "step": 12237
    },
    {
      "epoch": 12.25,
      "grad_norm": 17954.931640625,
      "learning_rate": 3.99484004127967e-05,
      "loss": 27.1794,
      "step": 12238
    },
    {
      "epoch": 12.25,
      "grad_norm": 8413.4326171875,
      "learning_rate": 3.994324045407637e-05,
      "loss": 34.1827,
      "step": 12239
    },
    {
      "epoch": 12.25,
      "grad_norm": 13181.4482421875,
      "learning_rate": 3.9938080495356037e-05,
      "loss": 37.11,
      "step": 12240
    },
    {
      "epoch": 12.25,
      "grad_norm": 62258.40234375,
      "learning_rate": 3.993292053663571e-05,
      "loss": 29.2666,
      "step": 12241
    },
    {
      "epoch": 12.25,
      "grad_norm": 62117.515625,
      "learning_rate": 3.992776057791538e-05,
      "loss": 43.252,
      "step": 12242
    },
    {
      "epoch": 12.26,
      "grad_norm": 8790.6982421875,
      "learning_rate": 3.9922600619195047e-05,
      "loss": 43.8982,
      "step": 12243
    },
    {
      "epoch": 12.26,
      "grad_norm": 10842.193359375,
      "learning_rate": 3.991744066047472e-05,
      "loss": 20.8447,
      "step": 12244
    },
    {
      "epoch": 12.26,
      "grad_norm": 24363.458984375,
      "learning_rate": 3.991228070175439e-05,
      "loss": 17.3855,
      "step": 12245
    },
    {
      "epoch": 12.26,
      "grad_norm": 8467.19140625,
      "learning_rate": 3.9907120743034057e-05,
      "loss": 36.0938,
      "step": 12246
    },
    {
      "epoch": 12.26,
      "grad_norm": 3459.1669921875,
      "learning_rate": 3.990196078431373e-05,
      "loss": 42.9883,
      "step": 12247
    },
    {
      "epoch": 12.26,
      "grad_norm": 6713.16455078125,
      "learning_rate": 3.989680082559339e-05,
      "loss": 34.1415,
      "step": 12248
    },
    {
      "epoch": 12.26,
      "grad_norm": 29751.205078125,
      "learning_rate": 3.9891640866873067e-05,
      "loss": 37.8139,
      "step": 12249
    },
    {
      "epoch": 12.26,
      "grad_norm": 26881.73046875,
      "learning_rate": 3.9886480908152734e-05,
      "loss": 18.9047,
      "step": 12250
    },
    {
      "epoch": 12.26,
      "grad_norm": 33376.95703125,
      "learning_rate": 3.98813209494324e-05,
      "loss": 33.4245,
      "step": 12251
    },
    {
      "epoch": 12.26,
      "grad_norm": 10015.0419921875,
      "learning_rate": 3.9876160990712077e-05,
      "loss": 30.8393,
      "step": 12252
    },
    {
      "epoch": 12.27,
      "grad_norm": 21702.9375,
      "learning_rate": 3.9871001031991744e-05,
      "loss": 32.2032,
      "step": 12253
    },
    {
      "epoch": 12.27,
      "grad_norm": 2221.182373046875,
      "learning_rate": 3.986584107327141e-05,
      "loss": 34.7613,
      "step": 12254
    },
    {
      "epoch": 12.27,
      "grad_norm": 9234.3203125,
      "learning_rate": 3.9860681114551087e-05,
      "loss": 20.2791,
      "step": 12255
    },
    {
      "epoch": 12.27,
      "grad_norm": 28635.962890625,
      "learning_rate": 3.9855521155830754e-05,
      "loss": 36.286,
      "step": 12256
    },
    {
      "epoch": 12.27,
      "grad_norm": 27160.63671875,
      "learning_rate": 3.985036119711042e-05,
      "loss": 14.6606,
      "step": 12257
    },
    {
      "epoch": 12.27,
      "grad_norm": 9979.4716796875,
      "learning_rate": 3.9845201238390097e-05,
      "loss": 32.3299,
      "step": 12258
    },
    {
      "epoch": 12.27,
      "grad_norm": 15818.5517578125,
      "learning_rate": 3.9840041279669764e-05,
      "loss": 18.1912,
      "step": 12259
    },
    {
      "epoch": 12.27,
      "grad_norm": 27483.4765625,
      "learning_rate": 3.983488132094943e-05,
      "loss": 41.7052,
      "step": 12260
    },
    {
      "epoch": 12.27,
      "grad_norm": 1472.25341796875,
      "learning_rate": 3.9829721362229107e-05,
      "loss": 23.2957,
      "step": 12261
    },
    {
      "epoch": 12.27,
      "grad_norm": 5317.86669921875,
      "learning_rate": 3.9824561403508774e-05,
      "loss": 29.4316,
      "step": 12262
    },
    {
      "epoch": 12.28,
      "grad_norm": 142523.34375,
      "learning_rate": 3.981940144478845e-05,
      "loss": 31.2811,
      "step": 12263
    },
    {
      "epoch": 12.28,
      "grad_norm": 23315.80078125,
      "learning_rate": 3.9814241486068117e-05,
      "loss": 30.5562,
      "step": 12264
    },
    {
      "epoch": 12.28,
      "grad_norm": 44355.74609375,
      "learning_rate": 3.9809081527347784e-05,
      "loss": 30.7768,
      "step": 12265
    },
    {
      "epoch": 12.28,
      "grad_norm": 142377.15625,
      "learning_rate": 3.980392156862745e-05,
      "loss": 41.2132,
      "step": 12266
    },
    {
      "epoch": 12.28,
      "grad_norm": 35618.30078125,
      "learning_rate": 3.979876160990712e-05,
      "loss": 32.4344,
      "step": 12267
    },
    {
      "epoch": 12.28,
      "grad_norm": 75080.453125,
      "learning_rate": 3.979360165118679e-05,
      "loss": 35.1757,
      "step": 12268
    },
    {
      "epoch": 12.28,
      "grad_norm": 11588.7099609375,
      "learning_rate": 3.978844169246646e-05,
      "loss": 24.8894,
      "step": 12269
    },
    {
      "epoch": 12.28,
      "grad_norm": 299698.0625,
      "learning_rate": 3.978328173374613e-05,
      "loss": 20.215,
      "step": 12270
    },
    {
      "epoch": 12.28,
      "grad_norm": 21852.185546875,
      "learning_rate": 3.97781217750258e-05,
      "loss": 31.652,
      "step": 12271
    },
    {
      "epoch": 12.28,
      "grad_norm": 35227.31640625,
      "learning_rate": 3.977296181630547e-05,
      "loss": 27.997,
      "step": 12272
    },
    {
      "epoch": 12.29,
      "grad_norm": 7954.33837890625,
      "learning_rate": 3.976780185758514e-05,
      "loss": 21.7751,
      "step": 12273
    },
    {
      "epoch": 12.29,
      "grad_norm": 21907.640625,
      "learning_rate": 3.976264189886481e-05,
      "loss": 27.1791,
      "step": 12274
    },
    {
      "epoch": 12.29,
      "grad_norm": 24308.564453125,
      "learning_rate": 3.975748194014448e-05,
      "loss": 26.582,
      "step": 12275
    },
    {
      "epoch": 12.29,
      "grad_norm": 7395.28955078125,
      "learning_rate": 3.975232198142415e-05,
      "loss": 22.349,
      "step": 12276
    },
    {
      "epoch": 12.29,
      "grad_norm": 9165.9208984375,
      "learning_rate": 3.9747162022703824e-05,
      "loss": 40.5769,
      "step": 12277
    },
    {
      "epoch": 12.29,
      "grad_norm": 35888.91015625,
      "learning_rate": 3.974200206398349e-05,
      "loss": 19.2039,
      "step": 12278
    },
    {
      "epoch": 12.29,
      "grad_norm": 8012.00732421875,
      "learning_rate": 3.973684210526316e-05,
      "loss": 27.8215,
      "step": 12279
    },
    {
      "epoch": 12.29,
      "grad_norm": 19168.09375,
      "learning_rate": 3.9731682146542834e-05,
      "loss": 36.3694,
      "step": 12280
    },
    {
      "epoch": 12.29,
      "grad_norm": 17177.353515625,
      "learning_rate": 3.97265221878225e-05,
      "loss": 41.6457,
      "step": 12281
    },
    {
      "epoch": 12.29,
      "grad_norm": 22172.77734375,
      "learning_rate": 3.972136222910217e-05,
      "loss": 17.0596,
      "step": 12282
    },
    {
      "epoch": 12.3,
      "grad_norm": 83813.546875,
      "learning_rate": 3.9716202270381844e-05,
      "loss": 41.7688,
      "step": 12283
    },
    {
      "epoch": 12.3,
      "grad_norm": 5235.0498046875,
      "learning_rate": 3.9711042311661505e-05,
      "loss": 42.4281,
      "step": 12284
    },
    {
      "epoch": 12.3,
      "grad_norm": 29811.98828125,
      "learning_rate": 3.970588235294117e-05,
      "loss": 31.8491,
      "step": 12285
    },
    {
      "epoch": 12.3,
      "grad_norm": 3602.485595703125,
      "learning_rate": 3.970072239422085e-05,
      "loss": 19.3349,
      "step": 12286
    },
    {
      "epoch": 12.3,
      "grad_norm": 6068.94775390625,
      "learning_rate": 3.9695562435500515e-05,
      "loss": 27.0939,
      "step": 12287
    },
    {
      "epoch": 12.3,
      "grad_norm": 82579.1484375,
      "learning_rate": 3.969040247678018e-05,
      "loss": 27.7086,
      "step": 12288
    },
    {
      "epoch": 12.3,
      "grad_norm": 4792.25,
      "learning_rate": 3.968524251805986e-05,
      "loss": 26.563,
      "step": 12289
    },
    {
      "epoch": 12.3,
      "grad_norm": 6193.3876953125,
      "learning_rate": 3.9680082559339525e-05,
      "loss": 45.7214,
      "step": 12290
    },
    {
      "epoch": 12.3,
      "grad_norm": 9665.6689453125,
      "learning_rate": 3.96749226006192e-05,
      "loss": 23.8244,
      "step": 12291
    },
    {
      "epoch": 12.3,
      "grad_norm": 22029.16796875,
      "learning_rate": 3.966976264189887e-05,
      "loss": 29.8915,
      "step": 12292
    },
    {
      "epoch": 12.31,
      "grad_norm": 32141.830078125,
      "learning_rate": 3.9664602683178535e-05,
      "loss": 36.7358,
      "step": 12293
    },
    {
      "epoch": 12.31,
      "grad_norm": 16498.39453125,
      "learning_rate": 3.965944272445821e-05,
      "loss": 35.7689,
      "step": 12294
    },
    {
      "epoch": 12.31,
      "grad_norm": 8531.32421875,
      "learning_rate": 3.965428276573788e-05,
      "loss": 26.5275,
      "step": 12295
    },
    {
      "epoch": 12.31,
      "grad_norm": 10870.703125,
      "learning_rate": 3.9649122807017545e-05,
      "loss": 33.4053,
      "step": 12296
    },
    {
      "epoch": 12.31,
      "grad_norm": 3707.876708984375,
      "learning_rate": 3.964396284829722e-05,
      "loss": 22.0905,
      "step": 12297
    },
    {
      "epoch": 12.31,
      "grad_norm": 29745.421875,
      "learning_rate": 3.963880288957689e-05,
      "loss": 37.7589,
      "step": 12298
    },
    {
      "epoch": 12.31,
      "grad_norm": 73800.6953125,
      "learning_rate": 3.9633642930856555e-05,
      "loss": 32.3202,
      "step": 12299
    },
    {
      "epoch": 12.31,
      "grad_norm": 8546.5537109375,
      "learning_rate": 3.962848297213623e-05,
      "loss": 23.0913,
      "step": 12300
    },
    {
      "epoch": 12.31,
      "grad_norm": 31777.814453125,
      "learning_rate": 3.96233230134159e-05,
      "loss": 39.812,
      "step": 12301
    },
    {
      "epoch": 12.31,
      "grad_norm": 13655.7041015625,
      "learning_rate": 3.961816305469556e-05,
      "loss": 20.8684,
      "step": 12302
    },
    {
      "epoch": 12.32,
      "grad_norm": 11911.91015625,
      "learning_rate": 3.961300309597523e-05,
      "loss": 35.2237,
      "step": 12303
    },
    {
      "epoch": 12.32,
      "grad_norm": 40411.87890625,
      "learning_rate": 3.96078431372549e-05,
      "loss": 25.4427,
      "step": 12304
    },
    {
      "epoch": 12.32,
      "grad_norm": 9878.5029296875,
      "learning_rate": 3.9602683178534575e-05,
      "loss": 36.378,
      "step": 12305
    },
    {
      "epoch": 12.32,
      "grad_norm": 8842.1533203125,
      "learning_rate": 3.959752321981424e-05,
      "loss": 34.5736,
      "step": 12306
    },
    {
      "epoch": 12.32,
      "grad_norm": 9282.8427734375,
      "learning_rate": 3.959236326109391e-05,
      "loss": 24.9633,
      "step": 12307
    },
    {
      "epoch": 12.32,
      "grad_norm": 228812.25,
      "learning_rate": 3.9587203302373585e-05,
      "loss": 34.6137,
      "step": 12308
    },
    {
      "epoch": 12.32,
      "grad_norm": 7075.54541015625,
      "learning_rate": 3.958204334365325e-05,
      "loss": 25.4793,
      "step": 12309
    },
    {
      "epoch": 12.32,
      "grad_norm": 5123.50830078125,
      "learning_rate": 3.957688338493292e-05,
      "loss": 40.5823,
      "step": 12310
    },
    {
      "epoch": 12.32,
      "grad_norm": 10156.7958984375,
      "learning_rate": 3.9571723426212595e-05,
      "loss": 38.1301,
      "step": 12311
    },
    {
      "epoch": 12.32,
      "grad_norm": 1594.14990234375,
      "learning_rate": 3.956656346749226e-05,
      "loss": 22.2189,
      "step": 12312
    },
    {
      "epoch": 12.33,
      "grad_norm": 14935.998046875,
      "learning_rate": 3.956140350877193e-05,
      "loss": 30.694,
      "step": 12313
    },
    {
      "epoch": 12.33,
      "grad_norm": 9766.5341796875,
      "learning_rate": 3.9556243550051605e-05,
      "loss": 39.07,
      "step": 12314
    },
    {
      "epoch": 12.33,
      "grad_norm": 18770.576171875,
      "learning_rate": 3.955108359133127e-05,
      "loss": 26.7138,
      "step": 12315
    },
    {
      "epoch": 12.33,
      "grad_norm": 27564.458984375,
      "learning_rate": 3.954592363261094e-05,
      "loss": 20.0465,
      "step": 12316
    },
    {
      "epoch": 12.33,
      "grad_norm": 22469.263671875,
      "learning_rate": 3.9540763673890615e-05,
      "loss": 41.069,
      "step": 12317
    },
    {
      "epoch": 12.33,
      "grad_norm": 4301.904296875,
      "learning_rate": 3.953560371517028e-05,
      "loss": 35.0129,
      "step": 12318
    },
    {
      "epoch": 12.33,
      "grad_norm": 8342.16796875,
      "learning_rate": 3.953044375644995e-05,
      "loss": 28.2546,
      "step": 12319
    },
    {
      "epoch": 12.33,
      "grad_norm": 19100.01171875,
      "learning_rate": 3.952528379772962e-05,
      "loss": 22.1539,
      "step": 12320
    },
    {
      "epoch": 12.33,
      "grad_norm": 3244.2919921875,
      "learning_rate": 3.9520123839009286e-05,
      "loss": 19.6563,
      "step": 12321
    },
    {
      "epoch": 12.33,
      "grad_norm": 3337.676513671875,
      "learning_rate": 3.951496388028896e-05,
      "loss": 39.321,
      "step": 12322
    },
    {
      "epoch": 12.34,
      "grad_norm": 66821.0390625,
      "learning_rate": 3.950980392156863e-05,
      "loss": 30.6636,
      "step": 12323
    },
    {
      "epoch": 12.34,
      "grad_norm": 11977.0166015625,
      "learning_rate": 3.9504643962848296e-05,
      "loss": 48.5665,
      "step": 12324
    },
    {
      "epoch": 12.34,
      "grad_norm": 16322.287109375,
      "learning_rate": 3.949948400412797e-05,
      "loss": 35.3382,
      "step": 12325
    },
    {
      "epoch": 12.34,
      "grad_norm": 6039.10107421875,
      "learning_rate": 3.949432404540764e-05,
      "loss": 34.5782,
      "step": 12326
    },
    {
      "epoch": 12.34,
      "grad_norm": 18734.0703125,
      "learning_rate": 3.9489164086687306e-05,
      "loss": 33.0986,
      "step": 12327
    },
    {
      "epoch": 12.34,
      "grad_norm": 9222.37890625,
      "learning_rate": 3.948400412796698e-05,
      "loss": 30.5713,
      "step": 12328
    },
    {
      "epoch": 12.34,
      "grad_norm": 26474.0625,
      "learning_rate": 3.947884416924665e-05,
      "loss": 43.7979,
      "step": 12329
    },
    {
      "epoch": 12.34,
      "grad_norm": 16907.283203125,
      "learning_rate": 3.9473684210526316e-05,
      "loss": 25.7357,
      "step": 12330
    },
    {
      "epoch": 12.34,
      "grad_norm": 9616.779296875,
      "learning_rate": 3.946852425180599e-05,
      "loss": 34.2124,
      "step": 12331
    },
    {
      "epoch": 12.34,
      "grad_norm": 23626.76953125,
      "learning_rate": 3.946336429308566e-05,
      "loss": 41.0513,
      "step": 12332
    },
    {
      "epoch": 12.35,
      "grad_norm": 17796.125,
      "learning_rate": 3.9458204334365326e-05,
      "loss": 14.6015,
      "step": 12333
    },
    {
      "epoch": 12.35,
      "grad_norm": 98380.1171875,
      "learning_rate": 3.9453044375645e-05,
      "loss": 32.4221,
      "step": 12334
    },
    {
      "epoch": 12.35,
      "grad_norm": 5649.87890625,
      "learning_rate": 3.944788441692467e-05,
      "loss": 36.0416,
      "step": 12335
    },
    {
      "epoch": 12.35,
      "grad_norm": 6954.43994140625,
      "learning_rate": 3.9442724458204336e-05,
      "loss": 40.1189,
      "step": 12336
    },
    {
      "epoch": 12.35,
      "grad_norm": 9124.650390625,
      "learning_rate": 3.943756449948401e-05,
      "loss": 34.451,
      "step": 12337
    },
    {
      "epoch": 12.35,
      "grad_norm": 14988.53515625,
      "learning_rate": 3.943240454076367e-05,
      "loss": 30.7075,
      "step": 12338
    },
    {
      "epoch": 12.35,
      "grad_norm": 2615.2275390625,
      "learning_rate": 3.9427244582043346e-05,
      "loss": 23.1792,
      "step": 12339
    },
    {
      "epoch": 12.35,
      "grad_norm": 2916.34326171875,
      "learning_rate": 3.9422084623323014e-05,
      "loss": 23.9859,
      "step": 12340
    },
    {
      "epoch": 12.35,
      "grad_norm": 24761.888671875,
      "learning_rate": 3.941692466460268e-05,
      "loss": 34.0051,
      "step": 12341
    },
    {
      "epoch": 12.35,
      "grad_norm": 2573.067138671875,
      "learning_rate": 3.9411764705882356e-05,
      "loss": 39.0308,
      "step": 12342
    },
    {
      "epoch": 12.36,
      "grad_norm": 20504.365234375,
      "learning_rate": 3.9406604747162024e-05,
      "loss": 42.685,
      "step": 12343
    },
    {
      "epoch": 12.36,
      "grad_norm": 11146.28125,
      "learning_rate": 3.940144478844169e-05,
      "loss": 30.3648,
      "step": 12344
    },
    {
      "epoch": 12.36,
      "grad_norm": 10243.15625,
      "learning_rate": 3.9396284829721366e-05,
      "loss": 35.5427,
      "step": 12345
    },
    {
      "epoch": 12.36,
      "grad_norm": 9954.3193359375,
      "learning_rate": 3.9391124871001034e-05,
      "loss": 21.2831,
      "step": 12346
    },
    {
      "epoch": 12.36,
      "grad_norm": 9073.5498046875,
      "learning_rate": 3.93859649122807e-05,
      "loss": 22.1794,
      "step": 12347
    },
    {
      "epoch": 12.36,
      "grad_norm": 32002.373046875,
      "learning_rate": 3.9380804953560376e-05,
      "loss": 35.3177,
      "step": 12348
    },
    {
      "epoch": 12.36,
      "grad_norm": 12715.8486328125,
      "learning_rate": 3.9375644994840044e-05,
      "loss": 32.6562,
      "step": 12349
    },
    {
      "epoch": 12.36,
      "grad_norm": 27835.724609375,
      "learning_rate": 3.937048503611971e-05,
      "loss": 31.8735,
      "step": 12350
    },
    {
      "epoch": 12.36,
      "grad_norm": 11854.828125,
      "learning_rate": 3.9365325077399386e-05,
      "loss": 35.4149,
      "step": 12351
    },
    {
      "epoch": 12.36,
      "grad_norm": 8232.595703125,
      "learning_rate": 3.9360165118679054e-05,
      "loss": 36.7465,
      "step": 12352
    },
    {
      "epoch": 12.37,
      "grad_norm": 53639.21484375,
      "learning_rate": 3.935500515995872e-05,
      "loss": 25.5035,
      "step": 12353
    },
    {
      "epoch": 12.37,
      "grad_norm": 4532.75244140625,
      "learning_rate": 3.9349845201238396e-05,
      "loss": 19.848,
      "step": 12354
    },
    {
      "epoch": 12.37,
      "grad_norm": 5540.34130859375,
      "learning_rate": 3.934468524251806e-05,
      "loss": 35.8158,
      "step": 12355
    },
    {
      "epoch": 12.37,
      "grad_norm": 68679.7109375,
      "learning_rate": 3.933952528379773e-05,
      "loss": 26.1257,
      "step": 12356
    },
    {
      "epoch": 12.37,
      "grad_norm": 62474.07421875,
      "learning_rate": 3.93343653250774e-05,
      "loss": 26.4542,
      "step": 12357
    },
    {
      "epoch": 12.37,
      "grad_norm": 31971.98046875,
      "learning_rate": 3.932920536635707e-05,
      "loss": 30.7371,
      "step": 12358
    },
    {
      "epoch": 12.37,
      "grad_norm": 10068.1591796875,
      "learning_rate": 3.932404540763674e-05,
      "loss": 36.2751,
      "step": 12359
    },
    {
      "epoch": 12.37,
      "grad_norm": 6091.57763671875,
      "learning_rate": 3.931888544891641e-05,
      "loss": 32.6726,
      "step": 12360
    },
    {
      "epoch": 12.37,
      "grad_norm": 63986.140625,
      "learning_rate": 3.931372549019608e-05,
      "loss": 14.4895,
      "step": 12361
    },
    {
      "epoch": 12.37,
      "grad_norm": 28731.083984375,
      "learning_rate": 3.930856553147575e-05,
      "loss": 39.468,
      "step": 12362
    },
    {
      "epoch": 12.38,
      "grad_norm": 15907.6982421875,
      "learning_rate": 3.930340557275542e-05,
      "loss": 20.8633,
      "step": 12363
    },
    {
      "epoch": 12.38,
      "grad_norm": 11688.2314453125,
      "learning_rate": 3.929824561403509e-05,
      "loss": 17.8355,
      "step": 12364
    },
    {
      "epoch": 12.38,
      "grad_norm": 11413.490234375,
      "learning_rate": 3.929308565531476e-05,
      "loss": 36.876,
      "step": 12365
    },
    {
      "epoch": 12.38,
      "grad_norm": 4636.34326171875,
      "learning_rate": 3.928792569659443e-05,
      "loss": 28.7472,
      "step": 12366
    },
    {
      "epoch": 12.38,
      "grad_norm": 5201.28076171875,
      "learning_rate": 3.92827657378741e-05,
      "loss": 35.8867,
      "step": 12367
    },
    {
      "epoch": 12.38,
      "grad_norm": 14788.935546875,
      "learning_rate": 3.927760577915377e-05,
      "loss": 40.5701,
      "step": 12368
    },
    {
      "epoch": 12.38,
      "grad_norm": 14347.46484375,
      "learning_rate": 3.927244582043344e-05,
      "loss": 25.3006,
      "step": 12369
    },
    {
      "epoch": 12.38,
      "grad_norm": 45301.7421875,
      "learning_rate": 3.926728586171311e-05,
      "loss": 20.2622,
      "step": 12370
    },
    {
      "epoch": 12.38,
      "grad_norm": 5299.18310546875,
      "learning_rate": 3.926212590299278e-05,
      "loss": 31.1878,
      "step": 12371
    },
    {
      "epoch": 12.38,
      "grad_norm": 5219.06396484375,
      "learning_rate": 3.925696594427245e-05,
      "loss": 25.4196,
      "step": 12372
    },
    {
      "epoch": 12.39,
      "grad_norm": 29350.484375,
      "learning_rate": 3.925180598555212e-05,
      "loss": 31.9769,
      "step": 12373
    },
    {
      "epoch": 12.39,
      "grad_norm": 66339.3515625,
      "learning_rate": 3.9246646026831785e-05,
      "loss": 30.7848,
      "step": 12374
    },
    {
      "epoch": 12.39,
      "grad_norm": 56600.421875,
      "learning_rate": 3.924148606811145e-05,
      "loss": 37.3913,
      "step": 12375
    },
    {
      "epoch": 12.39,
      "grad_norm": 17684.841796875,
      "learning_rate": 3.923632610939113e-05,
      "loss": 40.6262,
      "step": 12376
    },
    {
      "epoch": 12.39,
      "grad_norm": 10199.3369140625,
      "learning_rate": 3.9231166150670795e-05,
      "loss": 34.8166,
      "step": 12377
    },
    {
      "epoch": 12.39,
      "grad_norm": 8723.5322265625,
      "learning_rate": 3.922600619195046e-05,
      "loss": 40.5289,
      "step": 12378
    },
    {
      "epoch": 12.39,
      "grad_norm": 8142.22802734375,
      "learning_rate": 3.922084623323014e-05,
      "loss": 41.5545,
      "step": 12379
    },
    {
      "epoch": 12.39,
      "grad_norm": 29834.58984375,
      "learning_rate": 3.9215686274509805e-05,
      "loss": 27.1046,
      "step": 12380
    },
    {
      "epoch": 12.39,
      "grad_norm": 11984.69921875,
      "learning_rate": 3.921052631578947e-05,
      "loss": 40.4585,
      "step": 12381
    },
    {
      "epoch": 12.39,
      "grad_norm": 9262.951171875,
      "learning_rate": 3.920536635706915e-05,
      "loss": 34.1961,
      "step": 12382
    },
    {
      "epoch": 12.4,
      "grad_norm": 7678.9404296875,
      "learning_rate": 3.9200206398348815e-05,
      "loss": 38.0301,
      "step": 12383
    },
    {
      "epoch": 12.4,
      "grad_norm": 2943.600830078125,
      "learning_rate": 3.919504643962848e-05,
      "loss": 42.4278,
      "step": 12384
    },
    {
      "epoch": 12.4,
      "grad_norm": 22945.296875,
      "learning_rate": 3.918988648090816e-05,
      "loss": 29.3282,
      "step": 12385
    },
    {
      "epoch": 12.4,
      "grad_norm": 19719.140625,
      "learning_rate": 3.9184726522187825e-05,
      "loss": 30.0049,
      "step": 12386
    },
    {
      "epoch": 12.4,
      "grad_norm": 5315.90771484375,
      "learning_rate": 3.917956656346749e-05,
      "loss": 31.6107,
      "step": 12387
    },
    {
      "epoch": 12.4,
      "grad_norm": 23296.208984375,
      "learning_rate": 3.917440660474717e-05,
      "loss": 33.4452,
      "step": 12388
    },
    {
      "epoch": 12.4,
      "grad_norm": 14880.0703125,
      "learning_rate": 3.9169246646026835e-05,
      "loss": 46.463,
      "step": 12389
    },
    {
      "epoch": 12.4,
      "grad_norm": 26293.2578125,
      "learning_rate": 3.91640866873065e-05,
      "loss": 38.9643,
      "step": 12390
    },
    {
      "epoch": 12.4,
      "grad_norm": 6341.37158203125,
      "learning_rate": 3.915892672858617e-05,
      "loss": 36.3623,
      "step": 12391
    },
    {
      "epoch": 12.4,
      "grad_norm": 5760.91064453125,
      "learning_rate": 3.915376676986584e-05,
      "loss": 23.8966,
      "step": 12392
    },
    {
      "epoch": 12.41,
      "grad_norm": 9699.0380859375,
      "learning_rate": 3.914860681114551e-05,
      "loss": 18.0831,
      "step": 12393
    },
    {
      "epoch": 12.41,
      "grad_norm": 108127.4296875,
      "learning_rate": 3.914344685242518e-05,
      "loss": 28.1151,
      "step": 12394
    },
    {
      "epoch": 12.41,
      "grad_norm": 4891.5263671875,
      "learning_rate": 3.913828689370485e-05,
      "loss": 39.6841,
      "step": 12395
    },
    {
      "epoch": 12.41,
      "grad_norm": 46296.68359375,
      "learning_rate": 3.913312693498452e-05,
      "loss": 41.0423,
      "step": 12396
    },
    {
      "epoch": 12.41,
      "grad_norm": 18277.818359375,
      "learning_rate": 3.912796697626419e-05,
      "loss": 20.7968,
      "step": 12397
    },
    {
      "epoch": 12.41,
      "grad_norm": 20825.240234375,
      "learning_rate": 3.912280701754386e-05,
      "loss": 37.9264,
      "step": 12398
    },
    {
      "epoch": 12.41,
      "grad_norm": 33099.921875,
      "learning_rate": 3.911764705882353e-05,
      "loss": 39.5777,
      "step": 12399
    },
    {
      "epoch": 12.41,
      "grad_norm": 11279.34375,
      "learning_rate": 3.91124871001032e-05,
      "loss": 27.799,
      "step": 12400
    },
    {
      "epoch": 12.41,
      "grad_norm": 7304.75732421875,
      "learning_rate": 3.910732714138287e-05,
      "loss": 24.2311,
      "step": 12401
    },
    {
      "epoch": 12.41,
      "grad_norm": 36702.61328125,
      "learning_rate": 3.910216718266254e-05,
      "loss": 17.0932,
      "step": 12402
    },
    {
      "epoch": 12.42,
      "grad_norm": 6100.064453125,
      "learning_rate": 3.909700722394221e-05,
      "loss": 29.1479,
      "step": 12403
    },
    {
      "epoch": 12.42,
      "grad_norm": 13124.7666015625,
      "learning_rate": 3.909184726522188e-05,
      "loss": 28.4342,
      "step": 12404
    },
    {
      "epoch": 12.42,
      "grad_norm": 19204.240234375,
      "learning_rate": 3.908668730650155e-05,
      "loss": 30.0942,
      "step": 12405
    },
    {
      "epoch": 12.42,
      "grad_norm": 17834.556640625,
      "learning_rate": 3.908152734778122e-05,
      "loss": 23.189,
      "step": 12406
    },
    {
      "epoch": 12.42,
      "grad_norm": 10814.072265625,
      "learning_rate": 3.9076367389060895e-05,
      "loss": 28.8829,
      "step": 12407
    },
    {
      "epoch": 12.42,
      "grad_norm": 40326.5546875,
      "learning_rate": 3.907120743034056e-05,
      "loss": 34.0475,
      "step": 12408
    },
    {
      "epoch": 12.42,
      "grad_norm": 4127.01611328125,
      "learning_rate": 3.9066047471620224e-05,
      "loss": 36.4509,
      "step": 12409
    },
    {
      "epoch": 12.42,
      "grad_norm": 64556.97265625,
      "learning_rate": 3.90608875128999e-05,
      "loss": 33.5384,
      "step": 12410
    },
    {
      "epoch": 12.42,
      "grad_norm": 8401.3994140625,
      "learning_rate": 3.9055727554179566e-05,
      "loss": 19.7389,
      "step": 12411
    },
    {
      "epoch": 12.42,
      "grad_norm": 20286.2578125,
      "learning_rate": 3.9050567595459234e-05,
      "loss": 36.6035,
      "step": 12412
    },
    {
      "epoch": 12.43,
      "grad_norm": 3992.90087890625,
      "learning_rate": 3.904540763673891e-05,
      "loss": 38.6343,
      "step": 12413
    },
    {
      "epoch": 12.43,
      "grad_norm": 36243.6484375,
      "learning_rate": 3.9040247678018576e-05,
      "loss": 43.0676,
      "step": 12414
    },
    {
      "epoch": 12.43,
      "grad_norm": 11398.3037109375,
      "learning_rate": 3.9035087719298244e-05,
      "loss": 36.7323,
      "step": 12415
    },
    {
      "epoch": 12.43,
      "grad_norm": 24233.267578125,
      "learning_rate": 3.902992776057792e-05,
      "loss": 20.5987,
      "step": 12416
    },
    {
      "epoch": 12.43,
      "grad_norm": 6003.54052734375,
      "learning_rate": 3.9024767801857586e-05,
      "loss": 33.523,
      "step": 12417
    },
    {
      "epoch": 12.43,
      "grad_norm": 7154.81982421875,
      "learning_rate": 3.9019607843137254e-05,
      "loss": 19.0409,
      "step": 12418
    },
    {
      "epoch": 12.43,
      "grad_norm": 7052.35546875,
      "learning_rate": 3.901444788441693e-05,
      "loss": 25.2473,
      "step": 12419
    },
    {
      "epoch": 12.43,
      "grad_norm": 6714.62841796875,
      "learning_rate": 3.9009287925696596e-05,
      "loss": 29.6388,
      "step": 12420
    },
    {
      "epoch": 12.43,
      "grad_norm": 8223.4287109375,
      "learning_rate": 3.900412796697627e-05,
      "loss": 11.4725,
      "step": 12421
    },
    {
      "epoch": 12.43,
      "grad_norm": 2978.2265625,
      "learning_rate": 3.899896800825594e-05,
      "loss": 27.568,
      "step": 12422
    },
    {
      "epoch": 12.44,
      "grad_norm": 18300.08984375,
      "learning_rate": 3.8993808049535606e-05,
      "loss": 33.7963,
      "step": 12423
    },
    {
      "epoch": 12.44,
      "grad_norm": 8351.3935546875,
      "learning_rate": 3.898864809081528e-05,
      "loss": 33.2072,
      "step": 12424
    },
    {
      "epoch": 12.44,
      "grad_norm": 8580.447265625,
      "learning_rate": 3.898348813209495e-05,
      "loss": 31.8795,
      "step": 12425
    },
    {
      "epoch": 12.44,
      "grad_norm": 89917.6484375,
      "learning_rate": 3.8978328173374616e-05,
      "loss": 28.1121,
      "step": 12426
    },
    {
      "epoch": 12.44,
      "grad_norm": 32908.4375,
      "learning_rate": 3.8973168214654284e-05,
      "loss": 19.4129,
      "step": 12427
    },
    {
      "epoch": 12.44,
      "grad_norm": 4596.19140625,
      "learning_rate": 3.896800825593395e-05,
      "loss": 28.5361,
      "step": 12428
    },
    {
      "epoch": 12.44,
      "grad_norm": 10705.7099609375,
      "learning_rate": 3.896284829721362e-05,
      "loss": 22.4946,
      "step": 12429
    },
    {
      "epoch": 12.44,
      "grad_norm": 13853.16796875,
      "learning_rate": 3.8957688338493294e-05,
      "loss": 33.3407,
      "step": 12430
    },
    {
      "epoch": 12.44,
      "grad_norm": 7985.2021484375,
      "learning_rate": 3.895252837977296e-05,
      "loss": 34.4851,
      "step": 12431
    },
    {
      "epoch": 12.44,
      "grad_norm": 6263.01220703125,
      "learning_rate": 3.894736842105263e-05,
      "loss": 41.5606,
      "step": 12432
    },
    {
      "epoch": 12.45,
      "grad_norm": 31515.248046875,
      "learning_rate": 3.8942208462332304e-05,
      "loss": 29.3023,
      "step": 12433
    },
    {
      "epoch": 12.45,
      "grad_norm": 39392.70703125,
      "learning_rate": 3.893704850361197e-05,
      "loss": 24.2559,
      "step": 12434
    },
    {
      "epoch": 12.45,
      "grad_norm": 7627.50732421875,
      "learning_rate": 3.893188854489164e-05,
      "loss": 29.6624,
      "step": 12435
    },
    {
      "epoch": 12.45,
      "grad_norm": 2846.6669921875,
      "learning_rate": 3.8926728586171314e-05,
      "loss": 44.3837,
      "step": 12436
    },
    {
      "epoch": 12.45,
      "grad_norm": 32699.1328125,
      "learning_rate": 3.892156862745098e-05,
      "loss": 22.3621,
      "step": 12437
    },
    {
      "epoch": 12.45,
      "grad_norm": 16502.849609375,
      "learning_rate": 3.8916408668730656e-05,
      "loss": 44.8484,
      "step": 12438
    },
    {
      "epoch": 12.45,
      "grad_norm": 129646.4296875,
      "learning_rate": 3.8911248710010324e-05,
      "loss": 30.1481,
      "step": 12439
    },
    {
      "epoch": 12.45,
      "grad_norm": 9622.6884765625,
      "learning_rate": 3.890608875128999e-05,
      "loss": 31.0911,
      "step": 12440
    },
    {
      "epoch": 12.45,
      "grad_norm": 5805.26953125,
      "learning_rate": 3.8900928792569666e-05,
      "loss": 21.2742,
      "step": 12441
    },
    {
      "epoch": 12.45,
      "grad_norm": 64006.23046875,
      "learning_rate": 3.8895768833849334e-05,
      "loss": 34.8804,
      "step": 12442
    },
    {
      "epoch": 12.46,
      "grad_norm": 10532.0849609375,
      "learning_rate": 3.8890608875129e-05,
      "loss": 22.8078,
      "step": 12443
    },
    {
      "epoch": 12.46,
      "grad_norm": 20204.767578125,
      "learning_rate": 3.8885448916408676e-05,
      "loss": 36.4939,
      "step": 12444
    },
    {
      "epoch": 12.46,
      "grad_norm": 12119.9140625,
      "learning_rate": 3.888028895768834e-05,
      "loss": 37.9699,
      "step": 12445
    },
    {
      "epoch": 12.46,
      "grad_norm": 5771.83349609375,
      "learning_rate": 3.8875128998968005e-05,
      "loss": 18.8983,
      "step": 12446
    },
    {
      "epoch": 12.46,
      "grad_norm": 87918.3203125,
      "learning_rate": 3.886996904024768e-05,
      "loss": 50.0995,
      "step": 12447
    },
    {
      "epoch": 12.46,
      "grad_norm": 19375.66796875,
      "learning_rate": 3.886480908152735e-05,
      "loss": 18.0404,
      "step": 12448
    },
    {
      "epoch": 12.46,
      "grad_norm": 49558.25390625,
      "learning_rate": 3.8859649122807015e-05,
      "loss": 33.2883,
      "step": 12449
    },
    {
      "epoch": 12.46,
      "grad_norm": 8024.98828125,
      "learning_rate": 3.885448916408669e-05,
      "loss": 35.5974,
      "step": 12450
    },
    {
      "epoch": 12.46,
      "grad_norm": 9049.798828125,
      "learning_rate": 3.884932920536636e-05,
      "loss": 35.0423,
      "step": 12451
    },
    {
      "epoch": 12.46,
      "grad_norm": 32135.466796875,
      "learning_rate": 3.884416924664603e-05,
      "loss": 40.7203,
      "step": 12452
    },
    {
      "epoch": 12.47,
      "grad_norm": 40227.45703125,
      "learning_rate": 3.88390092879257e-05,
      "loss": 21.4899,
      "step": 12453
    },
    {
      "epoch": 12.47,
      "grad_norm": 62489.09765625,
      "learning_rate": 3.883384932920537e-05,
      "loss": 42.2252,
      "step": 12454
    },
    {
      "epoch": 12.47,
      "grad_norm": 25123.876953125,
      "learning_rate": 3.882868937048504e-05,
      "loss": 17.2883,
      "step": 12455
    },
    {
      "epoch": 12.47,
      "grad_norm": 4250.35205078125,
      "learning_rate": 3.882352941176471e-05,
      "loss": 27.4707,
      "step": 12456
    },
    {
      "epoch": 12.47,
      "grad_norm": 8344.7705078125,
      "learning_rate": 3.881836945304438e-05,
      "loss": 19.3753,
      "step": 12457
    },
    {
      "epoch": 12.47,
      "grad_norm": 17376.271484375,
      "learning_rate": 3.881320949432405e-05,
      "loss": 36.7586,
      "step": 12458
    },
    {
      "epoch": 12.47,
      "grad_norm": 81304.7421875,
      "learning_rate": 3.880804953560372e-05,
      "loss": 32.2486,
      "step": 12459
    },
    {
      "epoch": 12.47,
      "grad_norm": 4353.70166015625,
      "learning_rate": 3.880288957688339e-05,
      "loss": 32.1625,
      "step": 12460
    },
    {
      "epoch": 12.47,
      "grad_norm": 43166.60546875,
      "learning_rate": 3.879772961816306e-05,
      "loss": 32.4832,
      "step": 12461
    },
    {
      "epoch": 12.47,
      "grad_norm": 19607.607421875,
      "learning_rate": 3.879256965944273e-05,
      "loss": 38.6969,
      "step": 12462
    },
    {
      "epoch": 12.48,
      "grad_norm": 12074.8876953125,
      "learning_rate": 3.878740970072239e-05,
      "loss": 24.5483,
      "step": 12463
    },
    {
      "epoch": 12.48,
      "grad_norm": 3040.000244140625,
      "learning_rate": 3.8782249742002065e-05,
      "loss": 33.0923,
      "step": 12464
    },
    {
      "epoch": 12.48,
      "grad_norm": 36106.55859375,
      "learning_rate": 3.877708978328173e-05,
      "loss": 30.0957,
      "step": 12465
    },
    {
      "epoch": 12.48,
      "grad_norm": 8365.455078125,
      "learning_rate": 3.877192982456141e-05,
      "loss": 26.5754,
      "step": 12466
    },
    {
      "epoch": 12.48,
      "grad_norm": 5785.99951171875,
      "learning_rate": 3.8766769865841075e-05,
      "loss": 40.6663,
      "step": 12467
    },
    {
      "epoch": 12.48,
      "grad_norm": 40566.796875,
      "learning_rate": 3.876160990712074e-05,
      "loss": 24.6,
      "step": 12468
    },
    {
      "epoch": 12.48,
      "grad_norm": 6869.15576171875,
      "learning_rate": 3.875644994840042e-05,
      "loss": 32.2317,
      "step": 12469
    },
    {
      "epoch": 12.48,
      "grad_norm": 14646.353515625,
      "learning_rate": 3.8751289989680085e-05,
      "loss": 35.8808,
      "step": 12470
    },
    {
      "epoch": 12.48,
      "grad_norm": 12399.978515625,
      "learning_rate": 3.874613003095975e-05,
      "loss": 27.5958,
      "step": 12471
    },
    {
      "epoch": 12.48,
      "grad_norm": 10720.6494140625,
      "learning_rate": 3.874097007223943e-05,
      "loss": 34.7759,
      "step": 12472
    },
    {
      "epoch": 12.49,
      "grad_norm": 7755.2890625,
      "learning_rate": 3.8735810113519095e-05,
      "loss": 25.5566,
      "step": 12473
    },
    {
      "epoch": 12.49,
      "grad_norm": 86735.328125,
      "learning_rate": 3.873065015479876e-05,
      "loss": 27.8974,
      "step": 12474
    },
    {
      "epoch": 12.49,
      "grad_norm": 3489.2998046875,
      "learning_rate": 3.872549019607844e-05,
      "loss": 43.9342,
      "step": 12475
    },
    {
      "epoch": 12.49,
      "grad_norm": 21662.154296875,
      "learning_rate": 3.8720330237358105e-05,
      "loss": 26.6349,
      "step": 12476
    },
    {
      "epoch": 12.49,
      "grad_norm": 25688.09765625,
      "learning_rate": 3.871517027863777e-05,
      "loss": 26.6236,
      "step": 12477
    },
    {
      "epoch": 12.49,
      "grad_norm": 20267.36328125,
      "learning_rate": 3.871001031991745e-05,
      "loss": 36.0607,
      "step": 12478
    },
    {
      "epoch": 12.49,
      "grad_norm": 13689.03125,
      "learning_rate": 3.8704850361197115e-05,
      "loss": 38.5748,
      "step": 12479
    },
    {
      "epoch": 12.49,
      "grad_norm": 23558.232421875,
      "learning_rate": 3.869969040247678e-05,
      "loss": 29.681,
      "step": 12480
    },
    {
      "epoch": 12.49,
      "grad_norm": 4719.3818359375,
      "learning_rate": 3.869453044375645e-05,
      "loss": 33.5805,
      "step": 12481
    },
    {
      "epoch": 12.49,
      "grad_norm": 3711.472900390625,
      "learning_rate": 3.868937048503612e-05,
      "loss": 32.1741,
      "step": 12482
    },
    {
      "epoch": 12.5,
      "grad_norm": 7396.89306640625,
      "learning_rate": 3.868421052631579e-05,
      "loss": 20.5695,
      "step": 12483
    },
    {
      "epoch": 12.5,
      "grad_norm": 6320.64501953125,
      "learning_rate": 3.867905056759546e-05,
      "loss": 40.219,
      "step": 12484
    },
    {
      "epoch": 12.5,
      "grad_norm": 45867.5703125,
      "learning_rate": 3.867389060887513e-05,
      "loss": 39.4661,
      "step": 12485
    },
    {
      "epoch": 12.5,
      "grad_norm": 14092.408203125,
      "learning_rate": 3.86687306501548e-05,
      "loss": 36.7473,
      "step": 12486
    },
    {
      "epoch": 12.5,
      "grad_norm": 5797.93017578125,
      "learning_rate": 3.866357069143447e-05,
      "loss": 16.997,
      "step": 12487
    },
    {
      "epoch": 12.5,
      "grad_norm": 3586.738525390625,
      "learning_rate": 3.865841073271414e-05,
      "loss": 21.3369,
      "step": 12488
    },
    {
      "epoch": 12.5,
      "grad_norm": 10963.587890625,
      "learning_rate": 3.865325077399381e-05,
      "loss": 38.2078,
      "step": 12489
    },
    {
      "epoch": 12.5,
      "grad_norm": 6327.0244140625,
      "learning_rate": 3.864809081527348e-05,
      "loss": 36.5749,
      "step": 12490
    },
    {
      "epoch": 12.5,
      "grad_norm": 52590.38671875,
      "learning_rate": 3.864293085655315e-05,
      "loss": 34.7791,
      "step": 12491
    },
    {
      "epoch": 12.5,
      "grad_norm": 9996.2939453125,
      "learning_rate": 3.863777089783282e-05,
      "loss": 35.3385,
      "step": 12492
    },
    {
      "epoch": 12.51,
      "grad_norm": 3944.36572265625,
      "learning_rate": 3.863261093911249e-05,
      "loss": 33.0599,
      "step": 12493
    },
    {
      "epoch": 12.51,
      "grad_norm": 25713.587890625,
      "learning_rate": 3.862745098039216e-05,
      "loss": 24.4486,
      "step": 12494
    },
    {
      "epoch": 12.51,
      "grad_norm": 6234.69140625,
      "learning_rate": 3.862229102167183e-05,
      "loss": 44.7525,
      "step": 12495
    },
    {
      "epoch": 12.51,
      "grad_norm": 26503.607421875,
      "learning_rate": 3.86171310629515e-05,
      "loss": 39.6647,
      "step": 12496
    },
    {
      "epoch": 12.51,
      "grad_norm": 29002.62890625,
      "learning_rate": 3.861197110423117e-05,
      "loss": 30.3917,
      "step": 12497
    },
    {
      "epoch": 12.51,
      "grad_norm": 5190.96728515625,
      "learning_rate": 3.8606811145510836e-05,
      "loss": 39.6049,
      "step": 12498
    },
    {
      "epoch": 12.51,
      "grad_norm": 4730.27587890625,
      "learning_rate": 3.8601651186790503e-05,
      "loss": 40.7932,
      "step": 12499
    },
    {
      "epoch": 12.51,
      "grad_norm": 7691.7392578125,
      "learning_rate": 3.859649122807018e-05,
      "loss": 23.7876,
      "step": 12500
    },
    {
      "epoch": 12.51,
      "grad_norm": 20919.607421875,
      "learning_rate": 3.8591331269349846e-05,
      "loss": 39.4066,
      "step": 12501
    },
    {
      "epoch": 12.51,
      "grad_norm": 11577.37109375,
      "learning_rate": 3.8586171310629513e-05,
      "loss": 23.4461,
      "step": 12502
    },
    {
      "epoch": 12.52,
      "grad_norm": 8392.556640625,
      "learning_rate": 3.858101135190919e-05,
      "loss": 21.8768,
      "step": 12503
    },
    {
      "epoch": 12.52,
      "grad_norm": 184996.265625,
      "learning_rate": 3.8575851393188856e-05,
      "loss": 31.7406,
      "step": 12504
    },
    {
      "epoch": 12.52,
      "grad_norm": 8098.70751953125,
      "learning_rate": 3.8570691434468523e-05,
      "loss": 38.2384,
      "step": 12505
    },
    {
      "epoch": 12.52,
      "grad_norm": 11799.4775390625,
      "learning_rate": 3.85655314757482e-05,
      "loss": 42.9322,
      "step": 12506
    },
    {
      "epoch": 12.52,
      "grad_norm": 5867.46923828125,
      "learning_rate": 3.8560371517027866e-05,
      "loss": 31.3724,
      "step": 12507
    },
    {
      "epoch": 12.52,
      "grad_norm": 5156.70556640625,
      "learning_rate": 3.8555211558307533e-05,
      "loss": 47.3651,
      "step": 12508
    },
    {
      "epoch": 12.52,
      "grad_norm": 1853.9774169921875,
      "learning_rate": 3.855005159958721e-05,
      "loss": 28.657,
      "step": 12509
    },
    {
      "epoch": 12.52,
      "grad_norm": 6259.521484375,
      "learning_rate": 3.8544891640866876e-05,
      "loss": 34.2479,
      "step": 12510
    },
    {
      "epoch": 12.52,
      "grad_norm": 9392.7646484375,
      "learning_rate": 3.8539731682146543e-05,
      "loss": 28.337,
      "step": 12511
    },
    {
      "epoch": 12.52,
      "grad_norm": 45374.0625,
      "learning_rate": 3.853457172342622e-05,
      "loss": 47.6252,
      "step": 12512
    },
    {
      "epoch": 12.53,
      "grad_norm": 35757.98046875,
      "learning_rate": 3.8529411764705886e-05,
      "loss": 21.6713,
      "step": 12513
    },
    {
      "epoch": 12.53,
      "grad_norm": 14120.7578125,
      "learning_rate": 3.8524251805985553e-05,
      "loss": 44.434,
      "step": 12514
    },
    {
      "epoch": 12.53,
      "grad_norm": 23658.4296875,
      "learning_rate": 3.851909184726523e-05,
      "loss": 49.8448,
      "step": 12515
    },
    {
      "epoch": 12.53,
      "grad_norm": 11051.1005859375,
      "learning_rate": 3.851393188854489e-05,
      "loss": 38.845,
      "step": 12516
    },
    {
      "epoch": 12.53,
      "grad_norm": 7543.83447265625,
      "learning_rate": 3.8508771929824563e-05,
      "loss": 37.3733,
      "step": 12517
    },
    {
      "epoch": 12.53,
      "grad_norm": 6576.4423828125,
      "learning_rate": 3.850361197110423e-05,
      "loss": 49.0362,
      "step": 12518
    },
    {
      "epoch": 12.53,
      "grad_norm": 2847.564208984375,
      "learning_rate": 3.84984520123839e-05,
      "loss": 24.3306,
      "step": 12519
    },
    {
      "epoch": 12.53,
      "grad_norm": 5989.0302734375,
      "learning_rate": 3.8493292053663574e-05,
      "loss": 29.1792,
      "step": 12520
    },
    {
      "epoch": 12.53,
      "grad_norm": 39857.55859375,
      "learning_rate": 3.848813209494324e-05,
      "loss": 41.0674,
      "step": 12521
    },
    {
      "epoch": 12.53,
      "grad_norm": 24221.69140625,
      "learning_rate": 3.848297213622291e-05,
      "loss": 35.3285,
      "step": 12522
    },
    {
      "epoch": 12.54,
      "grad_norm": 7279.01953125,
      "learning_rate": 3.8477812177502584e-05,
      "loss": 39.3706,
      "step": 12523
    },
    {
      "epoch": 12.54,
      "grad_norm": 18285.0859375,
      "learning_rate": 3.847265221878225e-05,
      "loss": 24.4714,
      "step": 12524
    },
    {
      "epoch": 12.54,
      "grad_norm": 12905.189453125,
      "learning_rate": 3.846749226006192e-05,
      "loss": 36.8241,
      "step": 12525
    },
    {
      "epoch": 12.54,
      "grad_norm": 18302.75,
      "learning_rate": 3.8462332301341594e-05,
      "loss": 37.9573,
      "step": 12526
    },
    {
      "epoch": 12.54,
      "grad_norm": 20153.9296875,
      "learning_rate": 3.845717234262126e-05,
      "loss": 42.6183,
      "step": 12527
    },
    {
      "epoch": 12.54,
      "grad_norm": 6645.08984375,
      "learning_rate": 3.845201238390093e-05,
      "loss": 22.5207,
      "step": 12528
    },
    {
      "epoch": 12.54,
      "grad_norm": 7795.82568359375,
      "learning_rate": 3.8446852425180604e-05,
      "loss": 40.5943,
      "step": 12529
    },
    {
      "epoch": 12.54,
      "grad_norm": 3279.79931640625,
      "learning_rate": 3.844169246646027e-05,
      "loss": 24.1546,
      "step": 12530
    },
    {
      "epoch": 12.54,
      "grad_norm": 20247.349609375,
      "learning_rate": 3.843653250773994e-05,
      "loss": 38.1613,
      "step": 12531
    },
    {
      "epoch": 12.54,
      "grad_norm": 44705.1953125,
      "learning_rate": 3.8431372549019614e-05,
      "loss": 28.234,
      "step": 12532
    },
    {
      "epoch": 12.55,
      "grad_norm": 65843.96875,
      "learning_rate": 3.842621259029928e-05,
      "loss": 40.0753,
      "step": 12533
    },
    {
      "epoch": 12.55,
      "grad_norm": 9831.0322265625,
      "learning_rate": 3.842105263157895e-05,
      "loss": 33.6666,
      "step": 12534
    },
    {
      "epoch": 12.55,
      "grad_norm": 26541.46484375,
      "learning_rate": 3.841589267285862e-05,
      "loss": 39.1247,
      "step": 12535
    },
    {
      "epoch": 12.55,
      "grad_norm": 8064.732421875,
      "learning_rate": 3.8410732714138284e-05,
      "loss": 30.8876,
      "step": 12536
    },
    {
      "epoch": 12.55,
      "grad_norm": 524500.8125,
      "learning_rate": 3.840557275541796e-05,
      "loss": 43.6885,
      "step": 12537
    },
    {
      "epoch": 12.55,
      "grad_norm": 4099.0830078125,
      "learning_rate": 3.840041279669763e-05,
      "loss": 34.3149,
      "step": 12538
    },
    {
      "epoch": 12.55,
      "grad_norm": 11554.845703125,
      "learning_rate": 3.8395252837977294e-05,
      "loss": 36.8609,
      "step": 12539
    },
    {
      "epoch": 12.55,
      "grad_norm": 2475.603515625,
      "learning_rate": 3.839009287925697e-05,
      "loss": 36.2689,
      "step": 12540
    },
    {
      "epoch": 12.55,
      "grad_norm": 24355.470703125,
      "learning_rate": 3.838493292053664e-05,
      "loss": 27.1777,
      "step": 12541
    },
    {
      "epoch": 12.55,
      "grad_norm": 2920.5283203125,
      "learning_rate": 3.8379772961816304e-05,
      "loss": 35.4448,
      "step": 12542
    },
    {
      "epoch": 12.56,
      "grad_norm": 4448.36962890625,
      "learning_rate": 3.837461300309598e-05,
      "loss": 32.9755,
      "step": 12543
    },
    {
      "epoch": 12.56,
      "grad_norm": 1935.7529296875,
      "learning_rate": 3.836945304437565e-05,
      "loss": 41.7214,
      "step": 12544
    },
    {
      "epoch": 12.56,
      "grad_norm": 78782.1640625,
      "learning_rate": 3.8364293085655314e-05,
      "loss": 34.1787,
      "step": 12545
    },
    {
      "epoch": 12.56,
      "grad_norm": 72482.4140625,
      "learning_rate": 3.835913312693499e-05,
      "loss": 24.9255,
      "step": 12546
    },
    {
      "epoch": 12.56,
      "grad_norm": 2398.59228515625,
      "learning_rate": 3.835397316821466e-05,
      "loss": 38.7796,
      "step": 12547
    },
    {
      "epoch": 12.56,
      "grad_norm": 4701.23681640625,
      "learning_rate": 3.8348813209494324e-05,
      "loss": 24.4625,
      "step": 12548
    },
    {
      "epoch": 12.56,
      "grad_norm": 4440.16064453125,
      "learning_rate": 3.8343653250774e-05,
      "loss": 30.8815,
      "step": 12549
    },
    {
      "epoch": 12.56,
      "grad_norm": 6676.63916015625,
      "learning_rate": 3.833849329205367e-05,
      "loss": 35.3549,
      "step": 12550
    },
    {
      "epoch": 12.56,
      "grad_norm": 2088.726318359375,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 37.7906,
      "step": 12551
    },
    {
      "epoch": 12.56,
      "grad_norm": 11051.060546875,
      "learning_rate": 3.8328173374613e-05,
      "loss": 34.6572,
      "step": 12552
    },
    {
      "epoch": 12.57,
      "grad_norm": 2741.869140625,
      "learning_rate": 3.832301341589267e-05,
      "loss": 34.3399,
      "step": 12553
    },
    {
      "epoch": 12.57,
      "grad_norm": 63248.953125,
      "learning_rate": 3.8317853457172344e-05,
      "loss": 40.0615,
      "step": 12554
    },
    {
      "epoch": 12.57,
      "grad_norm": 23547.8125,
      "learning_rate": 3.831269349845201e-05,
      "loss": 34.7874,
      "step": 12555
    },
    {
      "epoch": 12.57,
      "grad_norm": 6984.26025390625,
      "learning_rate": 3.830753353973168e-05,
      "loss": 50.9747,
      "step": 12556
    },
    {
      "epoch": 12.57,
      "grad_norm": 2275.366455078125,
      "learning_rate": 3.8302373581011354e-05,
      "loss": 24.4657,
      "step": 12557
    },
    {
      "epoch": 12.57,
      "grad_norm": 19188.474609375,
      "learning_rate": 3.829721362229102e-05,
      "loss": 39.6655,
      "step": 12558
    },
    {
      "epoch": 12.57,
      "grad_norm": 18159.04296875,
      "learning_rate": 3.829205366357069e-05,
      "loss": 41.7945,
      "step": 12559
    },
    {
      "epoch": 12.57,
      "grad_norm": 17170.43359375,
      "learning_rate": 3.8286893704850365e-05,
      "loss": 39.5192,
      "step": 12560
    },
    {
      "epoch": 12.57,
      "grad_norm": 15223.4443359375,
      "learning_rate": 3.828173374613003e-05,
      "loss": 27.2316,
      "step": 12561
    },
    {
      "epoch": 12.57,
      "grad_norm": 19144.705078125,
      "learning_rate": 3.82765737874097e-05,
      "loss": 33.0889,
      "step": 12562
    },
    {
      "epoch": 12.58,
      "grad_norm": 17980.421875,
      "learning_rate": 3.8271413828689375e-05,
      "loss": 42.8037,
      "step": 12563
    },
    {
      "epoch": 12.58,
      "grad_norm": 22059.861328125,
      "learning_rate": 3.826625386996904e-05,
      "loss": 21.5876,
      "step": 12564
    },
    {
      "epoch": 12.58,
      "grad_norm": 68030.65625,
      "learning_rate": 3.826109391124871e-05,
      "loss": 30.878,
      "step": 12565
    },
    {
      "epoch": 12.58,
      "grad_norm": 26435.373046875,
      "learning_rate": 3.8255933952528385e-05,
      "loss": 32.7161,
      "step": 12566
    },
    {
      "epoch": 12.58,
      "grad_norm": 18874.58203125,
      "learning_rate": 3.825077399380805e-05,
      "loss": 42.3712,
      "step": 12567
    },
    {
      "epoch": 12.58,
      "grad_norm": 3532.8642578125,
      "learning_rate": 3.824561403508773e-05,
      "loss": 26.4609,
      "step": 12568
    },
    {
      "epoch": 12.58,
      "grad_norm": 1970.218017578125,
      "learning_rate": 3.8240454076367395e-05,
      "loss": 30.9759,
      "step": 12569
    },
    {
      "epoch": 12.58,
      "grad_norm": 15349.0693359375,
      "learning_rate": 3.8235294117647055e-05,
      "loss": 41.2982,
      "step": 12570
    },
    {
      "epoch": 12.58,
      "grad_norm": 2448357.75,
      "learning_rate": 3.823013415892673e-05,
      "loss": 24.167,
      "step": 12571
    },
    {
      "epoch": 12.58,
      "grad_norm": 13088.6396484375,
      "learning_rate": 3.82249742002064e-05,
      "loss": 35.4432,
      "step": 12572
    },
    {
      "epoch": 12.59,
      "grad_norm": 413772.9375,
      "learning_rate": 3.8219814241486065e-05,
      "loss": 38.1446,
      "step": 12573
    },
    {
      "epoch": 12.59,
      "grad_norm": 139463.828125,
      "learning_rate": 3.821465428276574e-05,
      "loss": 42.4886,
      "step": 12574
    },
    {
      "epoch": 12.59,
      "grad_norm": 5104.26611328125,
      "learning_rate": 3.820949432404541e-05,
      "loss": 32.006,
      "step": 12575
    },
    {
      "epoch": 12.59,
      "grad_norm": 22841.318359375,
      "learning_rate": 3.8204334365325075e-05,
      "loss": 26.333,
      "step": 12576
    },
    {
      "epoch": 12.59,
      "grad_norm": 11688.3857421875,
      "learning_rate": 3.819917440660475e-05,
      "loss": 37.5041,
      "step": 12577
    },
    {
      "epoch": 12.59,
      "grad_norm": 3411.580322265625,
      "learning_rate": 3.819401444788442e-05,
      "loss": 47.054,
      "step": 12578
    },
    {
      "epoch": 12.59,
      "grad_norm": 2532.009765625,
      "learning_rate": 3.8188854489164085e-05,
      "loss": 24.9394,
      "step": 12579
    },
    {
      "epoch": 12.59,
      "grad_norm": 18314.630859375,
      "learning_rate": 3.818369453044376e-05,
      "loss": 40.8259,
      "step": 12580
    },
    {
      "epoch": 12.59,
      "grad_norm": 31179.654296875,
      "learning_rate": 3.817853457172343e-05,
      "loss": 23.0995,
      "step": 12581
    },
    {
      "epoch": 12.59,
      "grad_norm": 5530.8388671875,
      "learning_rate": 3.81733746130031e-05,
      "loss": 33.6837,
      "step": 12582
    },
    {
      "epoch": 12.6,
      "grad_norm": 39682.921875,
      "learning_rate": 3.816821465428277e-05,
      "loss": 35.8254,
      "step": 12583
    },
    {
      "epoch": 12.6,
      "grad_norm": 22722.85546875,
      "learning_rate": 3.816305469556244e-05,
      "loss": 26.3874,
      "step": 12584
    },
    {
      "epoch": 12.6,
      "grad_norm": 6434.3037109375,
      "learning_rate": 3.815789473684211e-05,
      "loss": 37.1764,
      "step": 12585
    },
    {
      "epoch": 12.6,
      "grad_norm": 16601.7578125,
      "learning_rate": 3.815273477812178e-05,
      "loss": 39.9517,
      "step": 12586
    },
    {
      "epoch": 12.6,
      "grad_norm": 10674.2001953125,
      "learning_rate": 3.814757481940145e-05,
      "loss": 26.9586,
      "step": 12587
    },
    {
      "epoch": 12.6,
      "grad_norm": 3579.624755859375,
      "learning_rate": 3.8142414860681115e-05,
      "loss": 24.1077,
      "step": 12588
    },
    {
      "epoch": 12.6,
      "grad_norm": 60622.7265625,
      "learning_rate": 3.813725490196078e-05,
      "loss": 35.0437,
      "step": 12589
    },
    {
      "epoch": 12.6,
      "grad_norm": 3919.83056640625,
      "learning_rate": 3.813209494324045e-05,
      "loss": 37.9413,
      "step": 12590
    },
    {
      "epoch": 12.6,
      "grad_norm": 82595.5390625,
      "learning_rate": 3.8126934984520125e-05,
      "loss": 38.475,
      "step": 12591
    },
    {
      "epoch": 12.6,
      "grad_norm": 16884.458984375,
      "learning_rate": 3.812177502579979e-05,
      "loss": 46.4834,
      "step": 12592
    },
    {
      "epoch": 12.61,
      "grad_norm": 25399.18359375,
      "learning_rate": 3.811661506707946e-05,
      "loss": 40.3752,
      "step": 12593
    },
    {
      "epoch": 12.61,
      "grad_norm": 6769.30224609375,
      "learning_rate": 3.8111455108359135e-05,
      "loss": 30.9046,
      "step": 12594
    },
    {
      "epoch": 12.61,
      "grad_norm": 139358.890625,
      "learning_rate": 3.81062951496388e-05,
      "loss": 26.3877,
      "step": 12595
    },
    {
      "epoch": 12.61,
      "grad_norm": 12821.5390625,
      "learning_rate": 3.810113519091848e-05,
      "loss": 25.3461,
      "step": 12596
    },
    {
      "epoch": 12.61,
      "grad_norm": 9162.12890625,
      "learning_rate": 3.8095975232198145e-05,
      "loss": 24.5158,
      "step": 12597
    },
    {
      "epoch": 12.61,
      "grad_norm": 78401.953125,
      "learning_rate": 3.809081527347781e-05,
      "loss": 33.7295,
      "step": 12598
    },
    {
      "epoch": 12.61,
      "grad_norm": 10996.5263671875,
      "learning_rate": 3.808565531475749e-05,
      "loss": 49.0499,
      "step": 12599
    },
    {
      "epoch": 12.61,
      "grad_norm": 7129.63427734375,
      "learning_rate": 3.8080495356037155e-05,
      "loss": 43.714,
      "step": 12600
    },
    {
      "epoch": 12.61,
      "grad_norm": 5723.44189453125,
      "learning_rate": 3.807533539731682e-05,
      "loss": 41.6142,
      "step": 12601
    },
    {
      "epoch": 12.61,
      "grad_norm": 4476.32763671875,
      "learning_rate": 3.80701754385965e-05,
      "loss": 27.2502,
      "step": 12602
    },
    {
      "epoch": 12.62,
      "grad_norm": 31215.751953125,
      "learning_rate": 3.8065015479876166e-05,
      "loss": 51.4575,
      "step": 12603
    },
    {
      "epoch": 12.62,
      "grad_norm": 1571.607177734375,
      "learning_rate": 3.805985552115583e-05,
      "loss": 40.4169,
      "step": 12604
    },
    {
      "epoch": 12.62,
      "grad_norm": 8936.0380859375,
      "learning_rate": 3.805469556243551e-05,
      "loss": 30.7201,
      "step": 12605
    },
    {
      "epoch": 12.62,
      "grad_norm": 28998.076171875,
      "learning_rate": 3.804953560371517e-05,
      "loss": 30.7448,
      "step": 12606
    },
    {
      "epoch": 12.62,
      "grad_norm": 3033.10888671875,
      "learning_rate": 3.8044375644994836e-05,
      "loss": 27.2032,
      "step": 12607
    },
    {
      "epoch": 12.62,
      "grad_norm": 9939.8525390625,
      "learning_rate": 3.803921568627451e-05,
      "loss": 45.9181,
      "step": 12608
    },
    {
      "epoch": 12.62,
      "grad_norm": 28139.099609375,
      "learning_rate": 3.803405572755418e-05,
      "loss": 32.8644,
      "step": 12609
    },
    {
      "epoch": 12.62,
      "grad_norm": 4570.1181640625,
      "learning_rate": 3.8028895768833846e-05,
      "loss": 38.7826,
      "step": 12610
    },
    {
      "epoch": 12.62,
      "grad_norm": 23315.0234375,
      "learning_rate": 3.802373581011352e-05,
      "loss": 38.1952,
      "step": 12611
    },
    {
      "epoch": 12.62,
      "grad_norm": 15405.3876953125,
      "learning_rate": 3.801857585139319e-05,
      "loss": 26.4543,
      "step": 12612
    },
    {
      "epoch": 12.63,
      "grad_norm": 14599.0654296875,
      "learning_rate": 3.801341589267286e-05,
      "loss": 46.4914,
      "step": 12613
    },
    {
      "epoch": 12.63,
      "grad_norm": 35015.72265625,
      "learning_rate": 3.800825593395253e-05,
      "loss": 26.2743,
      "step": 12614
    },
    {
      "epoch": 12.63,
      "grad_norm": 32368.294921875,
      "learning_rate": 3.80030959752322e-05,
      "loss": 34.5968,
      "step": 12615
    },
    {
      "epoch": 12.63,
      "grad_norm": 4022.098388671875,
      "learning_rate": 3.799793601651187e-05,
      "loss": 45.3445,
      "step": 12616
    },
    {
      "epoch": 12.63,
      "grad_norm": 72217.9296875,
      "learning_rate": 3.799277605779154e-05,
      "loss": 37.9619,
      "step": 12617
    },
    {
      "epoch": 12.63,
      "grad_norm": 19058.509765625,
      "learning_rate": 3.798761609907121e-05,
      "loss": 27.1686,
      "step": 12618
    },
    {
      "epoch": 12.63,
      "grad_norm": 6523.04541015625,
      "learning_rate": 3.798245614035088e-05,
      "loss": 29.2517,
      "step": 12619
    },
    {
      "epoch": 12.63,
      "grad_norm": 9771.326171875,
      "learning_rate": 3.797729618163055e-05,
      "loss": 33.8311,
      "step": 12620
    },
    {
      "epoch": 12.63,
      "grad_norm": 5816.19287109375,
      "learning_rate": 3.797213622291022e-05,
      "loss": 42.3261,
      "step": 12621
    },
    {
      "epoch": 12.63,
      "grad_norm": 5650.4716796875,
      "learning_rate": 3.796697626418989e-05,
      "loss": 22.2089,
      "step": 12622
    },
    {
      "epoch": 12.64,
      "grad_norm": 11175.8798828125,
      "learning_rate": 3.796181630546956e-05,
      "loss": 22.3861,
      "step": 12623
    },
    {
      "epoch": 12.64,
      "grad_norm": 35985.625,
      "learning_rate": 3.795665634674922e-05,
      "loss": 43.1045,
      "step": 12624
    },
    {
      "epoch": 12.64,
      "grad_norm": 5258.5908203125,
      "learning_rate": 3.7951496388028896e-05,
      "loss": 40.6526,
      "step": 12625
    },
    {
      "epoch": 12.64,
      "grad_norm": 15406.234375,
      "learning_rate": 3.7946336429308564e-05,
      "loss": 27.9162,
      "step": 12626
    },
    {
      "epoch": 12.64,
      "grad_norm": 8462.923828125,
      "learning_rate": 3.794117647058824e-05,
      "loss": 34.5991,
      "step": 12627
    },
    {
      "epoch": 12.64,
      "grad_norm": 130567.1796875,
      "learning_rate": 3.7936016511867906e-05,
      "loss": 46.6517,
      "step": 12628
    },
    {
      "epoch": 12.64,
      "grad_norm": 51412.046875,
      "learning_rate": 3.7930856553147574e-05,
      "loss": 31.3161,
      "step": 12629
    },
    {
      "epoch": 12.64,
      "grad_norm": 3255.430419921875,
      "learning_rate": 3.792569659442725e-05,
      "loss": 37.3085,
      "step": 12630
    },
    {
      "epoch": 12.64,
      "grad_norm": 13440.369140625,
      "learning_rate": 3.7920536635706916e-05,
      "loss": 25.9882,
      "step": 12631
    },
    {
      "epoch": 12.64,
      "grad_norm": 133796.40625,
      "learning_rate": 3.7915376676986584e-05,
      "loss": 43.5313,
      "step": 12632
    },
    {
      "epoch": 12.65,
      "grad_norm": 37907.38671875,
      "learning_rate": 3.791021671826626e-05,
      "loss": 19.6786,
      "step": 12633
    },
    {
      "epoch": 12.65,
      "grad_norm": 20159.908203125,
      "learning_rate": 3.7905056759545926e-05,
      "loss": 40.3379,
      "step": 12634
    },
    {
      "epoch": 12.65,
      "grad_norm": 66870.2265625,
      "learning_rate": 3.7899896800825594e-05,
      "loss": 32.836,
      "step": 12635
    },
    {
      "epoch": 12.65,
      "grad_norm": 9332.9921875,
      "learning_rate": 3.789473684210527e-05,
      "loss": 46.6925,
      "step": 12636
    },
    {
      "epoch": 12.65,
      "grad_norm": 6187.3544921875,
      "learning_rate": 3.7889576883384936e-05,
      "loss": 55.4641,
      "step": 12637
    },
    {
      "epoch": 12.65,
      "grad_norm": 2782.708251953125,
      "learning_rate": 3.7884416924664604e-05,
      "loss": 38.0529,
      "step": 12638
    },
    {
      "epoch": 12.65,
      "grad_norm": 6329.74609375,
      "learning_rate": 3.787925696594428e-05,
      "loss": 25.9736,
      "step": 12639
    },
    {
      "epoch": 12.65,
      "grad_norm": 15770.0751953125,
      "learning_rate": 3.7874097007223946e-05,
      "loss": 33.442,
      "step": 12640
    },
    {
      "epoch": 12.65,
      "grad_norm": 59409.6796875,
      "learning_rate": 3.7868937048503614e-05,
      "loss": 33.5884,
      "step": 12641
    },
    {
      "epoch": 12.65,
      "grad_norm": 8693.8369140625,
      "learning_rate": 3.786377708978328e-05,
      "loss": 28.8536,
      "step": 12642
    },
    {
      "epoch": 12.66,
      "grad_norm": 15787.4853515625,
      "learning_rate": 3.785861713106295e-05,
      "loss": 41.1372,
      "step": 12643
    },
    {
      "epoch": 12.66,
      "grad_norm": 15367.2314453125,
      "learning_rate": 3.7853457172342624e-05,
      "loss": 33.0136,
      "step": 12644
    },
    {
      "epoch": 12.66,
      "grad_norm": 7368.865234375,
      "learning_rate": 3.784829721362229e-05,
      "loss": 28.3053,
      "step": 12645
    },
    {
      "epoch": 12.66,
      "grad_norm": 8248.5595703125,
      "learning_rate": 3.784313725490196e-05,
      "loss": 36.9331,
      "step": 12646
    },
    {
      "epoch": 12.66,
      "grad_norm": 29971.7109375,
      "learning_rate": 3.7837977296181634e-05,
      "loss": 43.1893,
      "step": 12647
    },
    {
      "epoch": 12.66,
      "grad_norm": 5487.80078125,
      "learning_rate": 3.78328173374613e-05,
      "loss": 40.4076,
      "step": 12648
    },
    {
      "epoch": 12.66,
      "grad_norm": 10264.50390625,
      "learning_rate": 3.782765737874097e-05,
      "loss": 35.7935,
      "step": 12649
    },
    {
      "epoch": 12.66,
      "grad_norm": 17408.662109375,
      "learning_rate": 3.7822497420020644e-05,
      "loss": 39.9686,
      "step": 12650
    },
    {
      "epoch": 12.66,
      "grad_norm": 158838.765625,
      "learning_rate": 3.781733746130031e-05,
      "loss": 28.5874,
      "step": 12651
    },
    {
      "epoch": 12.66,
      "grad_norm": 10683.5126953125,
      "learning_rate": 3.781217750257998e-05,
      "loss": 40.9431,
      "step": 12652
    },
    {
      "epoch": 12.67,
      "grad_norm": 4585.76318359375,
      "learning_rate": 3.7807017543859654e-05,
      "loss": 31.5862,
      "step": 12653
    },
    {
      "epoch": 12.67,
      "grad_norm": 3432.24609375,
      "learning_rate": 3.780185758513932e-05,
      "loss": 30.0846,
      "step": 12654
    },
    {
      "epoch": 12.67,
      "grad_norm": 27449.484375,
      "learning_rate": 3.779669762641899e-05,
      "loss": 40.0352,
      "step": 12655
    },
    {
      "epoch": 12.67,
      "grad_norm": 31566.849609375,
      "learning_rate": 3.7791537667698664e-05,
      "loss": 39.8012,
      "step": 12656
    },
    {
      "epoch": 12.67,
      "grad_norm": 31166.794921875,
      "learning_rate": 3.778637770897833e-05,
      "loss": 30.8495,
      "step": 12657
    },
    {
      "epoch": 12.67,
      "grad_norm": 5160.1669921875,
      "learning_rate": 3.7781217750258e-05,
      "loss": 35.5002,
      "step": 12658
    },
    {
      "epoch": 12.67,
      "grad_norm": 7078.18505859375,
      "learning_rate": 3.777605779153767e-05,
      "loss": 17.6748,
      "step": 12659
    },
    {
      "epoch": 12.67,
      "grad_norm": 12094.83984375,
      "learning_rate": 3.7770897832817335e-05,
      "loss": 16.805,
      "step": 12660
    },
    {
      "epoch": 12.67,
      "grad_norm": 1397.975341796875,
      "learning_rate": 3.776573787409701e-05,
      "loss": 22.0601,
      "step": 12661
    },
    {
      "epoch": 12.67,
      "grad_norm": 32262.796875,
      "learning_rate": 3.776057791537668e-05,
      "loss": 47.2824,
      "step": 12662
    },
    {
      "epoch": 12.68,
      "grad_norm": 6054.25390625,
      "learning_rate": 3.7755417956656345e-05,
      "loss": 34.147,
      "step": 12663
    },
    {
      "epoch": 12.68,
      "grad_norm": 10299.3447265625,
      "learning_rate": 3.775025799793602e-05,
      "loss": 47.8323,
      "step": 12664
    },
    {
      "epoch": 12.68,
      "grad_norm": 59804.90234375,
      "learning_rate": 3.774509803921569e-05,
      "loss": 46.8376,
      "step": 12665
    },
    {
      "epoch": 12.68,
      "grad_norm": 9222.5322265625,
      "learning_rate": 3.7739938080495355e-05,
      "loss": 35.0664,
      "step": 12666
    },
    {
      "epoch": 12.68,
      "grad_norm": 63726.3515625,
      "learning_rate": 3.773477812177503e-05,
      "loss": 32.6837,
      "step": 12667
    },
    {
      "epoch": 12.68,
      "grad_norm": 32432.748046875,
      "learning_rate": 3.77296181630547e-05,
      "loss": 31.5319,
      "step": 12668
    },
    {
      "epoch": 12.68,
      "grad_norm": 47654.28515625,
      "learning_rate": 3.7724458204334365e-05,
      "loss": 25.5412,
      "step": 12669
    },
    {
      "epoch": 12.68,
      "grad_norm": 15787.9267578125,
      "learning_rate": 3.771929824561404e-05,
      "loss": 20.1281,
      "step": 12670
    },
    {
      "epoch": 12.68,
      "grad_norm": 7502.26416015625,
      "learning_rate": 3.771413828689371e-05,
      "loss": 32.797,
      "step": 12671
    },
    {
      "epoch": 12.68,
      "grad_norm": 31920.15234375,
      "learning_rate": 3.7708978328173375e-05,
      "loss": 32.97,
      "step": 12672
    },
    {
      "epoch": 12.69,
      "grad_norm": 7946.0439453125,
      "learning_rate": 3.770381836945305e-05,
      "loss": 26.0223,
      "step": 12673
    },
    {
      "epoch": 12.69,
      "grad_norm": 5061.447265625,
      "learning_rate": 3.769865841073272e-05,
      "loss": 27.5265,
      "step": 12674
    },
    {
      "epoch": 12.69,
      "grad_norm": 7126.75927734375,
      "learning_rate": 3.7693498452012385e-05,
      "loss": 31.0398,
      "step": 12675
    },
    {
      "epoch": 12.69,
      "grad_norm": 18300.361328125,
      "learning_rate": 3.768833849329206e-05,
      "loss": 32.3162,
      "step": 12676
    },
    {
      "epoch": 12.69,
      "grad_norm": 5879.77880859375,
      "learning_rate": 3.768317853457172e-05,
      "loss": 39.6429,
      "step": 12677
    },
    {
      "epoch": 12.69,
      "grad_norm": 11112.5625,
      "learning_rate": 3.7678018575851395e-05,
      "loss": 34.2044,
      "step": 12678
    },
    {
      "epoch": 12.69,
      "grad_norm": 33223.67578125,
      "learning_rate": 3.767285861713106e-05,
      "loss": 38.4277,
      "step": 12679
    },
    {
      "epoch": 12.69,
      "grad_norm": 28029.091796875,
      "learning_rate": 3.766769865841073e-05,
      "loss": 37.7618,
      "step": 12680
    },
    {
      "epoch": 12.69,
      "grad_norm": 3296.488525390625,
      "learning_rate": 3.7662538699690405e-05,
      "loss": 21.2108,
      "step": 12681
    },
    {
      "epoch": 12.69,
      "grad_norm": 35002.7265625,
      "learning_rate": 3.765737874097007e-05,
      "loss": 41.0473,
      "step": 12682
    },
    {
      "epoch": 12.7,
      "grad_norm": 8085.478515625,
      "learning_rate": 3.765221878224974e-05,
      "loss": 35.4839,
      "step": 12683
    },
    {
      "epoch": 12.7,
      "grad_norm": 20111.576171875,
      "learning_rate": 3.7647058823529415e-05,
      "loss": 14.4563,
      "step": 12684
    },
    {
      "epoch": 12.7,
      "grad_norm": 92897.890625,
      "learning_rate": 3.764189886480908e-05,
      "loss": 34.919,
      "step": 12685
    },
    {
      "epoch": 12.7,
      "grad_norm": 17496.130859375,
      "learning_rate": 3.763673890608875e-05,
      "loss": 34.1194,
      "step": 12686
    },
    {
      "epoch": 12.7,
      "grad_norm": 13506.751953125,
      "learning_rate": 3.7631578947368425e-05,
      "loss": 31.876,
      "step": 12687
    },
    {
      "epoch": 12.7,
      "grad_norm": 109258.9453125,
      "learning_rate": 3.762641898864809e-05,
      "loss": 29.9028,
      "step": 12688
    },
    {
      "epoch": 12.7,
      "grad_norm": 2300.673828125,
      "learning_rate": 3.762125902992776e-05,
      "loss": 33.8531,
      "step": 12689
    },
    {
      "epoch": 12.7,
      "grad_norm": 24118.9375,
      "learning_rate": 3.7616099071207435e-05,
      "loss": 21.3967,
      "step": 12690
    },
    {
      "epoch": 12.7,
      "grad_norm": 28487.53515625,
      "learning_rate": 3.76109391124871e-05,
      "loss": 40.8873,
      "step": 12691
    },
    {
      "epoch": 12.7,
      "grad_norm": 2283.237060546875,
      "learning_rate": 3.760577915376677e-05,
      "loss": 30.9881,
      "step": 12692
    },
    {
      "epoch": 12.71,
      "grad_norm": 19413.576171875,
      "learning_rate": 3.7600619195046445e-05,
      "loss": 39.8444,
      "step": 12693
    },
    {
      "epoch": 12.71,
      "grad_norm": 9305.4208984375,
      "learning_rate": 3.759545923632611e-05,
      "loss": 42.1083,
      "step": 12694
    },
    {
      "epoch": 12.71,
      "grad_norm": 20593.716796875,
      "learning_rate": 3.759029927760578e-05,
      "loss": 37.3611,
      "step": 12695
    },
    {
      "epoch": 12.71,
      "grad_norm": 13077.9765625,
      "learning_rate": 3.758513931888545e-05,
      "loss": 46.748,
      "step": 12696
    },
    {
      "epoch": 12.71,
      "grad_norm": 27130.29296875,
      "learning_rate": 3.7579979360165116e-05,
      "loss": 43.4664,
      "step": 12697
    },
    {
      "epoch": 12.71,
      "grad_norm": 49746.26953125,
      "learning_rate": 3.757481940144479e-05,
      "loss": 47.4245,
      "step": 12698
    },
    {
      "epoch": 12.71,
      "grad_norm": 3259.10888671875,
      "learning_rate": 3.756965944272446e-05,
      "loss": 40.2782,
      "step": 12699
    },
    {
      "epoch": 12.71,
      "grad_norm": 2051.28271484375,
      "learning_rate": 3.7564499484004126e-05,
      "loss": 34.8209,
      "step": 12700
    },
    {
      "epoch": 12.71,
      "grad_norm": 5992.53759765625,
      "learning_rate": 3.75593395252838e-05,
      "loss": 42.6023,
      "step": 12701
    },
    {
      "epoch": 12.71,
      "grad_norm": 54290.078125,
      "learning_rate": 3.755417956656347e-05,
      "loss": 42.3157,
      "step": 12702
    },
    {
      "epoch": 12.72,
      "grad_norm": 21626.873046875,
      "learning_rate": 3.7549019607843136e-05,
      "loss": 35.3625,
      "step": 12703
    },
    {
      "epoch": 12.72,
      "grad_norm": 24152.80859375,
      "learning_rate": 3.754385964912281e-05,
      "loss": 26.7686,
      "step": 12704
    },
    {
      "epoch": 12.72,
      "grad_norm": 13223.830078125,
      "learning_rate": 3.753869969040248e-05,
      "loss": 40.4797,
      "step": 12705
    },
    {
      "epoch": 12.72,
      "grad_norm": 11925.888671875,
      "learning_rate": 3.7533539731682146e-05,
      "loss": 21.6254,
      "step": 12706
    },
    {
      "epoch": 12.72,
      "grad_norm": 32002.447265625,
      "learning_rate": 3.752837977296182e-05,
      "loss": 24.3608,
      "step": 12707
    },
    {
      "epoch": 12.72,
      "grad_norm": 139969.453125,
      "learning_rate": 3.752321981424149e-05,
      "loss": 24.6991,
      "step": 12708
    },
    {
      "epoch": 12.72,
      "grad_norm": 6590.96240234375,
      "learning_rate": 3.7518059855521156e-05,
      "loss": 20.9375,
      "step": 12709
    },
    {
      "epoch": 12.72,
      "grad_norm": 14163.0546875,
      "learning_rate": 3.751289989680083e-05,
      "loss": 42.5156,
      "step": 12710
    },
    {
      "epoch": 12.72,
      "grad_norm": 15903.228515625,
      "learning_rate": 3.75077399380805e-05,
      "loss": 32.303,
      "step": 12711
    },
    {
      "epoch": 12.72,
      "grad_norm": 17787.708984375,
      "learning_rate": 3.7502579979360166e-05,
      "loss": 22.3936,
      "step": 12712
    },
    {
      "epoch": 12.73,
      "grad_norm": 24098.24609375,
      "learning_rate": 3.7497420020639834e-05,
      "loss": 28.0873,
      "step": 12713
    },
    {
      "epoch": 12.73,
      "grad_norm": 2147.715576171875,
      "learning_rate": 3.74922600619195e-05,
      "loss": 35.4676,
      "step": 12714
    },
    {
      "epoch": 12.73,
      "grad_norm": 2963.887939453125,
      "learning_rate": 3.7487100103199176e-05,
      "loss": 40.7945,
      "step": 12715
    },
    {
      "epoch": 12.73,
      "grad_norm": 28006.443359375,
      "learning_rate": 3.7481940144478844e-05,
      "loss": 37.6565,
      "step": 12716
    },
    {
      "epoch": 12.73,
      "grad_norm": 24830.396484375,
      "learning_rate": 3.747678018575851e-05,
      "loss": 38.9445,
      "step": 12717
    },
    {
      "epoch": 12.73,
      "grad_norm": 53862.625,
      "learning_rate": 3.7471620227038186e-05,
      "loss": 22.2895,
      "step": 12718
    },
    {
      "epoch": 12.73,
      "grad_norm": 7248.3681640625,
      "learning_rate": 3.7466460268317854e-05,
      "loss": 37.0091,
      "step": 12719
    },
    {
      "epoch": 12.73,
      "grad_norm": 15169.55859375,
      "learning_rate": 3.746130030959752e-05,
      "loss": 43.5178,
      "step": 12720
    },
    {
      "epoch": 12.73,
      "grad_norm": 13121.2021484375,
      "learning_rate": 3.7456140350877196e-05,
      "loss": 54.3534,
      "step": 12721
    },
    {
      "epoch": 12.73,
      "grad_norm": 15877.6337890625,
      "learning_rate": 3.7450980392156864e-05,
      "loss": 27.8508,
      "step": 12722
    },
    {
      "epoch": 12.74,
      "grad_norm": 38842.3515625,
      "learning_rate": 3.744582043343653e-05,
      "loss": 23.3802,
      "step": 12723
    },
    {
      "epoch": 12.74,
      "grad_norm": 22685.5703125,
      "learning_rate": 3.7440660474716206e-05,
      "loss": 51.0365,
      "step": 12724
    },
    {
      "epoch": 12.74,
      "grad_norm": 17396.013671875,
      "learning_rate": 3.7435500515995874e-05,
      "loss": 36.8692,
      "step": 12725
    },
    {
      "epoch": 12.74,
      "grad_norm": 13452.841796875,
      "learning_rate": 3.743034055727554e-05,
      "loss": 37.0407,
      "step": 12726
    },
    {
      "epoch": 12.74,
      "grad_norm": 38057.57421875,
      "learning_rate": 3.7425180598555216e-05,
      "loss": 28.7782,
      "step": 12727
    },
    {
      "epoch": 12.74,
      "grad_norm": 18904.076171875,
      "learning_rate": 3.7420020639834884e-05,
      "loss": 43.8266,
      "step": 12728
    },
    {
      "epoch": 12.74,
      "grad_norm": 7665.91357421875,
      "learning_rate": 3.741486068111456e-05,
      "loss": 38.4394,
      "step": 12729
    },
    {
      "epoch": 12.74,
      "grad_norm": 6763.72509765625,
      "learning_rate": 3.7409700722394226e-05,
      "loss": 32.0378,
      "step": 12730
    },
    {
      "epoch": 12.74,
      "grad_norm": 4663.244140625,
      "learning_rate": 3.740454076367389e-05,
      "loss": 41.053,
      "step": 12731
    },
    {
      "epoch": 12.74,
      "grad_norm": 9490.0859375,
      "learning_rate": 3.739938080495356e-05,
      "loss": 34.1111,
      "step": 12732
    },
    {
      "epoch": 12.75,
      "grad_norm": 9312.544921875,
      "learning_rate": 3.739422084623323e-05,
      "loss": 35.2471,
      "step": 12733
    },
    {
      "epoch": 12.75,
      "grad_norm": 7287.529296875,
      "learning_rate": 3.73890608875129e-05,
      "loss": 21.6245,
      "step": 12734
    },
    {
      "epoch": 12.75,
      "grad_norm": 66149.078125,
      "learning_rate": 3.738390092879257e-05,
      "loss": 31.3113,
      "step": 12735
    },
    {
      "epoch": 12.75,
      "grad_norm": 3819.844970703125,
      "learning_rate": 3.737874097007224e-05,
      "loss": 36.2256,
      "step": 12736
    },
    {
      "epoch": 12.75,
      "grad_norm": 8837.8349609375,
      "learning_rate": 3.737358101135191e-05,
      "loss": 36.1415,
      "step": 12737
    },
    {
      "epoch": 12.75,
      "grad_norm": 3677.402099609375,
      "learning_rate": 3.736842105263158e-05,
      "loss": 37.6553,
      "step": 12738
    },
    {
      "epoch": 12.75,
      "grad_norm": 14286.765625,
      "learning_rate": 3.736326109391125e-05,
      "loss": 27.7445,
      "step": 12739
    },
    {
      "epoch": 12.75,
      "grad_norm": 20364.001953125,
      "learning_rate": 3.735810113519092e-05,
      "loss": 36.1861,
      "step": 12740
    },
    {
      "epoch": 12.75,
      "grad_norm": 24807.20703125,
      "learning_rate": 3.735294117647059e-05,
      "loss": 48.6003,
      "step": 12741
    },
    {
      "epoch": 12.75,
      "grad_norm": 13108.794921875,
      "learning_rate": 3.734778121775026e-05,
      "loss": 40.1996,
      "step": 12742
    },
    {
      "epoch": 12.76,
      "grad_norm": 42397.64453125,
      "learning_rate": 3.7342621259029934e-05,
      "loss": 39.706,
      "step": 12743
    },
    {
      "epoch": 12.76,
      "grad_norm": 7430.45751953125,
      "learning_rate": 3.73374613003096e-05,
      "loss": 19.1971,
      "step": 12744
    },
    {
      "epoch": 12.76,
      "grad_norm": 9288.8134765625,
      "learning_rate": 3.733230134158927e-05,
      "loss": 31.841,
      "step": 12745
    },
    {
      "epoch": 12.76,
      "grad_norm": 14933.0654296875,
      "learning_rate": 3.7327141382868944e-05,
      "loss": 33.0138,
      "step": 12746
    },
    {
      "epoch": 12.76,
      "grad_norm": 12410.0615234375,
      "learning_rate": 3.732198142414861e-05,
      "loss": 21.5442,
      "step": 12747
    },
    {
      "epoch": 12.76,
      "grad_norm": 44440.82421875,
      "learning_rate": 3.731682146542828e-05,
      "loss": 38.1841,
      "step": 12748
    },
    {
      "epoch": 12.76,
      "grad_norm": 2986.17138671875,
      "learning_rate": 3.731166150670795e-05,
      "loss": 49.2308,
      "step": 12749
    },
    {
      "epoch": 12.76,
      "grad_norm": 13412.9453125,
      "learning_rate": 3.7306501547987615e-05,
      "loss": 29.1786,
      "step": 12750
    },
    {
      "epoch": 12.76,
      "grad_norm": 27133.556640625,
      "learning_rate": 3.730134158926728e-05,
      "loss": 33.9434,
      "step": 12751
    },
    {
      "epoch": 12.76,
      "grad_norm": 12848.4560546875,
      "learning_rate": 3.729618163054696e-05,
      "loss": 28.8642,
      "step": 12752
    },
    {
      "epoch": 12.77,
      "grad_norm": 45939.6875,
      "learning_rate": 3.7291021671826625e-05,
      "loss": 31.9716,
      "step": 12753
    },
    {
      "epoch": 12.77,
      "grad_norm": 111303.5,
      "learning_rate": 3.728586171310629e-05,
      "loss": 29.731,
      "step": 12754
    },
    {
      "epoch": 12.77,
      "grad_norm": 4320.212890625,
      "learning_rate": 3.728070175438597e-05,
      "loss": 29.0645,
      "step": 12755
    },
    {
      "epoch": 12.77,
      "grad_norm": 5277.0087890625,
      "learning_rate": 3.7275541795665635e-05,
      "loss": 27.7206,
      "step": 12756
    },
    {
      "epoch": 12.77,
      "grad_norm": 7710.84423828125,
      "learning_rate": 3.727038183694531e-05,
      "loss": 29.5711,
      "step": 12757
    },
    {
      "epoch": 12.77,
      "grad_norm": 29552.822265625,
      "learning_rate": 3.726522187822498e-05,
      "loss": 42.1632,
      "step": 12758
    },
    {
      "epoch": 12.77,
      "grad_norm": 107584.234375,
      "learning_rate": 3.7260061919504645e-05,
      "loss": 46.5428,
      "step": 12759
    },
    {
      "epoch": 12.77,
      "grad_norm": 11593.6552734375,
      "learning_rate": 3.725490196078432e-05,
      "loss": 28.449,
      "step": 12760
    },
    {
      "epoch": 12.77,
      "grad_norm": 5689.61865234375,
      "learning_rate": 3.724974200206399e-05,
      "loss": 41.8496,
      "step": 12761
    },
    {
      "epoch": 12.77,
      "grad_norm": 16321.2080078125,
      "learning_rate": 3.7244582043343655e-05,
      "loss": 32.5322,
      "step": 12762
    },
    {
      "epoch": 12.78,
      "grad_norm": 2922.336181640625,
      "learning_rate": 3.723942208462333e-05,
      "loss": 37.8215,
      "step": 12763
    },
    {
      "epoch": 12.78,
      "grad_norm": 2012.4031982421875,
      "learning_rate": 3.7234262125903e-05,
      "loss": 35.2146,
      "step": 12764
    },
    {
      "epoch": 12.78,
      "grad_norm": 10709.427734375,
      "learning_rate": 3.7229102167182665e-05,
      "loss": 29.7217,
      "step": 12765
    },
    {
      "epoch": 12.78,
      "grad_norm": 55770.63671875,
      "learning_rate": 3.722394220846234e-05,
      "loss": 30.0152,
      "step": 12766
    },
    {
      "epoch": 12.78,
      "grad_norm": 7800.8388671875,
      "learning_rate": 3.7218782249742e-05,
      "loss": 40.4756,
      "step": 12767
    },
    {
      "epoch": 12.78,
      "grad_norm": 10071.923828125,
      "learning_rate": 3.721362229102167e-05,
      "loss": 53.3737,
      "step": 12768
    },
    {
      "epoch": 12.78,
      "grad_norm": 20607.96875,
      "learning_rate": 3.720846233230134e-05,
      "loss": 40.5427,
      "step": 12769
    },
    {
      "epoch": 12.78,
      "grad_norm": 18008.25,
      "learning_rate": 3.720330237358101e-05,
      "loss": 28.5605,
      "step": 12770
    },
    {
      "epoch": 12.78,
      "grad_norm": 2855.0234375,
      "learning_rate": 3.7198142414860685e-05,
      "loss": 41.4193,
      "step": 12771
    },
    {
      "epoch": 12.78,
      "grad_norm": 8044.4794921875,
      "learning_rate": 3.719298245614035e-05,
      "loss": 39.2083,
      "step": 12772
    },
    {
      "epoch": 12.79,
      "grad_norm": 14470.953125,
      "learning_rate": 3.718782249742002e-05,
      "loss": 38.1808,
      "step": 12773
    },
    {
      "epoch": 12.79,
      "grad_norm": 14574.77734375,
      "learning_rate": 3.7182662538699695e-05,
      "loss": 27.1685,
      "step": 12774
    },
    {
      "epoch": 12.79,
      "grad_norm": 13889.4375,
      "learning_rate": 3.717750257997936e-05,
      "loss": 42.4226,
      "step": 12775
    },
    {
      "epoch": 12.79,
      "grad_norm": 40832.51171875,
      "learning_rate": 3.717234262125903e-05,
      "loss": 26.5435,
      "step": 12776
    },
    {
      "epoch": 12.79,
      "grad_norm": 8906.7919921875,
      "learning_rate": 3.7167182662538705e-05,
      "loss": 31.6198,
      "step": 12777
    },
    {
      "epoch": 12.79,
      "grad_norm": 7562.3271484375,
      "learning_rate": 3.716202270381837e-05,
      "loss": 20.5995,
      "step": 12778
    },
    {
      "epoch": 12.79,
      "grad_norm": 28953.015625,
      "learning_rate": 3.715686274509804e-05,
      "loss": 30.8674,
      "step": 12779
    },
    {
      "epoch": 12.79,
      "grad_norm": 14748.23046875,
      "learning_rate": 3.7151702786377715e-05,
      "loss": 25.8289,
      "step": 12780
    },
    {
      "epoch": 12.79,
      "grad_norm": 30841.615234375,
      "learning_rate": 3.714654282765738e-05,
      "loss": 34.1101,
      "step": 12781
    },
    {
      "epoch": 12.79,
      "grad_norm": 129978.2890625,
      "learning_rate": 3.714138286893705e-05,
      "loss": 41.6904,
      "step": 12782
    },
    {
      "epoch": 12.8,
      "grad_norm": 14949.111328125,
      "learning_rate": 3.7136222910216725e-05,
      "loss": 15.5604,
      "step": 12783
    },
    {
      "epoch": 12.8,
      "grad_norm": 16096.7744140625,
      "learning_rate": 3.713106295149639e-05,
      "loss": 40.9618,
      "step": 12784
    },
    {
      "epoch": 12.8,
      "grad_norm": 1357.3896484375,
      "learning_rate": 3.712590299277606e-05,
      "loss": 50.9693,
      "step": 12785
    },
    {
      "epoch": 12.8,
      "grad_norm": 24443.19921875,
      "learning_rate": 3.712074303405573e-05,
      "loss": 18.7712,
      "step": 12786
    },
    {
      "epoch": 12.8,
      "grad_norm": 5610.23828125,
      "learning_rate": 3.7115583075335396e-05,
      "loss": 32.2366,
      "step": 12787
    },
    {
      "epoch": 12.8,
      "grad_norm": 10763.0810546875,
      "learning_rate": 3.711042311661507e-05,
      "loss": 26.5307,
      "step": 12788
    },
    {
      "epoch": 12.8,
      "grad_norm": 3239.24072265625,
      "learning_rate": 3.710526315789474e-05,
      "loss": 46.7616,
      "step": 12789
    },
    {
      "epoch": 12.8,
      "grad_norm": 11870.5185546875,
      "learning_rate": 3.7100103199174406e-05,
      "loss": 42.3823,
      "step": 12790
    },
    {
      "epoch": 12.8,
      "grad_norm": 13328.9736328125,
      "learning_rate": 3.709494324045408e-05,
      "loss": 28.8575,
      "step": 12791
    },
    {
      "epoch": 12.8,
      "grad_norm": 11750.0908203125,
      "learning_rate": 3.708978328173375e-05,
      "loss": 40.3308,
      "step": 12792
    },
    {
      "epoch": 12.81,
      "grad_norm": 8177.90771484375,
      "learning_rate": 3.7084623323013416e-05,
      "loss": 38.0211,
      "step": 12793
    },
    {
      "epoch": 12.81,
      "grad_norm": 5041.02685546875,
      "learning_rate": 3.707946336429309e-05,
      "loss": 26.1963,
      "step": 12794
    },
    {
      "epoch": 12.81,
      "grad_norm": 105739.765625,
      "learning_rate": 3.707430340557276e-05,
      "loss": 24.3885,
      "step": 12795
    },
    {
      "epoch": 12.81,
      "grad_norm": 7987.29052734375,
      "learning_rate": 3.7069143446852426e-05,
      "loss": 57.1496,
      "step": 12796
    },
    {
      "epoch": 12.81,
      "grad_norm": 29244.369140625,
      "learning_rate": 3.70639834881321e-05,
      "loss": 38.6172,
      "step": 12797
    },
    {
      "epoch": 12.81,
      "grad_norm": 68312.15625,
      "learning_rate": 3.705882352941177e-05,
      "loss": 48.1963,
      "step": 12798
    },
    {
      "epoch": 12.81,
      "grad_norm": 16580.7734375,
      "learning_rate": 3.7053663570691436e-05,
      "loss": 26.0088,
      "step": 12799
    },
    {
      "epoch": 12.81,
      "grad_norm": 5453.2900390625,
      "learning_rate": 3.704850361197111e-05,
      "loss": 38.4515,
      "step": 12800
    },
    {
      "epoch": 12.81,
      "grad_norm": 196358.421875,
      "learning_rate": 3.704334365325078e-05,
      "loss": 40.3879,
      "step": 12801
    },
    {
      "epoch": 12.81,
      "grad_norm": 3633.354248046875,
      "learning_rate": 3.7038183694530446e-05,
      "loss": 30.9269,
      "step": 12802
    },
    {
      "epoch": 12.82,
      "grad_norm": 47956.73828125,
      "learning_rate": 3.7033023735810114e-05,
      "loss": 38.6663,
      "step": 12803
    },
    {
      "epoch": 12.82,
      "grad_norm": 31004.81640625,
      "learning_rate": 3.702786377708978e-05,
      "loss": 43.8338,
      "step": 12804
    },
    {
      "epoch": 12.82,
      "grad_norm": 4589.2412109375,
      "learning_rate": 3.7022703818369456e-05,
      "loss": 46.134,
      "step": 12805
    },
    {
      "epoch": 12.82,
      "grad_norm": 18975.197265625,
      "learning_rate": 3.7017543859649124e-05,
      "loss": 19.8789,
      "step": 12806
    },
    {
      "epoch": 12.82,
      "grad_norm": 16773.208984375,
      "learning_rate": 3.701238390092879e-05,
      "loss": 45.7821,
      "step": 12807
    },
    {
      "epoch": 12.82,
      "grad_norm": 59309.15234375,
      "learning_rate": 3.7007223942208466e-05,
      "loss": 48.6851,
      "step": 12808
    },
    {
      "epoch": 12.82,
      "grad_norm": 209184.34375,
      "learning_rate": 3.7002063983488134e-05,
      "loss": 50.1297,
      "step": 12809
    },
    {
      "epoch": 12.82,
      "grad_norm": 23109.134765625,
      "learning_rate": 3.69969040247678e-05,
      "loss": 31.3628,
      "step": 12810
    },
    {
      "epoch": 12.82,
      "grad_norm": 11344.1640625,
      "learning_rate": 3.6991744066047476e-05,
      "loss": 40.5459,
      "step": 12811
    },
    {
      "epoch": 12.82,
      "grad_norm": 4931.40966796875,
      "learning_rate": 3.6986584107327144e-05,
      "loss": 45.5857,
      "step": 12812
    },
    {
      "epoch": 12.83,
      "grad_norm": 5911.025390625,
      "learning_rate": 3.698142414860681e-05,
      "loss": 40.8214,
      "step": 12813
    },
    {
      "epoch": 12.83,
      "grad_norm": 3480.002197265625,
      "learning_rate": 3.6976264189886486e-05,
      "loss": 37.2414,
      "step": 12814
    },
    {
      "epoch": 12.83,
      "grad_norm": 55642.53125,
      "learning_rate": 3.6971104231166154e-05,
      "loss": 31.5882,
      "step": 12815
    },
    {
      "epoch": 12.83,
      "grad_norm": 5679.75244140625,
      "learning_rate": 3.696594427244582e-05,
      "loss": 43.7483,
      "step": 12816
    },
    {
      "epoch": 12.83,
      "grad_norm": 4097.0205078125,
      "learning_rate": 3.6960784313725496e-05,
      "loss": 50.1908,
      "step": 12817
    },
    {
      "epoch": 12.83,
      "grad_norm": 3569.039794921875,
      "learning_rate": 3.6955624355005164e-05,
      "loss": 33.8406,
      "step": 12818
    },
    {
      "epoch": 12.83,
      "grad_norm": 40152.65234375,
      "learning_rate": 3.695046439628483e-05,
      "loss": 40.8672,
      "step": 12819
    },
    {
      "epoch": 12.83,
      "grad_norm": 14826.025390625,
      "learning_rate": 3.69453044375645e-05,
      "loss": 22.0798,
      "step": 12820
    },
    {
      "epoch": 12.83,
      "grad_norm": 9806.8955078125,
      "learning_rate": 3.694014447884417e-05,
      "loss": 29.7504,
      "step": 12821
    },
    {
      "epoch": 12.83,
      "grad_norm": 15076.6591796875,
      "learning_rate": 3.693498452012384e-05,
      "loss": 38.0249,
      "step": 12822
    },
    {
      "epoch": 12.84,
      "grad_norm": 13309.546875,
      "learning_rate": 3.692982456140351e-05,
      "loss": 33.9549,
      "step": 12823
    },
    {
      "epoch": 12.84,
      "grad_norm": 15093.02734375,
      "learning_rate": 3.692466460268318e-05,
      "loss": 21.4192,
      "step": 12824
    },
    {
      "epoch": 12.84,
      "grad_norm": 15731.5615234375,
      "learning_rate": 3.691950464396285e-05,
      "loss": 48.179,
      "step": 12825
    },
    {
      "epoch": 12.84,
      "grad_norm": 9560.484375,
      "learning_rate": 3.691434468524252e-05,
      "loss": 40.2617,
      "step": 12826
    },
    {
      "epoch": 12.84,
      "grad_norm": 3017.21337890625,
      "learning_rate": 3.690918472652219e-05,
      "loss": 22.3816,
      "step": 12827
    },
    {
      "epoch": 12.84,
      "grad_norm": 34563.03515625,
      "learning_rate": 3.690402476780186e-05,
      "loss": 31.7491,
      "step": 12828
    },
    {
      "epoch": 12.84,
      "grad_norm": 25727.998046875,
      "learning_rate": 3.689886480908153e-05,
      "loss": 37.3028,
      "step": 12829
    },
    {
      "epoch": 12.84,
      "grad_norm": 43592.609375,
      "learning_rate": 3.68937048503612e-05,
      "loss": 39.0979,
      "step": 12830
    },
    {
      "epoch": 12.84,
      "grad_norm": 7891.82470703125,
      "learning_rate": 3.688854489164087e-05,
      "loss": 33.702,
      "step": 12831
    },
    {
      "epoch": 12.84,
      "grad_norm": 17365.501953125,
      "learning_rate": 3.688338493292054e-05,
      "loss": 43.0287,
      "step": 12832
    },
    {
      "epoch": 12.85,
      "grad_norm": 24222.9296875,
      "learning_rate": 3.687822497420021e-05,
      "loss": 33.6448,
      "step": 12833
    },
    {
      "epoch": 12.85,
      "grad_norm": 10719.865234375,
      "learning_rate": 3.687306501547988e-05,
      "loss": 45.8552,
      "step": 12834
    },
    {
      "epoch": 12.85,
      "grad_norm": 3684.923095703125,
      "learning_rate": 3.686790505675955e-05,
      "loss": 49.3391,
      "step": 12835
    },
    {
      "epoch": 12.85,
      "grad_norm": 4388.36767578125,
      "learning_rate": 3.686274509803922e-05,
      "loss": 41.7569,
      "step": 12836
    },
    {
      "epoch": 12.85,
      "grad_norm": 9974.2568359375,
      "learning_rate": 3.685758513931889e-05,
      "loss": 40.2913,
      "step": 12837
    },
    {
      "epoch": 12.85,
      "grad_norm": 11924.65234375,
      "learning_rate": 3.685242518059855e-05,
      "loss": 54.9321,
      "step": 12838
    },
    {
      "epoch": 12.85,
      "grad_norm": 29629.826171875,
      "learning_rate": 3.684726522187823e-05,
      "loss": 31.056,
      "step": 12839
    },
    {
      "epoch": 12.85,
      "grad_norm": 10151.7021484375,
      "learning_rate": 3.6842105263157895e-05,
      "loss": 35.2391,
      "step": 12840
    },
    {
      "epoch": 12.85,
      "grad_norm": 21112.216796875,
      "learning_rate": 3.683694530443756e-05,
      "loss": 29.7099,
      "step": 12841
    },
    {
      "epoch": 12.85,
      "grad_norm": 19339.4453125,
      "learning_rate": 3.683178534571724e-05,
      "loss": 38.0768,
      "step": 12842
    },
    {
      "epoch": 12.86,
      "grad_norm": 9915.1533203125,
      "learning_rate": 3.6826625386996905e-05,
      "loss": 40.9326,
      "step": 12843
    },
    {
      "epoch": 12.86,
      "grad_norm": 433394.40625,
      "learning_rate": 3.682146542827657e-05,
      "loss": 38.1756,
      "step": 12844
    },
    {
      "epoch": 12.86,
      "grad_norm": 10301.2353515625,
      "learning_rate": 3.681630546955625e-05,
      "loss": 25.9887,
      "step": 12845
    },
    {
      "epoch": 12.86,
      "grad_norm": 11119.9765625,
      "learning_rate": 3.6811145510835915e-05,
      "loss": 45.1037,
      "step": 12846
    },
    {
      "epoch": 12.86,
      "grad_norm": 6823.12646484375,
      "learning_rate": 3.680598555211558e-05,
      "loss": 37.0993,
      "step": 12847
    },
    {
      "epoch": 12.86,
      "grad_norm": 14283.890625,
      "learning_rate": 3.680082559339526e-05,
      "loss": 33.8004,
      "step": 12848
    },
    {
      "epoch": 12.86,
      "grad_norm": 3311.465576171875,
      "learning_rate": 3.6795665634674925e-05,
      "loss": 50.5444,
      "step": 12849
    },
    {
      "epoch": 12.86,
      "grad_norm": 9159.1015625,
      "learning_rate": 3.679050567595459e-05,
      "loss": 30.4325,
      "step": 12850
    },
    {
      "epoch": 12.86,
      "grad_norm": 6052.572265625,
      "learning_rate": 3.678534571723427e-05,
      "loss": 49.8957,
      "step": 12851
    },
    {
      "epoch": 12.86,
      "grad_norm": 58644.953125,
      "learning_rate": 3.6780185758513935e-05,
      "loss": 47.4527,
      "step": 12852
    },
    {
      "epoch": 12.87,
      "grad_norm": 18794.861328125,
      "learning_rate": 3.67750257997936e-05,
      "loss": 44.6585,
      "step": 12853
    },
    {
      "epoch": 12.87,
      "grad_norm": 184525.765625,
      "learning_rate": 3.676986584107328e-05,
      "loss": 33.4851,
      "step": 12854
    },
    {
      "epoch": 12.87,
      "grad_norm": 8684.4306640625,
      "learning_rate": 3.6764705882352945e-05,
      "loss": 42.5098,
      "step": 12855
    },
    {
      "epoch": 12.87,
      "grad_norm": 34096.5625,
      "learning_rate": 3.675954592363261e-05,
      "loss": 41.979,
      "step": 12856
    },
    {
      "epoch": 12.87,
      "grad_norm": 5986.65478515625,
      "learning_rate": 3.675438596491228e-05,
      "loss": 30.6313,
      "step": 12857
    },
    {
      "epoch": 12.87,
      "grad_norm": 1698.026123046875,
      "learning_rate": 3.674922600619195e-05,
      "loss": 39.2804,
      "step": 12858
    },
    {
      "epoch": 12.87,
      "grad_norm": 12503.861328125,
      "learning_rate": 3.674406604747162e-05,
      "loss": 43.535,
      "step": 12859
    },
    {
      "epoch": 12.87,
      "grad_norm": 49218.640625,
      "learning_rate": 3.673890608875129e-05,
      "loss": 30.845,
      "step": 12860
    },
    {
      "epoch": 12.87,
      "grad_norm": 3709.8310546875,
      "learning_rate": 3.673374613003096e-05,
      "loss": 38.9392,
      "step": 12861
    },
    {
      "epoch": 12.87,
      "grad_norm": 3409.91064453125,
      "learning_rate": 3.672858617131063e-05,
      "loss": 52.0642,
      "step": 12862
    },
    {
      "epoch": 12.88,
      "grad_norm": 64466.78125,
      "learning_rate": 3.67234262125903e-05,
      "loss": 41.6776,
      "step": 12863
    },
    {
      "epoch": 12.88,
      "grad_norm": 11428.3701171875,
      "learning_rate": 3.671826625386997e-05,
      "loss": 45.2652,
      "step": 12864
    },
    {
      "epoch": 12.88,
      "grad_norm": 45757.7734375,
      "learning_rate": 3.671310629514964e-05,
      "loss": 35.6985,
      "step": 12865
    },
    {
      "epoch": 12.88,
      "grad_norm": 5075.82666015625,
      "learning_rate": 3.670794633642931e-05,
      "loss": 29.2106,
      "step": 12866
    },
    {
      "epoch": 12.88,
      "grad_norm": 12508.6923828125,
      "learning_rate": 3.670278637770898e-05,
      "loss": 32.2465,
      "step": 12867
    },
    {
      "epoch": 12.88,
      "grad_norm": 9684.6298828125,
      "learning_rate": 3.669762641898865e-05,
      "loss": 37.9056,
      "step": 12868
    },
    {
      "epoch": 12.88,
      "grad_norm": 5912.6435546875,
      "learning_rate": 3.669246646026832e-05,
      "loss": 19.4005,
      "step": 12869
    },
    {
      "epoch": 12.88,
      "grad_norm": 2934.62158203125,
      "learning_rate": 3.668730650154799e-05,
      "loss": 40.0933,
      "step": 12870
    },
    {
      "epoch": 12.88,
      "grad_norm": 28419.392578125,
      "learning_rate": 3.668214654282766e-05,
      "loss": 36.9266,
      "step": 12871
    },
    {
      "epoch": 12.88,
      "grad_norm": 12834.1650390625,
      "learning_rate": 3.667698658410733e-05,
      "loss": 24.534,
      "step": 12872
    },
    {
      "epoch": 12.89,
      "grad_norm": 14779.91796875,
      "learning_rate": 3.6671826625387e-05,
      "loss": 44.1044,
      "step": 12873
    },
    {
      "epoch": 12.89,
      "grad_norm": 5351.572265625,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 53.0242,
      "step": 12874
    },
    {
      "epoch": 12.89,
      "grad_norm": 33230.13671875,
      "learning_rate": 3.6661506707946333e-05,
      "loss": 48.7085,
      "step": 12875
    },
    {
      "epoch": 12.89,
      "grad_norm": 7723.4052734375,
      "learning_rate": 3.665634674922601e-05,
      "loss": 40.2681,
      "step": 12876
    },
    {
      "epoch": 12.89,
      "grad_norm": 7160.353515625,
      "learning_rate": 3.6651186790505676e-05,
      "loss": 29.6641,
      "step": 12877
    },
    {
      "epoch": 12.89,
      "grad_norm": 15896.2802734375,
      "learning_rate": 3.6646026831785343e-05,
      "loss": 47.8739,
      "step": 12878
    },
    {
      "epoch": 12.89,
      "grad_norm": 72089.40625,
      "learning_rate": 3.664086687306502e-05,
      "loss": 29.279,
      "step": 12879
    },
    {
      "epoch": 12.89,
      "grad_norm": 8769.8232421875,
      "learning_rate": 3.6635706914344686e-05,
      "loss": 31.1242,
      "step": 12880
    },
    {
      "epoch": 12.89,
      "grad_norm": 23397.52734375,
      "learning_rate": 3.6630546955624353e-05,
      "loss": 35.5976,
      "step": 12881
    },
    {
      "epoch": 12.89,
      "grad_norm": 20947.361328125,
      "learning_rate": 3.662538699690403e-05,
      "loss": 42.6606,
      "step": 12882
    },
    {
      "epoch": 12.9,
      "grad_norm": 8837.1171875,
      "learning_rate": 3.6620227038183696e-05,
      "loss": 36.9676,
      "step": 12883
    },
    {
      "epoch": 12.9,
      "grad_norm": 45613.64453125,
      "learning_rate": 3.6615067079463363e-05,
      "loss": 28.773,
      "step": 12884
    },
    {
      "epoch": 12.9,
      "grad_norm": 22385.5,
      "learning_rate": 3.660990712074304e-05,
      "loss": 42.7901,
      "step": 12885
    },
    {
      "epoch": 12.9,
      "grad_norm": 4607.990234375,
      "learning_rate": 3.6604747162022706e-05,
      "loss": 41.9643,
      "step": 12886
    },
    {
      "epoch": 12.9,
      "grad_norm": 5663.6220703125,
      "learning_rate": 3.6599587203302373e-05,
      "loss": 47.6457,
      "step": 12887
    },
    {
      "epoch": 12.9,
      "grad_norm": 25039.875,
      "learning_rate": 3.659442724458205e-05,
      "loss": 54.064,
      "step": 12888
    },
    {
      "epoch": 12.9,
      "grad_norm": 45935.7578125,
      "learning_rate": 3.6589267285861716e-05,
      "loss": 33.7787,
      "step": 12889
    },
    {
      "epoch": 12.9,
      "grad_norm": 51961.6875,
      "learning_rate": 3.658410732714139e-05,
      "loss": 29.9322,
      "step": 12890
    },
    {
      "epoch": 12.9,
      "grad_norm": 22376.876953125,
      "learning_rate": 3.657894736842106e-05,
      "loss": 30.9524,
      "step": 12891
    },
    {
      "epoch": 12.9,
      "grad_norm": 11620.9521484375,
      "learning_rate": 3.657378740970072e-05,
      "loss": 32.3906,
      "step": 12892
    },
    {
      "epoch": 12.91,
      "grad_norm": 26408.435546875,
      "learning_rate": 3.6568627450980393e-05,
      "loss": 41.719,
      "step": 12893
    },
    {
      "epoch": 12.91,
      "grad_norm": 7669.83251953125,
      "learning_rate": 3.656346749226006e-05,
      "loss": 56.0999,
      "step": 12894
    },
    {
      "epoch": 12.91,
      "grad_norm": 136832.328125,
      "learning_rate": 3.655830753353973e-05,
      "loss": 29.8941,
      "step": 12895
    },
    {
      "epoch": 12.91,
      "grad_norm": 17452.78515625,
      "learning_rate": 3.6553147574819403e-05,
      "loss": 36.6221,
      "step": 12896
    },
    {
      "epoch": 12.91,
      "grad_norm": 16842.818359375,
      "learning_rate": 3.654798761609907e-05,
      "loss": 38.0051,
      "step": 12897
    },
    {
      "epoch": 12.91,
      "grad_norm": 31220.529296875,
      "learning_rate": 3.654282765737874e-05,
      "loss": 34.1804,
      "step": 12898
    },
    {
      "epoch": 12.91,
      "grad_norm": 3487.649658203125,
      "learning_rate": 3.6537667698658413e-05,
      "loss": 40.978,
      "step": 12899
    },
    {
      "epoch": 12.91,
      "grad_norm": 6446.97900390625,
      "learning_rate": 3.653250773993808e-05,
      "loss": 33.6945,
      "step": 12900
    },
    {
      "epoch": 12.91,
      "grad_norm": 46728.71484375,
      "learning_rate": 3.652734778121775e-05,
      "loss": 35.699,
      "step": 12901
    },
    {
      "epoch": 12.91,
      "grad_norm": 25333.931640625,
      "learning_rate": 3.6522187822497423e-05,
      "loss": 36.4278,
      "step": 12902
    },
    {
      "epoch": 12.92,
      "grad_norm": 4045.447021484375,
      "learning_rate": 3.651702786377709e-05,
      "loss": 35.9721,
      "step": 12903
    },
    {
      "epoch": 12.92,
      "grad_norm": 12847.7275390625,
      "learning_rate": 3.6511867905056766e-05,
      "loss": 52.1651,
      "step": 12904
    },
    {
      "epoch": 12.92,
      "grad_norm": 32303.8203125,
      "learning_rate": 3.6506707946336433e-05,
      "loss": 35.3571,
      "step": 12905
    },
    {
      "epoch": 12.92,
      "grad_norm": 11894.21875,
      "learning_rate": 3.65015479876161e-05,
      "loss": 22.5276,
      "step": 12906
    },
    {
      "epoch": 12.92,
      "grad_norm": 12826.9052734375,
      "learning_rate": 3.6496388028895776e-05,
      "loss": 42.8142,
      "step": 12907
    },
    {
      "epoch": 12.92,
      "grad_norm": 13085.5361328125,
      "learning_rate": 3.6491228070175443e-05,
      "loss": 33.7348,
      "step": 12908
    },
    {
      "epoch": 12.92,
      "grad_norm": 12366.3291015625,
      "learning_rate": 3.648606811145511e-05,
      "loss": 27.0043,
      "step": 12909
    },
    {
      "epoch": 12.92,
      "grad_norm": 7433.06298828125,
      "learning_rate": 3.648090815273478e-05,
      "loss": 31.3946,
      "step": 12910
    },
    {
      "epoch": 12.92,
      "grad_norm": 20531.609375,
      "learning_rate": 3.647574819401445e-05,
      "loss": 50.4265,
      "step": 12911
    },
    {
      "epoch": 12.92,
      "grad_norm": 35855.1328125,
      "learning_rate": 3.6470588235294114e-05,
      "loss": 37.2974,
      "step": 12912
    },
    {
      "epoch": 12.93,
      "grad_norm": 77321.3671875,
      "learning_rate": 3.646542827657379e-05,
      "loss": 55.0813,
      "step": 12913
    },
    {
      "epoch": 12.93,
      "grad_norm": 14788.681640625,
      "learning_rate": 3.646026831785346e-05,
      "loss": 37.1537,
      "step": 12914
    },
    {
      "epoch": 12.93,
      "grad_norm": 49568.41796875,
      "learning_rate": 3.6455108359133124e-05,
      "loss": 38.1057,
      "step": 12915
    },
    {
      "epoch": 12.93,
      "grad_norm": 2096.94970703125,
      "learning_rate": 3.64499484004128e-05,
      "loss": 16.0419,
      "step": 12916
    },
    {
      "epoch": 12.93,
      "grad_norm": 32122.86328125,
      "learning_rate": 3.644478844169247e-05,
      "loss": 46.7446,
      "step": 12917
    },
    {
      "epoch": 12.93,
      "grad_norm": 5150.0703125,
      "learning_rate": 3.643962848297214e-05,
      "loss": 32.2593,
      "step": 12918
    },
    {
      "epoch": 12.93,
      "grad_norm": 9001.1259765625,
      "learning_rate": 3.643446852425181e-05,
      "loss": 41.6822,
      "step": 12919
    },
    {
      "epoch": 12.93,
      "grad_norm": 14315.10546875,
      "learning_rate": 3.642930856553148e-05,
      "loss": 36.3233,
      "step": 12920
    },
    {
      "epoch": 12.93,
      "grad_norm": 21655.169921875,
      "learning_rate": 3.642414860681115e-05,
      "loss": 34.0852,
      "step": 12921
    },
    {
      "epoch": 12.93,
      "grad_norm": 14009.080078125,
      "learning_rate": 3.641898864809082e-05,
      "loss": 52.4092,
      "step": 12922
    },
    {
      "epoch": 12.94,
      "grad_norm": 125631.921875,
      "learning_rate": 3.641382868937049e-05,
      "loss": 20.6913,
      "step": 12923
    },
    {
      "epoch": 12.94,
      "grad_norm": 51230.4921875,
      "learning_rate": 3.640866873065016e-05,
      "loss": 36.0124,
      "step": 12924
    },
    {
      "epoch": 12.94,
      "grad_norm": 18593.861328125,
      "learning_rate": 3.640350877192983e-05,
      "loss": 48.2353,
      "step": 12925
    },
    {
      "epoch": 12.94,
      "grad_norm": 16324.7939453125,
      "learning_rate": 3.63983488132095e-05,
      "loss": 38.0318,
      "step": 12926
    },
    {
      "epoch": 12.94,
      "grad_norm": 42339.76171875,
      "learning_rate": 3.639318885448917e-05,
      "loss": 35.8025,
      "step": 12927
    },
    {
      "epoch": 12.94,
      "grad_norm": 8191.07958984375,
      "learning_rate": 3.638802889576883e-05,
      "loss": 35.7755,
      "step": 12928
    },
    {
      "epoch": 12.94,
      "grad_norm": 3383.888916015625,
      "learning_rate": 3.63828689370485e-05,
      "loss": 36.8825,
      "step": 12929
    },
    {
      "epoch": 12.94,
      "grad_norm": 12316.17578125,
      "learning_rate": 3.6377708978328174e-05,
      "loss": 43.7854,
      "step": 12930
    },
    {
      "epoch": 12.94,
      "grad_norm": 26990.486328125,
      "learning_rate": 3.637254901960784e-05,
      "loss": 28.2517,
      "step": 12931
    },
    {
      "epoch": 12.94,
      "grad_norm": 12415.078125,
      "learning_rate": 3.636738906088752e-05,
      "loss": 39.8523,
      "step": 12932
    },
    {
      "epoch": 12.95,
      "grad_norm": 4302.1650390625,
      "learning_rate": 3.6362229102167184e-05,
      "loss": 41.7483,
      "step": 12933
    },
    {
      "epoch": 12.95,
      "grad_norm": 24910.609375,
      "learning_rate": 3.635706914344685e-05,
      "loss": 24.9646,
      "step": 12934
    },
    {
      "epoch": 12.95,
      "grad_norm": 3263.263427734375,
      "learning_rate": 3.635190918472653e-05,
      "loss": 31.5029,
      "step": 12935
    },
    {
      "epoch": 12.95,
      "grad_norm": 30983.796875,
      "learning_rate": 3.6346749226006194e-05,
      "loss": 33.9748,
      "step": 12936
    },
    {
      "epoch": 12.95,
      "grad_norm": 17635.791015625,
      "learning_rate": 3.634158926728586e-05,
      "loss": 38.279,
      "step": 12937
    },
    {
      "epoch": 12.95,
      "grad_norm": 11311.845703125,
      "learning_rate": 3.633642930856554e-05,
      "loss": 40.3803,
      "step": 12938
    },
    {
      "epoch": 12.95,
      "grad_norm": 105009.2578125,
      "learning_rate": 3.6331269349845204e-05,
      "loss": 42.7954,
      "step": 12939
    },
    {
      "epoch": 12.95,
      "grad_norm": 9245.2353515625,
      "learning_rate": 3.632610939112487e-05,
      "loss": 44.5868,
      "step": 12940
    },
    {
      "epoch": 12.95,
      "grad_norm": 11789.931640625,
      "learning_rate": 3.632094943240455e-05,
      "loss": 31.8495,
      "step": 12941
    },
    {
      "epoch": 12.95,
      "grad_norm": 10574.806640625,
      "learning_rate": 3.6315789473684214e-05,
      "loss": 26.246,
      "step": 12942
    },
    {
      "epoch": 12.96,
      "grad_norm": 22820.921875,
      "learning_rate": 3.631062951496388e-05,
      "loss": 30.0939,
      "step": 12943
    },
    {
      "epoch": 12.96,
      "grad_norm": 31435.30859375,
      "learning_rate": 3.630546955624356e-05,
      "loss": 51.9785,
      "step": 12944
    },
    {
      "epoch": 12.96,
      "grad_norm": 9041.3935546875,
      "learning_rate": 3.630030959752322e-05,
      "loss": 27.0453,
      "step": 12945
    },
    {
      "epoch": 12.96,
      "grad_norm": 15942.4541015625,
      "learning_rate": 3.629514963880289e-05,
      "loss": 46.0087,
      "step": 12946
    },
    {
      "epoch": 12.96,
      "grad_norm": 16012.2119140625,
      "learning_rate": 3.628998968008256e-05,
      "loss": 40.5171,
      "step": 12947
    },
    {
      "epoch": 12.96,
      "grad_norm": 12728.4375,
      "learning_rate": 3.628482972136223e-05,
      "loss": 45.4872,
      "step": 12948
    },
    {
      "epoch": 12.96,
      "grad_norm": 2048.150634765625,
      "learning_rate": 3.62796697626419e-05,
      "loss": 47.0679,
      "step": 12949
    },
    {
      "epoch": 12.96,
      "grad_norm": 57854.76953125,
      "learning_rate": 3.627450980392157e-05,
      "loss": 51.269,
      "step": 12950
    },
    {
      "epoch": 12.96,
      "grad_norm": 12302.27734375,
      "learning_rate": 3.626934984520124e-05,
      "loss": 23.0446,
      "step": 12951
    },
    {
      "epoch": 12.96,
      "grad_norm": 7177.25048828125,
      "learning_rate": 3.626418988648091e-05,
      "loss": 44.8324,
      "step": 12952
    },
    {
      "epoch": 12.97,
      "grad_norm": 45067.2890625,
      "learning_rate": 3.625902992776058e-05,
      "loss": 42.1575,
      "step": 12953
    },
    {
      "epoch": 12.97,
      "grad_norm": 2108.62060546875,
      "learning_rate": 3.625386996904025e-05,
      "loss": 45.6791,
      "step": 12954
    },
    {
      "epoch": 12.97,
      "grad_norm": 15901.2373046875,
      "learning_rate": 3.624871001031992e-05,
      "loss": 28.8738,
      "step": 12955
    },
    {
      "epoch": 12.97,
      "grad_norm": 71717.1484375,
      "learning_rate": 3.624355005159959e-05,
      "loss": 28.4736,
      "step": 12956
    },
    {
      "epoch": 12.97,
      "grad_norm": 9551.869140625,
      "learning_rate": 3.623839009287926e-05,
      "loss": 41.5779,
      "step": 12957
    },
    {
      "epoch": 12.97,
      "grad_norm": 211846.203125,
      "learning_rate": 3.623323013415893e-05,
      "loss": 31.8789,
      "step": 12958
    },
    {
      "epoch": 12.97,
      "grad_norm": 42555.81640625,
      "learning_rate": 3.62280701754386e-05,
      "loss": 54.3732,
      "step": 12959
    },
    {
      "epoch": 12.97,
      "grad_norm": 5649.0322265625,
      "learning_rate": 3.622291021671827e-05,
      "loss": 41.8209,
      "step": 12960
    },
    {
      "epoch": 12.97,
      "grad_norm": 12292.8466796875,
      "learning_rate": 3.621775025799794e-05,
      "loss": 33.6335,
      "step": 12961
    },
    {
      "epoch": 12.97,
      "grad_norm": 7211.6953125,
      "learning_rate": 3.621259029927761e-05,
      "loss": 11.7887,
      "step": 12962
    },
    {
      "epoch": 12.98,
      "grad_norm": 5624.8134765625,
      "learning_rate": 3.620743034055728e-05,
      "loss": 35.2769,
      "step": 12963
    },
    {
      "epoch": 12.98,
      "grad_norm": 275532.3125,
      "learning_rate": 3.6202270381836945e-05,
      "loss": 35.7946,
      "step": 12964
    },
    {
      "epoch": 12.98,
      "grad_norm": 5393.533203125,
      "learning_rate": 3.619711042311661e-05,
      "loss": 48.0935,
      "step": 12965
    },
    {
      "epoch": 12.98,
      "grad_norm": 39225.7734375,
      "learning_rate": 3.619195046439629e-05,
      "loss": 42.6432,
      "step": 12966
    },
    {
      "epoch": 12.98,
      "grad_norm": 25941.453125,
      "learning_rate": 3.6186790505675955e-05,
      "loss": 49.5032,
      "step": 12967
    },
    {
      "epoch": 12.98,
      "grad_norm": 7614.84765625,
      "learning_rate": 3.618163054695562e-05,
      "loss": 37.922,
      "step": 12968
    },
    {
      "epoch": 12.98,
      "grad_norm": 38565.3359375,
      "learning_rate": 3.61764705882353e-05,
      "loss": 42.662,
      "step": 12969
    },
    {
      "epoch": 12.98,
      "grad_norm": 61604.41015625,
      "learning_rate": 3.6171310629514965e-05,
      "loss": 41.3278,
      "step": 12970
    },
    {
      "epoch": 12.98,
      "grad_norm": 6588.7763671875,
      "learning_rate": 3.616615067079463e-05,
      "loss": 19.5713,
      "step": 12971
    },
    {
      "epoch": 12.98,
      "grad_norm": 3858.57958984375,
      "learning_rate": 3.616099071207431e-05,
      "loss": 32.4635,
      "step": 12972
    },
    {
      "epoch": 12.99,
      "grad_norm": 2996.934326171875,
      "learning_rate": 3.6155830753353975e-05,
      "loss": 35.2703,
      "step": 12973
    },
    {
      "epoch": 12.99,
      "grad_norm": 15498.28125,
      "learning_rate": 3.615067079463364e-05,
      "loss": 43.5067,
      "step": 12974
    },
    {
      "epoch": 12.99,
      "grad_norm": 24396.330078125,
      "learning_rate": 3.614551083591332e-05,
      "loss": 38.7071,
      "step": 12975
    },
    {
      "epoch": 12.99,
      "grad_norm": 9771.1259765625,
      "learning_rate": 3.6140350877192985e-05,
      "loss": 37.5691,
      "step": 12976
    },
    {
      "epoch": 12.99,
      "grad_norm": 13160.0625,
      "learning_rate": 3.613519091847265e-05,
      "loss": 30.7766,
      "step": 12977
    },
    {
      "epoch": 12.99,
      "grad_norm": 11230.1376953125,
      "learning_rate": 3.613003095975233e-05,
      "loss": 46.1092,
      "step": 12978
    },
    {
      "epoch": 12.99,
      "grad_norm": 38454.2734375,
      "learning_rate": 3.6124871001031995e-05,
      "loss": 29.6877,
      "step": 12979
    },
    {
      "epoch": 12.99,
      "grad_norm": 17037.2890625,
      "learning_rate": 3.611971104231166e-05,
      "loss": 35.4143,
      "step": 12980
    },
    {
      "epoch": 12.99,
      "grad_norm": 23133.80078125,
      "learning_rate": 3.611455108359133e-05,
      "loss": 45.2281,
      "step": 12981
    },
    {
      "epoch": 12.99,
      "grad_norm": 24170.654296875,
      "learning_rate": 3.6109391124871e-05,
      "loss": 41.4681,
      "step": 12982
    },
    {
      "epoch": 13.0,
      "grad_norm": 111284.6953125,
      "learning_rate": 3.610423116615067e-05,
      "loss": 40.6918,
      "step": 12983
    },
    {
      "epoch": 13.0,
      "grad_norm": 51339.71484375,
      "learning_rate": 3.609907120743034e-05,
      "loss": 41.5444,
      "step": 12984
    },
    {
      "epoch": 13.0,
      "grad_norm": 50407.99609375,
      "learning_rate": 3.609391124871001e-05,
      "loss": 34.1685,
      "step": 12985
    },
    {
      "epoch": 13.0,
      "grad_norm": 37020.41796875,
      "learning_rate": 3.608875128998968e-05,
      "loss": 37.5094,
      "step": 12986
    },
    {
      "epoch": 13.0,
      "grad_norm": 16921656.0,
      "learning_rate": 3.608359133126935e-05,
      "loss": 32.5263,
      "step": 12987
    },
    {
      "epoch": 13.0,
      "grad_norm": 5353.56640625,
      "learning_rate": 3.607843137254902e-05,
      "loss": 25.2659,
      "step": 12988
    },
    {
      "epoch": 13.0,
      "grad_norm": 13075.5380859375,
      "learning_rate": 3.607327141382869e-05,
      "loss": 28.1358,
      "step": 12989
    },
    {
      "epoch": 13.0,
      "grad_norm": 9143.6640625,
      "learning_rate": 3.606811145510836e-05,
      "loss": 33.881,
      "step": 12990
    },
    {
      "epoch": 13.0,
      "grad_norm": 25700.94921875,
      "learning_rate": 3.606295149638803e-05,
      "loss": 39.2313,
      "step": 12991
    },
    {
      "epoch": 13.01,
      "grad_norm": 28156.001953125,
      "learning_rate": 3.60577915376677e-05,
      "loss": 28.1552,
      "step": 12992
    },
    {
      "epoch": 13.01,
      "grad_norm": 5595.19921875,
      "learning_rate": 3.605263157894737e-05,
      "loss": 45.9594,
      "step": 12993
    },
    {
      "epoch": 13.01,
      "grad_norm": 24862.236328125,
      "learning_rate": 3.604747162022704e-05,
      "loss": 48.9831,
      "step": 12994
    },
    {
      "epoch": 13.01,
      "grad_norm": 48497.52734375,
      "learning_rate": 3.604231166150671e-05,
      "loss": 37.5891,
      "step": 12995
    },
    {
      "epoch": 13.01,
      "grad_norm": 7287.02001953125,
      "learning_rate": 3.603715170278638e-05,
      "loss": 34.4561,
      "step": 12996
    },
    {
      "epoch": 13.01,
      "grad_norm": 16313.81640625,
      "learning_rate": 3.603199174406605e-05,
      "loss": 29.0606,
      "step": 12997
    },
    {
      "epoch": 13.01,
      "grad_norm": 7659.8515625,
      "learning_rate": 3.602683178534572e-05,
      "loss": 44.0283,
      "step": 12998
    },
    {
      "epoch": 13.01,
      "grad_norm": 3082.8662109375,
      "learning_rate": 3.6021671826625384e-05,
      "loss": 41.7476,
      "step": 12999
    },
    {
      "epoch": 13.01,
      "grad_norm": 2864.406494140625,
      "learning_rate": 3.601651186790506e-05,
      "loss": 44.5909,
      "step": 13000
    },
    {
      "epoch": 13.01,
      "grad_norm": 11024.8203125,
      "learning_rate": 3.6011351909184726e-05,
      "loss": 29.1992,
      "step": 13001
    },
    {
      "epoch": 13.02,
      "grad_norm": 115792.9375,
      "learning_rate": 3.6006191950464394e-05,
      "loss": 48.2938,
      "step": 13002
    },
    {
      "epoch": 13.02,
      "grad_norm": 26714.8828125,
      "learning_rate": 3.600103199174407e-05,
      "loss": 39.8866,
      "step": 13003
    },
    {
      "epoch": 13.02,
      "grad_norm": 22351.537109375,
      "learning_rate": 3.5995872033023736e-05,
      "loss": 28.1721,
      "step": 13004
    },
    {
      "epoch": 13.02,
      "grad_norm": 14003.068359375,
      "learning_rate": 3.5990712074303404e-05,
      "loss": 26.763,
      "step": 13005
    },
    {
      "epoch": 13.02,
      "grad_norm": 8894.9189453125,
      "learning_rate": 3.598555211558308e-05,
      "loss": 41.5615,
      "step": 13006
    },
    {
      "epoch": 13.02,
      "grad_norm": 14644.2744140625,
      "learning_rate": 3.5980392156862746e-05,
      "loss": 35.4974,
      "step": 13007
    },
    {
      "epoch": 13.02,
      "grad_norm": 6839.30859375,
      "learning_rate": 3.5975232198142414e-05,
      "loss": 36.0949,
      "step": 13008
    },
    {
      "epoch": 13.02,
      "grad_norm": 17449.291015625,
      "learning_rate": 3.597007223942209e-05,
      "loss": 24.1872,
      "step": 13009
    },
    {
      "epoch": 13.02,
      "grad_norm": 14229.4287109375,
      "learning_rate": 3.5964912280701756e-05,
      "loss": 40.4057,
      "step": 13010
    },
    {
      "epoch": 13.02,
      "grad_norm": 11165.5556640625,
      "learning_rate": 3.5959752321981424e-05,
      "loss": 31.3543,
      "step": 13011
    },
    {
      "epoch": 13.03,
      "grad_norm": 30707.583984375,
      "learning_rate": 3.59545923632611e-05,
      "loss": 40.9383,
      "step": 13012
    },
    {
      "epoch": 13.03,
      "grad_norm": 4631.83984375,
      "learning_rate": 3.5949432404540766e-05,
      "loss": 43.6247,
      "step": 13013
    },
    {
      "epoch": 13.03,
      "grad_norm": 8484.1826171875,
      "learning_rate": 3.5944272445820434e-05,
      "loss": 41.5856,
      "step": 13014
    },
    {
      "epoch": 13.03,
      "grad_norm": 17176.37890625,
      "learning_rate": 3.593911248710011e-05,
      "loss": 42.1808,
      "step": 13015
    },
    {
      "epoch": 13.03,
      "grad_norm": 23990.822265625,
      "learning_rate": 3.5933952528379776e-05,
      "loss": 33.2726,
      "step": 13016
    },
    {
      "epoch": 13.03,
      "grad_norm": 7731.60693359375,
      "learning_rate": 3.5928792569659444e-05,
      "loss": 45.1753,
      "step": 13017
    },
    {
      "epoch": 13.03,
      "grad_norm": 26719.9375,
      "learning_rate": 3.592363261093911e-05,
      "loss": 43.6723,
      "step": 13018
    },
    {
      "epoch": 13.03,
      "grad_norm": 15230.498046875,
      "learning_rate": 3.591847265221878e-05,
      "loss": 47.9892,
      "step": 13019
    },
    {
      "epoch": 13.03,
      "grad_norm": 5019.5244140625,
      "learning_rate": 3.5913312693498454e-05,
      "loss": 49.9791,
      "step": 13020
    },
    {
      "epoch": 13.03,
      "grad_norm": 3130.507080078125,
      "learning_rate": 3.590815273477812e-05,
      "loss": 48.3216,
      "step": 13021
    },
    {
      "epoch": 13.04,
      "grad_norm": 6905.904296875,
      "learning_rate": 3.590299277605779e-05,
      "loss": 42.1953,
      "step": 13022
    },
    {
      "epoch": 13.04,
      "grad_norm": 17920.712890625,
      "learning_rate": 3.5897832817337464e-05,
      "loss": 41.8848,
      "step": 13023
    },
    {
      "epoch": 13.04,
      "grad_norm": 3895.937255859375,
      "learning_rate": 3.589267285861713e-05,
      "loss": 30.0752,
      "step": 13024
    },
    {
      "epoch": 13.04,
      "grad_norm": 2937.412841796875,
      "learning_rate": 3.58875128998968e-05,
      "loss": 37.4461,
      "step": 13025
    },
    {
      "epoch": 13.04,
      "grad_norm": 6843.27392578125,
      "learning_rate": 3.5882352941176474e-05,
      "loss": 44.797,
      "step": 13026
    },
    {
      "epoch": 13.04,
      "grad_norm": 14003.7001953125,
      "learning_rate": 3.587719298245614e-05,
      "loss": 35.342,
      "step": 13027
    },
    {
      "epoch": 13.04,
      "grad_norm": 14341.404296875,
      "learning_rate": 3.587203302373581e-05,
      "loss": 45.088,
      "step": 13028
    },
    {
      "epoch": 13.04,
      "grad_norm": 11507.0625,
      "learning_rate": 3.5866873065015484e-05,
      "loss": 43.6678,
      "step": 13029
    },
    {
      "epoch": 13.04,
      "grad_norm": 16742.552734375,
      "learning_rate": 3.586171310629515e-05,
      "loss": 21.0004,
      "step": 13030
    },
    {
      "epoch": 13.04,
      "grad_norm": 9917.3232421875,
      "learning_rate": 3.585655314757482e-05,
      "loss": 42.2881,
      "step": 13031
    },
    {
      "epoch": 13.05,
      "grad_norm": 6083.728515625,
      "learning_rate": 3.5851393188854494e-05,
      "loss": 35.1255,
      "step": 13032
    },
    {
      "epoch": 13.05,
      "grad_norm": 6824.5439453125,
      "learning_rate": 3.584623323013416e-05,
      "loss": 19.5231,
      "step": 13033
    },
    {
      "epoch": 13.05,
      "grad_norm": 10040.68359375,
      "learning_rate": 3.584107327141383e-05,
      "loss": 50.7271,
      "step": 13034
    },
    {
      "epoch": 13.05,
      "grad_norm": 17394.837890625,
      "learning_rate": 3.58359133126935e-05,
      "loss": 19.1075,
      "step": 13035
    },
    {
      "epoch": 13.05,
      "grad_norm": 51830.66015625,
      "learning_rate": 3.5830753353973165e-05,
      "loss": 44.2098,
      "step": 13036
    },
    {
      "epoch": 13.05,
      "grad_norm": 9489.18359375,
      "learning_rate": 3.582559339525284e-05,
      "loss": 52.6494,
      "step": 13037
    },
    {
      "epoch": 13.05,
      "grad_norm": 12224.1533203125,
      "learning_rate": 3.582043343653251e-05,
      "loss": 43.8707,
      "step": 13038
    },
    {
      "epoch": 13.05,
      "grad_norm": 56090.3359375,
      "learning_rate": 3.5815273477812175e-05,
      "loss": 17.9572,
      "step": 13039
    },
    {
      "epoch": 13.05,
      "grad_norm": 31748.02734375,
      "learning_rate": 3.581011351909185e-05,
      "loss": 40.49,
      "step": 13040
    },
    {
      "epoch": 13.05,
      "grad_norm": 13445.4169921875,
      "learning_rate": 3.580495356037152e-05,
      "loss": 36.2129,
      "step": 13041
    },
    {
      "epoch": 13.06,
      "grad_norm": 5203.34326171875,
      "learning_rate": 3.5799793601651185e-05,
      "loss": 48.3675,
      "step": 13042
    },
    {
      "epoch": 13.06,
      "grad_norm": 3458.2841796875,
      "learning_rate": 3.579463364293086e-05,
      "loss": 46.3692,
      "step": 13043
    },
    {
      "epoch": 13.06,
      "grad_norm": 11581.1708984375,
      "learning_rate": 3.578947368421053e-05,
      "loss": 36.7438,
      "step": 13044
    },
    {
      "epoch": 13.06,
      "grad_norm": 11220.2216796875,
      "learning_rate": 3.5784313725490195e-05,
      "loss": 46.0282,
      "step": 13045
    },
    {
      "epoch": 13.06,
      "grad_norm": 6305.142578125,
      "learning_rate": 3.577915376676987e-05,
      "loss": 45.1284,
      "step": 13046
    },
    {
      "epoch": 13.06,
      "grad_norm": 76874.6328125,
      "learning_rate": 3.577399380804954e-05,
      "loss": 30.6983,
      "step": 13047
    },
    {
      "epoch": 13.06,
      "grad_norm": 18140.779296875,
      "learning_rate": 3.5768833849329205e-05,
      "loss": 29.2374,
      "step": 13048
    },
    {
      "epoch": 13.06,
      "grad_norm": 9732.6787109375,
      "learning_rate": 3.576367389060888e-05,
      "loss": 41.0985,
      "step": 13049
    },
    {
      "epoch": 13.06,
      "grad_norm": 16813.029296875,
      "learning_rate": 3.575851393188855e-05,
      "loss": 32.9408,
      "step": 13050
    },
    {
      "epoch": 13.06,
      "grad_norm": 5462.28955078125,
      "learning_rate": 3.575335397316822e-05,
      "loss": 52.3076,
      "step": 13051
    },
    {
      "epoch": 13.07,
      "grad_norm": 95022.671875,
      "learning_rate": 3.574819401444789e-05,
      "loss": 40.9536,
      "step": 13052
    },
    {
      "epoch": 13.07,
      "grad_norm": 17144.232421875,
      "learning_rate": 3.574303405572755e-05,
      "loss": 44.3019,
      "step": 13053
    },
    {
      "epoch": 13.07,
      "grad_norm": 10680.787109375,
      "learning_rate": 3.5737874097007225e-05,
      "loss": 23.5657,
      "step": 13054
    },
    {
      "epoch": 13.07,
      "grad_norm": 4349.29248046875,
      "learning_rate": 3.573271413828689e-05,
      "loss": 33.7531,
      "step": 13055
    },
    {
      "epoch": 13.07,
      "grad_norm": 4134.8701171875,
      "learning_rate": 3.572755417956656e-05,
      "loss": 40.5899,
      "step": 13056
    },
    {
      "epoch": 13.07,
      "grad_norm": 129808.390625,
      "learning_rate": 3.5722394220846235e-05,
      "loss": 47.4009,
      "step": 13057
    },
    {
      "epoch": 13.07,
      "grad_norm": 165562.75,
      "learning_rate": 3.57172342621259e-05,
      "loss": 38.8315,
      "step": 13058
    },
    {
      "epoch": 13.07,
      "grad_norm": 11218.8095703125,
      "learning_rate": 3.571207430340557e-05,
      "loss": 34.1903,
      "step": 13059
    },
    {
      "epoch": 13.07,
      "grad_norm": 15652.0673828125,
      "learning_rate": 3.5706914344685245e-05,
      "loss": 38.928,
      "step": 13060
    },
    {
      "epoch": 13.07,
      "grad_norm": 8873.595703125,
      "learning_rate": 3.570175438596491e-05,
      "loss": 36.2526,
      "step": 13061
    },
    {
      "epoch": 13.08,
      "grad_norm": 27222.0546875,
      "learning_rate": 3.569659442724458e-05,
      "loss": 42.0796,
      "step": 13062
    },
    {
      "epoch": 13.08,
      "grad_norm": 10375.55078125,
      "learning_rate": 3.5691434468524255e-05,
      "loss": 54.1533,
      "step": 13063
    },
    {
      "epoch": 13.08,
      "grad_norm": 36168.046875,
      "learning_rate": 3.568627450980392e-05,
      "loss": 43.3101,
      "step": 13064
    },
    {
      "epoch": 13.08,
      "grad_norm": 24036.46484375,
      "learning_rate": 3.56811145510836e-05,
      "loss": 36.2111,
      "step": 13065
    },
    {
      "epoch": 13.08,
      "grad_norm": 327270.34375,
      "learning_rate": 3.5675954592363265e-05,
      "loss": 41.7986,
      "step": 13066
    },
    {
      "epoch": 13.08,
      "grad_norm": 26715.388671875,
      "learning_rate": 3.567079463364293e-05,
      "loss": 44.3668,
      "step": 13067
    },
    {
      "epoch": 13.08,
      "grad_norm": 2238.260498046875,
      "learning_rate": 3.566563467492261e-05,
      "loss": 37.9001,
      "step": 13068
    },
    {
      "epoch": 13.08,
      "grad_norm": 92094.0234375,
      "learning_rate": 3.5660474716202275e-05,
      "loss": 41.6133,
      "step": 13069
    },
    {
      "epoch": 13.08,
      "grad_norm": 31844.484375,
      "learning_rate": 3.565531475748194e-05,
      "loss": 40.0363,
      "step": 13070
    },
    {
      "epoch": 13.08,
      "grad_norm": 3839.208984375,
      "learning_rate": 3.565015479876161e-05,
      "loss": 40.3087,
      "step": 13071
    },
    {
      "epoch": 13.09,
      "grad_norm": 10605.69921875,
      "learning_rate": 3.564499484004128e-05,
      "loss": 44.8079,
      "step": 13072
    },
    {
      "epoch": 13.09,
      "grad_norm": 35085.8203125,
      "learning_rate": 3.5639834881320946e-05,
      "loss": 46.9792,
      "step": 13073
    },
    {
      "epoch": 13.09,
      "grad_norm": 9503.748046875,
      "learning_rate": 3.563467492260062e-05,
      "loss": 49.4227,
      "step": 13074
    },
    {
      "epoch": 13.09,
      "grad_norm": 2926.896728515625,
      "learning_rate": 3.562951496388029e-05,
      "loss": 33.0432,
      "step": 13075
    },
    {
      "epoch": 13.09,
      "grad_norm": 34131.19921875,
      "learning_rate": 3.5624355005159956e-05,
      "loss": 38.7634,
      "step": 13076
    },
    {
      "epoch": 13.09,
      "grad_norm": 88000.9765625,
      "learning_rate": 3.561919504643963e-05,
      "loss": 34.2376,
      "step": 13077
    },
    {
      "epoch": 13.09,
      "grad_norm": 27053.46484375,
      "learning_rate": 3.56140350877193e-05,
      "loss": 42.928,
      "step": 13078
    },
    {
      "epoch": 13.09,
      "grad_norm": 21934.30859375,
      "learning_rate": 3.560887512899897e-05,
      "loss": 38.9883,
      "step": 13079
    },
    {
      "epoch": 13.09,
      "grad_norm": 21310.81640625,
      "learning_rate": 3.560371517027864e-05,
      "loss": 54.2308,
      "step": 13080
    },
    {
      "epoch": 13.09,
      "grad_norm": 36957.578125,
      "learning_rate": 3.559855521155831e-05,
      "loss": 25.7455,
      "step": 13081
    },
    {
      "epoch": 13.1,
      "grad_norm": 15255.8525390625,
      "learning_rate": 3.559339525283798e-05,
      "loss": 38.3231,
      "step": 13082
    },
    {
      "epoch": 13.1,
      "grad_norm": 14270.595703125,
      "learning_rate": 3.558823529411765e-05,
      "loss": 28.5694,
      "step": 13083
    },
    {
      "epoch": 13.1,
      "grad_norm": 3000.44287109375,
      "learning_rate": 3.558307533539732e-05,
      "loss": 40.3814,
      "step": 13084
    },
    {
      "epoch": 13.1,
      "grad_norm": 14610.072265625,
      "learning_rate": 3.557791537667699e-05,
      "loss": 47.4832,
      "step": 13085
    },
    {
      "epoch": 13.1,
      "grad_norm": 20477.521484375,
      "learning_rate": 3.557275541795666e-05,
      "loss": 43.7553,
      "step": 13086
    },
    {
      "epoch": 13.1,
      "grad_norm": 7136.61962890625,
      "learning_rate": 3.556759545923633e-05,
      "loss": 38.0759,
      "step": 13087
    },
    {
      "epoch": 13.1,
      "grad_norm": 20666.82421875,
      "learning_rate": 3.5562435500515996e-05,
      "loss": 44.544,
      "step": 13088
    },
    {
      "epoch": 13.1,
      "grad_norm": 16030.0,
      "learning_rate": 3.5557275541795664e-05,
      "loss": 29.7731,
      "step": 13089
    },
    {
      "epoch": 13.1,
      "grad_norm": 33941.7890625,
      "learning_rate": 3.555211558307533e-05,
      "loss": 34.2465,
      "step": 13090
    },
    {
      "epoch": 13.1,
      "grad_norm": 12653.6201171875,
      "learning_rate": 3.5546955624355006e-05,
      "loss": 43.3018,
      "step": 13091
    },
    {
      "epoch": 13.11,
      "grad_norm": 44176.94921875,
      "learning_rate": 3.5541795665634674e-05,
      "loss": 38.6891,
      "step": 13092
    },
    {
      "epoch": 13.11,
      "grad_norm": 22219.615234375,
      "learning_rate": 3.553663570691435e-05,
      "loss": 35.8547,
      "step": 13093
    },
    {
      "epoch": 13.11,
      "grad_norm": 44841.98046875,
      "learning_rate": 3.5531475748194016e-05,
      "loss": 30.4621,
      "step": 13094
    },
    {
      "epoch": 13.11,
      "grad_norm": 115942.4765625,
      "learning_rate": 3.5526315789473684e-05,
      "loss": 45.1362,
      "step": 13095
    },
    {
      "epoch": 13.11,
      "grad_norm": 12290.736328125,
      "learning_rate": 3.552115583075336e-05,
      "loss": 39.5341,
      "step": 13096
    },
    {
      "epoch": 13.11,
      "grad_norm": 12019.9755859375,
      "learning_rate": 3.5515995872033026e-05,
      "loss": 37.7863,
      "step": 13097
    },
    {
      "epoch": 13.11,
      "grad_norm": 9675.1474609375,
      "learning_rate": 3.5510835913312694e-05,
      "loss": 45.6898,
      "step": 13098
    },
    {
      "epoch": 13.11,
      "grad_norm": 61310.734375,
      "learning_rate": 3.550567595459237e-05,
      "loss": 25.6679,
      "step": 13099
    },
    {
      "epoch": 13.11,
      "grad_norm": 7076.11865234375,
      "learning_rate": 3.5500515995872036e-05,
      "loss": 34.0019,
      "step": 13100
    },
    {
      "epoch": 13.11,
      "grad_norm": 61523.984375,
      "learning_rate": 3.5495356037151704e-05,
      "loss": 30.6032,
      "step": 13101
    },
    {
      "epoch": 13.12,
      "grad_norm": 9697.0234375,
      "learning_rate": 3.549019607843138e-05,
      "loss": 47.1329,
      "step": 13102
    },
    {
      "epoch": 13.12,
      "grad_norm": 171147.0625,
      "learning_rate": 3.5485036119711046e-05,
      "loss": 42.655,
      "step": 13103
    },
    {
      "epoch": 13.12,
      "grad_norm": 39075.234375,
      "learning_rate": 3.5479876160990714e-05,
      "loss": 41.8915,
      "step": 13104
    },
    {
      "epoch": 13.12,
      "grad_norm": 8066.51708984375,
      "learning_rate": 3.547471620227039e-05,
      "loss": 42.4153,
      "step": 13105
    },
    {
      "epoch": 13.12,
      "grad_norm": 15701.12109375,
      "learning_rate": 3.546955624355005e-05,
      "loss": 39.4355,
      "step": 13106
    },
    {
      "epoch": 13.12,
      "grad_norm": 42293.38671875,
      "learning_rate": 3.5464396284829724e-05,
      "loss": 42.1503,
      "step": 13107
    },
    {
      "epoch": 13.12,
      "grad_norm": 15633.9091796875,
      "learning_rate": 3.545923632610939e-05,
      "loss": 26.9757,
      "step": 13108
    },
    {
      "epoch": 13.12,
      "grad_norm": 58878.3359375,
      "learning_rate": 3.545407636738906e-05,
      "loss": 35.8394,
      "step": 13109
    },
    {
      "epoch": 13.12,
      "grad_norm": 3960.267333984375,
      "learning_rate": 3.5448916408668734e-05,
      "loss": 46.6761,
      "step": 13110
    },
    {
      "epoch": 13.12,
      "grad_norm": 7279.412109375,
      "learning_rate": 3.54437564499484e-05,
      "loss": 34.3081,
      "step": 13111
    },
    {
      "epoch": 13.13,
      "grad_norm": 23675.41796875,
      "learning_rate": 3.543859649122807e-05,
      "loss": 31.8647,
      "step": 13112
    },
    {
      "epoch": 13.13,
      "grad_norm": 40516.859375,
      "learning_rate": 3.5433436532507744e-05,
      "loss": 49.6066,
      "step": 13113
    },
    {
      "epoch": 13.13,
      "grad_norm": 12937.236328125,
      "learning_rate": 3.542827657378741e-05,
      "loss": 26.6155,
      "step": 13114
    },
    {
      "epoch": 13.13,
      "grad_norm": 24288.15625,
      "learning_rate": 3.542311661506708e-05,
      "loss": 27.8327,
      "step": 13115
    },
    {
      "epoch": 13.13,
      "grad_norm": 23814.490234375,
      "learning_rate": 3.5417956656346754e-05,
      "loss": 53.2006,
      "step": 13116
    },
    {
      "epoch": 13.13,
      "grad_norm": 21114.595703125,
      "learning_rate": 3.541279669762642e-05,
      "loss": 49.3517,
      "step": 13117
    },
    {
      "epoch": 13.13,
      "grad_norm": 12656.576171875,
      "learning_rate": 3.540763673890609e-05,
      "loss": 42.874,
      "step": 13118
    },
    {
      "epoch": 13.13,
      "grad_norm": 41882.09375,
      "learning_rate": 3.5402476780185764e-05,
      "loss": 53.427,
      "step": 13119
    },
    {
      "epoch": 13.13,
      "grad_norm": 23957.345703125,
      "learning_rate": 3.539731682146543e-05,
      "loss": 27.6799,
      "step": 13120
    },
    {
      "epoch": 13.13,
      "grad_norm": 26679.78125,
      "learning_rate": 3.53921568627451e-05,
      "loss": 35.3123,
      "step": 13121
    },
    {
      "epoch": 13.14,
      "grad_norm": 9930.2119140625,
      "learning_rate": 3.5386996904024774e-05,
      "loss": 51.5863,
      "step": 13122
    },
    {
      "epoch": 13.14,
      "grad_norm": 34647.91796875,
      "learning_rate": 3.538183694530444e-05,
      "loss": 33.1372,
      "step": 13123
    },
    {
      "epoch": 13.14,
      "grad_norm": 45632.53515625,
      "learning_rate": 3.537667698658411e-05,
      "loss": 46.1057,
      "step": 13124
    },
    {
      "epoch": 13.14,
      "grad_norm": 4945.015625,
      "learning_rate": 3.537151702786378e-05,
      "loss": 53.5015,
      "step": 13125
    },
    {
      "epoch": 13.14,
      "grad_norm": 8907.3125,
      "learning_rate": 3.5366357069143445e-05,
      "loss": 27.7748,
      "step": 13126
    },
    {
      "epoch": 13.14,
      "grad_norm": 12128.9111328125,
      "learning_rate": 3.536119711042312e-05,
      "loss": 41.4256,
      "step": 13127
    },
    {
      "epoch": 13.14,
      "grad_norm": 104855.6171875,
      "learning_rate": 3.535603715170279e-05,
      "loss": 41.7098,
      "step": 13128
    },
    {
      "epoch": 13.14,
      "grad_norm": 16134.3447265625,
      "learning_rate": 3.5350877192982455e-05,
      "loss": 36.6912,
      "step": 13129
    },
    {
      "epoch": 13.14,
      "grad_norm": 3713.13330078125,
      "learning_rate": 3.534571723426213e-05,
      "loss": 39.2624,
      "step": 13130
    },
    {
      "epoch": 13.14,
      "grad_norm": 9270.3466796875,
      "learning_rate": 3.53405572755418e-05,
      "loss": 35.3794,
      "step": 13131
    },
    {
      "epoch": 13.15,
      "grad_norm": 14450.615234375,
      "learning_rate": 3.5335397316821465e-05,
      "loss": 46.758,
      "step": 13132
    },
    {
      "epoch": 13.15,
      "grad_norm": 11794.5390625,
      "learning_rate": 3.533023735810114e-05,
      "loss": 16.9922,
      "step": 13133
    },
    {
      "epoch": 13.15,
      "grad_norm": 19946.744140625,
      "learning_rate": 3.532507739938081e-05,
      "loss": 35.3089,
      "step": 13134
    },
    {
      "epoch": 13.15,
      "grad_norm": 46520.4140625,
      "learning_rate": 3.5319917440660475e-05,
      "loss": 42.2638,
      "step": 13135
    },
    {
      "epoch": 13.15,
      "grad_norm": 8477.6923828125,
      "learning_rate": 3.531475748194015e-05,
      "loss": 41.6022,
      "step": 13136
    },
    {
      "epoch": 13.15,
      "grad_norm": 13398.025390625,
      "learning_rate": 3.530959752321982e-05,
      "loss": 42.7868,
      "step": 13137
    },
    {
      "epoch": 13.15,
      "grad_norm": 4530.95703125,
      "learning_rate": 3.5304437564499485e-05,
      "loss": 38.7137,
      "step": 13138
    },
    {
      "epoch": 13.15,
      "grad_norm": 10871.73828125,
      "learning_rate": 3.529927760577916e-05,
      "loss": 22.2818,
      "step": 13139
    },
    {
      "epoch": 13.15,
      "grad_norm": 66806.6796875,
      "learning_rate": 3.529411764705883e-05,
      "loss": 40.0375,
      "step": 13140
    },
    {
      "epoch": 13.15,
      "grad_norm": 11352.7626953125,
      "learning_rate": 3.5288957688338495e-05,
      "loss": 40.6353,
      "step": 13141
    },
    {
      "epoch": 13.16,
      "grad_norm": 20110.23046875,
      "learning_rate": 3.528379772961816e-05,
      "loss": 32.4099,
      "step": 13142
    },
    {
      "epoch": 13.16,
      "grad_norm": 23635.88671875,
      "learning_rate": 3.527863777089783e-05,
      "loss": 32.078,
      "step": 13143
    },
    {
      "epoch": 13.16,
      "grad_norm": 3437.4033203125,
      "learning_rate": 3.5273477812177505e-05,
      "loss": 38.361,
      "step": 13144
    },
    {
      "epoch": 13.16,
      "grad_norm": 5015.61279296875,
      "learning_rate": 3.526831785345717e-05,
      "loss": 38.1808,
      "step": 13145
    },
    {
      "epoch": 13.16,
      "grad_norm": 6946.7353515625,
      "learning_rate": 3.526315789473684e-05,
      "loss": 31.9877,
      "step": 13146
    },
    {
      "epoch": 13.16,
      "grad_norm": 32632.919921875,
      "learning_rate": 3.5257997936016515e-05,
      "loss": 43.9224,
      "step": 13147
    },
    {
      "epoch": 13.16,
      "grad_norm": 6618.97900390625,
      "learning_rate": 3.525283797729618e-05,
      "loss": 23.6157,
      "step": 13148
    },
    {
      "epoch": 13.16,
      "grad_norm": 44711.37109375,
      "learning_rate": 3.524767801857585e-05,
      "loss": 45.7835,
      "step": 13149
    },
    {
      "epoch": 13.16,
      "grad_norm": 15437.9404296875,
      "learning_rate": 3.5242518059855525e-05,
      "loss": 47.3815,
      "step": 13150
    },
    {
      "epoch": 13.16,
      "grad_norm": 20325.224609375,
      "learning_rate": 3.523735810113519e-05,
      "loss": 37.7475,
      "step": 13151
    },
    {
      "epoch": 13.17,
      "grad_norm": 28795.974609375,
      "learning_rate": 3.523219814241486e-05,
      "loss": 50.0254,
      "step": 13152
    },
    {
      "epoch": 13.17,
      "grad_norm": 78509.40625,
      "learning_rate": 3.5227038183694535e-05,
      "loss": 43.9628,
      "step": 13153
    },
    {
      "epoch": 13.17,
      "grad_norm": 7951.46630859375,
      "learning_rate": 3.52218782249742e-05,
      "loss": 27.8227,
      "step": 13154
    },
    {
      "epoch": 13.17,
      "grad_norm": 7599.1162109375,
      "learning_rate": 3.521671826625387e-05,
      "loss": 30.8352,
      "step": 13155
    },
    {
      "epoch": 13.17,
      "grad_norm": 7178.875,
      "learning_rate": 3.5211558307533545e-05,
      "loss": 27.419,
      "step": 13156
    },
    {
      "epoch": 13.17,
      "grad_norm": 73623.015625,
      "learning_rate": 3.520639834881321e-05,
      "loss": 42.5324,
      "step": 13157
    },
    {
      "epoch": 13.17,
      "grad_norm": 7067.67919921875,
      "learning_rate": 3.520123839009288e-05,
      "loss": 22.1575,
      "step": 13158
    },
    {
      "epoch": 13.17,
      "grad_norm": 17630.453125,
      "learning_rate": 3.5196078431372555e-05,
      "loss": 43.8564,
      "step": 13159
    },
    {
      "epoch": 13.17,
      "grad_norm": 73792.9296875,
      "learning_rate": 3.5190918472652216e-05,
      "loss": 48.679,
      "step": 13160
    },
    {
      "epoch": 13.17,
      "grad_norm": 16077.4150390625,
      "learning_rate": 3.518575851393189e-05,
      "loss": 40.0595,
      "step": 13161
    },
    {
      "epoch": 13.18,
      "grad_norm": 961.0303344726562,
      "learning_rate": 3.518059855521156e-05,
      "loss": 47.9983,
      "step": 13162
    },
    {
      "epoch": 13.18,
      "grad_norm": 2928.228759765625,
      "learning_rate": 3.5175438596491226e-05,
      "loss": 17.4251,
      "step": 13163
    },
    {
      "epoch": 13.18,
      "grad_norm": 1886.6807861328125,
      "learning_rate": 3.51702786377709e-05,
      "loss": 51.418,
      "step": 13164
    },
    {
      "epoch": 13.18,
      "grad_norm": 12135.162109375,
      "learning_rate": 3.516511867905057e-05,
      "loss": 26.917,
      "step": 13165
    },
    {
      "epoch": 13.18,
      "grad_norm": 7760.6240234375,
      "learning_rate": 3.5159958720330236e-05,
      "loss": 37.8278,
      "step": 13166
    },
    {
      "epoch": 13.18,
      "grad_norm": 1370.245361328125,
      "learning_rate": 3.515479876160991e-05,
      "loss": 28.694,
      "step": 13167
    },
    {
      "epoch": 13.18,
      "grad_norm": 21149.990234375,
      "learning_rate": 3.514963880288958e-05,
      "loss": 38.4957,
      "step": 13168
    },
    {
      "epoch": 13.18,
      "grad_norm": 16743.775390625,
      "learning_rate": 3.5144478844169246e-05,
      "loss": 12.8488,
      "step": 13169
    },
    {
      "epoch": 13.18,
      "grad_norm": 16751.857421875,
      "learning_rate": 3.513931888544892e-05,
      "loss": 41.6349,
      "step": 13170
    },
    {
      "epoch": 13.18,
      "grad_norm": 6606.00634765625,
      "learning_rate": 3.513415892672859e-05,
      "loss": 35.2594,
      "step": 13171
    },
    {
      "epoch": 13.19,
      "grad_norm": 50047.3671875,
      "learning_rate": 3.5128998968008256e-05,
      "loss": 37.5789,
      "step": 13172
    },
    {
      "epoch": 13.19,
      "grad_norm": 14120.3056640625,
      "learning_rate": 3.512383900928793e-05,
      "loss": 40.9057,
      "step": 13173
    },
    {
      "epoch": 13.19,
      "grad_norm": 4791.31494140625,
      "learning_rate": 3.51186790505676e-05,
      "loss": 43.1925,
      "step": 13174
    },
    {
      "epoch": 13.19,
      "grad_norm": 3212.13037109375,
      "learning_rate": 3.5113519091847266e-05,
      "loss": 38.899,
      "step": 13175
    },
    {
      "epoch": 13.19,
      "grad_norm": 19330.81640625,
      "learning_rate": 3.510835913312694e-05,
      "loss": 36.8043,
      "step": 13176
    },
    {
      "epoch": 13.19,
      "grad_norm": 7098.93701171875,
      "learning_rate": 3.510319917440661e-05,
      "loss": 45.0007,
      "step": 13177
    },
    {
      "epoch": 13.19,
      "grad_norm": 50655.15234375,
      "learning_rate": 3.5098039215686276e-05,
      "loss": 30.2988,
      "step": 13178
    },
    {
      "epoch": 13.19,
      "grad_norm": 792231.1875,
      "learning_rate": 3.5092879256965944e-05,
      "loss": 46.3597,
      "step": 13179
    },
    {
      "epoch": 13.19,
      "grad_norm": 2207.54345703125,
      "learning_rate": 3.508771929824561e-05,
      "loss": 39.4464,
      "step": 13180
    },
    {
      "epoch": 13.19,
      "grad_norm": 18661.291015625,
      "learning_rate": 3.5082559339525286e-05,
      "loss": 29.8428,
      "step": 13181
    },
    {
      "epoch": 13.2,
      "grad_norm": 20790.375,
      "learning_rate": 3.5077399380804954e-05,
      "loss": 48.0129,
      "step": 13182
    },
    {
      "epoch": 13.2,
      "grad_norm": 6268.650390625,
      "learning_rate": 3.507223942208462e-05,
      "loss": 36.3389,
      "step": 13183
    },
    {
      "epoch": 13.2,
      "grad_norm": 5390.07861328125,
      "learning_rate": 3.5067079463364296e-05,
      "loss": 36.4503,
      "step": 13184
    },
    {
      "epoch": 13.2,
      "grad_norm": 3815.471923828125,
      "learning_rate": 3.5061919504643964e-05,
      "loss": 53.073,
      "step": 13185
    },
    {
      "epoch": 13.2,
      "grad_norm": 56490.05078125,
      "learning_rate": 3.505675954592363e-05,
      "loss": 35.9301,
      "step": 13186
    },
    {
      "epoch": 13.2,
      "grad_norm": 57660.04296875,
      "learning_rate": 3.5051599587203306e-05,
      "loss": 46.1768,
      "step": 13187
    },
    {
      "epoch": 13.2,
      "grad_norm": 16434.1328125,
      "learning_rate": 3.5046439628482974e-05,
      "loss": 42.575,
      "step": 13188
    },
    {
      "epoch": 13.2,
      "grad_norm": 7252.02685546875,
      "learning_rate": 3.504127966976264e-05,
      "loss": 39.3338,
      "step": 13189
    },
    {
      "epoch": 13.2,
      "grad_norm": 47532.66015625,
      "learning_rate": 3.5036119711042316e-05,
      "loss": 45.6087,
      "step": 13190
    },
    {
      "epoch": 13.2,
      "grad_norm": 6158.0244140625,
      "learning_rate": 3.5030959752321984e-05,
      "loss": 48.1944,
      "step": 13191
    },
    {
      "epoch": 13.21,
      "grad_norm": 7206.16845703125,
      "learning_rate": 3.502579979360165e-05,
      "loss": 36.4777,
      "step": 13192
    },
    {
      "epoch": 13.21,
      "grad_norm": 38639.83984375,
      "learning_rate": 3.5020639834881326e-05,
      "loss": 49.9069,
      "step": 13193
    },
    {
      "epoch": 13.21,
      "grad_norm": 9620.8173828125,
      "learning_rate": 3.5015479876160994e-05,
      "loss": 41.6672,
      "step": 13194
    },
    {
      "epoch": 13.21,
      "grad_norm": 21157.400390625,
      "learning_rate": 3.501031991744067e-05,
      "loss": 43.5301,
      "step": 13195
    },
    {
      "epoch": 13.21,
      "grad_norm": 53732.03515625,
      "learning_rate": 3.500515995872033e-05,
      "loss": 17.4885,
      "step": 13196
    },
    {
      "epoch": 13.21,
      "grad_norm": 25439.3671875,
      "learning_rate": 3.5e-05,
      "loss": 36.5669,
      "step": 13197
    },
    {
      "epoch": 13.21,
      "grad_norm": 7016.08056640625,
      "learning_rate": 3.499484004127967e-05,
      "loss": 35.0037,
      "step": 13198
    },
    {
      "epoch": 13.21,
      "grad_norm": 7617.27587890625,
      "learning_rate": 3.498968008255934e-05,
      "loss": 43.2441,
      "step": 13199
    },
    {
      "epoch": 13.21,
      "grad_norm": 34468.15625,
      "learning_rate": 3.498452012383901e-05,
      "loss": 34.1059,
      "step": 13200
    },
    {
      "epoch": 13.21,
      "grad_norm": 14884.0244140625,
      "learning_rate": 3.497936016511868e-05,
      "loss": 46.7775,
      "step": 13201
    },
    {
      "epoch": 13.22,
      "grad_norm": 6957.900390625,
      "learning_rate": 3.497420020639835e-05,
      "loss": 39.7889,
      "step": 13202
    },
    {
      "epoch": 13.22,
      "grad_norm": 32250.7109375,
      "learning_rate": 3.496904024767802e-05,
      "loss": 43.7248,
      "step": 13203
    },
    {
      "epoch": 13.22,
      "grad_norm": 10968.900390625,
      "learning_rate": 3.496388028895769e-05,
      "loss": 42.5545,
      "step": 13204
    },
    {
      "epoch": 13.22,
      "grad_norm": 14632.0810546875,
      "learning_rate": 3.495872033023736e-05,
      "loss": 47.586,
      "step": 13205
    },
    {
      "epoch": 13.22,
      "grad_norm": 8083.06689453125,
      "learning_rate": 3.495356037151703e-05,
      "loss": 42.2397,
      "step": 13206
    },
    {
      "epoch": 13.22,
      "grad_norm": 7665.53466796875,
      "learning_rate": 3.49484004127967e-05,
      "loss": 39.4356,
      "step": 13207
    },
    {
      "epoch": 13.22,
      "grad_norm": 9093.17578125,
      "learning_rate": 3.494324045407637e-05,
      "loss": 40.3909,
      "step": 13208
    },
    {
      "epoch": 13.22,
      "grad_norm": 30929.609375,
      "learning_rate": 3.493808049535604e-05,
      "loss": 27.7266,
      "step": 13209
    },
    {
      "epoch": 13.22,
      "grad_norm": 14735.44921875,
      "learning_rate": 3.493292053663571e-05,
      "loss": 36.4312,
      "step": 13210
    },
    {
      "epoch": 13.22,
      "grad_norm": 21077.890625,
      "learning_rate": 3.492776057791538e-05,
      "loss": 32.9716,
      "step": 13211
    },
    {
      "epoch": 13.23,
      "grad_norm": 61756.91796875,
      "learning_rate": 3.4922600619195054e-05,
      "loss": 41.3105,
      "step": 13212
    },
    {
      "epoch": 13.23,
      "grad_norm": 5358.1650390625,
      "learning_rate": 3.491744066047472e-05,
      "loss": 42.3147,
      "step": 13213
    },
    {
      "epoch": 13.23,
      "grad_norm": 9377.13671875,
      "learning_rate": 3.491228070175438e-05,
      "loss": 43.6986,
      "step": 13214
    },
    {
      "epoch": 13.23,
      "grad_norm": 6416.8447265625,
      "learning_rate": 3.490712074303406e-05,
      "loss": 43.2192,
      "step": 13215
    },
    {
      "epoch": 13.23,
      "grad_norm": 29811.359375,
      "learning_rate": 3.4901960784313725e-05,
      "loss": 36.4439,
      "step": 13216
    },
    {
      "epoch": 13.23,
      "grad_norm": 58593.8359375,
      "learning_rate": 3.489680082559339e-05,
      "loss": 39.1853,
      "step": 13217
    },
    {
      "epoch": 13.23,
      "grad_norm": 25631.0703125,
      "learning_rate": 3.489164086687307e-05,
      "loss": 29.7756,
      "step": 13218
    },
    {
      "epoch": 13.23,
      "grad_norm": 38215.24609375,
      "learning_rate": 3.4886480908152735e-05,
      "loss": 46.05,
      "step": 13219
    },
    {
      "epoch": 13.23,
      "grad_norm": 23046.203125,
      "learning_rate": 3.48813209494324e-05,
      "loss": 41.4261,
      "step": 13220
    },
    {
      "epoch": 13.23,
      "grad_norm": 5904.60400390625,
      "learning_rate": 3.487616099071208e-05,
      "loss": 23.0376,
      "step": 13221
    },
    {
      "epoch": 13.24,
      "grad_norm": 16735.330078125,
      "learning_rate": 3.4871001031991745e-05,
      "loss": 46.6614,
      "step": 13222
    },
    {
      "epoch": 13.24,
      "grad_norm": 12531.462890625,
      "learning_rate": 3.486584107327141e-05,
      "loss": 19.9924,
      "step": 13223
    },
    {
      "epoch": 13.24,
      "grad_norm": 30513.236328125,
      "learning_rate": 3.486068111455109e-05,
      "loss": 40.3139,
      "step": 13224
    },
    {
      "epoch": 13.24,
      "grad_norm": 13649.61328125,
      "learning_rate": 3.4855521155830755e-05,
      "loss": 41.1381,
      "step": 13225
    },
    {
      "epoch": 13.24,
      "grad_norm": 2420.44873046875,
      "learning_rate": 3.485036119711043e-05,
      "loss": 40.5336,
      "step": 13226
    },
    {
      "epoch": 13.24,
      "grad_norm": 14649.59765625,
      "learning_rate": 3.48452012383901e-05,
      "loss": 54.0582,
      "step": 13227
    },
    {
      "epoch": 13.24,
      "grad_norm": 17481.103515625,
      "learning_rate": 3.4840041279669765e-05,
      "loss": 30.4288,
      "step": 13228
    },
    {
      "epoch": 13.24,
      "grad_norm": 9195.890625,
      "learning_rate": 3.483488132094944e-05,
      "loss": 40.6766,
      "step": 13229
    },
    {
      "epoch": 13.24,
      "grad_norm": 28127.91796875,
      "learning_rate": 3.482972136222911e-05,
      "loss": 48.1009,
      "step": 13230
    },
    {
      "epoch": 13.24,
      "grad_norm": 7885.85986328125,
      "learning_rate": 3.482456140350877e-05,
      "loss": 39.5002,
      "step": 13231
    },
    {
      "epoch": 13.25,
      "grad_norm": 4667.21435546875,
      "learning_rate": 3.481940144478844e-05,
      "loss": 43.5333,
      "step": 13232
    },
    {
      "epoch": 13.25,
      "grad_norm": 69169.1640625,
      "learning_rate": 3.481424148606811e-05,
      "loss": 39.9052,
      "step": 13233
    },
    {
      "epoch": 13.25,
      "grad_norm": 8208.970703125,
      "learning_rate": 3.480908152734778e-05,
      "loss": 49.0793,
      "step": 13234
    },
    {
      "epoch": 13.25,
      "grad_norm": 20563.6171875,
      "learning_rate": 3.480392156862745e-05,
      "loss": 35.7473,
      "step": 13235
    },
    {
      "epoch": 13.25,
      "grad_norm": 18893.01953125,
      "learning_rate": 3.479876160990712e-05,
      "loss": 41.5166,
      "step": 13236
    },
    {
      "epoch": 13.25,
      "grad_norm": 151778.234375,
      "learning_rate": 3.479360165118679e-05,
      "loss": 23.9941,
      "step": 13237
    },
    {
      "epoch": 13.25,
      "grad_norm": 31603.15234375,
      "learning_rate": 3.478844169246646e-05,
      "loss": 43.3055,
      "step": 13238
    },
    {
      "epoch": 13.25,
      "grad_norm": 25244.009765625,
      "learning_rate": 3.478328173374613e-05,
      "loss": 43.1991,
      "step": 13239
    },
    {
      "epoch": 13.25,
      "grad_norm": 21373.083984375,
      "learning_rate": 3.4778121775025805e-05,
      "loss": 31.2071,
      "step": 13240
    },
    {
      "epoch": 13.25,
      "grad_norm": 8778.2236328125,
      "learning_rate": 3.477296181630547e-05,
      "loss": 43.8148,
      "step": 13241
    },
    {
      "epoch": 13.26,
      "grad_norm": 12864.8466796875,
      "learning_rate": 3.476780185758514e-05,
      "loss": 21.4386,
      "step": 13242
    },
    {
      "epoch": 13.26,
      "grad_norm": 3384.16650390625,
      "learning_rate": 3.4762641898864815e-05,
      "loss": 15.3644,
      "step": 13243
    },
    {
      "epoch": 13.26,
      "grad_norm": 11306.8896484375,
      "learning_rate": 3.475748194014448e-05,
      "loss": 46.0662,
      "step": 13244
    },
    {
      "epoch": 13.26,
      "grad_norm": 8472.119140625,
      "learning_rate": 3.475232198142415e-05,
      "loss": 51.625,
      "step": 13245
    },
    {
      "epoch": 13.26,
      "grad_norm": 10038.9755859375,
      "learning_rate": 3.4747162022703825e-05,
      "loss": 39.6131,
      "step": 13246
    },
    {
      "epoch": 13.26,
      "grad_norm": 7852.94091796875,
      "learning_rate": 3.474200206398349e-05,
      "loss": 34.8833,
      "step": 13247
    },
    {
      "epoch": 13.26,
      "grad_norm": 8666.6748046875,
      "learning_rate": 3.473684210526316e-05,
      "loss": 44.8775,
      "step": 13248
    },
    {
      "epoch": 13.26,
      "grad_norm": 18789.224609375,
      "learning_rate": 3.473168214654283e-05,
      "loss": 29.3884,
      "step": 13249
    },
    {
      "epoch": 13.26,
      "grad_norm": 41041.80859375,
      "learning_rate": 3.4726522187822496e-05,
      "loss": 37.7722,
      "step": 13250
    },
    {
      "epoch": 13.26,
      "grad_norm": 8939.28125,
      "learning_rate": 3.4721362229102163e-05,
      "loss": 42.4469,
      "step": 13251
    },
    {
      "epoch": 13.27,
      "grad_norm": 9851.2197265625,
      "learning_rate": 3.471620227038184e-05,
      "loss": 51.797,
      "step": 13252
    },
    {
      "epoch": 13.27,
      "grad_norm": 14025.4130859375,
      "learning_rate": 3.4711042311661506e-05,
      "loss": 35.0632,
      "step": 13253
    },
    {
      "epoch": 13.27,
      "grad_norm": 7069.537109375,
      "learning_rate": 3.470588235294118e-05,
      "loss": 33.1986,
      "step": 13254
    },
    {
      "epoch": 13.27,
      "grad_norm": 467187.875,
      "learning_rate": 3.470072239422085e-05,
      "loss": 40.6132,
      "step": 13255
    },
    {
      "epoch": 13.27,
      "grad_norm": 7900.0380859375,
      "learning_rate": 3.4695562435500516e-05,
      "loss": 21.7724,
      "step": 13256
    },
    {
      "epoch": 13.27,
      "grad_norm": 1631.975830078125,
      "learning_rate": 3.469040247678019e-05,
      "loss": 34.3537,
      "step": 13257
    },
    {
      "epoch": 13.27,
      "grad_norm": 7856.46728515625,
      "learning_rate": 3.468524251805986e-05,
      "loss": 29.6131,
      "step": 13258
    },
    {
      "epoch": 13.27,
      "grad_norm": 9844.921875,
      "learning_rate": 3.4680082559339526e-05,
      "loss": 40.5506,
      "step": 13259
    },
    {
      "epoch": 13.27,
      "grad_norm": 6180.59130859375,
      "learning_rate": 3.46749226006192e-05,
      "loss": 47.7356,
      "step": 13260
    },
    {
      "epoch": 13.27,
      "grad_norm": 2349.2509765625,
      "learning_rate": 3.466976264189887e-05,
      "loss": 45.2185,
      "step": 13261
    },
    {
      "epoch": 13.28,
      "grad_norm": 21763.193359375,
      "learning_rate": 3.4664602683178536e-05,
      "loss": 41.9436,
      "step": 13262
    },
    {
      "epoch": 13.28,
      "grad_norm": 8619.078125,
      "learning_rate": 3.465944272445821e-05,
      "loss": 37.0814,
      "step": 13263
    },
    {
      "epoch": 13.28,
      "grad_norm": 7159.84423828125,
      "learning_rate": 3.465428276573788e-05,
      "loss": 34.2515,
      "step": 13264
    },
    {
      "epoch": 13.28,
      "grad_norm": 19591.59765625,
      "learning_rate": 3.4649122807017546e-05,
      "loss": 39.2225,
      "step": 13265
    },
    {
      "epoch": 13.28,
      "grad_norm": 41360.52734375,
      "learning_rate": 3.464396284829722e-05,
      "loss": 41.27,
      "step": 13266
    },
    {
      "epoch": 13.28,
      "grad_norm": 4324.94580078125,
      "learning_rate": 3.463880288957688e-05,
      "loss": 46.4679,
      "step": 13267
    },
    {
      "epoch": 13.28,
      "grad_norm": 5174.48583984375,
      "learning_rate": 3.4633642930856556e-05,
      "loss": 42.4115,
      "step": 13268
    },
    {
      "epoch": 13.28,
      "grad_norm": 2500.3330078125,
      "learning_rate": 3.4628482972136223e-05,
      "loss": 32.5512,
      "step": 13269
    },
    {
      "epoch": 13.28,
      "grad_norm": 7615.42919921875,
      "learning_rate": 3.462332301341589e-05,
      "loss": 46.139,
      "step": 13270
    },
    {
      "epoch": 13.28,
      "grad_norm": 6276.72314453125,
      "learning_rate": 3.4618163054695566e-05,
      "loss": 30.7089,
      "step": 13271
    },
    {
      "epoch": 13.29,
      "grad_norm": 2249.018798828125,
      "learning_rate": 3.4613003095975233e-05,
      "loss": 42.7884,
      "step": 13272
    },
    {
      "epoch": 13.29,
      "grad_norm": 141356.859375,
      "learning_rate": 3.46078431372549e-05,
      "loss": 46.8178,
      "step": 13273
    },
    {
      "epoch": 13.29,
      "grad_norm": 16215.3115234375,
      "learning_rate": 3.4602683178534576e-05,
      "loss": 42.0855,
      "step": 13274
    },
    {
      "epoch": 13.29,
      "grad_norm": 12050.349609375,
      "learning_rate": 3.4597523219814243e-05,
      "loss": 44.3336,
      "step": 13275
    },
    {
      "epoch": 13.29,
      "grad_norm": 8892.7509765625,
      "learning_rate": 3.459236326109391e-05,
      "loss": 34.0755,
      "step": 13276
    },
    {
      "epoch": 13.29,
      "grad_norm": 28332.83984375,
      "learning_rate": 3.4587203302373586e-05,
      "loss": 53.986,
      "step": 13277
    },
    {
      "epoch": 13.29,
      "grad_norm": 41843.05859375,
      "learning_rate": 3.4582043343653253e-05,
      "loss": 46.007,
      "step": 13278
    },
    {
      "epoch": 13.29,
      "grad_norm": 7760.5029296875,
      "learning_rate": 3.457688338493292e-05,
      "loss": 44.8771,
      "step": 13279
    },
    {
      "epoch": 13.29,
      "grad_norm": 39735.57421875,
      "learning_rate": 3.4571723426212596e-05,
      "loss": 47.5469,
      "step": 13280
    },
    {
      "epoch": 13.29,
      "grad_norm": 5197.73046875,
      "learning_rate": 3.4566563467492263e-05,
      "loss": 39.1361,
      "step": 13281
    },
    {
      "epoch": 13.3,
      "grad_norm": 8898.8427734375,
      "learning_rate": 3.456140350877193e-05,
      "loss": 41.7439,
      "step": 13282
    },
    {
      "epoch": 13.3,
      "grad_norm": 9500.150390625,
      "learning_rate": 3.4556243550051606e-05,
      "loss": 36.7093,
      "step": 13283
    },
    {
      "epoch": 13.3,
      "grad_norm": 12478.5537109375,
      "learning_rate": 3.4551083591331273e-05,
      "loss": 34.7417,
      "step": 13284
    },
    {
      "epoch": 13.3,
      "grad_norm": 19884.615234375,
      "learning_rate": 3.454592363261094e-05,
      "loss": 13.717,
      "step": 13285
    },
    {
      "epoch": 13.3,
      "grad_norm": 37485.33984375,
      "learning_rate": 3.454076367389061e-05,
      "loss": 46.7187,
      "step": 13286
    },
    {
      "epoch": 13.3,
      "grad_norm": 14910.111328125,
      "learning_rate": 3.453560371517028e-05,
      "loss": 40.9468,
      "step": 13287
    },
    {
      "epoch": 13.3,
      "grad_norm": 2413.723876953125,
      "learning_rate": 3.453044375644995e-05,
      "loss": 39.767,
      "step": 13288
    },
    {
      "epoch": 13.3,
      "grad_norm": 19417.51171875,
      "learning_rate": 3.452528379772962e-05,
      "loss": 43.6867,
      "step": 13289
    },
    {
      "epoch": 13.3,
      "grad_norm": 6979.583984375,
      "learning_rate": 3.452012383900929e-05,
      "loss": 50.4881,
      "step": 13290
    },
    {
      "epoch": 13.3,
      "grad_norm": 7415.71142578125,
      "learning_rate": 3.451496388028896e-05,
      "loss": 40.5854,
      "step": 13291
    },
    {
      "epoch": 13.31,
      "grad_norm": 57729.96484375,
      "learning_rate": 3.450980392156863e-05,
      "loss": 35.6577,
      "step": 13292
    },
    {
      "epoch": 13.31,
      "grad_norm": 12986.203125,
      "learning_rate": 3.45046439628483e-05,
      "loss": 48.3749,
      "step": 13293
    },
    {
      "epoch": 13.31,
      "grad_norm": 8111.41845703125,
      "learning_rate": 3.449948400412797e-05,
      "loss": 34.1111,
      "step": 13294
    },
    {
      "epoch": 13.31,
      "grad_norm": 4088.79052734375,
      "learning_rate": 3.449432404540764e-05,
      "loss": 38.7882,
      "step": 13295
    },
    {
      "epoch": 13.31,
      "grad_norm": 59847.84375,
      "learning_rate": 3.448916408668731e-05,
      "loss": 21.463,
      "step": 13296
    },
    {
      "epoch": 13.31,
      "grad_norm": 9383.140625,
      "learning_rate": 3.448400412796698e-05,
      "loss": 45.0503,
      "step": 13297
    },
    {
      "epoch": 13.31,
      "grad_norm": 7083.31298828125,
      "learning_rate": 3.447884416924665e-05,
      "loss": 43.2907,
      "step": 13298
    },
    {
      "epoch": 13.31,
      "grad_norm": 9842.8134765625,
      "learning_rate": 3.447368421052632e-05,
      "loss": 41.4342,
      "step": 13299
    },
    {
      "epoch": 13.31,
      "grad_norm": 14116.4423828125,
      "learning_rate": 3.446852425180599e-05,
      "loss": 48.3994,
      "step": 13300
    },
    {
      "epoch": 13.31,
      "grad_norm": 189643.28125,
      "learning_rate": 3.446336429308566e-05,
      "loss": 33.1277,
      "step": 13301
    },
    {
      "epoch": 13.32,
      "grad_norm": 35898.53515625,
      "learning_rate": 3.445820433436533e-05,
      "loss": 41.3447,
      "step": 13302
    },
    {
      "epoch": 13.32,
      "grad_norm": 55280.3671875,
      "learning_rate": 3.4453044375644994e-05,
      "loss": 27.3514,
      "step": 13303
    },
    {
      "epoch": 13.32,
      "grad_norm": 9437.857421875,
      "learning_rate": 3.444788441692466e-05,
      "loss": 36.5971,
      "step": 13304
    },
    {
      "epoch": 13.32,
      "grad_norm": 21674.009765625,
      "learning_rate": 3.444272445820434e-05,
      "loss": 45.4883,
      "step": 13305
    },
    {
      "epoch": 13.32,
      "grad_norm": 170729.796875,
      "learning_rate": 3.4437564499484004e-05,
      "loss": 44.5942,
      "step": 13306
    },
    {
      "epoch": 13.32,
      "grad_norm": 12630.06640625,
      "learning_rate": 3.443240454076367e-05,
      "loss": 46.4902,
      "step": 13307
    },
    {
      "epoch": 13.32,
      "grad_norm": 31293.8515625,
      "learning_rate": 3.442724458204335e-05,
      "loss": 18.0498,
      "step": 13308
    },
    {
      "epoch": 13.32,
      "grad_norm": 41756.91796875,
      "learning_rate": 3.4422084623323014e-05,
      "loss": 43.7736,
      "step": 13309
    },
    {
      "epoch": 13.32,
      "grad_norm": 89116.859375,
      "learning_rate": 3.441692466460268e-05,
      "loss": 38.2318,
      "step": 13310
    },
    {
      "epoch": 13.32,
      "grad_norm": 22868.4140625,
      "learning_rate": 3.441176470588236e-05,
      "loss": 48.6598,
      "step": 13311
    },
    {
      "epoch": 13.33,
      "grad_norm": 25648.33984375,
      "learning_rate": 3.4406604747162024e-05,
      "loss": 38.5609,
      "step": 13312
    },
    {
      "epoch": 13.33,
      "grad_norm": 7880.97802734375,
      "learning_rate": 3.440144478844169e-05,
      "loss": 28.1712,
      "step": 13313
    },
    {
      "epoch": 13.33,
      "grad_norm": 6815.939453125,
      "learning_rate": 3.439628482972137e-05,
      "loss": 37.5685,
      "step": 13314
    },
    {
      "epoch": 13.33,
      "grad_norm": 1412076.375,
      "learning_rate": 3.4391124871001034e-05,
      "loss": 36.1373,
      "step": 13315
    },
    {
      "epoch": 13.33,
      "grad_norm": 40259.0625,
      "learning_rate": 3.43859649122807e-05,
      "loss": 23.9506,
      "step": 13316
    },
    {
      "epoch": 13.33,
      "grad_norm": 44440.94921875,
      "learning_rate": 3.438080495356038e-05,
      "loss": 38.0435,
      "step": 13317
    },
    {
      "epoch": 13.33,
      "grad_norm": 5618.42431640625,
      "learning_rate": 3.4375644994840044e-05,
      "loss": 31.0479,
      "step": 13318
    },
    {
      "epoch": 13.33,
      "grad_norm": 2443.614013671875,
      "learning_rate": 3.437048503611971e-05,
      "loss": 38.738,
      "step": 13319
    },
    {
      "epoch": 13.33,
      "grad_norm": 19733.974609375,
      "learning_rate": 3.436532507739939e-05,
      "loss": 33.4628,
      "step": 13320
    },
    {
      "epoch": 13.33,
      "grad_norm": 5857.31103515625,
      "learning_rate": 3.436016511867905e-05,
      "loss": 47.6424,
      "step": 13321
    },
    {
      "epoch": 13.34,
      "grad_norm": 19984.16015625,
      "learning_rate": 3.435500515995872e-05,
      "loss": 49.5152,
      "step": 13322
    },
    {
      "epoch": 13.34,
      "grad_norm": 110288.7890625,
      "learning_rate": 3.434984520123839e-05,
      "loss": 46.4653,
      "step": 13323
    },
    {
      "epoch": 13.34,
      "grad_norm": 9548.36328125,
      "learning_rate": 3.434468524251806e-05,
      "loss": 47.295,
      "step": 13324
    },
    {
      "epoch": 13.34,
      "grad_norm": 70342.4921875,
      "learning_rate": 3.433952528379773e-05,
      "loss": 37.8568,
      "step": 13325
    },
    {
      "epoch": 13.34,
      "grad_norm": 279686.59375,
      "learning_rate": 3.43343653250774e-05,
      "loss": 38.3168,
      "step": 13326
    },
    {
      "epoch": 13.34,
      "grad_norm": 22685.701171875,
      "learning_rate": 3.432920536635707e-05,
      "loss": 48.4589,
      "step": 13327
    },
    {
      "epoch": 13.34,
      "grad_norm": 6598.9921875,
      "learning_rate": 3.432404540763674e-05,
      "loss": 25.8453,
      "step": 13328
    },
    {
      "epoch": 13.34,
      "grad_norm": 50333.82421875,
      "learning_rate": 3.431888544891641e-05,
      "loss": 48.326,
      "step": 13329
    },
    {
      "epoch": 13.34,
      "grad_norm": 42165.38671875,
      "learning_rate": 3.431372549019608e-05,
      "loss": 36.2651,
      "step": 13330
    },
    {
      "epoch": 13.34,
      "grad_norm": 15801.326171875,
      "learning_rate": 3.430856553147575e-05,
      "loss": 42.2908,
      "step": 13331
    },
    {
      "epoch": 13.35,
      "grad_norm": 10820.158203125,
      "learning_rate": 3.430340557275542e-05,
      "loss": 50.2604,
      "step": 13332
    },
    {
      "epoch": 13.35,
      "grad_norm": 7864.49853515625,
      "learning_rate": 3.429824561403509e-05,
      "loss": 39.4589,
      "step": 13333
    },
    {
      "epoch": 13.35,
      "grad_norm": 27086.671875,
      "learning_rate": 3.429308565531476e-05,
      "loss": 39.7239,
      "step": 13334
    },
    {
      "epoch": 13.35,
      "grad_norm": 3481.60498046875,
      "learning_rate": 3.428792569659443e-05,
      "loss": 33.6835,
      "step": 13335
    },
    {
      "epoch": 13.35,
      "grad_norm": 20767.671875,
      "learning_rate": 3.42827657378741e-05,
      "loss": 44.1458,
      "step": 13336
    },
    {
      "epoch": 13.35,
      "grad_norm": 13380.3447265625,
      "learning_rate": 3.427760577915377e-05,
      "loss": 33.1123,
      "step": 13337
    },
    {
      "epoch": 13.35,
      "grad_norm": 4067.974365234375,
      "learning_rate": 3.427244582043344e-05,
      "loss": 35.592,
      "step": 13338
    },
    {
      "epoch": 13.35,
      "grad_norm": 22194.80078125,
      "learning_rate": 3.426728586171311e-05,
      "loss": 26.3135,
      "step": 13339
    },
    {
      "epoch": 13.35,
      "grad_norm": 65222.5234375,
      "learning_rate": 3.4262125902992775e-05,
      "loss": 37.3689,
      "step": 13340
    },
    {
      "epoch": 13.35,
      "grad_norm": 50228.37890625,
      "learning_rate": 3.425696594427244e-05,
      "loss": 41.8875,
      "step": 13341
    },
    {
      "epoch": 13.36,
      "grad_norm": 8600.34375,
      "learning_rate": 3.425180598555212e-05,
      "loss": 42.2753,
      "step": 13342
    },
    {
      "epoch": 13.36,
      "grad_norm": 168999.5,
      "learning_rate": 3.4246646026831785e-05,
      "loss": 55.217,
      "step": 13343
    },
    {
      "epoch": 13.36,
      "grad_norm": 27867.798828125,
      "learning_rate": 3.424148606811145e-05,
      "loss": 39.6673,
      "step": 13344
    },
    {
      "epoch": 13.36,
      "grad_norm": 13781.541015625,
      "learning_rate": 3.423632610939113e-05,
      "loss": 21.8893,
      "step": 13345
    },
    {
      "epoch": 13.36,
      "grad_norm": 8193.150390625,
      "learning_rate": 3.4231166150670795e-05,
      "loss": 41.2755,
      "step": 13346
    },
    {
      "epoch": 13.36,
      "grad_norm": 30793.21875,
      "learning_rate": 3.422600619195046e-05,
      "loss": 43.3969,
      "step": 13347
    },
    {
      "epoch": 13.36,
      "grad_norm": 9393.72265625,
      "learning_rate": 3.422084623323014e-05,
      "loss": 45.9303,
      "step": 13348
    },
    {
      "epoch": 13.36,
      "grad_norm": 14008.392578125,
      "learning_rate": 3.4215686274509805e-05,
      "loss": 39.9515,
      "step": 13349
    },
    {
      "epoch": 13.36,
      "grad_norm": 27364.326171875,
      "learning_rate": 3.421052631578947e-05,
      "loss": 43.3105,
      "step": 13350
    },
    {
      "epoch": 13.36,
      "grad_norm": 38541.609375,
      "learning_rate": 3.420536635706915e-05,
      "loss": 46.7196,
      "step": 13351
    },
    {
      "epoch": 13.37,
      "grad_norm": 15873.388671875,
      "learning_rate": 3.4200206398348815e-05,
      "loss": 36.8971,
      "step": 13352
    },
    {
      "epoch": 13.37,
      "grad_norm": 145180.859375,
      "learning_rate": 3.419504643962848e-05,
      "loss": 36.2865,
      "step": 13353
    },
    {
      "epoch": 13.37,
      "grad_norm": 8588.8173828125,
      "learning_rate": 3.418988648090816e-05,
      "loss": 44.7269,
      "step": 13354
    },
    {
      "epoch": 13.37,
      "grad_norm": 21755.775390625,
      "learning_rate": 3.4184726522187825e-05,
      "loss": 24.4061,
      "step": 13355
    },
    {
      "epoch": 13.37,
      "grad_norm": 6009.47509765625,
      "learning_rate": 3.41795665634675e-05,
      "loss": 44.3391,
      "step": 13356
    },
    {
      "epoch": 13.37,
      "grad_norm": 18552.40625,
      "learning_rate": 3.417440660474716e-05,
      "loss": 13.334,
      "step": 13357
    },
    {
      "epoch": 13.37,
      "grad_norm": 9801.16015625,
      "learning_rate": 3.416924664602683e-05,
      "loss": 34.4774,
      "step": 13358
    },
    {
      "epoch": 13.37,
      "grad_norm": 26123.685546875,
      "learning_rate": 3.41640866873065e-05,
      "loss": 47.0453,
      "step": 13359
    },
    {
      "epoch": 13.37,
      "grad_norm": 22476.205078125,
      "learning_rate": 3.415892672858617e-05,
      "loss": 45.2177,
      "step": 13360
    },
    {
      "epoch": 13.37,
      "grad_norm": 7327.54541015625,
      "learning_rate": 3.415376676986584e-05,
      "loss": 46.1877,
      "step": 13361
    },
    {
      "epoch": 13.38,
      "grad_norm": 58936.7578125,
      "learning_rate": 3.414860681114551e-05,
      "loss": 54.1442,
      "step": 13362
    },
    {
      "epoch": 13.38,
      "grad_norm": 11284.2734375,
      "learning_rate": 3.414344685242518e-05,
      "loss": 34.6124,
      "step": 13363
    },
    {
      "epoch": 13.38,
      "grad_norm": 3636.2421875,
      "learning_rate": 3.413828689370485e-05,
      "loss": 31.9019,
      "step": 13364
    },
    {
      "epoch": 13.38,
      "grad_norm": 34288.44921875,
      "learning_rate": 3.413312693498452e-05,
      "loss": 42.4571,
      "step": 13365
    },
    {
      "epoch": 13.38,
      "grad_norm": 101025.0,
      "learning_rate": 3.412796697626419e-05,
      "loss": 40.2689,
      "step": 13366
    },
    {
      "epoch": 13.38,
      "grad_norm": 5156.45068359375,
      "learning_rate": 3.412280701754386e-05,
      "loss": 32.2659,
      "step": 13367
    },
    {
      "epoch": 13.38,
      "grad_norm": 11392.5947265625,
      "learning_rate": 3.411764705882353e-05,
      "loss": 42.3827,
      "step": 13368
    },
    {
      "epoch": 13.38,
      "grad_norm": 2579.864013671875,
      "learning_rate": 3.41124871001032e-05,
      "loss": 35.8512,
      "step": 13369
    },
    {
      "epoch": 13.38,
      "grad_norm": 34094.1328125,
      "learning_rate": 3.4107327141382875e-05,
      "loss": 23.7332,
      "step": 13370
    },
    {
      "epoch": 13.38,
      "grad_norm": 17293.2265625,
      "learning_rate": 3.410216718266254e-05,
      "loss": 41.346,
      "step": 13371
    },
    {
      "epoch": 13.39,
      "grad_norm": 12436.51171875,
      "learning_rate": 3.409700722394221e-05,
      "loss": 42.8431,
      "step": 13372
    },
    {
      "epoch": 13.39,
      "grad_norm": 16128.779296875,
      "learning_rate": 3.4091847265221885e-05,
      "loss": 46.2301,
      "step": 13373
    },
    {
      "epoch": 13.39,
      "grad_norm": 25815.287109375,
      "learning_rate": 3.4086687306501546e-05,
      "loss": 46.2129,
      "step": 13374
    },
    {
      "epoch": 13.39,
      "grad_norm": 10043.5859375,
      "learning_rate": 3.4081527347781214e-05,
      "loss": 36.9657,
      "step": 13375
    },
    {
      "epoch": 13.39,
      "grad_norm": 12919.287109375,
      "learning_rate": 3.407636738906089e-05,
      "loss": 55.203,
      "step": 13376
    },
    {
      "epoch": 13.39,
      "grad_norm": 7998.61865234375,
      "learning_rate": 3.4071207430340556e-05,
      "loss": 54.3411,
      "step": 13377
    },
    {
      "epoch": 13.39,
      "grad_norm": 5015.61962890625,
      "learning_rate": 3.4066047471620224e-05,
      "loss": 30.3654,
      "step": 13378
    },
    {
      "epoch": 13.39,
      "grad_norm": 105459.4375,
      "learning_rate": 3.40608875128999e-05,
      "loss": 35.3906,
      "step": 13379
    },
    {
      "epoch": 13.39,
      "grad_norm": 13359.5283203125,
      "learning_rate": 3.4055727554179566e-05,
      "loss": 39.1849,
      "step": 13380
    },
    {
      "epoch": 13.39,
      "grad_norm": 31552.732421875,
      "learning_rate": 3.4050567595459234e-05,
      "loss": 31.3878,
      "step": 13381
    },
    {
      "epoch": 13.4,
      "grad_norm": 14010.2861328125,
      "learning_rate": 3.404540763673891e-05,
      "loss": 31.0815,
      "step": 13382
    },
    {
      "epoch": 13.4,
      "grad_norm": 18765.75,
      "learning_rate": 3.4040247678018576e-05,
      "loss": 40.3568,
      "step": 13383
    },
    {
      "epoch": 13.4,
      "grad_norm": 1814.939208984375,
      "learning_rate": 3.403508771929825e-05,
      "loss": 44.9396,
      "step": 13384
    },
    {
      "epoch": 13.4,
      "grad_norm": 14034.5205078125,
      "learning_rate": 3.402992776057792e-05,
      "loss": 53.1931,
      "step": 13385
    },
    {
      "epoch": 13.4,
      "grad_norm": 105840.3984375,
      "learning_rate": 3.4024767801857586e-05,
      "loss": 43.6309,
      "step": 13386
    },
    {
      "epoch": 13.4,
      "grad_norm": 14050.3876953125,
      "learning_rate": 3.401960784313726e-05,
      "loss": 31.2543,
      "step": 13387
    },
    {
      "epoch": 13.4,
      "grad_norm": 16425.658203125,
      "learning_rate": 3.401444788441693e-05,
      "loss": 45.9678,
      "step": 13388
    },
    {
      "epoch": 13.4,
      "grad_norm": 12251.984375,
      "learning_rate": 3.4009287925696596e-05,
      "loss": 29.0021,
      "step": 13389
    },
    {
      "epoch": 13.4,
      "grad_norm": 6385.79931640625,
      "learning_rate": 3.400412796697627e-05,
      "loss": 33.5866,
      "step": 13390
    },
    {
      "epoch": 13.4,
      "grad_norm": 40280.8984375,
      "learning_rate": 3.399896800825594e-05,
      "loss": 42.0956,
      "step": 13391
    },
    {
      "epoch": 13.41,
      "grad_norm": 4801.42919921875,
      "learning_rate": 3.39938080495356e-05,
      "loss": 32.2348,
      "step": 13392
    },
    {
      "epoch": 13.41,
      "grad_norm": 37479.25390625,
      "learning_rate": 3.3988648090815274e-05,
      "loss": 37.6086,
      "step": 13393
    },
    {
      "epoch": 13.41,
      "grad_norm": 7841.94921875,
      "learning_rate": 3.398348813209494e-05,
      "loss": 45.6453,
      "step": 13394
    },
    {
      "epoch": 13.41,
      "grad_norm": 27167.03125,
      "learning_rate": 3.397832817337461e-05,
      "loss": 44.1695,
      "step": 13395
    },
    {
      "epoch": 13.41,
      "grad_norm": 8566.57421875,
      "learning_rate": 3.3973168214654284e-05,
      "loss": 46.8409,
      "step": 13396
    },
    {
      "epoch": 13.41,
      "grad_norm": 24458.197265625,
      "learning_rate": 3.396800825593395e-05,
      "loss": 42.0679,
      "step": 13397
    },
    {
      "epoch": 13.41,
      "grad_norm": 15766.865234375,
      "learning_rate": 3.396284829721362e-05,
      "loss": 44.068,
      "step": 13398
    },
    {
      "epoch": 13.41,
      "grad_norm": 9384.54296875,
      "learning_rate": 3.3957688338493294e-05,
      "loss": 34.7367,
      "step": 13399
    },
    {
      "epoch": 13.41,
      "grad_norm": 7028.76416015625,
      "learning_rate": 3.395252837977296e-05,
      "loss": 34.4559,
      "step": 13400
    },
    {
      "epoch": 13.41,
      "grad_norm": 1960.95361328125,
      "learning_rate": 3.3947368421052636e-05,
      "loss": 47.1881,
      "step": 13401
    },
    {
      "epoch": 13.42,
      "grad_norm": 15311.1865234375,
      "learning_rate": 3.3942208462332304e-05,
      "loss": 46.1472,
      "step": 13402
    },
    {
      "epoch": 13.42,
      "grad_norm": 3088.294921875,
      "learning_rate": 3.393704850361197e-05,
      "loss": 43.345,
      "step": 13403
    },
    {
      "epoch": 13.42,
      "grad_norm": 37169.1484375,
      "learning_rate": 3.3931888544891646e-05,
      "loss": 41.5588,
      "step": 13404
    },
    {
      "epoch": 13.42,
      "grad_norm": 114451.375,
      "learning_rate": 3.3926728586171314e-05,
      "loss": 33.9182,
      "step": 13405
    },
    {
      "epoch": 13.42,
      "grad_norm": 5912.8154296875,
      "learning_rate": 3.392156862745098e-05,
      "loss": 43.9863,
      "step": 13406
    },
    {
      "epoch": 13.42,
      "grad_norm": 14738.9462890625,
      "learning_rate": 3.3916408668730656e-05,
      "loss": 41.0422,
      "step": 13407
    },
    {
      "epoch": 13.42,
      "grad_norm": 10686.67578125,
      "learning_rate": 3.3911248710010324e-05,
      "loss": 31.257,
      "step": 13408
    },
    {
      "epoch": 13.42,
      "grad_norm": 12047.7373046875,
      "learning_rate": 3.390608875128999e-05,
      "loss": 43.2756,
      "step": 13409
    },
    {
      "epoch": 13.42,
      "grad_norm": 68342.7421875,
      "learning_rate": 3.390092879256966e-05,
      "loss": 41.391,
      "step": 13410
    },
    {
      "epoch": 13.42,
      "grad_norm": 9209.3515625,
      "learning_rate": 3.389576883384933e-05,
      "loss": 49.7262,
      "step": 13411
    },
    {
      "epoch": 13.43,
      "grad_norm": 3082.828125,
      "learning_rate": 3.3890608875128995e-05,
      "loss": 47.0772,
      "step": 13412
    },
    {
      "epoch": 13.43,
      "grad_norm": 77172.0390625,
      "learning_rate": 3.388544891640867e-05,
      "loss": 29.4563,
      "step": 13413
    },
    {
      "epoch": 13.43,
      "grad_norm": 6256.90771484375,
      "learning_rate": 3.388028895768834e-05,
      "loss": 39.7969,
      "step": 13414
    },
    {
      "epoch": 13.43,
      "grad_norm": 949.0953979492188,
      "learning_rate": 3.387512899896801e-05,
      "loss": 31.4004,
      "step": 13415
    },
    {
      "epoch": 13.43,
      "grad_norm": 62487.3984375,
      "learning_rate": 3.386996904024768e-05,
      "loss": 41.5065,
      "step": 13416
    },
    {
      "epoch": 13.43,
      "grad_norm": 10142.8720703125,
      "learning_rate": 3.386480908152735e-05,
      "loss": 45.4326,
      "step": 13417
    },
    {
      "epoch": 13.43,
      "grad_norm": 5116.0517578125,
      "learning_rate": 3.385964912280702e-05,
      "loss": 44.6685,
      "step": 13418
    },
    {
      "epoch": 13.43,
      "grad_norm": 34647.41015625,
      "learning_rate": 3.385448916408669e-05,
      "loss": 40.2371,
      "step": 13419
    },
    {
      "epoch": 13.43,
      "grad_norm": 4110.21142578125,
      "learning_rate": 3.384932920536636e-05,
      "loss": 37.2785,
      "step": 13420
    },
    {
      "epoch": 13.43,
      "grad_norm": 29081.689453125,
      "learning_rate": 3.384416924664603e-05,
      "loss": 42.6996,
      "step": 13421
    },
    {
      "epoch": 13.44,
      "grad_norm": 23289.69921875,
      "learning_rate": 3.38390092879257e-05,
      "loss": 20.7727,
      "step": 13422
    },
    {
      "epoch": 13.44,
      "grad_norm": 22449.66796875,
      "learning_rate": 3.383384932920537e-05,
      "loss": 42.0336,
      "step": 13423
    },
    {
      "epoch": 13.44,
      "grad_norm": 5979.310546875,
      "learning_rate": 3.382868937048504e-05,
      "loss": 49.9631,
      "step": 13424
    },
    {
      "epoch": 13.44,
      "grad_norm": 24205.53125,
      "learning_rate": 3.382352941176471e-05,
      "loss": 22.9446,
      "step": 13425
    },
    {
      "epoch": 13.44,
      "grad_norm": 4799.7666015625,
      "learning_rate": 3.381836945304438e-05,
      "loss": 42.0023,
      "step": 13426
    },
    {
      "epoch": 13.44,
      "grad_norm": 36378.2421875,
      "learning_rate": 3.381320949432405e-05,
      "loss": 37.5567,
      "step": 13427
    },
    {
      "epoch": 13.44,
      "grad_norm": 21801.453125,
      "learning_rate": 3.380804953560371e-05,
      "loss": 32.3608,
      "step": 13428
    },
    {
      "epoch": 13.44,
      "grad_norm": 5096.0380859375,
      "learning_rate": 3.380288957688339e-05,
      "loss": 47.3116,
      "step": 13429
    },
    {
      "epoch": 13.44,
      "grad_norm": 37387.69140625,
      "learning_rate": 3.3797729618163055e-05,
      "loss": 41.0876,
      "step": 13430
    },
    {
      "epoch": 13.44,
      "grad_norm": 11733.81640625,
      "learning_rate": 3.379256965944272e-05,
      "loss": 40.0302,
      "step": 13431
    },
    {
      "epoch": 13.45,
      "grad_norm": 34565.61328125,
      "learning_rate": 3.37874097007224e-05,
      "loss": 31.3556,
      "step": 13432
    },
    {
      "epoch": 13.45,
      "grad_norm": 9414.2607421875,
      "learning_rate": 3.3782249742002065e-05,
      "loss": 40.6681,
      "step": 13433
    },
    {
      "epoch": 13.45,
      "grad_norm": 7521.54541015625,
      "learning_rate": 3.377708978328173e-05,
      "loss": 46.3551,
      "step": 13434
    },
    {
      "epoch": 13.45,
      "grad_norm": 10266.41796875,
      "learning_rate": 3.377192982456141e-05,
      "loss": 43.4571,
      "step": 13435
    },
    {
      "epoch": 13.45,
      "grad_norm": 128367.6328125,
      "learning_rate": 3.3766769865841075e-05,
      "loss": 55.7518,
      "step": 13436
    },
    {
      "epoch": 13.45,
      "grad_norm": 39747.5,
      "learning_rate": 3.376160990712074e-05,
      "loss": 48.2463,
      "step": 13437
    },
    {
      "epoch": 13.45,
      "grad_norm": 16751.90625,
      "learning_rate": 3.375644994840042e-05,
      "loss": 50.0654,
      "step": 13438
    },
    {
      "epoch": 13.45,
      "grad_norm": 29561.474609375,
      "learning_rate": 3.3751289989680085e-05,
      "loss": 40.3442,
      "step": 13439
    },
    {
      "epoch": 13.45,
      "grad_norm": 48062.0625,
      "learning_rate": 3.374613003095975e-05,
      "loss": 22.5246,
      "step": 13440
    },
    {
      "epoch": 13.45,
      "grad_norm": 19415.794921875,
      "learning_rate": 3.374097007223943e-05,
      "loss": 42.618,
      "step": 13441
    },
    {
      "epoch": 13.46,
      "grad_norm": 35782.26953125,
      "learning_rate": 3.3735810113519095e-05,
      "loss": 36.5134,
      "step": 13442
    },
    {
      "epoch": 13.46,
      "grad_norm": 17135.837890625,
      "learning_rate": 3.373065015479876e-05,
      "loss": 45.7276,
      "step": 13443
    },
    {
      "epoch": 13.46,
      "grad_norm": 3884.760009765625,
      "learning_rate": 3.372549019607844e-05,
      "loss": 45.1678,
      "step": 13444
    },
    {
      "epoch": 13.46,
      "grad_norm": 23043.119140625,
      "learning_rate": 3.3720330237358105e-05,
      "loss": 39.5665,
      "step": 13445
    },
    {
      "epoch": 13.46,
      "grad_norm": 11815.3125,
      "learning_rate": 3.371517027863777e-05,
      "loss": 37.1065,
      "step": 13446
    },
    {
      "epoch": 13.46,
      "grad_norm": 12794.5302734375,
      "learning_rate": 3.371001031991744e-05,
      "loss": 36.5648,
      "step": 13447
    },
    {
      "epoch": 13.46,
      "grad_norm": 5068.6201171875,
      "learning_rate": 3.370485036119711e-05,
      "loss": 40.4409,
      "step": 13448
    },
    {
      "epoch": 13.46,
      "grad_norm": 30238.87890625,
      "learning_rate": 3.369969040247678e-05,
      "loss": 47.482,
      "step": 13449
    },
    {
      "epoch": 13.46,
      "grad_norm": 12196.572265625,
      "learning_rate": 3.369453044375645e-05,
      "loss": 49.7406,
      "step": 13450
    },
    {
      "epoch": 13.46,
      "grad_norm": 12087.9873046875,
      "learning_rate": 3.368937048503612e-05,
      "loss": 22.8734,
      "step": 13451
    },
    {
      "epoch": 13.47,
      "grad_norm": 29414.0234375,
      "learning_rate": 3.368421052631579e-05,
      "loss": 37.3534,
      "step": 13452
    },
    {
      "epoch": 13.47,
      "grad_norm": 55793.7578125,
      "learning_rate": 3.367905056759546e-05,
      "loss": 35.2782,
      "step": 13453
    },
    {
      "epoch": 13.47,
      "grad_norm": 23167.5625,
      "learning_rate": 3.367389060887513e-05,
      "loss": 37.8162,
      "step": 13454
    },
    {
      "epoch": 13.47,
      "grad_norm": 9246.259765625,
      "learning_rate": 3.36687306501548e-05,
      "loss": 50.6375,
      "step": 13455
    },
    {
      "epoch": 13.47,
      "grad_norm": 42074.17578125,
      "learning_rate": 3.366357069143447e-05,
      "loss": 41.9071,
      "step": 13456
    },
    {
      "epoch": 13.47,
      "grad_norm": 29973.466796875,
      "learning_rate": 3.365841073271414e-05,
      "loss": 52.9148,
      "step": 13457
    },
    {
      "epoch": 13.47,
      "grad_norm": 4282.6982421875,
      "learning_rate": 3.365325077399381e-05,
      "loss": 44.2083,
      "step": 13458
    },
    {
      "epoch": 13.47,
      "grad_norm": 20953.494140625,
      "learning_rate": 3.364809081527348e-05,
      "loss": 29.2224,
      "step": 13459
    },
    {
      "epoch": 13.47,
      "grad_norm": 11807.3583984375,
      "learning_rate": 3.364293085655315e-05,
      "loss": 48.503,
      "step": 13460
    },
    {
      "epoch": 13.47,
      "grad_norm": 6473.50732421875,
      "learning_rate": 3.363777089783282e-05,
      "loss": 41.3347,
      "step": 13461
    },
    {
      "epoch": 13.48,
      "grad_norm": 16777.498046875,
      "learning_rate": 3.363261093911249e-05,
      "loss": 49.9975,
      "step": 13462
    },
    {
      "epoch": 13.48,
      "grad_norm": 19814.23828125,
      "learning_rate": 3.362745098039216e-05,
      "loss": 35.3256,
      "step": 13463
    },
    {
      "epoch": 13.48,
      "grad_norm": 14693.591796875,
      "learning_rate": 3.3622291021671826e-05,
      "loss": 41.5329,
      "step": 13464
    },
    {
      "epoch": 13.48,
      "grad_norm": 27885.822265625,
      "learning_rate": 3.3617131062951494e-05,
      "loss": 23.5232,
      "step": 13465
    },
    {
      "epoch": 13.48,
      "grad_norm": 5140.03271484375,
      "learning_rate": 3.361197110423117e-05,
      "loss": 43.5524,
      "step": 13466
    },
    {
      "epoch": 13.48,
      "grad_norm": 149201.640625,
      "learning_rate": 3.3606811145510836e-05,
      "loss": 33.6501,
      "step": 13467
    },
    {
      "epoch": 13.48,
      "grad_norm": 13133.171875,
      "learning_rate": 3.3601651186790504e-05,
      "loss": 42.113,
      "step": 13468
    },
    {
      "epoch": 13.48,
      "grad_norm": 14213.2958984375,
      "learning_rate": 3.359649122807018e-05,
      "loss": 44.3455,
      "step": 13469
    },
    {
      "epoch": 13.48,
      "grad_norm": 47080.69140625,
      "learning_rate": 3.3591331269349846e-05,
      "loss": 40.2292,
      "step": 13470
    },
    {
      "epoch": 13.48,
      "grad_norm": 9635.8173828125,
      "learning_rate": 3.3586171310629514e-05,
      "loss": 48.3957,
      "step": 13471
    },
    {
      "epoch": 13.49,
      "grad_norm": 13257.767578125,
      "learning_rate": 3.358101135190919e-05,
      "loss": 43.7628,
      "step": 13472
    },
    {
      "epoch": 13.49,
      "grad_norm": 11508.5009765625,
      "learning_rate": 3.3575851393188856e-05,
      "loss": 45.6677,
      "step": 13473
    },
    {
      "epoch": 13.49,
      "grad_norm": 11605.169921875,
      "learning_rate": 3.3570691434468524e-05,
      "loss": 34.6317,
      "step": 13474
    },
    {
      "epoch": 13.49,
      "grad_norm": 6890.3232421875,
      "learning_rate": 3.35655314757482e-05,
      "loss": 40.042,
      "step": 13475
    },
    {
      "epoch": 13.49,
      "grad_norm": 36648.33984375,
      "learning_rate": 3.3560371517027866e-05,
      "loss": 46.0762,
      "step": 13476
    },
    {
      "epoch": 13.49,
      "grad_norm": 3499.987060546875,
      "learning_rate": 3.3555211558307534e-05,
      "loss": 43.4249,
      "step": 13477
    },
    {
      "epoch": 13.49,
      "grad_norm": 42140.7578125,
      "learning_rate": 3.355005159958721e-05,
      "loss": 43.6477,
      "step": 13478
    },
    {
      "epoch": 13.49,
      "grad_norm": 20861.2421875,
      "learning_rate": 3.3544891640866876e-05,
      "loss": 36.3771,
      "step": 13479
    },
    {
      "epoch": 13.49,
      "grad_norm": 64803.59375,
      "learning_rate": 3.3539731682146544e-05,
      "loss": 25.495,
      "step": 13480
    },
    {
      "epoch": 13.49,
      "grad_norm": 25920.185546875,
      "learning_rate": 3.353457172342622e-05,
      "loss": 28.6371,
      "step": 13481
    },
    {
      "epoch": 13.5,
      "grad_norm": 11065.5712890625,
      "learning_rate": 3.352941176470588e-05,
      "loss": 28.4235,
      "step": 13482
    },
    {
      "epoch": 13.5,
      "grad_norm": 14555.74609375,
      "learning_rate": 3.3524251805985554e-05,
      "loss": 41.8777,
      "step": 13483
    },
    {
      "epoch": 13.5,
      "grad_norm": 8940.2080078125,
      "learning_rate": 3.351909184726522e-05,
      "loss": 48.035,
      "step": 13484
    },
    {
      "epoch": 13.5,
      "grad_norm": 5626.38720703125,
      "learning_rate": 3.351393188854489e-05,
      "loss": 37.7832,
      "step": 13485
    },
    {
      "epoch": 13.5,
      "grad_norm": 26781.939453125,
      "learning_rate": 3.3508771929824564e-05,
      "loss": 45.7476,
      "step": 13486
    },
    {
      "epoch": 13.5,
      "grad_norm": 59992.83203125,
      "learning_rate": 3.350361197110423e-05,
      "loss": 42.1644,
      "step": 13487
    },
    {
      "epoch": 13.5,
      "grad_norm": 19214.748046875,
      "learning_rate": 3.34984520123839e-05,
      "loss": 47.0494,
      "step": 13488
    },
    {
      "epoch": 13.5,
      "grad_norm": 7174.2548828125,
      "learning_rate": 3.3493292053663574e-05,
      "loss": 39.0422,
      "step": 13489
    },
    {
      "epoch": 13.5,
      "grad_norm": 17215.677734375,
      "learning_rate": 3.348813209494324e-05,
      "loss": 35.6506,
      "step": 13490
    },
    {
      "epoch": 13.5,
      "grad_norm": 4571.1064453125,
      "learning_rate": 3.348297213622291e-05,
      "loss": 42.1168,
      "step": 13491
    },
    {
      "epoch": 13.51,
      "grad_norm": 21103.62109375,
      "learning_rate": 3.3477812177502584e-05,
      "loss": 24.6682,
      "step": 13492
    },
    {
      "epoch": 13.51,
      "grad_norm": 15076.7236328125,
      "learning_rate": 3.347265221878225e-05,
      "loss": 39.6777,
      "step": 13493
    },
    {
      "epoch": 13.51,
      "grad_norm": 9913.1591796875,
      "learning_rate": 3.346749226006192e-05,
      "loss": 41.0466,
      "step": 13494
    },
    {
      "epoch": 13.51,
      "grad_norm": 14753.890625,
      "learning_rate": 3.3462332301341594e-05,
      "loss": 42.0838,
      "step": 13495
    },
    {
      "epoch": 13.51,
      "grad_norm": 6896.83935546875,
      "learning_rate": 3.345717234262126e-05,
      "loss": 32.4257,
      "step": 13496
    },
    {
      "epoch": 13.51,
      "grad_norm": 9921.923828125,
      "learning_rate": 3.345201238390093e-05,
      "loss": 40.6466,
      "step": 13497
    },
    {
      "epoch": 13.51,
      "grad_norm": 14168.3525390625,
      "learning_rate": 3.3446852425180604e-05,
      "loss": 39.4843,
      "step": 13498
    },
    {
      "epoch": 13.51,
      "grad_norm": 16198.7451171875,
      "learning_rate": 3.344169246646027e-05,
      "loss": 46.4284,
      "step": 13499
    },
    {
      "epoch": 13.51,
      "grad_norm": 59475.9140625,
      "learning_rate": 3.343653250773994e-05,
      "loss": 42.9928,
      "step": 13500
    },
    {
      "epoch": 13.51,
      "grad_norm": 14393.6171875,
      "learning_rate": 3.343137254901961e-05,
      "loss": 27.669,
      "step": 13501
    },
    {
      "epoch": 13.52,
      "grad_norm": 15042.390625,
      "learning_rate": 3.3426212590299275e-05,
      "loss": 37.5112,
      "step": 13502
    },
    {
      "epoch": 13.52,
      "grad_norm": 119044.1953125,
      "learning_rate": 3.342105263157895e-05,
      "loss": 48.2489,
      "step": 13503
    },
    {
      "epoch": 13.52,
      "grad_norm": 39642.57421875,
      "learning_rate": 3.341589267285862e-05,
      "loss": 40.0497,
      "step": 13504
    },
    {
      "epoch": 13.52,
      "grad_norm": 7668.5947265625,
      "learning_rate": 3.3410732714138285e-05,
      "loss": 49.6644,
      "step": 13505
    },
    {
      "epoch": 13.52,
      "grad_norm": 39274.0703125,
      "learning_rate": 3.340557275541796e-05,
      "loss": 34.0488,
      "step": 13506
    },
    {
      "epoch": 13.52,
      "grad_norm": 18295.7109375,
      "learning_rate": 3.340041279669763e-05,
      "loss": 42.4575,
      "step": 13507
    },
    {
      "epoch": 13.52,
      "grad_norm": 184341.765625,
      "learning_rate": 3.3395252837977295e-05,
      "loss": 53.245,
      "step": 13508
    },
    {
      "epoch": 13.52,
      "grad_norm": 17864.263671875,
      "learning_rate": 3.339009287925697e-05,
      "loss": 40.7533,
      "step": 13509
    },
    {
      "epoch": 13.52,
      "grad_norm": 5890.69140625,
      "learning_rate": 3.338493292053664e-05,
      "loss": 34.5471,
      "step": 13510
    },
    {
      "epoch": 13.52,
      "grad_norm": 21983.533203125,
      "learning_rate": 3.3379772961816305e-05,
      "loss": 43.0774,
      "step": 13511
    },
    {
      "epoch": 13.53,
      "grad_norm": 6228.931640625,
      "learning_rate": 3.337461300309598e-05,
      "loss": 53.4925,
      "step": 13512
    },
    {
      "epoch": 13.53,
      "grad_norm": 5300.390625,
      "learning_rate": 3.336945304437565e-05,
      "loss": 42.4606,
      "step": 13513
    },
    {
      "epoch": 13.53,
      "grad_norm": 6630.958984375,
      "learning_rate": 3.3364293085655315e-05,
      "loss": 29.4703,
      "step": 13514
    },
    {
      "epoch": 13.53,
      "grad_norm": 1987.4520263671875,
      "learning_rate": 3.335913312693499e-05,
      "loss": 48.4364,
      "step": 13515
    },
    {
      "epoch": 13.53,
      "grad_norm": 12296.73828125,
      "learning_rate": 3.335397316821466e-05,
      "loss": 35.7284,
      "step": 13516
    },
    {
      "epoch": 13.53,
      "grad_norm": 109889.6640625,
      "learning_rate": 3.334881320949433e-05,
      "loss": 44.9644,
      "step": 13517
    },
    {
      "epoch": 13.53,
      "grad_norm": 12481.0380859375,
      "learning_rate": 3.334365325077399e-05,
      "loss": 50.3659,
      "step": 13518
    },
    {
      "epoch": 13.53,
      "grad_norm": 5784.83984375,
      "learning_rate": 3.333849329205366e-05,
      "loss": 21.8486,
      "step": 13519
    },
    {
      "epoch": 13.53,
      "grad_norm": 10272.171875,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 43.4998,
      "step": 13520
    },
    {
      "epoch": 13.53,
      "grad_norm": 29391.3046875,
      "learning_rate": 3.3328173374613e-05,
      "loss": 38.4423,
      "step": 13521
    },
    {
      "epoch": 13.54,
      "grad_norm": 13557.6572265625,
      "learning_rate": 3.332301341589267e-05,
      "loss": 42.9438,
      "step": 13522
    },
    {
      "epoch": 13.54,
      "grad_norm": 7715.7490234375,
      "learning_rate": 3.3317853457172345e-05,
      "loss": 43.0738,
      "step": 13523
    },
    {
      "epoch": 13.54,
      "grad_norm": 37140.3828125,
      "learning_rate": 3.331269349845201e-05,
      "loss": 49.7809,
      "step": 13524
    },
    {
      "epoch": 13.54,
      "grad_norm": 3239.225341796875,
      "learning_rate": 3.330753353973168e-05,
      "loss": 49.5879,
      "step": 13525
    },
    {
      "epoch": 13.54,
      "grad_norm": 29892.4453125,
      "learning_rate": 3.3302373581011355e-05,
      "loss": 40.8824,
      "step": 13526
    },
    {
      "epoch": 13.54,
      "grad_norm": 23394.12109375,
      "learning_rate": 3.329721362229102e-05,
      "loss": 50.3606,
      "step": 13527
    },
    {
      "epoch": 13.54,
      "grad_norm": 6493.36962890625,
      "learning_rate": 3.329205366357069e-05,
      "loss": 45.0934,
      "step": 13528
    },
    {
      "epoch": 13.54,
      "grad_norm": 14239.8876953125,
      "learning_rate": 3.3286893704850365e-05,
      "loss": 47.989,
      "step": 13529
    },
    {
      "epoch": 13.54,
      "grad_norm": 7676.8115234375,
      "learning_rate": 3.328173374613003e-05,
      "loss": 44.2789,
      "step": 13530
    },
    {
      "epoch": 13.54,
      "grad_norm": 8971.14453125,
      "learning_rate": 3.327657378740971e-05,
      "loss": 31.8561,
      "step": 13531
    },
    {
      "epoch": 13.55,
      "grad_norm": 15410.7421875,
      "learning_rate": 3.3271413828689375e-05,
      "loss": 43.2033,
      "step": 13532
    },
    {
      "epoch": 13.55,
      "grad_norm": 12529.0244140625,
      "learning_rate": 3.326625386996904e-05,
      "loss": 52.0591,
      "step": 13533
    },
    {
      "epoch": 13.55,
      "grad_norm": 28133.302734375,
      "learning_rate": 3.326109391124872e-05,
      "loss": 50.8038,
      "step": 13534
    },
    {
      "epoch": 13.55,
      "grad_norm": 10574.75,
      "learning_rate": 3.325593395252838e-05,
      "loss": 35.1299,
      "step": 13535
    },
    {
      "epoch": 13.55,
      "grad_norm": 3611.751708984375,
      "learning_rate": 3.3250773993808046e-05,
      "loss": 39.8714,
      "step": 13536
    },
    {
      "epoch": 13.55,
      "grad_norm": 24994.970703125,
      "learning_rate": 3.324561403508772e-05,
      "loss": 43.5002,
      "step": 13537
    },
    {
      "epoch": 13.55,
      "grad_norm": 13226.59375,
      "learning_rate": 3.324045407636739e-05,
      "loss": 40.1503,
      "step": 13538
    },
    {
      "epoch": 13.55,
      "grad_norm": 49823.06640625,
      "learning_rate": 3.3235294117647056e-05,
      "loss": 39.3597,
      "step": 13539
    },
    {
      "epoch": 13.55,
      "grad_norm": 10802.908203125,
      "learning_rate": 3.323013415892673e-05,
      "loss": 46.3109,
      "step": 13540
    },
    {
      "epoch": 13.55,
      "grad_norm": 14795.8544921875,
      "learning_rate": 3.32249742002064e-05,
      "loss": 39.6972,
      "step": 13541
    },
    {
      "epoch": 13.56,
      "grad_norm": 19077.361328125,
      "learning_rate": 3.3219814241486066e-05,
      "loss": 56.9343,
      "step": 13542
    },
    {
      "epoch": 13.56,
      "grad_norm": 8035.091796875,
      "learning_rate": 3.321465428276574e-05,
      "loss": 43.1014,
      "step": 13543
    },
    {
      "epoch": 13.56,
      "grad_norm": 4932.89111328125,
      "learning_rate": 3.320949432404541e-05,
      "loss": 25.8092,
      "step": 13544
    },
    {
      "epoch": 13.56,
      "grad_norm": 70194.9375,
      "learning_rate": 3.320433436532508e-05,
      "loss": 40.5751,
      "step": 13545
    },
    {
      "epoch": 13.56,
      "grad_norm": 31555.314453125,
      "learning_rate": 3.319917440660475e-05,
      "loss": 22.8573,
      "step": 13546
    },
    {
      "epoch": 13.56,
      "grad_norm": 55186.046875,
      "learning_rate": 3.319401444788442e-05,
      "loss": 41.6268,
      "step": 13547
    },
    {
      "epoch": 13.56,
      "grad_norm": 21633.08984375,
      "learning_rate": 3.318885448916409e-05,
      "loss": 31.9562,
      "step": 13548
    },
    {
      "epoch": 13.56,
      "grad_norm": 10215.54296875,
      "learning_rate": 3.318369453044376e-05,
      "loss": 43.4725,
      "step": 13549
    },
    {
      "epoch": 13.56,
      "grad_norm": 4906.5908203125,
      "learning_rate": 3.317853457172343e-05,
      "loss": 35.481,
      "step": 13550
    },
    {
      "epoch": 13.56,
      "grad_norm": 4379.0654296875,
      "learning_rate": 3.31733746130031e-05,
      "loss": 49.8283,
      "step": 13551
    },
    {
      "epoch": 13.57,
      "grad_norm": 8136.849609375,
      "learning_rate": 3.316821465428277e-05,
      "loss": 28.4267,
      "step": 13552
    },
    {
      "epoch": 13.57,
      "grad_norm": 6109.41748046875,
      "learning_rate": 3.316305469556243e-05,
      "loss": 36.2529,
      "step": 13553
    },
    {
      "epoch": 13.57,
      "grad_norm": 28305.087890625,
      "learning_rate": 3.3157894736842106e-05,
      "loss": 23.7164,
      "step": 13554
    },
    {
      "epoch": 13.57,
      "grad_norm": 6875.43798828125,
      "learning_rate": 3.3152734778121774e-05,
      "loss": 58.3116,
      "step": 13555
    },
    {
      "epoch": 13.57,
      "grad_norm": 7838.4873046875,
      "learning_rate": 3.314757481940144e-05,
      "loss": 33.6786,
      "step": 13556
    },
    {
      "epoch": 13.57,
      "grad_norm": 41287.8828125,
      "learning_rate": 3.3142414860681116e-05,
      "loss": 42.5631,
      "step": 13557
    },
    {
      "epoch": 13.57,
      "grad_norm": 9687.220703125,
      "learning_rate": 3.3137254901960784e-05,
      "loss": 44.4302,
      "step": 13558
    },
    {
      "epoch": 13.57,
      "grad_norm": 2900.98046875,
      "learning_rate": 3.313209494324046e-05,
      "loss": 39.2513,
      "step": 13559
    },
    {
      "epoch": 13.57,
      "grad_norm": 12583.3857421875,
      "learning_rate": 3.3126934984520126e-05,
      "loss": 42.5953,
      "step": 13560
    },
    {
      "epoch": 13.57,
      "grad_norm": 4320.482421875,
      "learning_rate": 3.3121775025799794e-05,
      "loss": 43.9056,
      "step": 13561
    },
    {
      "epoch": 13.58,
      "grad_norm": 34258.87890625,
      "learning_rate": 3.311661506707947e-05,
      "loss": 32.0111,
      "step": 13562
    },
    {
      "epoch": 13.58,
      "grad_norm": 1216.19189453125,
      "learning_rate": 3.3111455108359136e-05,
      "loss": 46.3131,
      "step": 13563
    },
    {
      "epoch": 13.58,
      "grad_norm": 35641.83984375,
      "learning_rate": 3.3106295149638804e-05,
      "loss": 24.3868,
      "step": 13564
    },
    {
      "epoch": 13.58,
      "grad_norm": 14859.451171875,
      "learning_rate": 3.310113519091848e-05,
      "loss": 50.9275,
      "step": 13565
    },
    {
      "epoch": 13.58,
      "grad_norm": 24303.8203125,
      "learning_rate": 3.3095975232198146e-05,
      "loss": 35.7722,
      "step": 13566
    },
    {
      "epoch": 13.58,
      "grad_norm": 29658.58984375,
      "learning_rate": 3.3090815273477814e-05,
      "loss": 25.7657,
      "step": 13567
    },
    {
      "epoch": 13.58,
      "grad_norm": 4803.7080078125,
      "learning_rate": 3.308565531475749e-05,
      "loss": 43.724,
      "step": 13568
    },
    {
      "epoch": 13.58,
      "grad_norm": 15241.9912109375,
      "learning_rate": 3.3080495356037156e-05,
      "loss": 41.5677,
      "step": 13569
    },
    {
      "epoch": 13.58,
      "grad_norm": 38591.41796875,
      "learning_rate": 3.3075335397316824e-05,
      "loss": 50.0546,
      "step": 13570
    },
    {
      "epoch": 13.58,
      "grad_norm": 11350.724609375,
      "learning_rate": 3.307017543859649e-05,
      "loss": 32.7038,
      "step": 13571
    },
    {
      "epoch": 13.59,
      "grad_norm": 86004.90625,
      "learning_rate": 3.306501547987616e-05,
      "loss": 40.972,
      "step": 13572
    },
    {
      "epoch": 13.59,
      "grad_norm": 3863.673095703125,
      "learning_rate": 3.305985552115583e-05,
      "loss": 47.9843,
      "step": 13573
    },
    {
      "epoch": 13.59,
      "grad_norm": 8171.32177734375,
      "learning_rate": 3.30546955624355e-05,
      "loss": 47.9196,
      "step": 13574
    },
    {
      "epoch": 13.59,
      "grad_norm": 9532.0205078125,
      "learning_rate": 3.304953560371517e-05,
      "loss": 33.9354,
      "step": 13575
    },
    {
      "epoch": 13.59,
      "grad_norm": 9379.4921875,
      "learning_rate": 3.3044375644994844e-05,
      "loss": 50.4193,
      "step": 13576
    },
    {
      "epoch": 13.59,
      "grad_norm": 2195.240234375,
      "learning_rate": 3.303921568627451e-05,
      "loss": 39.9108,
      "step": 13577
    },
    {
      "epoch": 13.59,
      "grad_norm": 16675.328125,
      "learning_rate": 3.303405572755418e-05,
      "loss": 46.6941,
      "step": 13578
    },
    {
      "epoch": 13.59,
      "grad_norm": 73481.25,
      "learning_rate": 3.3028895768833854e-05,
      "loss": 43.2421,
      "step": 13579
    },
    {
      "epoch": 13.59,
      "grad_norm": 6718.17724609375,
      "learning_rate": 3.302373581011352e-05,
      "loss": 36.7784,
      "step": 13580
    },
    {
      "epoch": 13.59,
      "grad_norm": 7423.7724609375,
      "learning_rate": 3.301857585139319e-05,
      "loss": 37.8424,
      "step": 13581
    },
    {
      "epoch": 13.6,
      "grad_norm": 10871.845703125,
      "learning_rate": 3.3013415892672864e-05,
      "loss": 55.9722,
      "step": 13582
    },
    {
      "epoch": 13.6,
      "grad_norm": 41952.33203125,
      "learning_rate": 3.300825593395253e-05,
      "loss": 32.1644,
      "step": 13583
    },
    {
      "epoch": 13.6,
      "grad_norm": 59677.578125,
      "learning_rate": 3.30030959752322e-05,
      "loss": 26.452,
      "step": 13584
    },
    {
      "epoch": 13.6,
      "grad_norm": 16562.49609375,
      "learning_rate": 3.2997936016511874e-05,
      "loss": 42.4163,
      "step": 13585
    },
    {
      "epoch": 13.6,
      "grad_norm": 2847.25830078125,
      "learning_rate": 3.299277605779154e-05,
      "loss": 55.708,
      "step": 13586
    },
    {
      "epoch": 13.6,
      "grad_norm": 9974.083984375,
      "learning_rate": 3.298761609907121e-05,
      "loss": 36.0804,
      "step": 13587
    },
    {
      "epoch": 13.6,
      "grad_norm": 5126.455078125,
      "learning_rate": 3.2982456140350884e-05,
      "loss": 36.2897,
      "step": 13588
    },
    {
      "epoch": 13.6,
      "grad_norm": 16963.7578125,
      "learning_rate": 3.2977296181630545e-05,
      "loss": 49.9965,
      "step": 13589
    },
    {
      "epoch": 13.6,
      "grad_norm": 11130.0029296875,
      "learning_rate": 3.297213622291022e-05,
      "loss": 34.3951,
      "step": 13590
    },
    {
      "epoch": 13.6,
      "grad_norm": 11793.39453125,
      "learning_rate": 3.296697626418989e-05,
      "loss": 38.3229,
      "step": 13591
    },
    {
      "epoch": 13.61,
      "grad_norm": 4485.3037109375,
      "learning_rate": 3.2961816305469555e-05,
      "loss": 41.7388,
      "step": 13592
    },
    {
      "epoch": 13.61,
      "grad_norm": 5964.58447265625,
      "learning_rate": 3.295665634674923e-05,
      "loss": 46.3429,
      "step": 13593
    },
    {
      "epoch": 13.61,
      "grad_norm": 2961.608642578125,
      "learning_rate": 3.29514963880289e-05,
      "loss": 40.9117,
      "step": 13594
    },
    {
      "epoch": 13.61,
      "grad_norm": 3243.991943359375,
      "learning_rate": 3.2946336429308565e-05,
      "loss": 37.3118,
      "step": 13595
    },
    {
      "epoch": 13.61,
      "grad_norm": 5126.34423828125,
      "learning_rate": 3.294117647058824e-05,
      "loss": 40.1948,
      "step": 13596
    },
    {
      "epoch": 13.61,
      "grad_norm": 19030.50390625,
      "learning_rate": 3.293601651186791e-05,
      "loss": 45.592,
      "step": 13597
    },
    {
      "epoch": 13.61,
      "grad_norm": 343870.75,
      "learning_rate": 3.2930856553147575e-05,
      "loss": 50.373,
      "step": 13598
    },
    {
      "epoch": 13.61,
      "grad_norm": 4064.634765625,
      "learning_rate": 3.292569659442725e-05,
      "loss": 37.0383,
      "step": 13599
    },
    {
      "epoch": 13.61,
      "grad_norm": 18712.021484375,
      "learning_rate": 3.292053663570692e-05,
      "loss": 41.5631,
      "step": 13600
    },
    {
      "epoch": 13.61,
      "grad_norm": 32747.90234375,
      "learning_rate": 3.2915376676986585e-05,
      "loss": 36.2428,
      "step": 13601
    },
    {
      "epoch": 13.62,
      "grad_norm": 13496.5966796875,
      "learning_rate": 3.291021671826626e-05,
      "loss": 51.614,
      "step": 13602
    },
    {
      "epoch": 13.62,
      "grad_norm": 5510.9677734375,
      "learning_rate": 3.290505675954593e-05,
      "loss": 47.4142,
      "step": 13603
    },
    {
      "epoch": 13.62,
      "grad_norm": 6127.552734375,
      "learning_rate": 3.2899896800825595e-05,
      "loss": 45.9807,
      "step": 13604
    },
    {
      "epoch": 13.62,
      "grad_norm": 11678.05859375,
      "learning_rate": 3.289473684210527e-05,
      "loss": 45.5409,
      "step": 13605
    },
    {
      "epoch": 13.62,
      "grad_norm": 35492.23046875,
      "learning_rate": 3.288957688338494e-05,
      "loss": 31.3799,
      "step": 13606
    },
    {
      "epoch": 13.62,
      "grad_norm": 12142.111328125,
      "learning_rate": 3.2884416924664605e-05,
      "loss": 49.7347,
      "step": 13607
    },
    {
      "epoch": 13.62,
      "grad_norm": 9893.21875,
      "learning_rate": 3.287925696594427e-05,
      "loss": 40.9614,
      "step": 13608
    },
    {
      "epoch": 13.62,
      "grad_norm": 107062.6015625,
      "learning_rate": 3.287409700722394e-05,
      "loss": 44.2363,
      "step": 13609
    },
    {
      "epoch": 13.62,
      "grad_norm": 44611.7578125,
      "learning_rate": 3.2868937048503615e-05,
      "loss": 26.8157,
      "step": 13610
    },
    {
      "epoch": 13.62,
      "grad_norm": 35215.89453125,
      "learning_rate": 3.286377708978328e-05,
      "loss": 46.4636,
      "step": 13611
    },
    {
      "epoch": 13.63,
      "grad_norm": 15918.4873046875,
      "learning_rate": 3.285861713106295e-05,
      "loss": 48.7303,
      "step": 13612
    },
    {
      "epoch": 13.63,
      "grad_norm": 9574.8544921875,
      "learning_rate": 3.2853457172342625e-05,
      "loss": 44.2985,
      "step": 13613
    },
    {
      "epoch": 13.63,
      "grad_norm": 4065.293212890625,
      "learning_rate": 3.284829721362229e-05,
      "loss": 29.4783,
      "step": 13614
    },
    {
      "epoch": 13.63,
      "grad_norm": 61118.4375,
      "learning_rate": 3.284313725490196e-05,
      "loss": 47.077,
      "step": 13615
    },
    {
      "epoch": 13.63,
      "grad_norm": 18500.091796875,
      "learning_rate": 3.2837977296181635e-05,
      "loss": 36.7935,
      "step": 13616
    },
    {
      "epoch": 13.63,
      "grad_norm": 19042.259765625,
      "learning_rate": 3.28328173374613e-05,
      "loss": 40.5425,
      "step": 13617
    },
    {
      "epoch": 13.63,
      "grad_norm": 128765.625,
      "learning_rate": 3.282765737874097e-05,
      "loss": 46.5512,
      "step": 13618
    },
    {
      "epoch": 13.63,
      "grad_norm": 56565.9453125,
      "learning_rate": 3.2822497420020645e-05,
      "loss": 34.8046,
      "step": 13619
    },
    {
      "epoch": 13.63,
      "grad_norm": 22924.529296875,
      "learning_rate": 3.281733746130031e-05,
      "loss": 47.3771,
      "step": 13620
    },
    {
      "epoch": 13.63,
      "grad_norm": 6751.6533203125,
      "learning_rate": 3.281217750257998e-05,
      "loss": 47.7166,
      "step": 13621
    },
    {
      "epoch": 13.64,
      "grad_norm": 9704.6845703125,
      "learning_rate": 3.2807017543859655e-05,
      "loss": 44.8962,
      "step": 13622
    },
    {
      "epoch": 13.64,
      "grad_norm": 6235.3544921875,
      "learning_rate": 3.280185758513932e-05,
      "loss": 26.4892,
      "step": 13623
    },
    {
      "epoch": 13.64,
      "grad_norm": 4915.36376953125,
      "learning_rate": 3.279669762641899e-05,
      "loss": 36.0426,
      "step": 13624
    },
    {
      "epoch": 13.64,
      "grad_norm": 13572.4921875,
      "learning_rate": 3.279153766769866e-05,
      "loss": 40.798,
      "step": 13625
    },
    {
      "epoch": 13.64,
      "grad_norm": 11618.1005859375,
      "learning_rate": 3.2786377708978326e-05,
      "loss": 35.5352,
      "step": 13626
    },
    {
      "epoch": 13.64,
      "grad_norm": 29626.283203125,
      "learning_rate": 3.2781217750258e-05,
      "loss": 40.5457,
      "step": 13627
    },
    {
      "epoch": 13.64,
      "grad_norm": 12236.087890625,
      "learning_rate": 3.277605779153767e-05,
      "loss": 20.3486,
      "step": 13628
    },
    {
      "epoch": 13.64,
      "grad_norm": 32501.982421875,
      "learning_rate": 3.2770897832817336e-05,
      "loss": 52.209,
      "step": 13629
    },
    {
      "epoch": 13.64,
      "grad_norm": 8086.46826171875,
      "learning_rate": 3.276573787409701e-05,
      "loss": 16.2407,
      "step": 13630
    },
    {
      "epoch": 13.64,
      "grad_norm": 32920.296875,
      "learning_rate": 3.276057791537668e-05,
      "loss": 31.5129,
      "step": 13631
    },
    {
      "epoch": 13.65,
      "grad_norm": 7911.330078125,
      "learning_rate": 3.2755417956656346e-05,
      "loss": 41.0462,
      "step": 13632
    },
    {
      "epoch": 13.65,
      "grad_norm": 18668.9375,
      "learning_rate": 3.275025799793602e-05,
      "loss": 36.6822,
      "step": 13633
    },
    {
      "epoch": 13.65,
      "grad_norm": 10067.923828125,
      "learning_rate": 3.274509803921569e-05,
      "loss": 40.218,
      "step": 13634
    },
    {
      "epoch": 13.65,
      "grad_norm": 17265.19921875,
      "learning_rate": 3.2739938080495356e-05,
      "loss": 31.4958,
      "step": 13635
    },
    {
      "epoch": 13.65,
      "grad_norm": 7322.77685546875,
      "learning_rate": 3.273477812177503e-05,
      "loss": 50.9629,
      "step": 13636
    },
    {
      "epoch": 13.65,
      "grad_norm": 10895.9775390625,
      "learning_rate": 3.27296181630547e-05,
      "loss": 35.9845,
      "step": 13637
    },
    {
      "epoch": 13.65,
      "grad_norm": 23955.658203125,
      "learning_rate": 3.2724458204334366e-05,
      "loss": 41.5674,
      "step": 13638
    },
    {
      "epoch": 13.65,
      "grad_norm": 12606.3017578125,
      "learning_rate": 3.271929824561404e-05,
      "loss": 40.1443,
      "step": 13639
    },
    {
      "epoch": 13.65,
      "grad_norm": 13840.8603515625,
      "learning_rate": 3.271413828689371e-05,
      "loss": 18.965,
      "step": 13640
    },
    {
      "epoch": 13.65,
      "grad_norm": 30411.26171875,
      "learning_rate": 3.2708978328173376e-05,
      "loss": 35.7905,
      "step": 13641
    },
    {
      "epoch": 13.66,
      "grad_norm": 8521.134765625,
      "learning_rate": 3.270381836945305e-05,
      "loss": 48.7889,
      "step": 13642
    },
    {
      "epoch": 13.66,
      "grad_norm": 37121.890625,
      "learning_rate": 3.269865841073271e-05,
      "loss": 49.2849,
      "step": 13643
    },
    {
      "epoch": 13.66,
      "grad_norm": 24645.771484375,
      "learning_rate": 3.2693498452012386e-05,
      "loss": 38.7672,
      "step": 13644
    },
    {
      "epoch": 13.66,
      "grad_norm": 7198.96337890625,
      "learning_rate": 3.2688338493292053e-05,
      "loss": 52.9304,
      "step": 13645
    },
    {
      "epoch": 13.66,
      "grad_norm": 7132.4521484375,
      "learning_rate": 3.268317853457172e-05,
      "loss": 52.1092,
      "step": 13646
    },
    {
      "epoch": 13.66,
      "grad_norm": 32118.8515625,
      "learning_rate": 3.2678018575851396e-05,
      "loss": 35.0188,
      "step": 13647
    },
    {
      "epoch": 13.66,
      "grad_norm": 8093.20849609375,
      "learning_rate": 3.2672858617131063e-05,
      "loss": 42.0702,
      "step": 13648
    },
    {
      "epoch": 13.66,
      "grad_norm": 4169.76806640625,
      "learning_rate": 3.266769865841073e-05,
      "loss": 41.6066,
      "step": 13649
    },
    {
      "epoch": 13.66,
      "grad_norm": 10450.205078125,
      "learning_rate": 3.2662538699690406e-05,
      "loss": 43.5881,
      "step": 13650
    },
    {
      "epoch": 13.66,
      "grad_norm": 12012.33203125,
      "learning_rate": 3.2657378740970073e-05,
      "loss": 40.0714,
      "step": 13651
    },
    {
      "epoch": 13.67,
      "grad_norm": 13636.193359375,
      "learning_rate": 3.265221878224974e-05,
      "loss": 51.3618,
      "step": 13652
    },
    {
      "epoch": 13.67,
      "grad_norm": 41998.51171875,
      "learning_rate": 3.2647058823529416e-05,
      "loss": 45.2602,
      "step": 13653
    },
    {
      "epoch": 13.67,
      "grad_norm": 14719.9345703125,
      "learning_rate": 3.2641898864809083e-05,
      "loss": 43.5828,
      "step": 13654
    },
    {
      "epoch": 13.67,
      "grad_norm": 32039.064453125,
      "learning_rate": 3.263673890608875e-05,
      "loss": 21.2457,
      "step": 13655
    },
    {
      "epoch": 13.67,
      "grad_norm": 7708.26318359375,
      "learning_rate": 3.2631578947368426e-05,
      "loss": 45.7451,
      "step": 13656
    },
    {
      "epoch": 13.67,
      "grad_norm": 11729.9873046875,
      "learning_rate": 3.2626418988648093e-05,
      "loss": 24.5659,
      "step": 13657
    },
    {
      "epoch": 13.67,
      "grad_norm": 3021.04638671875,
      "learning_rate": 3.262125902992776e-05,
      "loss": 53.0966,
      "step": 13658
    },
    {
      "epoch": 13.67,
      "grad_norm": 2077.922119140625,
      "learning_rate": 3.2616099071207436e-05,
      "loss": 51.7526,
      "step": 13659
    },
    {
      "epoch": 13.67,
      "grad_norm": 8198.1728515625,
      "learning_rate": 3.2610939112487103e-05,
      "loss": 30.6264,
      "step": 13660
    },
    {
      "epoch": 13.67,
      "grad_norm": 3126.23828125,
      "learning_rate": 3.260577915376677e-05,
      "loss": 29.4363,
      "step": 13661
    },
    {
      "epoch": 13.68,
      "grad_norm": 8858.490234375,
      "learning_rate": 3.260061919504644e-05,
      "loss": 51.9221,
      "step": 13662
    },
    {
      "epoch": 13.68,
      "grad_norm": 66763.4609375,
      "learning_rate": 3.259545923632611e-05,
      "loss": 49.8237,
      "step": 13663
    },
    {
      "epoch": 13.68,
      "grad_norm": 6111.07177734375,
      "learning_rate": 3.259029927760578e-05,
      "loss": 25.9563,
      "step": 13664
    },
    {
      "epoch": 13.68,
      "grad_norm": 16274.8359375,
      "learning_rate": 3.258513931888545e-05,
      "loss": 23.9345,
      "step": 13665
    },
    {
      "epoch": 13.68,
      "grad_norm": 24870.681640625,
      "learning_rate": 3.257997936016512e-05,
      "loss": 57.4344,
      "step": 13666
    },
    {
      "epoch": 13.68,
      "grad_norm": 41936.2578125,
      "learning_rate": 3.257481940144479e-05,
      "loss": 30.0116,
      "step": 13667
    },
    {
      "epoch": 13.68,
      "grad_norm": 6062.8427734375,
      "learning_rate": 3.256965944272446e-05,
      "loss": 48.6219,
      "step": 13668
    },
    {
      "epoch": 13.68,
      "grad_norm": 68251.7734375,
      "learning_rate": 3.256449948400413e-05,
      "loss": 41.1597,
      "step": 13669
    },
    {
      "epoch": 13.68,
      "grad_norm": 2889.41748046875,
      "learning_rate": 3.25593395252838e-05,
      "loss": 30.2889,
      "step": 13670
    },
    {
      "epoch": 13.68,
      "grad_norm": 8347.16015625,
      "learning_rate": 3.255417956656347e-05,
      "loss": 34.3464,
      "step": 13671
    },
    {
      "epoch": 13.69,
      "grad_norm": 190775.0,
      "learning_rate": 3.254901960784314e-05,
      "loss": 36.7787,
      "step": 13672
    },
    {
      "epoch": 13.69,
      "grad_norm": 7257.53466796875,
      "learning_rate": 3.254385964912281e-05,
      "loss": 42.7081,
      "step": 13673
    },
    {
      "epoch": 13.69,
      "grad_norm": 19739.291015625,
      "learning_rate": 3.253869969040248e-05,
      "loss": 48.7579,
      "step": 13674
    },
    {
      "epoch": 13.69,
      "grad_norm": 4755.9013671875,
      "learning_rate": 3.253353973168215e-05,
      "loss": 28.0784,
      "step": 13675
    },
    {
      "epoch": 13.69,
      "grad_norm": 9634.53125,
      "learning_rate": 3.252837977296182e-05,
      "loss": 34.6914,
      "step": 13676
    },
    {
      "epoch": 13.69,
      "grad_norm": 25764.56640625,
      "learning_rate": 3.252321981424149e-05,
      "loss": 22.341,
      "step": 13677
    },
    {
      "epoch": 13.69,
      "grad_norm": 26220.4296875,
      "learning_rate": 3.251805985552116e-05,
      "loss": 38.7881,
      "step": 13678
    },
    {
      "epoch": 13.69,
      "grad_norm": 37735.91796875,
      "learning_rate": 3.2512899896800824e-05,
      "loss": 49.8337,
      "step": 13679
    },
    {
      "epoch": 13.69,
      "grad_norm": 7312.60986328125,
      "learning_rate": 3.250773993808049e-05,
      "loss": 32.1207,
      "step": 13680
    },
    {
      "epoch": 13.69,
      "grad_norm": 36001.92578125,
      "learning_rate": 3.250257997936017e-05,
      "loss": 51.4048,
      "step": 13681
    },
    {
      "epoch": 13.7,
      "grad_norm": 13414.578125,
      "learning_rate": 3.2497420020639834e-05,
      "loss": 45.4536,
      "step": 13682
    },
    {
      "epoch": 13.7,
      "grad_norm": 39541.0,
      "learning_rate": 3.24922600619195e-05,
      "loss": 34.518,
      "step": 13683
    },
    {
      "epoch": 13.7,
      "grad_norm": 7375.6123046875,
      "learning_rate": 3.248710010319918e-05,
      "loss": 52.1271,
      "step": 13684
    },
    {
      "epoch": 13.7,
      "grad_norm": 22541.68359375,
      "learning_rate": 3.2481940144478844e-05,
      "loss": 47.0049,
      "step": 13685
    },
    {
      "epoch": 13.7,
      "grad_norm": 45545.65234375,
      "learning_rate": 3.247678018575851e-05,
      "loss": 40.3394,
      "step": 13686
    },
    {
      "epoch": 13.7,
      "grad_norm": 34406.125,
      "learning_rate": 3.247162022703819e-05,
      "loss": 45.5779,
      "step": 13687
    },
    {
      "epoch": 13.7,
      "grad_norm": 9929.84765625,
      "learning_rate": 3.2466460268317854e-05,
      "loss": 35.0982,
      "step": 13688
    },
    {
      "epoch": 13.7,
      "grad_norm": 20485.08203125,
      "learning_rate": 3.246130030959752e-05,
      "loss": 42.2399,
      "step": 13689
    },
    {
      "epoch": 13.7,
      "grad_norm": 15225.494140625,
      "learning_rate": 3.24561403508772e-05,
      "loss": 42.2226,
      "step": 13690
    },
    {
      "epoch": 13.7,
      "grad_norm": 5914.11865234375,
      "learning_rate": 3.2450980392156864e-05,
      "loss": 40.875,
      "step": 13691
    },
    {
      "epoch": 13.71,
      "grad_norm": 149369.234375,
      "learning_rate": 3.244582043343654e-05,
      "loss": 45.8053,
      "step": 13692
    },
    {
      "epoch": 13.71,
      "grad_norm": 17063.083984375,
      "learning_rate": 3.244066047471621e-05,
      "loss": 44.3711,
      "step": 13693
    },
    {
      "epoch": 13.71,
      "grad_norm": 66939.953125,
      "learning_rate": 3.2435500515995874e-05,
      "loss": 37.3318,
      "step": 13694
    },
    {
      "epoch": 13.71,
      "grad_norm": 30387.634765625,
      "learning_rate": 3.243034055727555e-05,
      "loss": 46.4985,
      "step": 13695
    },
    {
      "epoch": 13.71,
      "grad_norm": 171515.875,
      "learning_rate": 3.242518059855521e-05,
      "loss": 36.4316,
      "step": 13696
    },
    {
      "epoch": 13.71,
      "grad_norm": 8293.8349609375,
      "learning_rate": 3.242002063983488e-05,
      "loss": 54.5926,
      "step": 13697
    },
    {
      "epoch": 13.71,
      "grad_norm": 5439.099609375,
      "learning_rate": 3.241486068111455e-05,
      "loss": 30.1796,
      "step": 13698
    },
    {
      "epoch": 13.71,
      "grad_norm": 14948.5341796875,
      "learning_rate": 3.240970072239422e-05,
      "loss": 44.318,
      "step": 13699
    },
    {
      "epoch": 13.71,
      "grad_norm": 34752.91015625,
      "learning_rate": 3.240454076367389e-05,
      "loss": 38.1385,
      "step": 13700
    },
    {
      "epoch": 13.71,
      "grad_norm": 26234.765625,
      "learning_rate": 3.239938080495356e-05,
      "loss": 31.2422,
      "step": 13701
    },
    {
      "epoch": 13.72,
      "grad_norm": 21042.40625,
      "learning_rate": 3.239422084623323e-05,
      "loss": 45.5702,
      "step": 13702
    },
    {
      "epoch": 13.72,
      "grad_norm": 18035.5625,
      "learning_rate": 3.23890608875129e-05,
      "loss": 47.3735,
      "step": 13703
    },
    {
      "epoch": 13.72,
      "grad_norm": 11958.6865234375,
      "learning_rate": 3.238390092879257e-05,
      "loss": 43.7149,
      "step": 13704
    },
    {
      "epoch": 13.72,
      "grad_norm": 41368.4609375,
      "learning_rate": 3.237874097007224e-05,
      "loss": 50.0208,
      "step": 13705
    },
    {
      "epoch": 13.72,
      "grad_norm": 7550.73486328125,
      "learning_rate": 3.2373581011351914e-05,
      "loss": 45.3864,
      "step": 13706
    },
    {
      "epoch": 13.72,
      "grad_norm": 2391.600341796875,
      "learning_rate": 3.236842105263158e-05,
      "loss": 49.8536,
      "step": 13707
    },
    {
      "epoch": 13.72,
      "grad_norm": 6628.947265625,
      "learning_rate": 3.236326109391125e-05,
      "loss": 28.6511,
      "step": 13708
    },
    {
      "epoch": 13.72,
      "grad_norm": 12727.46484375,
      "learning_rate": 3.2358101135190924e-05,
      "loss": 54.3717,
      "step": 13709
    },
    {
      "epoch": 13.72,
      "grad_norm": 3791.698486328125,
      "learning_rate": 3.235294117647059e-05,
      "loss": 31.2341,
      "step": 13710
    },
    {
      "epoch": 13.72,
      "grad_norm": 25222.123046875,
      "learning_rate": 3.234778121775026e-05,
      "loss": 31.3046,
      "step": 13711
    },
    {
      "epoch": 13.73,
      "grad_norm": 14571.6865234375,
      "learning_rate": 3.2342621259029934e-05,
      "loss": 33.1182,
      "step": 13712
    },
    {
      "epoch": 13.73,
      "grad_norm": 16314.9599609375,
      "learning_rate": 3.23374613003096e-05,
      "loss": 51.6469,
      "step": 13713
    },
    {
      "epoch": 13.73,
      "grad_norm": 91428.8671875,
      "learning_rate": 3.233230134158926e-05,
      "loss": 47.1876,
      "step": 13714
    },
    {
      "epoch": 13.73,
      "grad_norm": 5401.29150390625,
      "learning_rate": 3.232714138286894e-05,
      "loss": 35.8595,
      "step": 13715
    },
    {
      "epoch": 13.73,
      "grad_norm": 14611.7529296875,
      "learning_rate": 3.2321981424148605e-05,
      "loss": 52.6882,
      "step": 13716
    },
    {
      "epoch": 13.73,
      "grad_norm": 4371.04052734375,
      "learning_rate": 3.231682146542827e-05,
      "loss": 49.714,
      "step": 13717
    },
    {
      "epoch": 13.73,
      "grad_norm": 97929.9296875,
      "learning_rate": 3.231166150670795e-05,
      "loss": 34.8199,
      "step": 13718
    },
    {
      "epoch": 13.73,
      "grad_norm": 110102.59375,
      "learning_rate": 3.2306501547987615e-05,
      "loss": 44.0143,
      "step": 13719
    },
    {
      "epoch": 13.73,
      "grad_norm": 16232.8896484375,
      "learning_rate": 3.230134158926729e-05,
      "loss": 45.7715,
      "step": 13720
    },
    {
      "epoch": 13.73,
      "grad_norm": 18273.765625,
      "learning_rate": 3.229618163054696e-05,
      "loss": 45.0844,
      "step": 13721
    },
    {
      "epoch": 13.74,
      "grad_norm": 7816.03271484375,
      "learning_rate": 3.2291021671826625e-05,
      "loss": 41.7336,
      "step": 13722
    },
    {
      "epoch": 13.74,
      "grad_norm": 16671.912109375,
      "learning_rate": 3.22858617131063e-05,
      "loss": 45.6255,
      "step": 13723
    },
    {
      "epoch": 13.74,
      "grad_norm": 8149.671875,
      "learning_rate": 3.228070175438597e-05,
      "loss": 47.7704,
      "step": 13724
    },
    {
      "epoch": 13.74,
      "grad_norm": 19044.943359375,
      "learning_rate": 3.2275541795665635e-05,
      "loss": 44.4504,
      "step": 13725
    },
    {
      "epoch": 13.74,
      "grad_norm": 32234.712890625,
      "learning_rate": 3.227038183694531e-05,
      "loss": 38.6396,
      "step": 13726
    },
    {
      "epoch": 13.74,
      "grad_norm": 4281.3427734375,
      "learning_rate": 3.226522187822498e-05,
      "loss": 39.1383,
      "step": 13727
    },
    {
      "epoch": 13.74,
      "grad_norm": 9029.3349609375,
      "learning_rate": 3.2260061919504645e-05,
      "loss": 52.7882,
      "step": 13728
    },
    {
      "epoch": 13.74,
      "grad_norm": 846219.5,
      "learning_rate": 3.225490196078432e-05,
      "loss": 53.0301,
      "step": 13729
    },
    {
      "epoch": 13.74,
      "grad_norm": 28825.947265625,
      "learning_rate": 3.224974200206399e-05,
      "loss": 32.8325,
      "step": 13730
    },
    {
      "epoch": 13.74,
      "grad_norm": 6266.09130859375,
      "learning_rate": 3.2244582043343655e-05,
      "loss": 39.1719,
      "step": 13731
    },
    {
      "epoch": 13.75,
      "grad_norm": 83577.296875,
      "learning_rate": 3.223942208462332e-05,
      "loss": 55.0569,
      "step": 13732
    },
    {
      "epoch": 13.75,
      "grad_norm": 58387.984375,
      "learning_rate": 3.223426212590299e-05,
      "loss": 28.8487,
      "step": 13733
    },
    {
      "epoch": 13.75,
      "grad_norm": 16670.08984375,
      "learning_rate": 3.2229102167182665e-05,
      "loss": 42.8176,
      "step": 13734
    },
    {
      "epoch": 13.75,
      "grad_norm": 25608.4609375,
      "learning_rate": 3.222394220846233e-05,
      "loss": 41.496,
      "step": 13735
    },
    {
      "epoch": 13.75,
      "grad_norm": 5223.19921875,
      "learning_rate": 3.2218782249742e-05,
      "loss": 47.3191,
      "step": 13736
    },
    {
      "epoch": 13.75,
      "grad_norm": 17046.609375,
      "learning_rate": 3.2213622291021675e-05,
      "loss": 55.7265,
      "step": 13737
    },
    {
      "epoch": 13.75,
      "grad_norm": 11488.5732421875,
      "learning_rate": 3.220846233230134e-05,
      "loss": 48.7711,
      "step": 13738
    },
    {
      "epoch": 13.75,
      "grad_norm": 5210.67919921875,
      "learning_rate": 3.220330237358101e-05,
      "loss": 42.341,
      "step": 13739
    },
    {
      "epoch": 13.75,
      "grad_norm": 6844.38330078125,
      "learning_rate": 3.2198142414860685e-05,
      "loss": 32.145,
      "step": 13740
    },
    {
      "epoch": 13.75,
      "grad_norm": 5227.0927734375,
      "learning_rate": 3.219298245614035e-05,
      "loss": 31.3693,
      "step": 13741
    },
    {
      "epoch": 13.76,
      "grad_norm": 4011.278076171875,
      "learning_rate": 3.218782249742002e-05,
      "loss": 43.9505,
      "step": 13742
    },
    {
      "epoch": 13.76,
      "grad_norm": 5028.296875,
      "learning_rate": 3.2182662538699695e-05,
      "loss": 46.7603,
      "step": 13743
    },
    {
      "epoch": 13.76,
      "grad_norm": 12218.638671875,
      "learning_rate": 3.217750257997936e-05,
      "loss": 42.8303,
      "step": 13744
    },
    {
      "epoch": 13.76,
      "grad_norm": 14245.7265625,
      "learning_rate": 3.217234262125903e-05,
      "loss": 41.9697,
      "step": 13745
    },
    {
      "epoch": 13.76,
      "grad_norm": 34959.11328125,
      "learning_rate": 3.2167182662538705e-05,
      "loss": 45.972,
      "step": 13746
    },
    {
      "epoch": 13.76,
      "grad_norm": 14434.7998046875,
      "learning_rate": 3.216202270381837e-05,
      "loss": 42.2796,
      "step": 13747
    },
    {
      "epoch": 13.76,
      "grad_norm": 178970.328125,
      "learning_rate": 3.215686274509804e-05,
      "loss": 48.1393,
      "step": 13748
    },
    {
      "epoch": 13.76,
      "grad_norm": 5345.07958984375,
      "learning_rate": 3.2151702786377715e-05,
      "loss": 42.1995,
      "step": 13749
    },
    {
      "epoch": 13.76,
      "grad_norm": 37249.7890625,
      "learning_rate": 3.2146542827657376e-05,
      "loss": 47.3988,
      "step": 13750
    },
    {
      "epoch": 13.76,
      "grad_norm": 8074.9013671875,
      "learning_rate": 3.214138286893705e-05,
      "loss": 40.6607,
      "step": 13751
    },
    {
      "epoch": 13.77,
      "grad_norm": 9056.0625,
      "learning_rate": 3.213622291021672e-05,
      "loss": 43.0152,
      "step": 13752
    },
    {
      "epoch": 13.77,
      "grad_norm": 30371.072265625,
      "learning_rate": 3.2131062951496386e-05,
      "loss": 48.5939,
      "step": 13753
    },
    {
      "epoch": 13.77,
      "grad_norm": 36968.6015625,
      "learning_rate": 3.212590299277606e-05,
      "loss": 42.4027,
      "step": 13754
    },
    {
      "epoch": 13.77,
      "grad_norm": 18599.033203125,
      "learning_rate": 3.212074303405573e-05,
      "loss": 37.9244,
      "step": 13755
    },
    {
      "epoch": 13.77,
      "grad_norm": 60370.18359375,
      "learning_rate": 3.2115583075335396e-05,
      "loss": 35.9514,
      "step": 13756
    },
    {
      "epoch": 13.77,
      "grad_norm": 15120.1865234375,
      "learning_rate": 3.211042311661507e-05,
      "loss": 43.0893,
      "step": 13757
    },
    {
      "epoch": 13.77,
      "grad_norm": 10711.236328125,
      "learning_rate": 3.210526315789474e-05,
      "loss": 50.8832,
      "step": 13758
    },
    {
      "epoch": 13.77,
      "grad_norm": 3947.672119140625,
      "learning_rate": 3.2100103199174406e-05,
      "loss": 21.5509,
      "step": 13759
    },
    {
      "epoch": 13.77,
      "grad_norm": 10964.7216796875,
      "learning_rate": 3.209494324045408e-05,
      "loss": 55.2325,
      "step": 13760
    },
    {
      "epoch": 13.77,
      "grad_norm": 22370.205078125,
      "learning_rate": 3.208978328173375e-05,
      "loss": 37.7317,
      "step": 13761
    },
    {
      "epoch": 13.78,
      "grad_norm": 18881.58203125,
      "learning_rate": 3.2084623323013416e-05,
      "loss": 20.3266,
      "step": 13762
    },
    {
      "epoch": 13.78,
      "grad_norm": 5465.84716796875,
      "learning_rate": 3.207946336429309e-05,
      "loss": 37.5689,
      "step": 13763
    },
    {
      "epoch": 13.78,
      "grad_norm": 167287.234375,
      "learning_rate": 3.207430340557276e-05,
      "loss": 52.3567,
      "step": 13764
    },
    {
      "epoch": 13.78,
      "grad_norm": 107151.1015625,
      "learning_rate": 3.2069143446852426e-05,
      "loss": 42.7109,
      "step": 13765
    },
    {
      "epoch": 13.78,
      "grad_norm": 4231.96826171875,
      "learning_rate": 3.20639834881321e-05,
      "loss": 38.4402,
      "step": 13766
    },
    {
      "epoch": 13.78,
      "grad_norm": 7384.85302734375,
      "learning_rate": 3.205882352941177e-05,
      "loss": 31.005,
      "step": 13767
    },
    {
      "epoch": 13.78,
      "grad_norm": 19020.548828125,
      "learning_rate": 3.2053663570691436e-05,
      "loss": 38.9474,
      "step": 13768
    },
    {
      "epoch": 13.78,
      "grad_norm": 8160.46826171875,
      "learning_rate": 3.2048503611971104e-05,
      "loss": 46.7626,
      "step": 13769
    },
    {
      "epoch": 13.78,
      "grad_norm": 10217.4755859375,
      "learning_rate": 3.204334365325077e-05,
      "loss": 40.3914,
      "step": 13770
    },
    {
      "epoch": 13.78,
      "grad_norm": 3893.85546875,
      "learning_rate": 3.2038183694530446e-05,
      "loss": 48.1147,
      "step": 13771
    },
    {
      "epoch": 13.79,
      "grad_norm": 4473.34423828125,
      "learning_rate": 3.2033023735810114e-05,
      "loss": 40.597,
      "step": 13772
    },
    {
      "epoch": 13.79,
      "grad_norm": 37310.86328125,
      "learning_rate": 3.202786377708978e-05,
      "loss": 37.9618,
      "step": 13773
    },
    {
      "epoch": 13.79,
      "grad_norm": 3911.90478515625,
      "learning_rate": 3.2022703818369456e-05,
      "loss": 48.9702,
      "step": 13774
    },
    {
      "epoch": 13.79,
      "grad_norm": 24740.056640625,
      "learning_rate": 3.2017543859649124e-05,
      "loss": 48.0223,
      "step": 13775
    },
    {
      "epoch": 13.79,
      "grad_norm": 17711.673828125,
      "learning_rate": 3.201238390092879e-05,
      "loss": 46.7703,
      "step": 13776
    },
    {
      "epoch": 13.79,
      "grad_norm": 25138.611328125,
      "learning_rate": 3.2007223942208466e-05,
      "loss": 41.4892,
      "step": 13777
    },
    {
      "epoch": 13.79,
      "grad_norm": 7889.67041015625,
      "learning_rate": 3.2002063983488134e-05,
      "loss": 31.4596,
      "step": 13778
    },
    {
      "epoch": 13.79,
      "grad_norm": 49155.35546875,
      "learning_rate": 3.19969040247678e-05,
      "loss": 48.1432,
      "step": 13779
    },
    {
      "epoch": 13.79,
      "grad_norm": 6940.17529296875,
      "learning_rate": 3.1991744066047476e-05,
      "loss": 36.9534,
      "step": 13780
    },
    {
      "epoch": 13.79,
      "grad_norm": 7457.76171875,
      "learning_rate": 3.1986584107327144e-05,
      "loss": 33.5157,
      "step": 13781
    },
    {
      "epoch": 13.8,
      "grad_norm": 5303.400390625,
      "learning_rate": 3.198142414860681e-05,
      "loss": 45.5883,
      "step": 13782
    },
    {
      "epoch": 13.8,
      "grad_norm": 15099.2734375,
      "learning_rate": 3.1976264189886486e-05,
      "loss": 49.9926,
      "step": 13783
    },
    {
      "epoch": 13.8,
      "grad_norm": 2705.742431640625,
      "learning_rate": 3.1971104231166154e-05,
      "loss": 43.7144,
      "step": 13784
    },
    {
      "epoch": 13.8,
      "grad_norm": 4632.287109375,
      "learning_rate": 3.196594427244582e-05,
      "loss": 51.3141,
      "step": 13785
    },
    {
      "epoch": 13.8,
      "grad_norm": 18735.76171875,
      "learning_rate": 3.196078431372549e-05,
      "loss": 37.8348,
      "step": 13786
    },
    {
      "epoch": 13.8,
      "grad_norm": 18866.287109375,
      "learning_rate": 3.195562435500516e-05,
      "loss": 51.2193,
      "step": 13787
    },
    {
      "epoch": 13.8,
      "grad_norm": 5139.734375,
      "learning_rate": 3.195046439628483e-05,
      "loss": 50.3088,
      "step": 13788
    },
    {
      "epoch": 13.8,
      "grad_norm": 6038.8544921875,
      "learning_rate": 3.19453044375645e-05,
      "loss": 43.2418,
      "step": 13789
    },
    {
      "epoch": 13.8,
      "grad_norm": 5383.326171875,
      "learning_rate": 3.194014447884417e-05,
      "loss": 56.9642,
      "step": 13790
    },
    {
      "epoch": 13.8,
      "grad_norm": 7708.66845703125,
      "learning_rate": 3.193498452012384e-05,
      "loss": 57.6788,
      "step": 13791
    },
    {
      "epoch": 13.81,
      "grad_norm": 119221.734375,
      "learning_rate": 3.192982456140351e-05,
      "loss": 53.8238,
      "step": 13792
    },
    {
      "epoch": 13.81,
      "grad_norm": 36483.90234375,
      "learning_rate": 3.192466460268318e-05,
      "loss": 50.1616,
      "step": 13793
    },
    {
      "epoch": 13.81,
      "grad_norm": 8471.9248046875,
      "learning_rate": 3.191950464396285e-05,
      "loss": 45.632,
      "step": 13794
    },
    {
      "epoch": 13.81,
      "grad_norm": 2414.861572265625,
      "learning_rate": 3.191434468524252e-05,
      "loss": 50.5151,
      "step": 13795
    },
    {
      "epoch": 13.81,
      "grad_norm": 37974.3203125,
      "learning_rate": 3.190918472652219e-05,
      "loss": 49.0473,
      "step": 13796
    },
    {
      "epoch": 13.81,
      "grad_norm": 8341.005859375,
      "learning_rate": 3.190402476780186e-05,
      "loss": 54.399,
      "step": 13797
    },
    {
      "epoch": 13.81,
      "grad_norm": 8955.75390625,
      "learning_rate": 3.189886480908153e-05,
      "loss": 41.4195,
      "step": 13798
    },
    {
      "epoch": 13.81,
      "grad_norm": 17017.57421875,
      "learning_rate": 3.18937048503612e-05,
      "loss": 49.5521,
      "step": 13799
    },
    {
      "epoch": 13.81,
      "grad_norm": 67114.6953125,
      "learning_rate": 3.188854489164087e-05,
      "loss": 45.2236,
      "step": 13800
    },
    {
      "epoch": 13.81,
      "grad_norm": 21798.349609375,
      "learning_rate": 3.188338493292054e-05,
      "loss": 34.868,
      "step": 13801
    },
    {
      "epoch": 13.82,
      "grad_norm": 26514.427734375,
      "learning_rate": 3.187822497420021e-05,
      "loss": 37.1147,
      "step": 13802
    },
    {
      "epoch": 13.82,
      "grad_norm": 9871.02734375,
      "learning_rate": 3.187306501547988e-05,
      "loss": 54.2179,
      "step": 13803
    },
    {
      "epoch": 13.82,
      "grad_norm": 30362.984375,
      "learning_rate": 3.186790505675954e-05,
      "loss": 49.9713,
      "step": 13804
    },
    {
      "epoch": 13.82,
      "grad_norm": 18788.6015625,
      "learning_rate": 3.186274509803922e-05,
      "loss": 40.0395,
      "step": 13805
    },
    {
      "epoch": 13.82,
      "grad_norm": 2741.732666015625,
      "learning_rate": 3.1857585139318885e-05,
      "loss": 37.0274,
      "step": 13806
    },
    {
      "epoch": 13.82,
      "grad_norm": 35986.80078125,
      "learning_rate": 3.185242518059855e-05,
      "loss": 49.8145,
      "step": 13807
    },
    {
      "epoch": 13.82,
      "grad_norm": 43566.3828125,
      "learning_rate": 3.184726522187823e-05,
      "loss": 44.3125,
      "step": 13808
    },
    {
      "epoch": 13.82,
      "grad_norm": 13176.9892578125,
      "learning_rate": 3.1842105263157895e-05,
      "loss": 42.5201,
      "step": 13809
    },
    {
      "epoch": 13.82,
      "grad_norm": 11039.185546875,
      "learning_rate": 3.183694530443756e-05,
      "loss": 42.5929,
      "step": 13810
    },
    {
      "epoch": 13.82,
      "grad_norm": 5810.32275390625,
      "learning_rate": 3.183178534571724e-05,
      "loss": 49.4366,
      "step": 13811
    },
    {
      "epoch": 13.83,
      "grad_norm": 15584.041015625,
      "learning_rate": 3.1826625386996905e-05,
      "loss": 18.232,
      "step": 13812
    },
    {
      "epoch": 13.83,
      "grad_norm": 9962.8642578125,
      "learning_rate": 3.182146542827657e-05,
      "loss": 37.5001,
      "step": 13813
    },
    {
      "epoch": 13.83,
      "grad_norm": 30467.59765625,
      "learning_rate": 3.181630546955625e-05,
      "loss": 44.3445,
      "step": 13814
    },
    {
      "epoch": 13.83,
      "grad_norm": 28883.5234375,
      "learning_rate": 3.1811145510835915e-05,
      "loss": 47.6959,
      "step": 13815
    },
    {
      "epoch": 13.83,
      "grad_norm": 42890.203125,
      "learning_rate": 3.180598555211558e-05,
      "loss": 24.0011,
      "step": 13816
    },
    {
      "epoch": 13.83,
      "grad_norm": 18124.658203125,
      "learning_rate": 3.180082559339526e-05,
      "loss": 42.9125,
      "step": 13817
    },
    {
      "epoch": 13.83,
      "grad_norm": 5769.42138671875,
      "learning_rate": 3.1795665634674925e-05,
      "loss": 54.2195,
      "step": 13818
    },
    {
      "epoch": 13.83,
      "grad_norm": 17676.630859375,
      "learning_rate": 3.179050567595459e-05,
      "loss": 46.0605,
      "step": 13819
    },
    {
      "epoch": 13.83,
      "grad_norm": 8168.505859375,
      "learning_rate": 3.178534571723427e-05,
      "loss": 39.5685,
      "step": 13820
    },
    {
      "epoch": 13.83,
      "grad_norm": 7188.45947265625,
      "learning_rate": 3.178018575851393e-05,
      "loss": 37.2758,
      "step": 13821
    },
    {
      "epoch": 13.84,
      "grad_norm": 16222.3466796875,
      "learning_rate": 3.17750257997936e-05,
      "loss": 36.285,
      "step": 13822
    },
    {
      "epoch": 13.84,
      "grad_norm": 16783.09765625,
      "learning_rate": 3.176986584107327e-05,
      "loss": 45.8943,
      "step": 13823
    },
    {
      "epoch": 13.84,
      "grad_norm": 10456.611328125,
      "learning_rate": 3.176470588235294e-05,
      "loss": 41.2916,
      "step": 13824
    },
    {
      "epoch": 13.84,
      "grad_norm": 21920.708984375,
      "learning_rate": 3.175954592363261e-05,
      "loss": 48.4028,
      "step": 13825
    },
    {
      "epoch": 13.84,
      "grad_norm": 45331.390625,
      "learning_rate": 3.175438596491228e-05,
      "loss": 51.7291,
      "step": 13826
    },
    {
      "epoch": 13.84,
      "grad_norm": 9294.1875,
      "learning_rate": 3.174922600619195e-05,
      "loss": 37.5394,
      "step": 13827
    },
    {
      "epoch": 13.84,
      "grad_norm": 40725.15234375,
      "learning_rate": 3.174406604747162e-05,
      "loss": 31.635,
      "step": 13828
    },
    {
      "epoch": 13.84,
      "grad_norm": 269982.65625,
      "learning_rate": 3.173890608875129e-05,
      "loss": 36.5137,
      "step": 13829
    },
    {
      "epoch": 13.84,
      "grad_norm": 46271.3515625,
      "learning_rate": 3.173374613003096e-05,
      "loss": 35.4148,
      "step": 13830
    },
    {
      "epoch": 13.84,
      "grad_norm": 27930.408203125,
      "learning_rate": 3.172858617131063e-05,
      "loss": 41.0619,
      "step": 13831
    },
    {
      "epoch": 13.85,
      "grad_norm": 9708.3662109375,
      "learning_rate": 3.17234262125903e-05,
      "loss": 56.6944,
      "step": 13832
    },
    {
      "epoch": 13.85,
      "grad_norm": 28378.578125,
      "learning_rate": 3.171826625386997e-05,
      "loss": 49.2733,
      "step": 13833
    },
    {
      "epoch": 13.85,
      "grad_norm": 4961.22021484375,
      "learning_rate": 3.171310629514964e-05,
      "loss": 22.5685,
      "step": 13834
    },
    {
      "epoch": 13.85,
      "grad_norm": 10004.978515625,
      "learning_rate": 3.170794633642931e-05,
      "loss": 58.8781,
      "step": 13835
    },
    {
      "epoch": 13.85,
      "grad_norm": 48229.12109375,
      "learning_rate": 3.170278637770898e-05,
      "loss": 48.7348,
      "step": 13836
    },
    {
      "epoch": 13.85,
      "grad_norm": 17035.08203125,
      "learning_rate": 3.169762641898865e-05,
      "loss": 45.7805,
      "step": 13837
    },
    {
      "epoch": 13.85,
      "grad_norm": 38781.95703125,
      "learning_rate": 3.169246646026832e-05,
      "loss": 42.8908,
      "step": 13838
    },
    {
      "epoch": 13.85,
      "grad_norm": 7760.29833984375,
      "learning_rate": 3.168730650154799e-05,
      "loss": 46.8036,
      "step": 13839
    },
    {
      "epoch": 13.85,
      "grad_norm": 17836.05078125,
      "learning_rate": 3.1682146542827656e-05,
      "loss": 42.0875,
      "step": 13840
    },
    {
      "epoch": 13.85,
      "grad_norm": 4903.72314453125,
      "learning_rate": 3.1676986584107324e-05,
      "loss": 55.9414,
      "step": 13841
    },
    {
      "epoch": 13.86,
      "grad_norm": 3320.218994140625,
      "learning_rate": 3.1671826625387e-05,
      "loss": 54.65,
      "step": 13842
    },
    {
      "epoch": 13.86,
      "grad_norm": 22416.369140625,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 36.8142,
      "step": 13843
    },
    {
      "epoch": 13.86,
      "grad_norm": 38835.9921875,
      "learning_rate": 3.1661506707946334e-05,
      "loss": 41.2051,
      "step": 13844
    },
    {
      "epoch": 13.86,
      "grad_norm": 11703.583984375,
      "learning_rate": 3.165634674922601e-05,
      "loss": 42.4982,
      "step": 13845
    },
    {
      "epoch": 13.86,
      "grad_norm": 10163.232421875,
      "learning_rate": 3.1651186790505676e-05,
      "loss": 39.881,
      "step": 13846
    },
    {
      "epoch": 13.86,
      "grad_norm": 2372.0283203125,
      "learning_rate": 3.1646026831785344e-05,
      "loss": 48.739,
      "step": 13847
    },
    {
      "epoch": 13.86,
      "grad_norm": 6113.94189453125,
      "learning_rate": 3.164086687306502e-05,
      "loss": 58.6286,
      "step": 13848
    },
    {
      "epoch": 13.86,
      "grad_norm": 17000.974609375,
      "learning_rate": 3.1635706914344686e-05,
      "loss": 36.289,
      "step": 13849
    },
    {
      "epoch": 13.86,
      "grad_norm": 102835.984375,
      "learning_rate": 3.1630546955624354e-05,
      "loss": 31.141,
      "step": 13850
    },
    {
      "epoch": 13.86,
      "grad_norm": 21301.5,
      "learning_rate": 3.162538699690403e-05,
      "loss": 24.6586,
      "step": 13851
    },
    {
      "epoch": 13.87,
      "grad_norm": 16685.546875,
      "learning_rate": 3.1620227038183696e-05,
      "loss": 39.305,
      "step": 13852
    },
    {
      "epoch": 13.87,
      "grad_norm": 12684.546875,
      "learning_rate": 3.161506707946337e-05,
      "loss": 31.029,
      "step": 13853
    },
    {
      "epoch": 13.87,
      "grad_norm": 132763.875,
      "learning_rate": 3.160990712074304e-05,
      "loss": 37.4167,
      "step": 13854
    },
    {
      "epoch": 13.87,
      "grad_norm": 42861.0703125,
      "learning_rate": 3.1604747162022706e-05,
      "loss": 30.9355,
      "step": 13855
    },
    {
      "epoch": 13.87,
      "grad_norm": 9803.6865234375,
      "learning_rate": 3.159958720330238e-05,
      "loss": 44.2022,
      "step": 13856
    },
    {
      "epoch": 13.87,
      "grad_norm": 6457.25732421875,
      "learning_rate": 3.159442724458204e-05,
      "loss": 53.9548,
      "step": 13857
    },
    {
      "epoch": 13.87,
      "grad_norm": 13516.8291015625,
      "learning_rate": 3.158926728586171e-05,
      "loss": 24.882,
      "step": 13858
    },
    {
      "epoch": 13.87,
      "grad_norm": 6915.18310546875,
      "learning_rate": 3.1584107327141384e-05,
      "loss": 22.5995,
      "step": 13859
    },
    {
      "epoch": 13.87,
      "grad_norm": 5943.1376953125,
      "learning_rate": 3.157894736842105e-05,
      "loss": 46.4325,
      "step": 13860
    },
    {
      "epoch": 13.87,
      "grad_norm": 6236.2080078125,
      "learning_rate": 3.157378740970072e-05,
      "loss": 47.1033,
      "step": 13861
    },
    {
      "epoch": 13.88,
      "grad_norm": 7168.8134765625,
      "learning_rate": 3.1568627450980394e-05,
      "loss": 50.5435,
      "step": 13862
    },
    {
      "epoch": 13.88,
      "grad_norm": 4639.884765625,
      "learning_rate": 3.156346749226006e-05,
      "loss": 46.3486,
      "step": 13863
    },
    {
      "epoch": 13.88,
      "grad_norm": 5136.294921875,
      "learning_rate": 3.155830753353973e-05,
      "loss": 44.0752,
      "step": 13864
    },
    {
      "epoch": 13.88,
      "grad_norm": 56964.98828125,
      "learning_rate": 3.1553147574819404e-05,
      "loss": 20.9998,
      "step": 13865
    },
    {
      "epoch": 13.88,
      "grad_norm": 4510.2099609375,
      "learning_rate": 3.154798761609907e-05,
      "loss": 51.9833,
      "step": 13866
    },
    {
      "epoch": 13.88,
      "grad_norm": 11937.943359375,
      "learning_rate": 3.1542827657378746e-05,
      "loss": 47.3001,
      "step": 13867
    },
    {
      "epoch": 13.88,
      "grad_norm": 3140.908203125,
      "learning_rate": 3.1537667698658414e-05,
      "loss": 38.6821,
      "step": 13868
    },
    {
      "epoch": 13.88,
      "grad_norm": 26305.5625,
      "learning_rate": 3.153250773993808e-05,
      "loss": 41.9842,
      "step": 13869
    },
    {
      "epoch": 13.88,
      "grad_norm": 16384.201171875,
      "learning_rate": 3.1527347781217756e-05,
      "loss": 25.3912,
      "step": 13870
    },
    {
      "epoch": 13.88,
      "grad_norm": 8891.2392578125,
      "learning_rate": 3.1522187822497424e-05,
      "loss": 49.6012,
      "step": 13871
    },
    {
      "epoch": 13.89,
      "grad_norm": 15238.78515625,
      "learning_rate": 3.151702786377709e-05,
      "loss": 51.3965,
      "step": 13872
    },
    {
      "epoch": 13.89,
      "grad_norm": 7995.1201171875,
      "learning_rate": 3.1511867905056766e-05,
      "loss": 45.5856,
      "step": 13873
    },
    {
      "epoch": 13.89,
      "grad_norm": 10138.115234375,
      "learning_rate": 3.1506707946336434e-05,
      "loss": 34.4744,
      "step": 13874
    },
    {
      "epoch": 13.89,
      "grad_norm": 95306.8984375,
      "learning_rate": 3.1501547987616095e-05,
      "loss": 49.1035,
      "step": 13875
    },
    {
      "epoch": 13.89,
      "grad_norm": 4463.5556640625,
      "learning_rate": 3.149638802889577e-05,
      "loss": 40.8284,
      "step": 13876
    },
    {
      "epoch": 13.89,
      "grad_norm": 6560.3291015625,
      "learning_rate": 3.149122807017544e-05,
      "loss": 45.3189,
      "step": 13877
    },
    {
      "epoch": 13.89,
      "grad_norm": 50994.85546875,
      "learning_rate": 3.1486068111455105e-05,
      "loss": 38.3789,
      "step": 13878
    },
    {
      "epoch": 13.89,
      "grad_norm": 326725.5625,
      "learning_rate": 3.148090815273478e-05,
      "loss": 38.7641,
      "step": 13879
    },
    {
      "epoch": 13.89,
      "grad_norm": 5140.23974609375,
      "learning_rate": 3.147574819401445e-05,
      "loss": 52.1502,
      "step": 13880
    },
    {
      "epoch": 13.89,
      "grad_norm": 8776.591796875,
      "learning_rate": 3.147058823529412e-05,
      "loss": 47.9382,
      "step": 13881
    },
    {
      "epoch": 13.9,
      "grad_norm": 20116.244140625,
      "learning_rate": 3.146542827657379e-05,
      "loss": 16.6337,
      "step": 13882
    },
    {
      "epoch": 13.9,
      "grad_norm": 4970.1259765625,
      "learning_rate": 3.146026831785346e-05,
      "loss": 48.4134,
      "step": 13883
    },
    {
      "epoch": 13.9,
      "grad_norm": 26510.775390625,
      "learning_rate": 3.145510835913313e-05,
      "loss": 23.3035,
      "step": 13884
    },
    {
      "epoch": 13.9,
      "grad_norm": 11399.615234375,
      "learning_rate": 3.14499484004128e-05,
      "loss": 40.884,
      "step": 13885
    },
    {
      "epoch": 13.9,
      "grad_norm": 20635.90234375,
      "learning_rate": 3.144478844169247e-05,
      "loss": 50.3944,
      "step": 13886
    },
    {
      "epoch": 13.9,
      "grad_norm": 3378.951904296875,
      "learning_rate": 3.143962848297214e-05,
      "loss": 52.0431,
      "step": 13887
    },
    {
      "epoch": 13.9,
      "grad_norm": 44169.2734375,
      "learning_rate": 3.143446852425181e-05,
      "loss": 18.2563,
      "step": 13888
    },
    {
      "epoch": 13.9,
      "grad_norm": 5772.73095703125,
      "learning_rate": 3.142930856553148e-05,
      "loss": 45.2627,
      "step": 13889
    },
    {
      "epoch": 13.9,
      "grad_norm": 7055.97265625,
      "learning_rate": 3.142414860681115e-05,
      "loss": 42.2802,
      "step": 13890
    },
    {
      "epoch": 13.9,
      "grad_norm": 8571.4970703125,
      "learning_rate": 3.141898864809082e-05,
      "loss": 33.0292,
      "step": 13891
    },
    {
      "epoch": 13.91,
      "grad_norm": 10599.1025390625,
      "learning_rate": 3.141382868937049e-05,
      "loss": 48.243,
      "step": 13892
    },
    {
      "epoch": 13.91,
      "grad_norm": 3903.732421875,
      "learning_rate": 3.1408668730650155e-05,
      "loss": 48.9531,
      "step": 13893
    },
    {
      "epoch": 13.91,
      "grad_norm": 9334.728515625,
      "learning_rate": 3.140350877192982e-05,
      "loss": 47.8827,
      "step": 13894
    },
    {
      "epoch": 13.91,
      "grad_norm": 13667.8203125,
      "learning_rate": 3.13983488132095e-05,
      "loss": 51.434,
      "step": 13895
    },
    {
      "epoch": 13.91,
      "grad_norm": 21560.537109375,
      "learning_rate": 3.1393188854489165e-05,
      "loss": 55.6503,
      "step": 13896
    },
    {
      "epoch": 13.91,
      "grad_norm": 11390.64453125,
      "learning_rate": 3.138802889576883e-05,
      "loss": 29.7742,
      "step": 13897
    },
    {
      "epoch": 13.91,
      "grad_norm": 24665.626953125,
      "learning_rate": 3.138286893704851e-05,
      "loss": 47.1237,
      "step": 13898
    },
    {
      "epoch": 13.91,
      "grad_norm": 5829.99365234375,
      "learning_rate": 3.1377708978328175e-05,
      "loss": 50.6928,
      "step": 13899
    },
    {
      "epoch": 13.91,
      "grad_norm": 3792.31689453125,
      "learning_rate": 3.137254901960784e-05,
      "loss": 43.929,
      "step": 13900
    },
    {
      "epoch": 13.91,
      "grad_norm": 23143.140625,
      "learning_rate": 3.136738906088752e-05,
      "loss": 40.6851,
      "step": 13901
    },
    {
      "epoch": 13.92,
      "grad_norm": 22360.955078125,
      "learning_rate": 3.1362229102167185e-05,
      "loss": 47.5111,
      "step": 13902
    },
    {
      "epoch": 13.92,
      "grad_norm": 43995.03515625,
      "learning_rate": 3.135706914344685e-05,
      "loss": 42.9492,
      "step": 13903
    },
    {
      "epoch": 13.92,
      "grad_norm": 7484.27490234375,
      "learning_rate": 3.135190918472653e-05,
      "loss": 48.8384,
      "step": 13904
    },
    {
      "epoch": 13.92,
      "grad_norm": 9750.373046875,
      "learning_rate": 3.1346749226006195e-05,
      "loss": 37.4076,
      "step": 13905
    },
    {
      "epoch": 13.92,
      "grad_norm": 16378.2919921875,
      "learning_rate": 3.134158926728586e-05,
      "loss": 34.0581,
      "step": 13906
    },
    {
      "epoch": 13.92,
      "grad_norm": 71369.3515625,
      "learning_rate": 3.133642930856554e-05,
      "loss": 29.1462,
      "step": 13907
    },
    {
      "epoch": 13.92,
      "grad_norm": 6059.65234375,
      "learning_rate": 3.1331269349845205e-05,
      "loss": 34.023,
      "step": 13908
    },
    {
      "epoch": 13.92,
      "grad_norm": 2058.831298828125,
      "learning_rate": 3.132610939112487e-05,
      "loss": 28.1313,
      "step": 13909
    },
    {
      "epoch": 13.92,
      "grad_norm": 16003.3017578125,
      "learning_rate": 3.132094943240455e-05,
      "loss": 29.7076,
      "step": 13910
    },
    {
      "epoch": 13.92,
      "grad_norm": 13938.92578125,
      "learning_rate": 3.131578947368421e-05,
      "loss": 41.9012,
      "step": 13911
    },
    {
      "epoch": 13.93,
      "grad_norm": 30295.9296875,
      "learning_rate": 3.131062951496388e-05,
      "loss": 25.6065,
      "step": 13912
    },
    {
      "epoch": 13.93,
      "grad_norm": 43881.69140625,
      "learning_rate": 3.130546955624355e-05,
      "loss": 40.9386,
      "step": 13913
    },
    {
      "epoch": 13.93,
      "grad_norm": 454945.40625,
      "learning_rate": 3.130030959752322e-05,
      "loss": 46.8322,
      "step": 13914
    },
    {
      "epoch": 13.93,
      "grad_norm": 25389.642578125,
      "learning_rate": 3.129514963880289e-05,
      "loss": 53.612,
      "step": 13915
    },
    {
      "epoch": 13.93,
      "grad_norm": 56983.4609375,
      "learning_rate": 3.128998968008256e-05,
      "loss": 42.0414,
      "step": 13916
    },
    {
      "epoch": 13.93,
      "grad_norm": 33522.296875,
      "learning_rate": 3.128482972136223e-05,
      "loss": 51.0906,
      "step": 13917
    },
    {
      "epoch": 13.93,
      "grad_norm": 11110.2744140625,
      "learning_rate": 3.12796697626419e-05,
      "loss": 38.3256,
      "step": 13918
    },
    {
      "epoch": 13.93,
      "grad_norm": 11740.2421875,
      "learning_rate": 3.127450980392157e-05,
      "loss": 27.3325,
      "step": 13919
    },
    {
      "epoch": 13.93,
      "grad_norm": 4136.24609375,
      "learning_rate": 3.126934984520124e-05,
      "loss": 47.9456,
      "step": 13920
    },
    {
      "epoch": 13.93,
      "grad_norm": 11265.5048828125,
      "learning_rate": 3.126418988648091e-05,
      "loss": 41.3746,
      "step": 13921
    },
    {
      "epoch": 13.94,
      "grad_norm": 3406.119384765625,
      "learning_rate": 3.125902992776058e-05,
      "loss": 45.0816,
      "step": 13922
    },
    {
      "epoch": 13.94,
      "grad_norm": 13519.8095703125,
      "learning_rate": 3.125386996904025e-05,
      "loss": 46.451,
      "step": 13923
    },
    {
      "epoch": 13.94,
      "grad_norm": 37636.03125,
      "learning_rate": 3.124871001031992e-05,
      "loss": 43.7812,
      "step": 13924
    },
    {
      "epoch": 13.94,
      "grad_norm": 16388.9375,
      "learning_rate": 3.124355005159959e-05,
      "loss": 40.5511,
      "step": 13925
    },
    {
      "epoch": 13.94,
      "grad_norm": 41599.875,
      "learning_rate": 3.123839009287926e-05,
      "loss": 54.1513,
      "step": 13926
    },
    {
      "epoch": 13.94,
      "grad_norm": 53357.32421875,
      "learning_rate": 3.123323013415893e-05,
      "loss": 41.4864,
      "step": 13927
    },
    {
      "epoch": 13.94,
      "grad_norm": 24682.755859375,
      "learning_rate": 3.12280701754386e-05,
      "loss": 49.004,
      "step": 13928
    },
    {
      "epoch": 13.94,
      "grad_norm": 7915.2294921875,
      "learning_rate": 3.122291021671827e-05,
      "loss": 44.3151,
      "step": 13929
    },
    {
      "epoch": 13.94,
      "grad_norm": 24822.306640625,
      "learning_rate": 3.1217750257997936e-05,
      "loss": 18.8466,
      "step": 13930
    },
    {
      "epoch": 13.94,
      "grad_norm": 49980.54296875,
      "learning_rate": 3.1212590299277604e-05,
      "loss": 49.1686,
      "step": 13931
    },
    {
      "epoch": 13.95,
      "grad_norm": 30404.662109375,
      "learning_rate": 3.120743034055728e-05,
      "loss": 43.3459,
      "step": 13932
    },
    {
      "epoch": 13.95,
      "grad_norm": 15410.1572265625,
      "learning_rate": 3.1202270381836946e-05,
      "loss": 45.0254,
      "step": 13933
    },
    {
      "epoch": 13.95,
      "grad_norm": 26574.21484375,
      "learning_rate": 3.1197110423116614e-05,
      "loss": 54.2525,
      "step": 13934
    },
    {
      "epoch": 13.95,
      "grad_norm": 21025.97265625,
      "learning_rate": 3.119195046439629e-05,
      "loss": 19.2201,
      "step": 13935
    },
    {
      "epoch": 13.95,
      "grad_norm": 6091.71826171875,
      "learning_rate": 3.1186790505675956e-05,
      "loss": 52.3856,
      "step": 13936
    },
    {
      "epoch": 13.95,
      "grad_norm": 1790.9686279296875,
      "learning_rate": 3.1181630546955624e-05,
      "loss": 51.8091,
      "step": 13937
    },
    {
      "epoch": 13.95,
      "grad_norm": 328063.46875,
      "learning_rate": 3.11764705882353e-05,
      "loss": 39.5799,
      "step": 13938
    },
    {
      "epoch": 13.95,
      "grad_norm": 72916.6640625,
      "learning_rate": 3.1171310629514966e-05,
      "loss": 31.2971,
      "step": 13939
    },
    {
      "epoch": 13.95,
      "grad_norm": 37924.5390625,
      "learning_rate": 3.1166150670794634e-05,
      "loss": 37.558,
      "step": 13940
    },
    {
      "epoch": 13.95,
      "grad_norm": 9480.501953125,
      "learning_rate": 3.116099071207431e-05,
      "loss": 51.6914,
      "step": 13941
    },
    {
      "epoch": 13.96,
      "grad_norm": 9747.78125,
      "learning_rate": 3.1155830753353976e-05,
      "loss": 38.4195,
      "step": 13942
    },
    {
      "epoch": 13.96,
      "grad_norm": 35906.44921875,
      "learning_rate": 3.1150670794633644e-05,
      "loss": 32.6381,
      "step": 13943
    },
    {
      "epoch": 13.96,
      "grad_norm": 18204.7890625,
      "learning_rate": 3.114551083591332e-05,
      "loss": 51.9024,
      "step": 13944
    },
    {
      "epoch": 13.96,
      "grad_norm": 39850.859375,
      "learning_rate": 3.1140350877192986e-05,
      "loss": 37.444,
      "step": 13945
    },
    {
      "epoch": 13.96,
      "grad_norm": 7217.18798828125,
      "learning_rate": 3.1135190918472654e-05,
      "loss": 48.4552,
      "step": 13946
    },
    {
      "epoch": 13.96,
      "grad_norm": 24723.380859375,
      "learning_rate": 3.113003095975232e-05,
      "loss": 54.3298,
      "step": 13947
    },
    {
      "epoch": 13.96,
      "grad_norm": 20292.701171875,
      "learning_rate": 3.112487100103199e-05,
      "loss": 51.894,
      "step": 13948
    },
    {
      "epoch": 13.96,
      "grad_norm": 2527.531005859375,
      "learning_rate": 3.1119711042311664e-05,
      "loss": 37.6604,
      "step": 13949
    },
    {
      "epoch": 13.96,
      "grad_norm": 34739.75390625,
      "learning_rate": 3.111455108359133e-05,
      "loss": 44.0654,
      "step": 13950
    },
    {
      "epoch": 13.96,
      "grad_norm": 22340.599609375,
      "learning_rate": 3.1109391124871e-05,
      "loss": 48.0451,
      "step": 13951
    },
    {
      "epoch": 13.97,
      "grad_norm": 4976.39697265625,
      "learning_rate": 3.1104231166150674e-05,
      "loss": 28.6221,
      "step": 13952
    },
    {
      "epoch": 13.97,
      "grad_norm": 6044.44140625,
      "learning_rate": 3.109907120743034e-05,
      "loss": 51.3683,
      "step": 13953
    },
    {
      "epoch": 13.97,
      "grad_norm": 49439.65234375,
      "learning_rate": 3.109391124871001e-05,
      "loss": 32.5577,
      "step": 13954
    },
    {
      "epoch": 13.97,
      "grad_norm": 8808.4697265625,
      "learning_rate": 3.1088751289989684e-05,
      "loss": 47.5286,
      "step": 13955
    },
    {
      "epoch": 13.97,
      "grad_norm": 69580.609375,
      "learning_rate": 3.108359133126935e-05,
      "loss": 46.3001,
      "step": 13956
    },
    {
      "epoch": 13.97,
      "grad_norm": 25785.271484375,
      "learning_rate": 3.107843137254902e-05,
      "loss": 44.9871,
      "step": 13957
    },
    {
      "epoch": 13.97,
      "grad_norm": 22634.193359375,
      "learning_rate": 3.1073271413828694e-05,
      "loss": 51.6337,
      "step": 13958
    },
    {
      "epoch": 13.97,
      "grad_norm": 11674.3681640625,
      "learning_rate": 3.106811145510836e-05,
      "loss": 40.2714,
      "step": 13959
    },
    {
      "epoch": 13.97,
      "grad_norm": 16014.150390625,
      "learning_rate": 3.106295149638803e-05,
      "loss": 35.0908,
      "step": 13960
    },
    {
      "epoch": 13.97,
      "grad_norm": 20096.193359375,
      "learning_rate": 3.1057791537667704e-05,
      "loss": 45.2886,
      "step": 13961
    },
    {
      "epoch": 13.98,
      "grad_norm": 965775.9375,
      "learning_rate": 3.105263157894737e-05,
      "loss": 41.7999,
      "step": 13962
    },
    {
      "epoch": 13.98,
      "grad_norm": 9162.86328125,
      "learning_rate": 3.104747162022704e-05,
      "loss": 49.9642,
      "step": 13963
    },
    {
      "epoch": 13.98,
      "grad_norm": 69139.515625,
      "learning_rate": 3.104231166150671e-05,
      "loss": 45.4616,
      "step": 13964
    },
    {
      "epoch": 13.98,
      "grad_norm": 8776.2919921875,
      "learning_rate": 3.1037151702786375e-05,
      "loss": 50.58,
      "step": 13965
    },
    {
      "epoch": 13.98,
      "grad_norm": 51463.9375,
      "learning_rate": 3.103199174406605e-05,
      "loss": 45.6409,
      "step": 13966
    },
    {
      "epoch": 13.98,
      "grad_norm": 12804.150390625,
      "learning_rate": 3.102683178534572e-05,
      "loss": 39.521,
      "step": 13967
    },
    {
      "epoch": 13.98,
      "grad_norm": 55121.0546875,
      "learning_rate": 3.1021671826625385e-05,
      "loss": 37.4488,
      "step": 13968
    },
    {
      "epoch": 13.98,
      "grad_norm": 2272.953369140625,
      "learning_rate": 3.101651186790506e-05,
      "loss": 35.9683,
      "step": 13969
    },
    {
      "epoch": 13.98,
      "grad_norm": 10022.2705078125,
      "learning_rate": 3.101135190918473e-05,
      "loss": 46.4847,
      "step": 13970
    },
    {
      "epoch": 13.98,
      "grad_norm": 22465.65625,
      "learning_rate": 3.1006191950464395e-05,
      "loss": 22.871,
      "step": 13971
    },
    {
      "epoch": 13.99,
      "grad_norm": 23244.66015625,
      "learning_rate": 3.100103199174407e-05,
      "loss": 44.7098,
      "step": 13972
    },
    {
      "epoch": 13.99,
      "grad_norm": 18304.123046875,
      "learning_rate": 3.099587203302374e-05,
      "loss": 46.6037,
      "step": 13973
    },
    {
      "epoch": 13.99,
      "grad_norm": 39971.83984375,
      "learning_rate": 3.0990712074303405e-05,
      "loss": 50.5748,
      "step": 13974
    },
    {
      "epoch": 13.99,
      "grad_norm": 10894.626953125,
      "learning_rate": 3.098555211558308e-05,
      "loss": 44.0158,
      "step": 13975
    },
    {
      "epoch": 13.99,
      "grad_norm": 6069.5927734375,
      "learning_rate": 3.098039215686275e-05,
      "loss": 41.8436,
      "step": 13976
    },
    {
      "epoch": 13.99,
      "grad_norm": 7671.36376953125,
      "learning_rate": 3.0975232198142415e-05,
      "loss": 38.3316,
      "step": 13977
    },
    {
      "epoch": 13.99,
      "grad_norm": 55436.28515625,
      "learning_rate": 3.097007223942209e-05,
      "loss": 36.8254,
      "step": 13978
    },
    {
      "epoch": 13.99,
      "grad_norm": 52042.6640625,
      "learning_rate": 3.096491228070176e-05,
      "loss": 48.4272,
      "step": 13979
    },
    {
      "epoch": 13.99,
      "grad_norm": 178406.0,
      "learning_rate": 3.0959752321981425e-05,
      "loss": 30.7221,
      "step": 13980
    },
    {
      "epoch": 13.99,
      "grad_norm": 6077.45751953125,
      "learning_rate": 3.09545923632611e-05,
      "loss": 39.8133,
      "step": 13981
    },
    {
      "epoch": 14.0,
      "grad_norm": 21223.359375,
      "learning_rate": 3.094943240454076e-05,
      "loss": 39.9068,
      "step": 13982
    },
    {
      "epoch": 14.0,
      "grad_norm": 147573.546875,
      "learning_rate": 3.0944272445820435e-05,
      "loss": 34.4196,
      "step": 13983
    },
    {
      "epoch": 14.0,
      "grad_norm": 9590.9384765625,
      "learning_rate": 3.09391124871001e-05,
      "loss": 35.1192,
      "step": 13984
    },
    {
      "epoch": 14.0,
      "grad_norm": 3332.68994140625,
      "learning_rate": 3.093395252837977e-05,
      "loss": 39.0655,
      "step": 13985
    },
    {
      "epoch": 14.0,
      "grad_norm": 206626.796875,
      "learning_rate": 3.0928792569659445e-05,
      "loss": 52.1579,
      "step": 13986
    },
    {
      "epoch": 14.0,
      "grad_norm": 34942.95703125,
      "learning_rate": 3.092363261093911e-05,
      "loss": 42.3031,
      "step": 13987
    },
    {
      "epoch": 14.0,
      "grad_norm": 20883.734375,
      "learning_rate": 3.091847265221878e-05,
      "loss": 47.9284,
      "step": 13988
    },
    {
      "epoch": 14.0,
      "grad_norm": 23342.39453125,
      "learning_rate": 3.0913312693498455e-05,
      "loss": 33.7262,
      "step": 13989
    },
    {
      "epoch": 14.0,
      "grad_norm": 11405.580078125,
      "learning_rate": 3.090815273477812e-05,
      "loss": 39.4667,
      "step": 13990
    },
    {
      "epoch": 14.01,
      "grad_norm": 36401.56640625,
      "learning_rate": 3.090299277605779e-05,
      "loss": 40.3774,
      "step": 13991
    },
    {
      "epoch": 14.01,
      "grad_norm": 7146.59326171875,
      "learning_rate": 3.0897832817337465e-05,
      "loss": 41.8463,
      "step": 13992
    },
    {
      "epoch": 14.01,
      "grad_norm": 9774.046875,
      "learning_rate": 3.089267285861713e-05,
      "loss": 53.2137,
      "step": 13993
    },
    {
      "epoch": 14.01,
      "grad_norm": 165985.25,
      "learning_rate": 3.08875128998968e-05,
      "loss": 41.0537,
      "step": 13994
    },
    {
      "epoch": 14.01,
      "grad_norm": 17833.04296875,
      "learning_rate": 3.0882352941176475e-05,
      "loss": 46.554,
      "step": 13995
    },
    {
      "epoch": 14.01,
      "grad_norm": 2931.1005859375,
      "learning_rate": 3.087719298245614e-05,
      "loss": 49.4852,
      "step": 13996
    },
    {
      "epoch": 14.01,
      "grad_norm": 12818.5712890625,
      "learning_rate": 3.087203302373581e-05,
      "loss": 36.6301,
      "step": 13997
    },
    {
      "epoch": 14.01,
      "grad_norm": 13962.564453125,
      "learning_rate": 3.0866873065015485e-05,
      "loss": 50.1483,
      "step": 13998
    },
    {
      "epoch": 14.01,
      "grad_norm": 5928.26123046875,
      "learning_rate": 3.086171310629515e-05,
      "loss": 49.5641,
      "step": 13999
    },
    {
      "epoch": 14.01,
      "grad_norm": 9977.8173828125,
      "learning_rate": 3.085655314757482e-05,
      "loss": 41.3632,
      "step": 14000
    },
    {
      "epoch": 14.02,
      "grad_norm": 26230.16015625,
      "learning_rate": 3.085139318885449e-05,
      "loss": 49.3332,
      "step": 14001
    },
    {
      "epoch": 14.02,
      "grad_norm": 6422.55908203125,
      "learning_rate": 3.0846233230134156e-05,
      "loss": 46.8803,
      "step": 14002
    },
    {
      "epoch": 14.02,
      "grad_norm": 7241.75341796875,
      "learning_rate": 3.084107327141383e-05,
      "loss": 36.9714,
      "step": 14003
    },
    {
      "epoch": 14.02,
      "grad_norm": 57278.1796875,
      "learning_rate": 3.08359133126935e-05,
      "loss": 44.8047,
      "step": 14004
    },
    {
      "epoch": 14.02,
      "grad_norm": 22984.896484375,
      "learning_rate": 3.0830753353973166e-05,
      "loss": 35.1223,
      "step": 14005
    },
    {
      "epoch": 14.02,
      "grad_norm": 11666.1435546875,
      "learning_rate": 3.082559339525284e-05,
      "loss": 47.8727,
      "step": 14006
    },
    {
      "epoch": 14.02,
      "grad_norm": 5485.8203125,
      "learning_rate": 3.082043343653251e-05,
      "loss": 39.3811,
      "step": 14007
    },
    {
      "epoch": 14.02,
      "grad_norm": 9253.6396484375,
      "learning_rate": 3.0815273477812176e-05,
      "loss": 51.1858,
      "step": 14008
    },
    {
      "epoch": 14.02,
      "grad_norm": 27684.251953125,
      "learning_rate": 3.081011351909185e-05,
      "loss": 36.9557,
      "step": 14009
    },
    {
      "epoch": 14.02,
      "grad_norm": 18830.73828125,
      "learning_rate": 3.080495356037152e-05,
      "loss": 37.8956,
      "step": 14010
    },
    {
      "epoch": 14.03,
      "grad_norm": 28671.7109375,
      "learning_rate": 3.0799793601651186e-05,
      "loss": 45.4658,
      "step": 14011
    },
    {
      "epoch": 14.03,
      "grad_norm": 14319.166015625,
      "learning_rate": 3.079463364293086e-05,
      "loss": 61.7505,
      "step": 14012
    },
    {
      "epoch": 14.03,
      "grad_norm": 8390.6982421875,
      "learning_rate": 3.078947368421053e-05,
      "loss": 42.2284,
      "step": 14013
    },
    {
      "epoch": 14.03,
      "grad_norm": 7395.44091796875,
      "learning_rate": 3.07843137254902e-05,
      "loss": 42.5248,
      "step": 14014
    },
    {
      "epoch": 14.03,
      "grad_norm": 782787.25,
      "learning_rate": 3.077915376676987e-05,
      "loss": 30.1893,
      "step": 14015
    },
    {
      "epoch": 14.03,
      "grad_norm": 52160.08203125,
      "learning_rate": 3.077399380804954e-05,
      "loss": 52.5204,
      "step": 14016
    },
    {
      "epoch": 14.03,
      "grad_norm": 18538.1484375,
      "learning_rate": 3.076883384932921e-05,
      "loss": 33.7911,
      "step": 14017
    },
    {
      "epoch": 14.03,
      "grad_norm": 42590.17578125,
      "learning_rate": 3.076367389060887e-05,
      "loss": 30.4512,
      "step": 14018
    },
    {
      "epoch": 14.03,
      "grad_norm": 20025.462890625,
      "learning_rate": 3.075851393188854e-05,
      "loss": 45.6455,
      "step": 14019
    },
    {
      "epoch": 14.03,
      "grad_norm": 19716.6640625,
      "learning_rate": 3.0753353973168216e-05,
      "loss": 25.4742,
      "step": 14020
    },
    {
      "epoch": 14.04,
      "grad_norm": 14031.7978515625,
      "learning_rate": 3.074819401444788e-05,
      "loss": 51.228,
      "step": 14021
    },
    {
      "epoch": 14.04,
      "grad_norm": 15923.591796875,
      "learning_rate": 3.074303405572755e-05,
      "loss": 28.954,
      "step": 14022
    },
    {
      "epoch": 14.04,
      "grad_norm": 32216.33984375,
      "learning_rate": 3.0737874097007226e-05,
      "loss": 25.6724,
      "step": 14023
    },
    {
      "epoch": 14.04,
      "grad_norm": 50542.75390625,
      "learning_rate": 3.0732714138286893e-05,
      "loss": 54.972,
      "step": 14024
    },
    {
      "epoch": 14.04,
      "grad_norm": 7714.2421875,
      "learning_rate": 3.072755417956656e-05,
      "loss": 46.4854,
      "step": 14025
    },
    {
      "epoch": 14.04,
      "grad_norm": 24657.06640625,
      "learning_rate": 3.0722394220846236e-05,
      "loss": 40.7896,
      "step": 14026
    },
    {
      "epoch": 14.04,
      "grad_norm": 22659.279296875,
      "learning_rate": 3.0717234262125903e-05,
      "loss": 31.5288,
      "step": 14027
    },
    {
      "epoch": 14.04,
      "grad_norm": 5470.97900390625,
      "learning_rate": 3.071207430340558e-05,
      "loss": 35.9108,
      "step": 14028
    },
    {
      "epoch": 14.04,
      "grad_norm": 10468.2919921875,
      "learning_rate": 3.0706914344685246e-05,
      "loss": 50.7374,
      "step": 14029
    },
    {
      "epoch": 14.04,
      "grad_norm": 32111.51953125,
      "learning_rate": 3.0701754385964913e-05,
      "loss": 52.8512,
      "step": 14030
    },
    {
      "epoch": 14.05,
      "grad_norm": 45686.98046875,
      "learning_rate": 3.069659442724459e-05,
      "loss": 33.8918,
      "step": 14031
    },
    {
      "epoch": 14.05,
      "grad_norm": 37162.8984375,
      "learning_rate": 3.0691434468524256e-05,
      "loss": 55.369,
      "step": 14032
    },
    {
      "epoch": 14.05,
      "grad_norm": 14566.349609375,
      "learning_rate": 3.0686274509803923e-05,
      "loss": 40.8123,
      "step": 14033
    },
    {
      "epoch": 14.05,
      "grad_norm": 40146.33984375,
      "learning_rate": 3.06811145510836e-05,
      "loss": 54.5573,
      "step": 14034
    },
    {
      "epoch": 14.05,
      "grad_norm": 74949.6953125,
      "learning_rate": 3.0675954592363266e-05,
      "loss": 51.7044,
      "step": 14035
    },
    {
      "epoch": 14.05,
      "grad_norm": 39010.6484375,
      "learning_rate": 3.067079463364293e-05,
      "loss": 38.5631,
      "step": 14036
    },
    {
      "epoch": 14.05,
      "grad_norm": 64446.0625,
      "learning_rate": 3.06656346749226e-05,
      "loss": 55.797,
      "step": 14037
    },
    {
      "epoch": 14.05,
      "grad_norm": 57703.37890625,
      "learning_rate": 3.066047471620227e-05,
      "loss": 40.0972,
      "step": 14038
    },
    {
      "epoch": 14.05,
      "grad_norm": 3952.913818359375,
      "learning_rate": 3.065531475748194e-05,
      "loss": 51.5227,
      "step": 14039
    },
    {
      "epoch": 14.05,
      "grad_norm": 228639.359375,
      "learning_rate": 3.065015479876161e-05,
      "loss": 41.4439,
      "step": 14040
    },
    {
      "epoch": 14.06,
      "grad_norm": 31350.494140625,
      "learning_rate": 3.064499484004128e-05,
      "loss": 44.3239,
      "step": 14041
    },
    {
      "epoch": 14.06,
      "grad_norm": 7591.2392578125,
      "learning_rate": 3.0639834881320953e-05,
      "loss": 51.22,
      "step": 14042
    },
    {
      "epoch": 14.06,
      "grad_norm": 19691.953125,
      "learning_rate": 3.063467492260062e-05,
      "loss": 27.9767,
      "step": 14043
    },
    {
      "epoch": 14.06,
      "grad_norm": 33606.38671875,
      "learning_rate": 3.062951496388029e-05,
      "loss": 49.84,
      "step": 14044
    },
    {
      "epoch": 14.06,
      "grad_norm": 7017.58544921875,
      "learning_rate": 3.0624355005159963e-05,
      "loss": 47.791,
      "step": 14045
    },
    {
      "epoch": 14.06,
      "grad_norm": 17108.826171875,
      "learning_rate": 3.061919504643963e-05,
      "loss": 39.688,
      "step": 14046
    },
    {
      "epoch": 14.06,
      "grad_norm": 7751.240234375,
      "learning_rate": 3.06140350877193e-05,
      "loss": 31.9457,
      "step": 14047
    },
    {
      "epoch": 14.06,
      "grad_norm": 7705.64697265625,
      "learning_rate": 3.0608875128998973e-05,
      "loss": 38.0203,
      "step": 14048
    },
    {
      "epoch": 14.06,
      "grad_norm": 42760.57421875,
      "learning_rate": 3.060371517027864e-05,
      "loss": 42.8947,
      "step": 14049
    },
    {
      "epoch": 14.06,
      "grad_norm": 8501.3876953125,
      "learning_rate": 3.059855521155831e-05,
      "loss": 45.9586,
      "step": 14050
    },
    {
      "epoch": 14.07,
      "grad_norm": 8871.0263671875,
      "learning_rate": 3.0593395252837983e-05,
      "loss": 56.4706,
      "step": 14051
    },
    {
      "epoch": 14.07,
      "grad_norm": 41359.96875,
      "learning_rate": 3.058823529411765e-05,
      "loss": 45.1682,
      "step": 14052
    },
    {
      "epoch": 14.07,
      "grad_norm": 11980.32421875,
      "learning_rate": 3.058307533539732e-05,
      "loss": 48.5817,
      "step": 14053
    },
    {
      "epoch": 14.07,
      "grad_norm": 22469.96875,
      "learning_rate": 3.057791537667699e-05,
      "loss": 41.2631,
      "step": 14054
    },
    {
      "epoch": 14.07,
      "grad_norm": 4932.24169921875,
      "learning_rate": 3.0572755417956654e-05,
      "loss": 41.7508,
      "step": 14055
    },
    {
      "epoch": 14.07,
      "grad_norm": 31962.173828125,
      "learning_rate": 3.056759545923633e-05,
      "loss": 32.3901,
      "step": 14056
    },
    {
      "epoch": 14.07,
      "grad_norm": 23692.79296875,
      "learning_rate": 3.0562435500516e-05,
      "loss": 25.3283,
      "step": 14057
    },
    {
      "epoch": 14.07,
      "grad_norm": 7797.228515625,
      "learning_rate": 3.0557275541795664e-05,
      "loss": 29.4281,
      "step": 14058
    },
    {
      "epoch": 14.07,
      "grad_norm": 3593.552978515625,
      "learning_rate": 3.055211558307534e-05,
      "loss": 53.8527,
      "step": 14059
    },
    {
      "epoch": 14.07,
      "grad_norm": 12232.2373046875,
      "learning_rate": 3.054695562435501e-05,
      "loss": 50.6705,
      "step": 14060
    },
    {
      "epoch": 14.08,
      "grad_norm": 19153.8828125,
      "learning_rate": 3.0541795665634674e-05,
      "loss": 46.8287,
      "step": 14061
    },
    {
      "epoch": 14.08,
      "grad_norm": 13180.3974609375,
      "learning_rate": 3.053663570691435e-05,
      "loss": 52.4075,
      "step": 14062
    },
    {
      "epoch": 14.08,
      "grad_norm": 8724.1943359375,
      "learning_rate": 3.053147574819402e-05,
      "loss": 28.024,
      "step": 14063
    },
    {
      "epoch": 14.08,
      "grad_norm": 3795.712890625,
      "learning_rate": 3.0526315789473684e-05,
      "loss": 58.6858,
      "step": 14064
    },
    {
      "epoch": 14.08,
      "grad_norm": 11691.08203125,
      "learning_rate": 3.052115583075336e-05,
      "loss": 37.2154,
      "step": 14065
    },
    {
      "epoch": 14.08,
      "grad_norm": 36276.50390625,
      "learning_rate": 3.0515995872033027e-05,
      "loss": 49.4157,
      "step": 14066
    },
    {
      "epoch": 14.08,
      "grad_norm": 18483.982421875,
      "learning_rate": 3.0510835913312698e-05,
      "loss": 38.5723,
      "step": 14067
    },
    {
      "epoch": 14.08,
      "grad_norm": 2806.430419921875,
      "learning_rate": 3.0505675954592366e-05,
      "loss": 57.4359,
      "step": 14068
    },
    {
      "epoch": 14.08,
      "grad_norm": 14195.373046875,
      "learning_rate": 3.0500515995872037e-05,
      "loss": 41.5507,
      "step": 14069
    },
    {
      "epoch": 14.08,
      "grad_norm": 5340.029296875,
      "learning_rate": 3.0495356037151708e-05,
      "loss": 34.3436,
      "step": 14070
    },
    {
      "epoch": 14.09,
      "grad_norm": 15235.0859375,
      "learning_rate": 3.0490196078431376e-05,
      "loss": 47.0515,
      "step": 14071
    },
    {
      "epoch": 14.09,
      "grad_norm": 17478.912109375,
      "learning_rate": 3.048503611971104e-05,
      "loss": 51.737,
      "step": 14072
    },
    {
      "epoch": 14.09,
      "grad_norm": 10673.5615234375,
      "learning_rate": 3.047987616099071e-05,
      "loss": 43.7151,
      "step": 14073
    },
    {
      "epoch": 14.09,
      "grad_norm": 9990.712890625,
      "learning_rate": 3.0474716202270382e-05,
      "loss": 30.1731,
      "step": 14074
    },
    {
      "epoch": 14.09,
      "grad_norm": 17134.2578125,
      "learning_rate": 3.046955624355005e-05,
      "loss": 53.7065,
      "step": 14075
    },
    {
      "epoch": 14.09,
      "grad_norm": 17953.22265625,
      "learning_rate": 3.046439628482972e-05,
      "loss": 45.8126,
      "step": 14076
    },
    {
      "epoch": 14.09,
      "grad_norm": 51682.77734375,
      "learning_rate": 3.0459236326109392e-05,
      "loss": 35.4378,
      "step": 14077
    },
    {
      "epoch": 14.09,
      "grad_norm": 1984.9478759765625,
      "learning_rate": 3.0454076367389063e-05,
      "loss": 50.2093,
      "step": 14078
    },
    {
      "epoch": 14.09,
      "grad_norm": 24598.349609375,
      "learning_rate": 3.044891640866873e-05,
      "loss": 48.9294,
      "step": 14079
    },
    {
      "epoch": 14.09,
      "grad_norm": 32591.771484375,
      "learning_rate": 3.0443756449948402e-05,
      "loss": 47.5697,
      "step": 14080
    },
    {
      "epoch": 14.1,
      "grad_norm": 18169.98046875,
      "learning_rate": 3.0438596491228073e-05,
      "loss": 49.0851,
      "step": 14081
    },
    {
      "epoch": 14.1,
      "grad_norm": 325348.09375,
      "learning_rate": 3.043343653250774e-05,
      "loss": 56.3112,
      "step": 14082
    },
    {
      "epoch": 14.1,
      "grad_norm": 45515.953125,
      "learning_rate": 3.0428276573787412e-05,
      "loss": 56.0031,
      "step": 14083
    },
    {
      "epoch": 14.1,
      "grad_norm": 33404.50390625,
      "learning_rate": 3.0423116615067083e-05,
      "loss": 49.6726,
      "step": 14084
    },
    {
      "epoch": 14.1,
      "grad_norm": 30214.00390625,
      "learning_rate": 3.041795665634675e-05,
      "loss": 47.49,
      "step": 14085
    },
    {
      "epoch": 14.1,
      "grad_norm": 20085.962890625,
      "learning_rate": 3.0412796697626422e-05,
      "loss": 35.6465,
      "step": 14086
    },
    {
      "epoch": 14.1,
      "grad_norm": 170802.265625,
      "learning_rate": 3.0407636738906093e-05,
      "loss": 49.3594,
      "step": 14087
    },
    {
      "epoch": 14.1,
      "grad_norm": 28517.712890625,
      "learning_rate": 3.040247678018576e-05,
      "loss": 42.4903,
      "step": 14088
    },
    {
      "epoch": 14.1,
      "grad_norm": 7518.95263671875,
      "learning_rate": 3.0397316821465432e-05,
      "loss": 44.2881,
      "step": 14089
    },
    {
      "epoch": 14.1,
      "grad_norm": 9665.09375,
      "learning_rate": 3.0392156862745097e-05,
      "loss": 41.0908,
      "step": 14090
    },
    {
      "epoch": 14.11,
      "grad_norm": 13772.0009765625,
      "learning_rate": 3.0386996904024768e-05,
      "loss": 52.3755,
      "step": 14091
    },
    {
      "epoch": 14.11,
      "grad_norm": 10079.24609375,
      "learning_rate": 3.038183694530444e-05,
      "loss": 48.025,
      "step": 14092
    },
    {
      "epoch": 14.11,
      "grad_norm": 8093.54931640625,
      "learning_rate": 3.0376676986584107e-05,
      "loss": 45.9746,
      "step": 14093
    },
    {
      "epoch": 14.11,
      "grad_norm": 22457.46484375,
      "learning_rate": 3.0371517027863778e-05,
      "loss": 52.0135,
      "step": 14094
    },
    {
      "epoch": 14.11,
      "grad_norm": 23856.47265625,
      "learning_rate": 3.036635706914345e-05,
      "loss": 39.3334,
      "step": 14095
    },
    {
      "epoch": 14.11,
      "grad_norm": 1777.0693359375,
      "learning_rate": 3.0361197110423117e-05,
      "loss": 40.9706,
      "step": 14096
    },
    {
      "epoch": 14.11,
      "grad_norm": 7202.46240234375,
      "learning_rate": 3.0356037151702788e-05,
      "loss": 43.0507,
      "step": 14097
    },
    {
      "epoch": 14.11,
      "grad_norm": 23743.90625,
      "learning_rate": 3.035087719298246e-05,
      "loss": 49.0006,
      "step": 14098
    },
    {
      "epoch": 14.11,
      "grad_norm": 10550.9619140625,
      "learning_rate": 3.0345717234262127e-05,
      "loss": 37.8968,
      "step": 14099
    },
    {
      "epoch": 14.11,
      "grad_norm": 16845.234375,
      "learning_rate": 3.0340557275541798e-05,
      "loss": 41.0979,
      "step": 14100
    },
    {
      "epoch": 14.12,
      "grad_norm": 6048.83154296875,
      "learning_rate": 3.033539731682147e-05,
      "loss": 38.926,
      "step": 14101
    },
    {
      "epoch": 14.12,
      "grad_norm": 41443.265625,
      "learning_rate": 3.0330237358101137e-05,
      "loss": 30.4686,
      "step": 14102
    },
    {
      "epoch": 14.12,
      "grad_norm": 13704.9833984375,
      "learning_rate": 3.0325077399380808e-05,
      "loss": 49.7096,
      "step": 14103
    },
    {
      "epoch": 14.12,
      "grad_norm": 16235.849609375,
      "learning_rate": 3.031991744066048e-05,
      "loss": 24.3358,
      "step": 14104
    },
    {
      "epoch": 14.12,
      "grad_norm": 25067.689453125,
      "learning_rate": 3.0314757481940147e-05,
      "loss": 24.4429,
      "step": 14105
    },
    {
      "epoch": 14.12,
      "grad_norm": 22042.98046875,
      "learning_rate": 3.0309597523219818e-05,
      "loss": 57.6868,
      "step": 14106
    },
    {
      "epoch": 14.12,
      "grad_norm": 6195.31787109375,
      "learning_rate": 3.030443756449949e-05,
      "loss": 46.9238,
      "step": 14107
    },
    {
      "epoch": 14.12,
      "grad_norm": 49216.15234375,
      "learning_rate": 3.0299277605779153e-05,
      "loss": 41.5307,
      "step": 14108
    },
    {
      "epoch": 14.12,
      "grad_norm": 13106.7138671875,
      "learning_rate": 3.0294117647058824e-05,
      "loss": 44.5439,
      "step": 14109
    },
    {
      "epoch": 14.12,
      "grad_norm": 12552.14453125,
      "learning_rate": 3.0288957688338492e-05,
      "loss": 54.6566,
      "step": 14110
    },
    {
      "epoch": 14.13,
      "grad_norm": 3298.667724609375,
      "learning_rate": 3.0283797729618163e-05,
      "loss": 40.6344,
      "step": 14111
    },
    {
      "epoch": 14.13,
      "grad_norm": 1631.6669921875,
      "learning_rate": 3.0278637770897834e-05,
      "loss": 50.0158,
      "step": 14112
    },
    {
      "epoch": 14.13,
      "grad_norm": 8489.4345703125,
      "learning_rate": 3.0273477812177502e-05,
      "loss": 56.7178,
      "step": 14113
    },
    {
      "epoch": 14.13,
      "grad_norm": 34495.40625,
      "learning_rate": 3.0268317853457173e-05,
      "loss": 40.6912,
      "step": 14114
    },
    {
      "epoch": 14.13,
      "grad_norm": 14654.548828125,
      "learning_rate": 3.0263157894736844e-05,
      "loss": 46.368,
      "step": 14115
    },
    {
      "epoch": 14.13,
      "grad_norm": 18852.59375,
      "learning_rate": 3.0257997936016512e-05,
      "loss": 49.9032,
      "step": 14116
    },
    {
      "epoch": 14.13,
      "grad_norm": 8109.01220703125,
      "learning_rate": 3.0252837977296183e-05,
      "loss": 53.1007,
      "step": 14117
    },
    {
      "epoch": 14.13,
      "grad_norm": 12699.7236328125,
      "learning_rate": 3.0247678018575854e-05,
      "loss": 35.2924,
      "step": 14118
    },
    {
      "epoch": 14.13,
      "grad_norm": 4637.3212890625,
      "learning_rate": 3.0242518059855522e-05,
      "loss": 43.8652,
      "step": 14119
    },
    {
      "epoch": 14.13,
      "grad_norm": 15213.708984375,
      "learning_rate": 3.0237358101135193e-05,
      "loss": 49.2605,
      "step": 14120
    },
    {
      "epoch": 14.14,
      "grad_norm": 19681.75,
      "learning_rate": 3.0232198142414864e-05,
      "loss": 38.5708,
      "step": 14121
    },
    {
      "epoch": 14.14,
      "grad_norm": 2871.228515625,
      "learning_rate": 3.0227038183694535e-05,
      "loss": 58.8147,
      "step": 14122
    },
    {
      "epoch": 14.14,
      "grad_norm": 6046.4228515625,
      "learning_rate": 3.0221878224974203e-05,
      "loss": 44.6621,
      "step": 14123
    },
    {
      "epoch": 14.14,
      "grad_norm": 12105.25,
      "learning_rate": 3.0216718266253874e-05,
      "loss": 45.3943,
      "step": 14124
    },
    {
      "epoch": 14.14,
      "grad_norm": 59238.20703125,
      "learning_rate": 3.021155830753354e-05,
      "loss": 44.7745,
      "step": 14125
    },
    {
      "epoch": 14.14,
      "grad_norm": 2063.497802734375,
      "learning_rate": 3.020639834881321e-05,
      "loss": 46.0057,
      "step": 14126
    },
    {
      "epoch": 14.14,
      "grad_norm": 1499.40576171875,
      "learning_rate": 3.0201238390092878e-05,
      "loss": 45.4935,
      "step": 14127
    },
    {
      "epoch": 14.14,
      "grad_norm": 35594.11328125,
      "learning_rate": 3.019607843137255e-05,
      "loss": 41.479,
      "step": 14128
    },
    {
      "epoch": 14.14,
      "grad_norm": 4058.51806640625,
      "learning_rate": 3.019091847265222e-05,
      "loss": 47.4855,
      "step": 14129
    },
    {
      "epoch": 14.14,
      "grad_norm": 11703.1875,
      "learning_rate": 3.0185758513931888e-05,
      "loss": 37.9958,
      "step": 14130
    },
    {
      "epoch": 14.15,
      "grad_norm": 30800.109375,
      "learning_rate": 3.018059855521156e-05,
      "loss": 37.6973,
      "step": 14131
    },
    {
      "epoch": 14.15,
      "grad_norm": 18137.951171875,
      "learning_rate": 3.017543859649123e-05,
      "loss": 47.1101,
      "step": 14132
    },
    {
      "epoch": 14.15,
      "grad_norm": 20426.513671875,
      "learning_rate": 3.0170278637770898e-05,
      "loss": 44.4446,
      "step": 14133
    },
    {
      "epoch": 14.15,
      "grad_norm": 9550.455078125,
      "learning_rate": 3.016511867905057e-05,
      "loss": 41.8886,
      "step": 14134
    },
    {
      "epoch": 14.15,
      "grad_norm": 29764.865234375,
      "learning_rate": 3.015995872033024e-05,
      "loss": 38.2059,
      "step": 14135
    },
    {
      "epoch": 14.15,
      "grad_norm": 93547.2578125,
      "learning_rate": 3.015479876160991e-05,
      "loss": 45.8472,
      "step": 14136
    },
    {
      "epoch": 14.15,
      "grad_norm": 3408.81689453125,
      "learning_rate": 3.014963880288958e-05,
      "loss": 54.0195,
      "step": 14137
    },
    {
      "epoch": 14.15,
      "grad_norm": 30137.52734375,
      "learning_rate": 3.014447884416925e-05,
      "loss": 34.3599,
      "step": 14138
    },
    {
      "epoch": 14.15,
      "grad_norm": 32824.484375,
      "learning_rate": 3.013931888544892e-05,
      "loss": 57.581,
      "step": 14139
    },
    {
      "epoch": 14.15,
      "grad_norm": 15128.6650390625,
      "learning_rate": 3.013415892672859e-05,
      "loss": 37.1171,
      "step": 14140
    },
    {
      "epoch": 14.16,
      "grad_norm": 18460.234375,
      "learning_rate": 3.012899896800826e-05,
      "loss": 42.3663,
      "step": 14141
    },
    {
      "epoch": 14.16,
      "grad_norm": 2720.670654296875,
      "learning_rate": 3.012383900928793e-05,
      "loss": 45.396,
      "step": 14142
    },
    {
      "epoch": 14.16,
      "grad_norm": 17101.345703125,
      "learning_rate": 3.0118679050567595e-05,
      "loss": 50.2241,
      "step": 14143
    },
    {
      "epoch": 14.16,
      "grad_norm": 76591.6953125,
      "learning_rate": 3.0113519091847263e-05,
      "loss": 50.009,
      "step": 14144
    },
    {
      "epoch": 14.16,
      "grad_norm": 15980.0849609375,
      "learning_rate": 3.0108359133126934e-05,
      "loss": 49.3993,
      "step": 14145
    },
    {
      "epoch": 14.16,
      "grad_norm": 25138.837890625,
      "learning_rate": 3.0103199174406605e-05,
      "loss": 31.2579,
      "step": 14146
    },
    {
      "epoch": 14.16,
      "grad_norm": 15426.5458984375,
      "learning_rate": 3.0098039215686273e-05,
      "loss": 39.986,
      "step": 14147
    },
    {
      "epoch": 14.16,
      "grad_norm": 21381.328125,
      "learning_rate": 3.0092879256965944e-05,
      "loss": 34.7796,
      "step": 14148
    },
    {
      "epoch": 14.16,
      "grad_norm": 7625.9443359375,
      "learning_rate": 3.0087719298245615e-05,
      "loss": 57.7836,
      "step": 14149
    },
    {
      "epoch": 14.16,
      "grad_norm": 6994.77880859375,
      "learning_rate": 3.0082559339525286e-05,
      "loss": 32.2691,
      "step": 14150
    },
    {
      "epoch": 14.17,
      "grad_norm": 1968.8267822265625,
      "learning_rate": 3.0077399380804954e-05,
      "loss": 49.1264,
      "step": 14151
    },
    {
      "epoch": 14.17,
      "grad_norm": 15277.796875,
      "learning_rate": 3.0072239422084625e-05,
      "loss": 54.3238,
      "step": 14152
    },
    {
      "epoch": 14.17,
      "grad_norm": 33146.49609375,
      "learning_rate": 3.0067079463364296e-05,
      "loss": 48.0217,
      "step": 14153
    },
    {
      "epoch": 14.17,
      "grad_norm": 16911.673828125,
      "learning_rate": 3.0061919504643964e-05,
      "loss": 38.6137,
      "step": 14154
    },
    {
      "epoch": 14.17,
      "grad_norm": 12620.361328125,
      "learning_rate": 3.0056759545923635e-05,
      "loss": 35.1437,
      "step": 14155
    },
    {
      "epoch": 14.17,
      "grad_norm": 21642.376953125,
      "learning_rate": 3.0051599587203306e-05,
      "loss": 30.9321,
      "step": 14156
    },
    {
      "epoch": 14.17,
      "grad_norm": 42591.6875,
      "learning_rate": 3.0046439628482974e-05,
      "loss": 58.2233,
      "step": 14157
    },
    {
      "epoch": 14.17,
      "grad_norm": 6138.5595703125,
      "learning_rate": 3.0041279669762645e-05,
      "loss": 40.5208,
      "step": 14158
    },
    {
      "epoch": 14.17,
      "grad_norm": 5611.6552734375,
      "learning_rate": 3.0036119711042316e-05,
      "loss": 50.2156,
      "step": 14159
    },
    {
      "epoch": 14.17,
      "grad_norm": 16924.4921875,
      "learning_rate": 3.0030959752321984e-05,
      "loss": 32.6044,
      "step": 14160
    },
    {
      "epoch": 14.18,
      "grad_norm": 19721.814453125,
      "learning_rate": 3.002579979360165e-05,
      "loss": 26.8894,
      "step": 14161
    },
    {
      "epoch": 14.18,
      "grad_norm": 7111.5830078125,
      "learning_rate": 3.002063983488132e-05,
      "loss": 44.0785,
      "step": 14162
    },
    {
      "epoch": 14.18,
      "grad_norm": 5306.9794921875,
      "learning_rate": 3.001547987616099e-05,
      "loss": 50.1152,
      "step": 14163
    },
    {
      "epoch": 14.18,
      "grad_norm": 8398.1513671875,
      "learning_rate": 3.001031991744066e-05,
      "loss": 41.5276,
      "step": 14164
    },
    {
      "epoch": 14.18,
      "grad_norm": 18231.001953125,
      "learning_rate": 3.000515995872033e-05,
      "loss": 40.9953,
      "step": 14165
    },
    {
      "epoch": 14.18,
      "grad_norm": 7429.6552734375,
      "learning_rate": 3e-05,
      "loss": 50.7214,
      "step": 14166
    },
    {
      "epoch": 14.18,
      "grad_norm": 61322.09765625,
      "learning_rate": 2.9994840041279672e-05,
      "loss": 21.8771,
      "step": 14167
    },
    {
      "epoch": 14.18,
      "grad_norm": 4640.388671875,
      "learning_rate": 2.998968008255934e-05,
      "loss": 45.5021,
      "step": 14168
    },
    {
      "epoch": 14.18,
      "grad_norm": 54335.59765625,
      "learning_rate": 2.998452012383901e-05,
      "loss": 44.9089,
      "step": 14169
    },
    {
      "epoch": 14.18,
      "grad_norm": 43186.40625,
      "learning_rate": 2.9979360165118682e-05,
      "loss": 43.7164,
      "step": 14170
    },
    {
      "epoch": 14.19,
      "grad_norm": 4104.92919921875,
      "learning_rate": 2.997420020639835e-05,
      "loss": 46.6877,
      "step": 14171
    },
    {
      "epoch": 14.19,
      "grad_norm": 7443.8818359375,
      "learning_rate": 2.996904024767802e-05,
      "loss": 51.4873,
      "step": 14172
    },
    {
      "epoch": 14.19,
      "grad_norm": 5552.123046875,
      "learning_rate": 2.9963880288957692e-05,
      "loss": 53.6741,
      "step": 14173
    },
    {
      "epoch": 14.19,
      "grad_norm": 9609.9189453125,
      "learning_rate": 2.995872033023736e-05,
      "loss": 27.1908,
      "step": 14174
    },
    {
      "epoch": 14.19,
      "grad_norm": 9702.173828125,
      "learning_rate": 2.995356037151703e-05,
      "loss": 38.0434,
      "step": 14175
    },
    {
      "epoch": 14.19,
      "grad_norm": 17605.810546875,
      "learning_rate": 2.9948400412796702e-05,
      "loss": 32.9978,
      "step": 14176
    },
    {
      "epoch": 14.19,
      "grad_norm": 24247.94140625,
      "learning_rate": 2.994324045407637e-05,
      "loss": 53.1547,
      "step": 14177
    },
    {
      "epoch": 14.19,
      "grad_norm": 17271.361328125,
      "learning_rate": 2.993808049535604e-05,
      "loss": 46.8424,
      "step": 14178
    },
    {
      "epoch": 14.19,
      "grad_norm": 30235.62109375,
      "learning_rate": 2.9932920536635705e-05,
      "loss": 42.9106,
      "step": 14179
    },
    {
      "epoch": 14.19,
      "grad_norm": 38890.25,
      "learning_rate": 2.9927760577915376e-05,
      "loss": 45.4692,
      "step": 14180
    },
    {
      "epoch": 14.2,
      "grad_norm": 6764.16552734375,
      "learning_rate": 2.9922600619195047e-05,
      "loss": 52.6547,
      "step": 14181
    },
    {
      "epoch": 14.2,
      "grad_norm": 89415.1015625,
      "learning_rate": 2.9917440660474715e-05,
      "loss": 42.363,
      "step": 14182
    },
    {
      "epoch": 14.2,
      "grad_norm": 24545.17578125,
      "learning_rate": 2.9912280701754386e-05,
      "loss": 43.2909,
      "step": 14183
    },
    {
      "epoch": 14.2,
      "grad_norm": 6837.138671875,
      "learning_rate": 2.9907120743034057e-05,
      "loss": 39.3449,
      "step": 14184
    },
    {
      "epoch": 14.2,
      "grad_norm": 38723.6328125,
      "learning_rate": 2.9901960784313725e-05,
      "loss": 43.7176,
      "step": 14185
    },
    {
      "epoch": 14.2,
      "grad_norm": 9324.8642578125,
      "learning_rate": 2.9896800825593396e-05,
      "loss": 38.0736,
      "step": 14186
    },
    {
      "epoch": 14.2,
      "grad_norm": 15629.1181640625,
      "learning_rate": 2.9891640866873067e-05,
      "loss": 43.754,
      "step": 14187
    },
    {
      "epoch": 14.2,
      "grad_norm": 12879.3759765625,
      "learning_rate": 2.9886480908152735e-05,
      "loss": 31.474,
      "step": 14188
    },
    {
      "epoch": 14.2,
      "grad_norm": 136300.421875,
      "learning_rate": 2.9881320949432406e-05,
      "loss": 31.4013,
      "step": 14189
    },
    {
      "epoch": 14.2,
      "grad_norm": 209967.890625,
      "learning_rate": 2.9876160990712077e-05,
      "loss": 58.1478,
      "step": 14190
    },
    {
      "epoch": 14.21,
      "grad_norm": 32703.58984375,
      "learning_rate": 2.9871001031991745e-05,
      "loss": 56.0098,
      "step": 14191
    },
    {
      "epoch": 14.21,
      "grad_norm": 3519.213623046875,
      "learning_rate": 2.9865841073271416e-05,
      "loss": 45.2124,
      "step": 14192
    },
    {
      "epoch": 14.21,
      "grad_norm": 9893.701171875,
      "learning_rate": 2.9860681114551087e-05,
      "loss": 31.8524,
      "step": 14193
    },
    {
      "epoch": 14.21,
      "grad_norm": 4849.3642578125,
      "learning_rate": 2.9855521155830755e-05,
      "loss": 42.0414,
      "step": 14194
    },
    {
      "epoch": 14.21,
      "grad_norm": 5669.99609375,
      "learning_rate": 2.9850361197110426e-05,
      "loss": 41.9458,
      "step": 14195
    },
    {
      "epoch": 14.21,
      "grad_norm": 33052.640625,
      "learning_rate": 2.9845201238390097e-05,
      "loss": 26.6797,
      "step": 14196
    },
    {
      "epoch": 14.21,
      "grad_norm": 11760.404296875,
      "learning_rate": 2.9840041279669762e-05,
      "loss": 50.3439,
      "step": 14197
    },
    {
      "epoch": 14.21,
      "grad_norm": 5012.68408203125,
      "learning_rate": 2.9834881320949433e-05,
      "loss": 47.7592,
      "step": 14198
    },
    {
      "epoch": 14.21,
      "grad_norm": 42150.28125,
      "learning_rate": 2.98297213622291e-05,
      "loss": 38.0663,
      "step": 14199
    },
    {
      "epoch": 14.21,
      "grad_norm": 104875.25,
      "learning_rate": 2.9824561403508772e-05,
      "loss": 32.7908,
      "step": 14200
    },
    {
      "epoch": 14.22,
      "grad_norm": 4921.67333984375,
      "learning_rate": 2.9819401444788443e-05,
      "loss": 45.8922,
      "step": 14201
    },
    {
      "epoch": 14.22,
      "grad_norm": 8347.3740234375,
      "learning_rate": 2.981424148606811e-05,
      "loss": 49.9974,
      "step": 14202
    },
    {
      "epoch": 14.22,
      "grad_norm": 4953.46142578125,
      "learning_rate": 2.9809081527347782e-05,
      "loss": 35.9273,
      "step": 14203
    },
    {
      "epoch": 14.22,
      "grad_norm": 145155.359375,
      "learning_rate": 2.9803921568627453e-05,
      "loss": 23.7309,
      "step": 14204
    },
    {
      "epoch": 14.22,
      "grad_norm": 73414.0625,
      "learning_rate": 2.979876160990712e-05,
      "loss": 21.7014,
      "step": 14205
    },
    {
      "epoch": 14.22,
      "grad_norm": 4469.92236328125,
      "learning_rate": 2.9793601651186792e-05,
      "loss": 40.8473,
      "step": 14206
    },
    {
      "epoch": 14.22,
      "grad_norm": 4823.26611328125,
      "learning_rate": 2.9788441692466463e-05,
      "loss": 33.6103,
      "step": 14207
    },
    {
      "epoch": 14.22,
      "grad_norm": 15750.30859375,
      "learning_rate": 2.978328173374613e-05,
      "loss": 43.8867,
      "step": 14208
    },
    {
      "epoch": 14.22,
      "grad_norm": 18751.494140625,
      "learning_rate": 2.9778121775025802e-05,
      "loss": 55.743,
      "step": 14209
    },
    {
      "epoch": 14.22,
      "grad_norm": 8644.08984375,
      "learning_rate": 2.9772961816305473e-05,
      "loss": 44.4555,
      "step": 14210
    },
    {
      "epoch": 14.23,
      "grad_norm": 14511.8017578125,
      "learning_rate": 2.9767801857585144e-05,
      "loss": 52.3197,
      "step": 14211
    },
    {
      "epoch": 14.23,
      "grad_norm": 12109.8310546875,
      "learning_rate": 2.9762641898864812e-05,
      "loss": 51.4732,
      "step": 14212
    },
    {
      "epoch": 14.23,
      "grad_norm": 14263.3291015625,
      "learning_rate": 2.9757481940144483e-05,
      "loss": 52.5106,
      "step": 14213
    },
    {
      "epoch": 14.23,
      "grad_norm": 11169.359375,
      "learning_rate": 2.9752321981424154e-05,
      "loss": 35.9107,
      "step": 14214
    },
    {
      "epoch": 14.23,
      "grad_norm": 17410.201171875,
      "learning_rate": 2.974716202270382e-05,
      "loss": 30.0412,
      "step": 14215
    },
    {
      "epoch": 14.23,
      "grad_norm": 45825.73828125,
      "learning_rate": 2.9742002063983486e-05,
      "loss": 41.8949,
      "step": 14216
    },
    {
      "epoch": 14.23,
      "grad_norm": 27979.189453125,
      "learning_rate": 2.9736842105263157e-05,
      "loss": 46.0411,
      "step": 14217
    },
    {
      "epoch": 14.23,
      "grad_norm": 26757.994140625,
      "learning_rate": 2.973168214654283e-05,
      "loss": 31.6933,
      "step": 14218
    },
    {
      "epoch": 14.23,
      "grad_norm": 56224.890625,
      "learning_rate": 2.9726522187822496e-05,
      "loss": 40.9537,
      "step": 14219
    },
    {
      "epoch": 14.23,
      "grad_norm": 372244.3125,
      "learning_rate": 2.9721362229102167e-05,
      "loss": 24.6731,
      "step": 14220
    },
    {
      "epoch": 14.24,
      "grad_norm": 10258.017578125,
      "learning_rate": 2.971620227038184e-05,
      "loss": 43.3897,
      "step": 14221
    },
    {
      "epoch": 14.24,
      "grad_norm": 5289.14453125,
      "learning_rate": 2.9711042311661506e-05,
      "loss": 42.7536,
      "step": 14222
    },
    {
      "epoch": 14.24,
      "grad_norm": 5561.02001953125,
      "learning_rate": 2.9705882352941177e-05,
      "loss": 46.9179,
      "step": 14223
    },
    {
      "epoch": 14.24,
      "grad_norm": 38839.6953125,
      "learning_rate": 2.970072239422085e-05,
      "loss": 49.9168,
      "step": 14224
    },
    {
      "epoch": 14.24,
      "grad_norm": 6869.64013671875,
      "learning_rate": 2.969556243550052e-05,
      "loss": 54.0593,
      "step": 14225
    },
    {
      "epoch": 14.24,
      "grad_norm": 103219.59375,
      "learning_rate": 2.9690402476780187e-05,
      "loss": 56.2675,
      "step": 14226
    },
    {
      "epoch": 14.24,
      "grad_norm": 13648.052734375,
      "learning_rate": 2.968524251805986e-05,
      "loss": 39.1164,
      "step": 14227
    },
    {
      "epoch": 14.24,
      "grad_norm": 9911.4833984375,
      "learning_rate": 2.968008255933953e-05,
      "loss": 30.9146,
      "step": 14228
    },
    {
      "epoch": 14.24,
      "grad_norm": 7069.6240234375,
      "learning_rate": 2.9674922600619197e-05,
      "loss": 53.8043,
      "step": 14229
    },
    {
      "epoch": 14.24,
      "grad_norm": 17392.7578125,
      "learning_rate": 2.966976264189887e-05,
      "loss": 40.4659,
      "step": 14230
    },
    {
      "epoch": 14.25,
      "grad_norm": 25234.9375,
      "learning_rate": 2.966460268317854e-05,
      "loss": 53.2135,
      "step": 14231
    },
    {
      "epoch": 14.25,
      "grad_norm": 1465.9205322265625,
      "learning_rate": 2.9659442724458207e-05,
      "loss": 42.3177,
      "step": 14232
    },
    {
      "epoch": 14.25,
      "grad_norm": 39938.6171875,
      "learning_rate": 2.965428276573787e-05,
      "loss": 32.9657,
      "step": 14233
    },
    {
      "epoch": 14.25,
      "grad_norm": 7026.92919921875,
      "learning_rate": 2.9649122807017543e-05,
      "loss": 54.0264,
      "step": 14234
    },
    {
      "epoch": 14.25,
      "grad_norm": 67958.8046875,
      "learning_rate": 2.9643962848297214e-05,
      "loss": 39.912,
      "step": 14235
    },
    {
      "epoch": 14.25,
      "grad_norm": 16729.662109375,
      "learning_rate": 2.963880288957688e-05,
      "loss": 49.4268,
      "step": 14236
    },
    {
      "epoch": 14.25,
      "grad_norm": 4951.2119140625,
      "learning_rate": 2.9633642930856553e-05,
      "loss": 43.103,
      "step": 14237
    },
    {
      "epoch": 14.25,
      "grad_norm": 495902.3125,
      "learning_rate": 2.9628482972136224e-05,
      "loss": 48.2865,
      "step": 14238
    },
    {
      "epoch": 14.25,
      "grad_norm": 11323.177734375,
      "learning_rate": 2.9623323013415895e-05,
      "loss": 49.6429,
      "step": 14239
    },
    {
      "epoch": 14.25,
      "grad_norm": 9972.7060546875,
      "learning_rate": 2.9618163054695563e-05,
      "loss": 51.3632,
      "step": 14240
    },
    {
      "epoch": 14.26,
      "grad_norm": 47846.8984375,
      "learning_rate": 2.9613003095975234e-05,
      "loss": 45.7392,
      "step": 14241
    },
    {
      "epoch": 14.26,
      "grad_norm": 14030.90234375,
      "learning_rate": 2.9607843137254905e-05,
      "loss": 48.5389,
      "step": 14242
    },
    {
      "epoch": 14.26,
      "grad_norm": 19404.458984375,
      "learning_rate": 2.9602683178534573e-05,
      "loss": 53.7276,
      "step": 14243
    },
    {
      "epoch": 14.26,
      "grad_norm": 11077.09765625,
      "learning_rate": 2.9597523219814244e-05,
      "loss": 30.7355,
      "step": 14244
    },
    {
      "epoch": 14.26,
      "grad_norm": 3084.534912109375,
      "learning_rate": 2.9592363261093915e-05,
      "loss": 55.1451,
      "step": 14245
    },
    {
      "epoch": 14.26,
      "grad_norm": 202869.921875,
      "learning_rate": 2.9587203302373583e-05,
      "loss": 50.2948,
      "step": 14246
    },
    {
      "epoch": 14.26,
      "grad_norm": 38995.7421875,
      "learning_rate": 2.9582043343653254e-05,
      "loss": 40.8399,
      "step": 14247
    },
    {
      "epoch": 14.26,
      "grad_norm": 19392.732421875,
      "learning_rate": 2.9576883384932925e-05,
      "loss": 49.4527,
      "step": 14248
    },
    {
      "epoch": 14.26,
      "grad_norm": 28697.740234375,
      "learning_rate": 2.9571723426212593e-05,
      "loss": 38.9137,
      "step": 14249
    },
    {
      "epoch": 14.26,
      "grad_norm": 59104.35546875,
      "learning_rate": 2.9566563467492264e-05,
      "loss": 35.7497,
      "step": 14250
    },
    {
      "epoch": 14.27,
      "grad_norm": 7833.1943359375,
      "learning_rate": 2.9561403508771928e-05,
      "loss": 52.8008,
      "step": 14251
    },
    {
      "epoch": 14.27,
      "grad_norm": 14444.3837890625,
      "learning_rate": 2.95562435500516e-05,
      "loss": 39.0813,
      "step": 14252
    },
    {
      "epoch": 14.27,
      "grad_norm": 25771.107421875,
      "learning_rate": 2.955108359133127e-05,
      "loss": 45.8411,
      "step": 14253
    },
    {
      "epoch": 14.27,
      "grad_norm": 13551.4501953125,
      "learning_rate": 2.9545923632610938e-05,
      "loss": 51.1235,
      "step": 14254
    },
    {
      "epoch": 14.27,
      "grad_norm": 23782.74609375,
      "learning_rate": 2.954076367389061e-05,
      "loss": 41.2243,
      "step": 14255
    },
    {
      "epoch": 14.27,
      "grad_norm": 28433.755859375,
      "learning_rate": 2.953560371517028e-05,
      "loss": 30.6449,
      "step": 14256
    },
    {
      "epoch": 14.27,
      "grad_norm": 18666.802734375,
      "learning_rate": 2.9530443756449948e-05,
      "loss": 41.921,
      "step": 14257
    },
    {
      "epoch": 14.27,
      "grad_norm": 52495.66015625,
      "learning_rate": 2.952528379772962e-05,
      "loss": 34.5465,
      "step": 14258
    },
    {
      "epoch": 14.27,
      "grad_norm": 20080.37890625,
      "learning_rate": 2.952012383900929e-05,
      "loss": 46.7591,
      "step": 14259
    },
    {
      "epoch": 14.27,
      "grad_norm": 3456.56787109375,
      "learning_rate": 2.9514963880288958e-05,
      "loss": 37.8775,
      "step": 14260
    },
    {
      "epoch": 14.28,
      "grad_norm": 22362.8046875,
      "learning_rate": 2.950980392156863e-05,
      "loss": 33.8623,
      "step": 14261
    },
    {
      "epoch": 14.28,
      "grad_norm": 159761.6875,
      "learning_rate": 2.95046439628483e-05,
      "loss": 29.5682,
      "step": 14262
    },
    {
      "epoch": 14.28,
      "grad_norm": 10026.2548828125,
      "learning_rate": 2.9499484004127968e-05,
      "loss": 52.4329,
      "step": 14263
    },
    {
      "epoch": 14.28,
      "grad_norm": 25175.474609375,
      "learning_rate": 2.949432404540764e-05,
      "loss": 43.1698,
      "step": 14264
    },
    {
      "epoch": 14.28,
      "grad_norm": 18539.724609375,
      "learning_rate": 2.948916408668731e-05,
      "loss": 38.517,
      "step": 14265
    },
    {
      "epoch": 14.28,
      "grad_norm": 4169.18896484375,
      "learning_rate": 2.9484004127966978e-05,
      "loss": 45.69,
      "step": 14266
    },
    {
      "epoch": 14.28,
      "grad_norm": 31348.501953125,
      "learning_rate": 2.947884416924665e-05,
      "loss": 42.7797,
      "step": 14267
    },
    {
      "epoch": 14.28,
      "grad_norm": 26866.041015625,
      "learning_rate": 2.9473684210526314e-05,
      "loss": 43.1366,
      "step": 14268
    },
    {
      "epoch": 14.28,
      "grad_norm": 12991.3671875,
      "learning_rate": 2.9468524251805985e-05,
      "loss": 50.3726,
      "step": 14269
    },
    {
      "epoch": 14.28,
      "grad_norm": 6629.93603515625,
      "learning_rate": 2.9463364293085656e-05,
      "loss": 39.3922,
      "step": 14270
    },
    {
      "epoch": 14.29,
      "grad_norm": 43679.02734375,
      "learning_rate": 2.9458204334365324e-05,
      "loss": 56.4334,
      "step": 14271
    },
    {
      "epoch": 14.29,
      "grad_norm": 9005.1513671875,
      "learning_rate": 2.9453044375644995e-05,
      "loss": 50.4549,
      "step": 14272
    },
    {
      "epoch": 14.29,
      "grad_norm": 84430.9375,
      "learning_rate": 2.9447884416924666e-05,
      "loss": 53.3124,
      "step": 14273
    },
    {
      "epoch": 14.29,
      "grad_norm": 7135.75341796875,
      "learning_rate": 2.9442724458204334e-05,
      "loss": 48.0304,
      "step": 14274
    },
    {
      "epoch": 14.29,
      "grad_norm": 3209.974365234375,
      "learning_rate": 2.9437564499484005e-05,
      "loss": 46.8904,
      "step": 14275
    },
    {
      "epoch": 14.29,
      "grad_norm": 188137.984375,
      "learning_rate": 2.9432404540763676e-05,
      "loss": 27.5732,
      "step": 14276
    },
    {
      "epoch": 14.29,
      "grad_norm": 5619.732421875,
      "learning_rate": 2.9427244582043344e-05,
      "loss": 44.9178,
      "step": 14277
    },
    {
      "epoch": 14.29,
      "grad_norm": 11573.17578125,
      "learning_rate": 2.9422084623323015e-05,
      "loss": 42.9604,
      "step": 14278
    },
    {
      "epoch": 14.29,
      "grad_norm": 79257.71875,
      "learning_rate": 2.9416924664602686e-05,
      "loss": 40.9701,
      "step": 14279
    },
    {
      "epoch": 14.29,
      "grad_norm": 4487.80615234375,
      "learning_rate": 2.9411764705882354e-05,
      "loss": 49.2432,
      "step": 14280
    },
    {
      "epoch": 14.3,
      "grad_norm": 7743.60791015625,
      "learning_rate": 2.9406604747162025e-05,
      "loss": 49.6518,
      "step": 14281
    },
    {
      "epoch": 14.3,
      "grad_norm": 60749.82421875,
      "learning_rate": 2.9401444788441696e-05,
      "loss": 49.6137,
      "step": 14282
    },
    {
      "epoch": 14.3,
      "grad_norm": 5214.7021484375,
      "learning_rate": 2.9396284829721367e-05,
      "loss": 46.2092,
      "step": 14283
    },
    {
      "epoch": 14.3,
      "grad_norm": 40193.71875,
      "learning_rate": 2.9391124871001035e-05,
      "loss": 53.9831,
      "step": 14284
    },
    {
      "epoch": 14.3,
      "grad_norm": 19542.3125,
      "learning_rate": 2.9385964912280706e-05,
      "loss": 50.6415,
      "step": 14285
    },
    {
      "epoch": 14.3,
      "grad_norm": 3708.61572265625,
      "learning_rate": 2.938080495356037e-05,
      "loss": 24.875,
      "step": 14286
    },
    {
      "epoch": 14.3,
      "grad_norm": 6501.7236328125,
      "learning_rate": 2.937564499484004e-05,
      "loss": 40.8723,
      "step": 14287
    },
    {
      "epoch": 14.3,
      "grad_norm": 69777.640625,
      "learning_rate": 2.937048503611971e-05,
      "loss": 44.5235,
      "step": 14288
    },
    {
      "epoch": 14.3,
      "grad_norm": 22226.177734375,
      "learning_rate": 2.936532507739938e-05,
      "loss": 43.7937,
      "step": 14289
    },
    {
      "epoch": 14.3,
      "grad_norm": 162257.640625,
      "learning_rate": 2.936016511867905e-05,
      "loss": 36.4333,
      "step": 14290
    },
    {
      "epoch": 14.31,
      "grad_norm": 11112.1826171875,
      "learning_rate": 2.935500515995872e-05,
      "loss": 48.326,
      "step": 14291
    },
    {
      "epoch": 14.31,
      "grad_norm": 74672.90625,
      "learning_rate": 2.934984520123839e-05,
      "loss": 29.9557,
      "step": 14292
    },
    {
      "epoch": 14.31,
      "grad_norm": 13208.15234375,
      "learning_rate": 2.934468524251806e-05,
      "loss": 50.6423,
      "step": 14293
    },
    {
      "epoch": 14.31,
      "grad_norm": 4384.37109375,
      "learning_rate": 2.933952528379773e-05,
      "loss": 56.2673,
      "step": 14294
    },
    {
      "epoch": 14.31,
      "grad_norm": 4005.750732421875,
      "learning_rate": 2.93343653250774e-05,
      "loss": 43.461,
      "step": 14295
    },
    {
      "epoch": 14.31,
      "grad_norm": 7729.03515625,
      "learning_rate": 2.932920536635707e-05,
      "loss": 45.2167,
      "step": 14296
    },
    {
      "epoch": 14.31,
      "grad_norm": 42672.75390625,
      "learning_rate": 2.9324045407636743e-05,
      "loss": 31.5603,
      "step": 14297
    },
    {
      "epoch": 14.31,
      "grad_norm": 18128.41796875,
      "learning_rate": 2.931888544891641e-05,
      "loss": 39.0305,
      "step": 14298
    },
    {
      "epoch": 14.31,
      "grad_norm": 67206.2890625,
      "learning_rate": 2.931372549019608e-05,
      "loss": 46.6416,
      "step": 14299
    },
    {
      "epoch": 14.31,
      "grad_norm": 5581.7294921875,
      "learning_rate": 2.9308565531475753e-05,
      "loss": 55.7702,
      "step": 14300
    },
    {
      "epoch": 14.32,
      "grad_norm": 4598.94189453125,
      "learning_rate": 2.930340557275542e-05,
      "loss": 38.5714,
      "step": 14301
    },
    {
      "epoch": 14.32,
      "grad_norm": 3929.49609375,
      "learning_rate": 2.929824561403509e-05,
      "loss": 54.0755,
      "step": 14302
    },
    {
      "epoch": 14.32,
      "grad_norm": 4952.1298828125,
      "learning_rate": 2.9293085655314763e-05,
      "loss": 47.6409,
      "step": 14303
    },
    {
      "epoch": 14.32,
      "grad_norm": 5644.3232421875,
      "learning_rate": 2.9287925696594427e-05,
      "loss": 51.8419,
      "step": 14304
    },
    {
      "epoch": 14.32,
      "grad_norm": 5460.6669921875,
      "learning_rate": 2.9282765737874095e-05,
      "loss": 43.3027,
      "step": 14305
    },
    {
      "epoch": 14.32,
      "grad_norm": 19327.26953125,
      "learning_rate": 2.9277605779153766e-05,
      "loss": 43.9015,
      "step": 14306
    },
    {
      "epoch": 14.32,
      "grad_norm": 14045.380859375,
      "learning_rate": 2.9272445820433437e-05,
      "loss": 42.2898,
      "step": 14307
    },
    {
      "epoch": 14.32,
      "grad_norm": 13041.26171875,
      "learning_rate": 2.9267285861713105e-05,
      "loss": 46.8414,
      "step": 14308
    },
    {
      "epoch": 14.32,
      "grad_norm": 9341.12109375,
      "learning_rate": 2.9262125902992776e-05,
      "loss": 44.8421,
      "step": 14309
    },
    {
      "epoch": 14.32,
      "grad_norm": 12815.4833984375,
      "learning_rate": 2.9256965944272447e-05,
      "loss": 52.4886,
      "step": 14310
    },
    {
      "epoch": 14.33,
      "grad_norm": 33790.98828125,
      "learning_rate": 2.9251805985552118e-05,
      "loss": 24.234,
      "step": 14311
    },
    {
      "epoch": 14.33,
      "grad_norm": 8913.98046875,
      "learning_rate": 2.9246646026831786e-05,
      "loss": 40.0908,
      "step": 14312
    },
    {
      "epoch": 14.33,
      "grad_norm": 10386.2744140625,
      "learning_rate": 2.9241486068111457e-05,
      "loss": 47.0769,
      "step": 14313
    },
    {
      "epoch": 14.33,
      "grad_norm": 116874.234375,
      "learning_rate": 2.9236326109391128e-05,
      "loss": 54.2454,
      "step": 14314
    },
    {
      "epoch": 14.33,
      "grad_norm": 11003.447265625,
      "learning_rate": 2.9231166150670796e-05,
      "loss": 45.4758,
      "step": 14315
    },
    {
      "epoch": 14.33,
      "grad_norm": 11127.115234375,
      "learning_rate": 2.9226006191950467e-05,
      "loss": 52.3898,
      "step": 14316
    },
    {
      "epoch": 14.33,
      "grad_norm": 62360.765625,
      "learning_rate": 2.9220846233230138e-05,
      "loss": 41.1054,
      "step": 14317
    },
    {
      "epoch": 14.33,
      "grad_norm": 9626.009765625,
      "learning_rate": 2.9215686274509806e-05,
      "loss": 46.9277,
      "step": 14318
    },
    {
      "epoch": 14.33,
      "grad_norm": 16529.134765625,
      "learning_rate": 2.9210526315789477e-05,
      "loss": 55.1831,
      "step": 14319
    },
    {
      "epoch": 14.33,
      "grad_norm": 23169.982421875,
      "learning_rate": 2.9205366357069148e-05,
      "loss": 48.879,
      "step": 14320
    },
    {
      "epoch": 14.34,
      "grad_norm": 10002.6845703125,
      "learning_rate": 2.9200206398348816e-05,
      "loss": 42.8958,
      "step": 14321
    },
    {
      "epoch": 14.34,
      "grad_norm": 36231.0234375,
      "learning_rate": 2.919504643962848e-05,
      "loss": 43.6965,
      "step": 14322
    },
    {
      "epoch": 14.34,
      "grad_norm": 26021.705078125,
      "learning_rate": 2.918988648090815e-05,
      "loss": 17.055,
      "step": 14323
    },
    {
      "epoch": 14.34,
      "grad_norm": 33681.71484375,
      "learning_rate": 2.9184726522187822e-05,
      "loss": 22.3572,
      "step": 14324
    },
    {
      "epoch": 14.34,
      "grad_norm": 16628.73828125,
      "learning_rate": 2.9179566563467494e-05,
      "loss": 40.5446,
      "step": 14325
    },
    {
      "epoch": 14.34,
      "grad_norm": 11664.69921875,
      "learning_rate": 2.917440660474716e-05,
      "loss": 35.5573,
      "step": 14326
    },
    {
      "epoch": 14.34,
      "grad_norm": 42373.58984375,
      "learning_rate": 2.9169246646026832e-05,
      "loss": 20.8834,
      "step": 14327
    },
    {
      "epoch": 14.34,
      "grad_norm": 95198.34375,
      "learning_rate": 2.9164086687306504e-05,
      "loss": 50.2931,
      "step": 14328
    },
    {
      "epoch": 14.34,
      "grad_norm": 19965.671875,
      "learning_rate": 2.915892672858617e-05,
      "loss": 55.0311,
      "step": 14329
    },
    {
      "epoch": 14.34,
      "grad_norm": 85885.6171875,
      "learning_rate": 2.9153766769865843e-05,
      "loss": 14.1034,
      "step": 14330
    },
    {
      "epoch": 14.35,
      "grad_norm": 35567.8671875,
      "learning_rate": 2.9148606811145514e-05,
      "loss": 47.0806,
      "step": 14331
    },
    {
      "epoch": 14.35,
      "grad_norm": 23337.701171875,
      "learning_rate": 2.914344685242518e-05,
      "loss": 50.1847,
      "step": 14332
    },
    {
      "epoch": 14.35,
      "grad_norm": 47986.03125,
      "learning_rate": 2.9138286893704853e-05,
      "loss": 53.79,
      "step": 14333
    },
    {
      "epoch": 14.35,
      "grad_norm": 72234.1875,
      "learning_rate": 2.9133126934984524e-05,
      "loss": 46.7723,
      "step": 14334
    },
    {
      "epoch": 14.35,
      "grad_norm": 22335.60546875,
      "learning_rate": 2.912796697626419e-05,
      "loss": 50.6993,
      "step": 14335
    },
    {
      "epoch": 14.35,
      "grad_norm": 3581.360595703125,
      "learning_rate": 2.9122807017543863e-05,
      "loss": 42.7285,
      "step": 14336
    },
    {
      "epoch": 14.35,
      "grad_norm": 56706.6171875,
      "learning_rate": 2.9117647058823534e-05,
      "loss": 56.3224,
      "step": 14337
    },
    {
      "epoch": 14.35,
      "grad_norm": 40234.12109375,
      "learning_rate": 2.91124871001032e-05,
      "loss": 32.1121,
      "step": 14338
    },
    {
      "epoch": 14.35,
      "grad_norm": 9188.6767578125,
      "learning_rate": 2.9107327141382873e-05,
      "loss": 36.677,
      "step": 14339
    },
    {
      "epoch": 14.35,
      "grad_norm": 49786.265625,
      "learning_rate": 2.9102167182662537e-05,
      "loss": 48.458,
      "step": 14340
    },
    {
      "epoch": 14.36,
      "grad_norm": 5317.3955078125,
      "learning_rate": 2.9097007223942208e-05,
      "loss": 57.6025,
      "step": 14341
    },
    {
      "epoch": 14.36,
      "grad_norm": 49796.984375,
      "learning_rate": 2.909184726522188e-05,
      "loss": 53.5562,
      "step": 14342
    },
    {
      "epoch": 14.36,
      "grad_norm": 34876.01953125,
      "learning_rate": 2.9086687306501547e-05,
      "loss": 41.3088,
      "step": 14343
    },
    {
      "epoch": 14.36,
      "grad_norm": 164210.5,
      "learning_rate": 2.9081527347781218e-05,
      "loss": 47.8614,
      "step": 14344
    },
    {
      "epoch": 14.36,
      "grad_norm": 6915.5576171875,
      "learning_rate": 2.907636738906089e-05,
      "loss": 27.3649,
      "step": 14345
    },
    {
      "epoch": 14.36,
      "grad_norm": 7110.8798828125,
      "learning_rate": 2.9071207430340557e-05,
      "loss": 46.7449,
      "step": 14346
    },
    {
      "epoch": 14.36,
      "grad_norm": 13311.337890625,
      "learning_rate": 2.9066047471620228e-05,
      "loss": 44.7903,
      "step": 14347
    },
    {
      "epoch": 14.36,
      "grad_norm": 51655.90625,
      "learning_rate": 2.90608875128999e-05,
      "loss": 28.4936,
      "step": 14348
    },
    {
      "epoch": 14.36,
      "grad_norm": 72027.796875,
      "learning_rate": 2.9055727554179567e-05,
      "loss": 44.7611,
      "step": 14349
    },
    {
      "epoch": 14.36,
      "grad_norm": 4760.42041015625,
      "learning_rate": 2.9050567595459238e-05,
      "loss": 49.2797,
      "step": 14350
    },
    {
      "epoch": 14.37,
      "grad_norm": 43625.3515625,
      "learning_rate": 2.904540763673891e-05,
      "loss": 35.3788,
      "step": 14351
    },
    {
      "epoch": 14.37,
      "grad_norm": 22961.7265625,
      "learning_rate": 2.9040247678018577e-05,
      "loss": 45.8094,
      "step": 14352
    },
    {
      "epoch": 14.37,
      "grad_norm": 8365.681640625,
      "learning_rate": 2.9035087719298248e-05,
      "loss": 48.4073,
      "step": 14353
    },
    {
      "epoch": 14.37,
      "grad_norm": 18298.298828125,
      "learning_rate": 2.902992776057792e-05,
      "loss": 50.6268,
      "step": 14354
    },
    {
      "epoch": 14.37,
      "grad_norm": 25922.349609375,
      "learning_rate": 2.902476780185759e-05,
      "loss": 45.9976,
      "step": 14355
    },
    {
      "epoch": 14.37,
      "grad_norm": 3396.892333984375,
      "learning_rate": 2.9019607843137258e-05,
      "loss": 57.0056,
      "step": 14356
    },
    {
      "epoch": 14.37,
      "grad_norm": 6701.48388671875,
      "learning_rate": 2.901444788441693e-05,
      "loss": 37.2995,
      "step": 14357
    },
    {
      "epoch": 14.37,
      "grad_norm": 5629.96142578125,
      "learning_rate": 2.9009287925696593e-05,
      "loss": 48.9809,
      "step": 14358
    },
    {
      "epoch": 14.37,
      "grad_norm": 4346.85693359375,
      "learning_rate": 2.9004127966976265e-05,
      "loss": 35.277,
      "step": 14359
    },
    {
      "epoch": 14.37,
      "grad_norm": 1731.5015869140625,
      "learning_rate": 2.8998968008255932e-05,
      "loss": 47.6208,
      "step": 14360
    },
    {
      "epoch": 14.38,
      "grad_norm": 1625.171630859375,
      "learning_rate": 2.8993808049535603e-05,
      "loss": 48.9445,
      "step": 14361
    },
    {
      "epoch": 14.38,
      "grad_norm": 10489.4404296875,
      "learning_rate": 2.8988648090815275e-05,
      "loss": 40.4543,
      "step": 14362
    },
    {
      "epoch": 14.38,
      "grad_norm": 21885.9453125,
      "learning_rate": 2.8983488132094942e-05,
      "loss": 47.0098,
      "step": 14363
    },
    {
      "epoch": 14.38,
      "grad_norm": 16794.169921875,
      "learning_rate": 2.8978328173374613e-05,
      "loss": 40.5325,
      "step": 14364
    },
    {
      "epoch": 14.38,
      "grad_norm": 81199.9453125,
      "learning_rate": 2.8973168214654285e-05,
      "loss": 43.5381,
      "step": 14365
    },
    {
      "epoch": 14.38,
      "grad_norm": 4700.6669921875,
      "learning_rate": 2.8968008255933952e-05,
      "loss": 44.5256,
      "step": 14366
    },
    {
      "epoch": 14.38,
      "grad_norm": 17661.576171875,
      "learning_rate": 2.8962848297213623e-05,
      "loss": 48.8682,
      "step": 14367
    },
    {
      "epoch": 14.38,
      "grad_norm": 11444.4111328125,
      "learning_rate": 2.8957688338493295e-05,
      "loss": 56.066,
      "step": 14368
    },
    {
      "epoch": 14.38,
      "grad_norm": 12051.677734375,
      "learning_rate": 2.8952528379772962e-05,
      "loss": 55.3747,
      "step": 14369
    },
    {
      "epoch": 14.38,
      "grad_norm": 14250.6533203125,
      "learning_rate": 2.8947368421052634e-05,
      "loss": 36.3,
      "step": 14370
    },
    {
      "epoch": 14.39,
      "grad_norm": 8578.2666015625,
      "learning_rate": 2.8942208462332305e-05,
      "loss": 49.1603,
      "step": 14371
    },
    {
      "epoch": 14.39,
      "grad_norm": 29996.06640625,
      "learning_rate": 2.8937048503611976e-05,
      "loss": 45.6457,
      "step": 14372
    },
    {
      "epoch": 14.39,
      "grad_norm": 25925.501953125,
      "learning_rate": 2.8931888544891644e-05,
      "loss": 53.763,
      "step": 14373
    },
    {
      "epoch": 14.39,
      "grad_norm": 4328.50341796875,
      "learning_rate": 2.8926728586171315e-05,
      "loss": 39.1807,
      "step": 14374
    },
    {
      "epoch": 14.39,
      "grad_norm": 4665.7490234375,
      "learning_rate": 2.8921568627450986e-05,
      "loss": 49.547,
      "step": 14375
    },
    {
      "epoch": 14.39,
      "grad_norm": 42436.65234375,
      "learning_rate": 2.891640866873065e-05,
      "loss": 38.9948,
      "step": 14376
    },
    {
      "epoch": 14.39,
      "grad_norm": 10029.421875,
      "learning_rate": 2.8911248710010318e-05,
      "loss": 49.8963,
      "step": 14377
    },
    {
      "epoch": 14.39,
      "grad_norm": 11982.326171875,
      "learning_rate": 2.890608875128999e-05,
      "loss": 47.8853,
      "step": 14378
    },
    {
      "epoch": 14.39,
      "grad_norm": 18392.900390625,
      "learning_rate": 2.890092879256966e-05,
      "loss": 39.3446,
      "step": 14379
    },
    {
      "epoch": 14.39,
      "grad_norm": 36678.3125,
      "learning_rate": 2.8895768833849328e-05,
      "loss": 31.5252,
      "step": 14380
    },
    {
      "epoch": 14.4,
      "grad_norm": 8116.89892578125,
      "learning_rate": 2.8890608875129e-05,
      "loss": 42.1553,
      "step": 14381
    },
    {
      "epoch": 14.4,
      "grad_norm": 4869.916015625,
      "learning_rate": 2.888544891640867e-05,
      "loss": 19.5424,
      "step": 14382
    },
    {
      "epoch": 14.4,
      "grad_norm": 16556.69921875,
      "learning_rate": 2.8880288957688338e-05,
      "loss": 53.3753,
      "step": 14383
    },
    {
      "epoch": 14.4,
      "grad_norm": 15748.33984375,
      "learning_rate": 2.887512899896801e-05,
      "loss": 55.0451,
      "step": 14384
    },
    {
      "epoch": 14.4,
      "grad_norm": 22507.6796875,
      "learning_rate": 2.886996904024768e-05,
      "loss": 41.739,
      "step": 14385
    },
    {
      "epoch": 14.4,
      "grad_norm": 12271.6064453125,
      "learning_rate": 2.886480908152735e-05,
      "loss": 51.2434,
      "step": 14386
    },
    {
      "epoch": 14.4,
      "grad_norm": 41758.00390625,
      "learning_rate": 2.885964912280702e-05,
      "loss": 30.442,
      "step": 14387
    },
    {
      "epoch": 14.4,
      "grad_norm": 8963.6787109375,
      "learning_rate": 2.885448916408669e-05,
      "loss": 45.1469,
      "step": 14388
    },
    {
      "epoch": 14.4,
      "grad_norm": 20155.81640625,
      "learning_rate": 2.884932920536636e-05,
      "loss": 24.6701,
      "step": 14389
    },
    {
      "epoch": 14.4,
      "grad_norm": 677187.9375,
      "learning_rate": 2.884416924664603e-05,
      "loss": 46.7779,
      "step": 14390
    },
    {
      "epoch": 14.41,
      "grad_norm": 8695.6015625,
      "learning_rate": 2.88390092879257e-05,
      "loss": 35.2056,
      "step": 14391
    },
    {
      "epoch": 14.41,
      "grad_norm": 37976.984375,
      "learning_rate": 2.883384932920537e-05,
      "loss": 33.3477,
      "step": 14392
    },
    {
      "epoch": 14.41,
      "grad_norm": 6026.263671875,
      "learning_rate": 2.882868937048504e-05,
      "loss": 41.8883,
      "step": 14393
    },
    {
      "epoch": 14.41,
      "grad_norm": 9301.6044921875,
      "learning_rate": 2.8823529411764703e-05,
      "loss": 40.9689,
      "step": 14394
    },
    {
      "epoch": 14.41,
      "grad_norm": 3808.912353515625,
      "learning_rate": 2.8818369453044374e-05,
      "loss": 35.9041,
      "step": 14395
    },
    {
      "epoch": 14.41,
      "grad_norm": 3371.495849609375,
      "learning_rate": 2.8813209494324046e-05,
      "loss": 54.5405,
      "step": 14396
    },
    {
      "epoch": 14.41,
      "grad_norm": 53559.50390625,
      "learning_rate": 2.8808049535603713e-05,
      "loss": 49.765,
      "step": 14397
    },
    {
      "epoch": 14.41,
      "grad_norm": 9343.375,
      "learning_rate": 2.8802889576883384e-05,
      "loss": 59.516,
      "step": 14398
    },
    {
      "epoch": 14.41,
      "grad_norm": 5695.294921875,
      "learning_rate": 2.8797729618163056e-05,
      "loss": 22.5256,
      "step": 14399
    },
    {
      "epoch": 14.41,
      "grad_norm": 3623.64453125,
      "learning_rate": 2.8792569659442727e-05,
      "loss": 48.4098,
      "step": 14400
    },
    {
      "epoch": 14.42,
      "grad_norm": 4242.9521484375,
      "learning_rate": 2.8787409700722394e-05,
      "loss": 49.6307,
      "step": 14401
    },
    {
      "epoch": 14.42,
      "grad_norm": 13936.0986328125,
      "learning_rate": 2.8782249742002066e-05,
      "loss": 55.8351,
      "step": 14402
    },
    {
      "epoch": 14.42,
      "grad_norm": 155682.984375,
      "learning_rate": 2.8777089783281737e-05,
      "loss": 43.868,
      "step": 14403
    },
    {
      "epoch": 14.42,
      "grad_norm": 9886.70703125,
      "learning_rate": 2.8771929824561404e-05,
      "loss": 38.3309,
      "step": 14404
    },
    {
      "epoch": 14.42,
      "grad_norm": 43315.30859375,
      "learning_rate": 2.8766769865841076e-05,
      "loss": 45.0727,
      "step": 14405
    },
    {
      "epoch": 14.42,
      "grad_norm": 71235.234375,
      "learning_rate": 2.8761609907120747e-05,
      "loss": 34.485,
      "step": 14406
    },
    {
      "epoch": 14.42,
      "grad_norm": 22846.548828125,
      "learning_rate": 2.8756449948400414e-05,
      "loss": 50.8015,
      "step": 14407
    },
    {
      "epoch": 14.42,
      "grad_norm": 13069.8203125,
      "learning_rate": 2.8751289989680086e-05,
      "loss": 49.4579,
      "step": 14408
    },
    {
      "epoch": 14.42,
      "grad_norm": 8772.4716796875,
      "learning_rate": 2.8746130030959757e-05,
      "loss": 50.323,
      "step": 14409
    },
    {
      "epoch": 14.42,
      "grad_norm": 42953.9140625,
      "learning_rate": 2.8740970072239425e-05,
      "loss": 33.8329,
      "step": 14410
    },
    {
      "epoch": 14.43,
      "grad_norm": 32108.298828125,
      "learning_rate": 2.873581011351909e-05,
      "loss": 47.8976,
      "step": 14411
    },
    {
      "epoch": 14.43,
      "grad_norm": 7073.51220703125,
      "learning_rate": 2.873065015479876e-05,
      "loss": 52.8371,
      "step": 14412
    },
    {
      "epoch": 14.43,
      "grad_norm": 3910.70361328125,
      "learning_rate": 2.872549019607843e-05,
      "loss": 38.9107,
      "step": 14413
    },
    {
      "epoch": 14.43,
      "grad_norm": 17666.87109375,
      "learning_rate": 2.8720330237358102e-05,
      "loss": 43.0955,
      "step": 14414
    },
    {
      "epoch": 14.43,
      "grad_norm": 6345.99853515625,
      "learning_rate": 2.871517027863777e-05,
      "loss": 39.4921,
      "step": 14415
    },
    {
      "epoch": 14.43,
      "grad_norm": 22088.14453125,
      "learning_rate": 2.871001031991744e-05,
      "loss": 50.7691,
      "step": 14416
    },
    {
      "epoch": 14.43,
      "grad_norm": 7237.2763671875,
      "learning_rate": 2.8704850361197112e-05,
      "loss": 52.3192,
      "step": 14417
    },
    {
      "epoch": 14.43,
      "grad_norm": 16692.98828125,
      "learning_rate": 2.869969040247678e-05,
      "loss": 41.1133,
      "step": 14418
    },
    {
      "epoch": 14.43,
      "grad_norm": 12113.9150390625,
      "learning_rate": 2.869453044375645e-05,
      "loss": 53.7151,
      "step": 14419
    },
    {
      "epoch": 14.43,
      "grad_norm": 11433.052734375,
      "learning_rate": 2.8689370485036122e-05,
      "loss": 36.2838,
      "step": 14420
    },
    {
      "epoch": 14.44,
      "grad_norm": 16814.521484375,
      "learning_rate": 2.868421052631579e-05,
      "loss": 51.6131,
      "step": 14421
    },
    {
      "epoch": 14.44,
      "grad_norm": 6696.3857421875,
      "learning_rate": 2.867905056759546e-05,
      "loss": 45.5759,
      "step": 14422
    },
    {
      "epoch": 14.44,
      "grad_norm": 13039.599609375,
      "learning_rate": 2.8673890608875132e-05,
      "loss": 46.5986,
      "step": 14423
    },
    {
      "epoch": 14.44,
      "grad_norm": 19376.845703125,
      "learning_rate": 2.86687306501548e-05,
      "loss": 34.7593,
      "step": 14424
    },
    {
      "epoch": 14.44,
      "grad_norm": 36815.046875,
      "learning_rate": 2.866357069143447e-05,
      "loss": 35.4834,
      "step": 14425
    },
    {
      "epoch": 14.44,
      "grad_norm": 488136.34375,
      "learning_rate": 2.8658410732714142e-05,
      "loss": 47.0122,
      "step": 14426
    },
    {
      "epoch": 14.44,
      "grad_norm": 26163.8046875,
      "learning_rate": 2.865325077399381e-05,
      "loss": 43.6435,
      "step": 14427
    },
    {
      "epoch": 14.44,
      "grad_norm": 1332.690185546875,
      "learning_rate": 2.864809081527348e-05,
      "loss": 53.4868,
      "step": 14428
    },
    {
      "epoch": 14.44,
      "grad_norm": 44831.1875,
      "learning_rate": 2.8642930856553145e-05,
      "loss": 48.3312,
      "step": 14429
    },
    {
      "epoch": 14.44,
      "grad_norm": 36212.8046875,
      "learning_rate": 2.8637770897832817e-05,
      "loss": 37.308,
      "step": 14430
    },
    {
      "epoch": 14.45,
      "grad_norm": 41834.0,
      "learning_rate": 2.8632610939112488e-05,
      "loss": 43.327,
      "step": 14431
    },
    {
      "epoch": 14.45,
      "grad_norm": 4781.5556640625,
      "learning_rate": 2.8627450980392155e-05,
      "loss": 32.6123,
      "step": 14432
    },
    {
      "epoch": 14.45,
      "grad_norm": 40655.97265625,
      "learning_rate": 2.8622291021671827e-05,
      "loss": 40.4475,
      "step": 14433
    },
    {
      "epoch": 14.45,
      "grad_norm": 5154.5263671875,
      "learning_rate": 2.8617131062951498e-05,
      "loss": 33.8469,
      "step": 14434
    },
    {
      "epoch": 14.45,
      "grad_norm": 10098.7099609375,
      "learning_rate": 2.8611971104231165e-05,
      "loss": 48.3709,
      "step": 14435
    },
    {
      "epoch": 14.45,
      "grad_norm": 19371.494140625,
      "learning_rate": 2.8606811145510837e-05,
      "loss": 50.7711,
      "step": 14436
    },
    {
      "epoch": 14.45,
      "grad_norm": 16591.578125,
      "learning_rate": 2.8601651186790508e-05,
      "loss": 56.3603,
      "step": 14437
    },
    {
      "epoch": 14.45,
      "grad_norm": 42414.32421875,
      "learning_rate": 2.8596491228070175e-05,
      "loss": 42.3813,
      "step": 14438
    },
    {
      "epoch": 14.45,
      "grad_norm": 5862.869140625,
      "learning_rate": 2.8591331269349847e-05,
      "loss": 49.136,
      "step": 14439
    },
    {
      "epoch": 14.45,
      "grad_norm": 8672.431640625,
      "learning_rate": 2.8586171310629518e-05,
      "loss": 30.884,
      "step": 14440
    },
    {
      "epoch": 14.46,
      "grad_norm": 9656.0166015625,
      "learning_rate": 2.8581011351909185e-05,
      "loss": 49.1143,
      "step": 14441
    },
    {
      "epoch": 14.46,
      "grad_norm": 10562.1083984375,
      "learning_rate": 2.8575851393188857e-05,
      "loss": 58.9659,
      "step": 14442
    },
    {
      "epoch": 14.46,
      "grad_norm": 3101.6396484375,
      "learning_rate": 2.8570691434468528e-05,
      "loss": 38.8589,
      "step": 14443
    },
    {
      "epoch": 14.46,
      "grad_norm": 26962.857421875,
      "learning_rate": 2.85655314757482e-05,
      "loss": 48.5633,
      "step": 14444
    },
    {
      "epoch": 14.46,
      "grad_norm": 6481.11376953125,
      "learning_rate": 2.8560371517027867e-05,
      "loss": 31.3672,
      "step": 14445
    },
    {
      "epoch": 14.46,
      "grad_norm": 19304.373046875,
      "learning_rate": 2.8555211558307538e-05,
      "loss": 44.1834,
      "step": 14446
    },
    {
      "epoch": 14.46,
      "grad_norm": 20549.84765625,
      "learning_rate": 2.8550051599587202e-05,
      "loss": 44.3719,
      "step": 14447
    },
    {
      "epoch": 14.46,
      "grad_norm": 2761.52587890625,
      "learning_rate": 2.8544891640866873e-05,
      "loss": 52.8385,
      "step": 14448
    },
    {
      "epoch": 14.46,
      "grad_norm": 3263.74609375,
      "learning_rate": 2.853973168214654e-05,
      "loss": 45.9167,
      "step": 14449
    },
    {
      "epoch": 14.46,
      "grad_norm": 11622.95703125,
      "learning_rate": 2.8534571723426212e-05,
      "loss": 52.2773,
      "step": 14450
    },
    {
      "epoch": 14.47,
      "grad_norm": 21121.6484375,
      "learning_rate": 2.8529411764705883e-05,
      "loss": 39.7488,
      "step": 14451
    },
    {
      "epoch": 14.47,
      "grad_norm": 2681.10595703125,
      "learning_rate": 2.852425180598555e-05,
      "loss": 44.9604,
      "step": 14452
    },
    {
      "epoch": 14.47,
      "grad_norm": 5415.17822265625,
      "learning_rate": 2.8519091847265222e-05,
      "loss": 29.9581,
      "step": 14453
    },
    {
      "epoch": 14.47,
      "grad_norm": 12418.556640625,
      "learning_rate": 2.8513931888544893e-05,
      "loss": 44.1267,
      "step": 14454
    },
    {
      "epoch": 14.47,
      "grad_norm": 10296.4892578125,
      "learning_rate": 2.850877192982456e-05,
      "loss": 41.3566,
      "step": 14455
    },
    {
      "epoch": 14.47,
      "grad_norm": 39086.77734375,
      "learning_rate": 2.8503611971104232e-05,
      "loss": 45.7599,
      "step": 14456
    },
    {
      "epoch": 14.47,
      "grad_norm": 7195.81689453125,
      "learning_rate": 2.8498452012383903e-05,
      "loss": 50.2125,
      "step": 14457
    },
    {
      "epoch": 14.47,
      "grad_norm": 3926.498046875,
      "learning_rate": 2.8493292053663574e-05,
      "loss": 56.4511,
      "step": 14458
    },
    {
      "epoch": 14.47,
      "grad_norm": 6389.5576171875,
      "learning_rate": 2.8488132094943242e-05,
      "loss": 38.4572,
      "step": 14459
    },
    {
      "epoch": 14.47,
      "grad_norm": 7441.27197265625,
      "learning_rate": 2.8482972136222913e-05,
      "loss": 50.7114,
      "step": 14460
    },
    {
      "epoch": 14.48,
      "grad_norm": 30557.169921875,
      "learning_rate": 2.8477812177502584e-05,
      "loss": 58.2332,
      "step": 14461
    },
    {
      "epoch": 14.48,
      "grad_norm": 6907.21728515625,
      "learning_rate": 2.8472652218782252e-05,
      "loss": 45.6478,
      "step": 14462
    },
    {
      "epoch": 14.48,
      "grad_norm": 59189.078125,
      "learning_rate": 2.8467492260061923e-05,
      "loss": 33.735,
      "step": 14463
    },
    {
      "epoch": 14.48,
      "grad_norm": 4361.95703125,
      "learning_rate": 2.8462332301341594e-05,
      "loss": 26.0832,
      "step": 14464
    },
    {
      "epoch": 14.48,
      "grad_norm": 34774.97265625,
      "learning_rate": 2.845717234262126e-05,
      "loss": 30.6074,
      "step": 14465
    },
    {
      "epoch": 14.48,
      "grad_norm": 23745.01953125,
      "learning_rate": 2.8452012383900926e-05,
      "loss": 55.7796,
      "step": 14466
    },
    {
      "epoch": 14.48,
      "grad_norm": 5329.4951171875,
      "learning_rate": 2.8446852425180598e-05,
      "loss": 49.4,
      "step": 14467
    },
    {
      "epoch": 14.48,
      "grad_norm": 18254.134765625,
      "learning_rate": 2.844169246646027e-05,
      "loss": 46.2939,
      "step": 14468
    },
    {
      "epoch": 14.48,
      "grad_norm": 11383.7958984375,
      "learning_rate": 2.8436532507739936e-05,
      "loss": 54.7134,
      "step": 14469
    },
    {
      "epoch": 14.48,
      "grad_norm": 33619.578125,
      "learning_rate": 2.8431372549019608e-05,
      "loss": 57.6632,
      "step": 14470
    },
    {
      "epoch": 14.49,
      "grad_norm": 10533.7275390625,
      "learning_rate": 2.842621259029928e-05,
      "loss": 39.3362,
      "step": 14471
    },
    {
      "epoch": 14.49,
      "grad_norm": 17236.158203125,
      "learning_rate": 2.842105263157895e-05,
      "loss": 57.8578,
      "step": 14472
    },
    {
      "epoch": 14.49,
      "grad_norm": 13621.4091796875,
      "learning_rate": 2.8415892672858618e-05,
      "loss": 44.7257,
      "step": 14473
    },
    {
      "epoch": 14.49,
      "grad_norm": 41012.40234375,
      "learning_rate": 2.841073271413829e-05,
      "loss": 50.3335,
      "step": 14474
    },
    {
      "epoch": 14.49,
      "grad_norm": 3396.86474609375,
      "learning_rate": 2.840557275541796e-05,
      "loss": 48.3527,
      "step": 14475
    },
    {
      "epoch": 14.49,
      "grad_norm": 13442.275390625,
      "learning_rate": 2.8400412796697628e-05,
      "loss": 20.6551,
      "step": 14476
    },
    {
      "epoch": 14.49,
      "grad_norm": 3045.8447265625,
      "learning_rate": 2.83952528379773e-05,
      "loss": 38.8687,
      "step": 14477
    },
    {
      "epoch": 14.49,
      "grad_norm": 3233.6142578125,
      "learning_rate": 2.839009287925697e-05,
      "loss": 61.7276,
      "step": 14478
    },
    {
      "epoch": 14.49,
      "grad_norm": 44079.08984375,
      "learning_rate": 2.8384932920536638e-05,
      "loss": 44.9768,
      "step": 14479
    },
    {
      "epoch": 14.49,
      "grad_norm": 34366.34375,
      "learning_rate": 2.837977296181631e-05,
      "loss": 48.76,
      "step": 14480
    },
    {
      "epoch": 14.5,
      "grad_norm": 8957.28515625,
      "learning_rate": 2.837461300309598e-05,
      "loss": 44.525,
      "step": 14481
    },
    {
      "epoch": 14.5,
      "grad_norm": 3867.526611328125,
      "learning_rate": 2.8369453044375648e-05,
      "loss": 46.3911,
      "step": 14482
    },
    {
      "epoch": 14.5,
      "grad_norm": 10068.0712890625,
      "learning_rate": 2.8364293085655312e-05,
      "loss": 54.8222,
      "step": 14483
    },
    {
      "epoch": 14.5,
      "grad_norm": 14583.6328125,
      "learning_rate": 2.8359133126934983e-05,
      "loss": 47.5094,
      "step": 14484
    },
    {
      "epoch": 14.5,
      "grad_norm": 2932.35888671875,
      "learning_rate": 2.8353973168214654e-05,
      "loss": 55.4493,
      "step": 14485
    },
    {
      "epoch": 14.5,
      "grad_norm": 21074.66796875,
      "learning_rate": 2.8348813209494325e-05,
      "loss": 42.1972,
      "step": 14486
    },
    {
      "epoch": 14.5,
      "grad_norm": 239420.953125,
      "learning_rate": 2.8343653250773993e-05,
      "loss": 45.8911,
      "step": 14487
    },
    {
      "epoch": 14.5,
      "grad_norm": 65688.421875,
      "learning_rate": 2.8338493292053664e-05,
      "loss": 45.7027,
      "step": 14488
    },
    {
      "epoch": 14.5,
      "grad_norm": 8523.4169921875,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 47.4289,
      "step": 14489
    },
    {
      "epoch": 14.5,
      "grad_norm": 6716.93603515625,
      "learning_rate": 2.8328173374613003e-05,
      "loss": 51.1601,
      "step": 14490
    },
    {
      "epoch": 14.51,
      "grad_norm": 18936.662109375,
      "learning_rate": 2.8323013415892674e-05,
      "loss": 34.6212,
      "step": 14491
    },
    {
      "epoch": 14.51,
      "grad_norm": 4845.96923828125,
      "learning_rate": 2.8317853457172345e-05,
      "loss": 52.4765,
      "step": 14492
    },
    {
      "epoch": 14.51,
      "grad_norm": 68615.4375,
      "learning_rate": 2.8312693498452013e-05,
      "loss": 52.4745,
      "step": 14493
    },
    {
      "epoch": 14.51,
      "grad_norm": 24564.6640625,
      "learning_rate": 2.8307533539731684e-05,
      "loss": 38.5065,
      "step": 14494
    },
    {
      "epoch": 14.51,
      "grad_norm": 10486.2275390625,
      "learning_rate": 2.8302373581011355e-05,
      "loss": 59.4038,
      "step": 14495
    },
    {
      "epoch": 14.51,
      "grad_norm": 8081.24609375,
      "learning_rate": 2.8297213622291023e-05,
      "loss": 47.3026,
      "step": 14496
    },
    {
      "epoch": 14.51,
      "grad_norm": 18674.873046875,
      "learning_rate": 2.8292053663570694e-05,
      "loss": 50.7379,
      "step": 14497
    },
    {
      "epoch": 14.51,
      "grad_norm": 7559.0029296875,
      "learning_rate": 2.8286893704850365e-05,
      "loss": 42.0232,
      "step": 14498
    },
    {
      "epoch": 14.51,
      "grad_norm": 10307.4482421875,
      "learning_rate": 2.8281733746130033e-05,
      "loss": 48.8177,
      "step": 14499
    },
    {
      "epoch": 14.51,
      "grad_norm": 12737.15234375,
      "learning_rate": 2.8276573787409704e-05,
      "loss": 27.501,
      "step": 14500
    },
    {
      "epoch": 14.52,
      "grad_norm": 3812.80810546875,
      "learning_rate": 2.827141382868937e-05,
      "loss": 45.2302,
      "step": 14501
    },
    {
      "epoch": 14.52,
      "grad_norm": 14163.0947265625,
      "learning_rate": 2.826625386996904e-05,
      "loss": 45.1752,
      "step": 14502
    },
    {
      "epoch": 14.52,
      "grad_norm": 9486.896484375,
      "learning_rate": 2.826109391124871e-05,
      "loss": 33.5671,
      "step": 14503
    },
    {
      "epoch": 14.52,
      "grad_norm": 30842.310546875,
      "learning_rate": 2.825593395252838e-05,
      "loss": 50.3219,
      "step": 14504
    },
    {
      "epoch": 14.52,
      "grad_norm": 91278.59375,
      "learning_rate": 2.825077399380805e-05,
      "loss": 20.257,
      "step": 14505
    },
    {
      "epoch": 14.52,
      "grad_norm": 8676.2333984375,
      "learning_rate": 2.824561403508772e-05,
      "loss": 46.732,
      "step": 14506
    },
    {
      "epoch": 14.52,
      "grad_norm": 6431.29638671875,
      "learning_rate": 2.824045407636739e-05,
      "loss": 53.651,
      "step": 14507
    },
    {
      "epoch": 14.52,
      "grad_norm": 26746.990234375,
      "learning_rate": 2.823529411764706e-05,
      "loss": 43.1336,
      "step": 14508
    },
    {
      "epoch": 14.52,
      "grad_norm": 5027.64794921875,
      "learning_rate": 2.823013415892673e-05,
      "loss": 50.983,
      "step": 14509
    },
    {
      "epoch": 14.52,
      "grad_norm": 19188.3671875,
      "learning_rate": 2.82249742002064e-05,
      "loss": 39.5736,
      "step": 14510
    },
    {
      "epoch": 14.53,
      "grad_norm": 6860.35205078125,
      "learning_rate": 2.821981424148607e-05,
      "loss": 41.7308,
      "step": 14511
    },
    {
      "epoch": 14.53,
      "grad_norm": 63577.23828125,
      "learning_rate": 2.821465428276574e-05,
      "loss": 53.6629,
      "step": 14512
    },
    {
      "epoch": 14.53,
      "grad_norm": 54250.7890625,
      "learning_rate": 2.820949432404541e-05,
      "loss": 54.4747,
      "step": 14513
    },
    {
      "epoch": 14.53,
      "grad_norm": 13749.501953125,
      "learning_rate": 2.820433436532508e-05,
      "loss": 46.1334,
      "step": 14514
    },
    {
      "epoch": 14.53,
      "grad_norm": 11960.25,
      "learning_rate": 2.819917440660475e-05,
      "loss": 51.7963,
      "step": 14515
    },
    {
      "epoch": 14.53,
      "grad_norm": 20069.35546875,
      "learning_rate": 2.8194014447884422e-05,
      "loss": 50.8708,
      "step": 14516
    },
    {
      "epoch": 14.53,
      "grad_norm": 10517.3515625,
      "learning_rate": 2.818885448916409e-05,
      "loss": 44.7557,
      "step": 14517
    },
    {
      "epoch": 14.53,
      "grad_norm": 65542.6796875,
      "learning_rate": 2.818369453044376e-05,
      "loss": 48.3172,
      "step": 14518
    },
    {
      "epoch": 14.53,
      "grad_norm": 8846.640625,
      "learning_rate": 2.8178534571723425e-05,
      "loss": 55.1138,
      "step": 14519
    },
    {
      "epoch": 14.53,
      "grad_norm": 8732.9716796875,
      "learning_rate": 2.8173374613003096e-05,
      "loss": 28.3013,
      "step": 14520
    },
    {
      "epoch": 14.54,
      "grad_norm": 4797.66796875,
      "learning_rate": 2.8168214654282764e-05,
      "loss": 48.5967,
      "step": 14521
    },
    {
      "epoch": 14.54,
      "grad_norm": 7384.15625,
      "learning_rate": 2.8163054695562435e-05,
      "loss": 51.0845,
      "step": 14522
    },
    {
      "epoch": 14.54,
      "grad_norm": 12721.5556640625,
      "learning_rate": 2.8157894736842106e-05,
      "loss": 44.6147,
      "step": 14523
    },
    {
      "epoch": 14.54,
      "grad_norm": 15187.8505859375,
      "learning_rate": 2.8152734778121774e-05,
      "loss": 51.2045,
      "step": 14524
    },
    {
      "epoch": 14.54,
      "grad_norm": 6767.2783203125,
      "learning_rate": 2.8147574819401445e-05,
      "loss": 44.5665,
      "step": 14525
    },
    {
      "epoch": 14.54,
      "grad_norm": 15682.767578125,
      "learning_rate": 2.8142414860681116e-05,
      "loss": 49.8329,
      "step": 14526
    },
    {
      "epoch": 14.54,
      "grad_norm": 9781.60546875,
      "learning_rate": 2.8137254901960784e-05,
      "loss": 50.1045,
      "step": 14527
    },
    {
      "epoch": 14.54,
      "grad_norm": 7466.44873046875,
      "learning_rate": 2.8132094943240455e-05,
      "loss": 56.073,
      "step": 14528
    },
    {
      "epoch": 14.54,
      "grad_norm": 36319.3984375,
      "learning_rate": 2.8126934984520126e-05,
      "loss": 49.8637,
      "step": 14529
    },
    {
      "epoch": 14.54,
      "grad_norm": 9398.8076171875,
      "learning_rate": 2.8121775025799797e-05,
      "loss": 40.9594,
      "step": 14530
    },
    {
      "epoch": 14.55,
      "grad_norm": 8222.8193359375,
      "learning_rate": 2.8116615067079465e-05,
      "loss": 55.5445,
      "step": 14531
    },
    {
      "epoch": 14.55,
      "grad_norm": 31847.572265625,
      "learning_rate": 2.8111455108359136e-05,
      "loss": 52.6531,
      "step": 14532
    },
    {
      "epoch": 14.55,
      "grad_norm": 3954.95556640625,
      "learning_rate": 2.8106295149638808e-05,
      "loss": 48.6853,
      "step": 14533
    },
    {
      "epoch": 14.55,
      "grad_norm": 7843.30029296875,
      "learning_rate": 2.8101135190918475e-05,
      "loss": 42.0202,
      "step": 14534
    },
    {
      "epoch": 14.55,
      "grad_norm": 25669.578125,
      "learning_rate": 2.8095975232198146e-05,
      "loss": 46.6894,
      "step": 14535
    },
    {
      "epoch": 14.55,
      "grad_norm": 8769.8515625,
      "learning_rate": 2.8090815273477818e-05,
      "loss": 35.9911,
      "step": 14536
    },
    {
      "epoch": 14.55,
      "grad_norm": 12145.9638671875,
      "learning_rate": 2.8085655314757482e-05,
      "loss": 41.5302,
      "step": 14537
    },
    {
      "epoch": 14.55,
      "grad_norm": 186768.59375,
      "learning_rate": 2.808049535603715e-05,
      "loss": 44.6321,
      "step": 14538
    },
    {
      "epoch": 14.55,
      "grad_norm": 30123.658203125,
      "learning_rate": 2.807533539731682e-05,
      "loss": 35.476,
      "step": 14539
    },
    {
      "epoch": 14.55,
      "grad_norm": 2016.6820068359375,
      "learning_rate": 2.8070175438596492e-05,
      "loss": 57.9426,
      "step": 14540
    },
    {
      "epoch": 14.56,
      "grad_norm": 3331.460693359375,
      "learning_rate": 2.806501547987616e-05,
      "loss": 55.4895,
      "step": 14541
    },
    {
      "epoch": 14.56,
      "grad_norm": 3730.541015625,
      "learning_rate": 2.805985552115583e-05,
      "loss": 60.6323,
      "step": 14542
    },
    {
      "epoch": 14.56,
      "grad_norm": 14677.7431640625,
      "learning_rate": 2.8054695562435502e-05,
      "loss": 32.2921,
      "step": 14543
    },
    {
      "epoch": 14.56,
      "grad_norm": 240197.8125,
      "learning_rate": 2.8049535603715173e-05,
      "loss": 38.7471,
      "step": 14544
    },
    {
      "epoch": 14.56,
      "grad_norm": 8773.4697265625,
      "learning_rate": 2.804437564499484e-05,
      "loss": 52.3456,
      "step": 14545
    },
    {
      "epoch": 14.56,
      "grad_norm": 50519.99609375,
      "learning_rate": 2.8039215686274512e-05,
      "loss": 41.8648,
      "step": 14546
    },
    {
      "epoch": 14.56,
      "grad_norm": 15614.0732421875,
      "learning_rate": 2.8034055727554183e-05,
      "loss": 40.7287,
      "step": 14547
    },
    {
      "epoch": 14.56,
      "grad_norm": 13411.8359375,
      "learning_rate": 2.802889576883385e-05,
      "loss": 50.5281,
      "step": 14548
    },
    {
      "epoch": 14.56,
      "grad_norm": 10767.3359375,
      "learning_rate": 2.8023735810113522e-05,
      "loss": 43.8481,
      "step": 14549
    },
    {
      "epoch": 14.56,
      "grad_norm": 69068.921875,
      "learning_rate": 2.8018575851393193e-05,
      "loss": 52.0191,
      "step": 14550
    },
    {
      "epoch": 14.57,
      "grad_norm": 28035.41796875,
      "learning_rate": 2.801341589267286e-05,
      "loss": 49.3495,
      "step": 14551
    },
    {
      "epoch": 14.57,
      "grad_norm": 7360.57177734375,
      "learning_rate": 2.8008255933952532e-05,
      "loss": 49.4461,
      "step": 14552
    },
    {
      "epoch": 14.57,
      "grad_norm": 61776.98828125,
      "learning_rate": 2.8003095975232203e-05,
      "loss": 31.2022,
      "step": 14553
    },
    {
      "epoch": 14.57,
      "grad_norm": 2113.2197265625,
      "learning_rate": 2.7997936016511867e-05,
      "loss": 56.2452,
      "step": 14554
    },
    {
      "epoch": 14.57,
      "grad_norm": 26038.29296875,
      "learning_rate": 2.7992776057791535e-05,
      "loss": 47.0913,
      "step": 14555
    },
    {
      "epoch": 14.57,
      "grad_norm": 24738.98046875,
      "learning_rate": 2.7987616099071206e-05,
      "loss": 47.8318,
      "step": 14556
    },
    {
      "epoch": 14.57,
      "grad_norm": 5009.24365234375,
      "learning_rate": 2.7982456140350877e-05,
      "loss": 42.3095,
      "step": 14557
    },
    {
      "epoch": 14.57,
      "grad_norm": 21597.869140625,
      "learning_rate": 2.7977296181630545e-05,
      "loss": 45.7433,
      "step": 14558
    },
    {
      "epoch": 14.57,
      "grad_norm": 9169.2890625,
      "learning_rate": 2.7972136222910216e-05,
      "loss": 50.5408,
      "step": 14559
    },
    {
      "epoch": 14.57,
      "grad_norm": 14385.255859375,
      "learning_rate": 2.7966976264189887e-05,
      "loss": 36.4371,
      "step": 14560
    },
    {
      "epoch": 14.58,
      "grad_norm": 357589.0,
      "learning_rate": 2.796181630546956e-05,
      "loss": 30.9685,
      "step": 14561
    },
    {
      "epoch": 14.58,
      "grad_norm": 12548.919921875,
      "learning_rate": 2.7956656346749226e-05,
      "loss": 48.0405,
      "step": 14562
    },
    {
      "epoch": 14.58,
      "grad_norm": 28985.736328125,
      "learning_rate": 2.7951496388028897e-05,
      "loss": 57.0142,
      "step": 14563
    },
    {
      "epoch": 14.58,
      "grad_norm": 25415.337890625,
      "learning_rate": 2.794633642930857e-05,
      "loss": 55.0163,
      "step": 14564
    },
    {
      "epoch": 14.58,
      "grad_norm": 12562.7587890625,
      "learning_rate": 2.7941176470588236e-05,
      "loss": 56.297,
      "step": 14565
    },
    {
      "epoch": 14.58,
      "grad_norm": 26928.51953125,
      "learning_rate": 2.7936016511867907e-05,
      "loss": 45.1807,
      "step": 14566
    },
    {
      "epoch": 14.58,
      "grad_norm": 10084.7431640625,
      "learning_rate": 2.793085655314758e-05,
      "loss": 48.337,
      "step": 14567
    },
    {
      "epoch": 14.58,
      "grad_norm": 122059.7890625,
      "learning_rate": 2.7925696594427246e-05,
      "loss": 43.333,
      "step": 14568
    },
    {
      "epoch": 14.58,
      "grad_norm": 1903.755615234375,
      "learning_rate": 2.7920536635706917e-05,
      "loss": 42.4769,
      "step": 14569
    },
    {
      "epoch": 14.58,
      "grad_norm": 4530.91015625,
      "learning_rate": 2.791537667698659e-05,
      "loss": 47.2127,
      "step": 14570
    },
    {
      "epoch": 14.59,
      "grad_norm": 4410.8154296875,
      "learning_rate": 2.7910216718266256e-05,
      "loss": 54.9644,
      "step": 14571
    },
    {
      "epoch": 14.59,
      "grad_norm": 14921.478515625,
      "learning_rate": 2.790505675954592e-05,
      "loss": 43.7604,
      "step": 14572
    },
    {
      "epoch": 14.59,
      "grad_norm": 10796.091796875,
      "learning_rate": 2.7899896800825592e-05,
      "loss": 17.5616,
      "step": 14573
    },
    {
      "epoch": 14.59,
      "grad_norm": 9779.2294921875,
      "learning_rate": 2.7894736842105263e-05,
      "loss": 51.8088,
      "step": 14574
    },
    {
      "epoch": 14.59,
      "grad_norm": 5178.91552734375,
      "learning_rate": 2.7889576883384934e-05,
      "loss": 40.9963,
      "step": 14575
    },
    {
      "epoch": 14.59,
      "grad_norm": 5224.265625,
      "learning_rate": 2.7884416924664602e-05,
      "loss": 44.7585,
      "step": 14576
    },
    {
      "epoch": 14.59,
      "grad_norm": 8261.3310546875,
      "learning_rate": 2.7879256965944273e-05,
      "loss": 49.5971,
      "step": 14577
    },
    {
      "epoch": 14.59,
      "grad_norm": 4965.60693359375,
      "learning_rate": 2.7874097007223944e-05,
      "loss": 53.4938,
      "step": 14578
    },
    {
      "epoch": 14.59,
      "grad_norm": 31031.591796875,
      "learning_rate": 2.7868937048503612e-05,
      "loss": 59.3918,
      "step": 14579
    },
    {
      "epoch": 14.59,
      "grad_norm": 283606.0,
      "learning_rate": 2.7863777089783283e-05,
      "loss": 51.1232,
      "step": 14580
    },
    {
      "epoch": 14.6,
      "grad_norm": 17483.322265625,
      "learning_rate": 2.7858617131062954e-05,
      "loss": 53.878,
      "step": 14581
    },
    {
      "epoch": 14.6,
      "grad_norm": 2272.788818359375,
      "learning_rate": 2.7853457172342622e-05,
      "loss": 42.1216,
      "step": 14582
    },
    {
      "epoch": 14.6,
      "grad_norm": 14170.009765625,
      "learning_rate": 2.7848297213622293e-05,
      "loss": 48.6183,
      "step": 14583
    },
    {
      "epoch": 14.6,
      "grad_norm": 12048.4228515625,
      "learning_rate": 2.7843137254901964e-05,
      "loss": 37.6707,
      "step": 14584
    },
    {
      "epoch": 14.6,
      "grad_norm": 3107.249755859375,
      "learning_rate": 2.7837977296181632e-05,
      "loss": 51.1893,
      "step": 14585
    },
    {
      "epoch": 14.6,
      "grad_norm": 7065.2666015625,
      "learning_rate": 2.7832817337461303e-05,
      "loss": 35.8354,
      "step": 14586
    },
    {
      "epoch": 14.6,
      "grad_norm": 108490.7109375,
      "learning_rate": 2.7827657378740974e-05,
      "loss": 34.3625,
      "step": 14587
    },
    {
      "epoch": 14.6,
      "grad_norm": 4243.51904296875,
      "learning_rate": 2.7822497420020642e-05,
      "loss": 37.2254,
      "step": 14588
    },
    {
      "epoch": 14.6,
      "grad_norm": 21983.068359375,
      "learning_rate": 2.7817337461300313e-05,
      "loss": 48.9275,
      "step": 14589
    },
    {
      "epoch": 14.6,
      "grad_norm": 30161.603515625,
      "learning_rate": 2.7812177502579977e-05,
      "loss": 47.8054,
      "step": 14590
    },
    {
      "epoch": 14.61,
      "grad_norm": 10387.857421875,
      "learning_rate": 2.780701754385965e-05,
      "loss": 40.8873,
      "step": 14591
    },
    {
      "epoch": 14.61,
      "grad_norm": 8078.9326171875,
      "learning_rate": 2.780185758513932e-05,
      "loss": 50.1564,
      "step": 14592
    },
    {
      "epoch": 14.61,
      "grad_norm": 4199.08251953125,
      "learning_rate": 2.7796697626418987e-05,
      "loss": 46.7548,
      "step": 14593
    },
    {
      "epoch": 14.61,
      "grad_norm": 20581.6953125,
      "learning_rate": 2.779153766769866e-05,
      "loss": 38.8636,
      "step": 14594
    },
    {
      "epoch": 14.61,
      "grad_norm": 5796.66015625,
      "learning_rate": 2.778637770897833e-05,
      "loss": 37.0119,
      "step": 14595
    },
    {
      "epoch": 14.61,
      "grad_norm": 2394.90478515625,
      "learning_rate": 2.7781217750257997e-05,
      "loss": 55.8835,
      "step": 14596
    },
    {
      "epoch": 14.61,
      "grad_norm": 4509.35986328125,
      "learning_rate": 2.777605779153767e-05,
      "loss": 48.1464,
      "step": 14597
    },
    {
      "epoch": 14.61,
      "grad_norm": 13214.0166015625,
      "learning_rate": 2.777089783281734e-05,
      "loss": 52.6799,
      "step": 14598
    },
    {
      "epoch": 14.61,
      "grad_norm": 11277.921875,
      "learning_rate": 2.7765737874097007e-05,
      "loss": 46.3544,
      "step": 14599
    },
    {
      "epoch": 14.61,
      "grad_norm": 7069.94580078125,
      "learning_rate": 2.776057791537668e-05,
      "loss": 37.6717,
      "step": 14600
    },
    {
      "epoch": 14.62,
      "grad_norm": 8671.853515625,
      "learning_rate": 2.775541795665635e-05,
      "loss": 47.6633,
      "step": 14601
    },
    {
      "epoch": 14.62,
      "grad_norm": 9774.6689453125,
      "learning_rate": 2.7750257997936017e-05,
      "loss": 47.6781,
      "step": 14602
    },
    {
      "epoch": 14.62,
      "grad_norm": 3361.91796875,
      "learning_rate": 2.774509803921569e-05,
      "loss": 55.7132,
      "step": 14603
    },
    {
      "epoch": 14.62,
      "grad_norm": 15383.7890625,
      "learning_rate": 2.773993808049536e-05,
      "loss": 47.6856,
      "step": 14604
    },
    {
      "epoch": 14.62,
      "grad_norm": 10702.9970703125,
      "learning_rate": 2.773477812177503e-05,
      "loss": 48.3683,
      "step": 14605
    },
    {
      "epoch": 14.62,
      "grad_norm": 20456.046875,
      "learning_rate": 2.77296181630547e-05,
      "loss": 41.6086,
      "step": 14606
    },
    {
      "epoch": 14.62,
      "grad_norm": 11691.2060546875,
      "learning_rate": 2.772445820433437e-05,
      "loss": 54.7087,
      "step": 14607
    },
    {
      "epoch": 14.62,
      "grad_norm": 1706.6046142578125,
      "learning_rate": 2.7719298245614034e-05,
      "loss": 55.3703,
      "step": 14608
    },
    {
      "epoch": 14.62,
      "grad_norm": 23933.1328125,
      "learning_rate": 2.7714138286893705e-05,
      "loss": 46.0737,
      "step": 14609
    },
    {
      "epoch": 14.62,
      "grad_norm": 8315.73828125,
      "learning_rate": 2.7708978328173373e-05,
      "loss": 52.4287,
      "step": 14610
    },
    {
      "epoch": 14.63,
      "grad_norm": 12356.2197265625,
      "learning_rate": 2.7703818369453044e-05,
      "loss": 45.7825,
      "step": 14611
    },
    {
      "epoch": 14.63,
      "grad_norm": 8736.849609375,
      "learning_rate": 2.7698658410732715e-05,
      "loss": 49.206,
      "step": 14612
    },
    {
      "epoch": 14.63,
      "grad_norm": 7800.48291015625,
      "learning_rate": 2.7693498452012383e-05,
      "loss": 55.6061,
      "step": 14613
    },
    {
      "epoch": 14.63,
      "grad_norm": 17224.439453125,
      "learning_rate": 2.7688338493292054e-05,
      "loss": 43.6958,
      "step": 14614
    },
    {
      "epoch": 14.63,
      "grad_norm": 21083.81640625,
      "learning_rate": 2.7683178534571725e-05,
      "loss": 36.9374,
      "step": 14615
    },
    {
      "epoch": 14.63,
      "grad_norm": 31634.98828125,
      "learning_rate": 2.7678018575851393e-05,
      "loss": 20.1449,
      "step": 14616
    },
    {
      "epoch": 14.63,
      "grad_norm": 12101.4931640625,
      "learning_rate": 2.7672858617131064e-05,
      "loss": 46.3357,
      "step": 14617
    },
    {
      "epoch": 14.63,
      "grad_norm": 13344.416015625,
      "learning_rate": 2.7667698658410735e-05,
      "loss": 42.3857,
      "step": 14618
    },
    {
      "epoch": 14.63,
      "grad_norm": 12122.7294921875,
      "learning_rate": 2.7662538699690406e-05,
      "loss": 48.0377,
      "step": 14619
    },
    {
      "epoch": 14.63,
      "grad_norm": 22455.029296875,
      "learning_rate": 2.7657378740970074e-05,
      "loss": 43.2902,
      "step": 14620
    },
    {
      "epoch": 14.64,
      "grad_norm": 107098.8359375,
      "learning_rate": 2.7652218782249745e-05,
      "loss": 45.0038,
      "step": 14621
    },
    {
      "epoch": 14.64,
      "grad_norm": 41368.546875,
      "learning_rate": 2.7647058823529416e-05,
      "loss": 48.2362,
      "step": 14622
    },
    {
      "epoch": 14.64,
      "grad_norm": 10825.470703125,
      "learning_rate": 2.7641898864809084e-05,
      "loss": 45.4486,
      "step": 14623
    },
    {
      "epoch": 14.64,
      "grad_norm": 5895.72265625,
      "learning_rate": 2.7636738906088755e-05,
      "loss": 44.7887,
      "step": 14624
    },
    {
      "epoch": 14.64,
      "grad_norm": 1267.071533203125,
      "learning_rate": 2.7631578947368426e-05,
      "loss": 46.5887,
      "step": 14625
    },
    {
      "epoch": 14.64,
      "grad_norm": 16014.763671875,
      "learning_rate": 2.762641898864809e-05,
      "loss": 45.6042,
      "step": 14626
    },
    {
      "epoch": 14.64,
      "grad_norm": 36672.2890625,
      "learning_rate": 2.7621259029927758e-05,
      "loss": 56.4279,
      "step": 14627
    },
    {
      "epoch": 14.64,
      "grad_norm": 8371.3388671875,
      "learning_rate": 2.761609907120743e-05,
      "loss": 50.7455,
      "step": 14628
    },
    {
      "epoch": 14.64,
      "grad_norm": 6497.87158203125,
      "learning_rate": 2.76109391124871e-05,
      "loss": 31.0028,
      "step": 14629
    },
    {
      "epoch": 14.64,
      "grad_norm": 59235.28125,
      "learning_rate": 2.7605779153766768e-05,
      "loss": 48.099,
      "step": 14630
    },
    {
      "epoch": 14.65,
      "grad_norm": 3505.427490234375,
      "learning_rate": 2.760061919504644e-05,
      "loss": 50.5786,
      "step": 14631
    },
    {
      "epoch": 14.65,
      "grad_norm": 8765.1474609375,
      "learning_rate": 2.759545923632611e-05,
      "loss": 45.6196,
      "step": 14632
    },
    {
      "epoch": 14.65,
      "grad_norm": 8648.01171875,
      "learning_rate": 2.759029927760578e-05,
      "loss": 49.3642,
      "step": 14633
    },
    {
      "epoch": 14.65,
      "grad_norm": 37764.375,
      "learning_rate": 2.758513931888545e-05,
      "loss": 41.0366,
      "step": 14634
    },
    {
      "epoch": 14.65,
      "grad_norm": 11588.0625,
      "learning_rate": 2.757997936016512e-05,
      "loss": 42.9403,
      "step": 14635
    },
    {
      "epoch": 14.65,
      "grad_norm": 5469.19287109375,
      "learning_rate": 2.757481940144479e-05,
      "loss": 40.3804,
      "step": 14636
    },
    {
      "epoch": 14.65,
      "grad_norm": 7417.90771484375,
      "learning_rate": 2.756965944272446e-05,
      "loss": 46.0799,
      "step": 14637
    },
    {
      "epoch": 14.65,
      "grad_norm": 28672.90234375,
      "learning_rate": 2.756449948400413e-05,
      "loss": 56.3288,
      "step": 14638
    },
    {
      "epoch": 14.65,
      "grad_norm": 2785.994140625,
      "learning_rate": 2.75593395252838e-05,
      "loss": 51.1092,
      "step": 14639
    },
    {
      "epoch": 14.65,
      "grad_norm": 38564.47265625,
      "learning_rate": 2.755417956656347e-05,
      "loss": 32.0157,
      "step": 14640
    },
    {
      "epoch": 14.66,
      "grad_norm": 143506.21875,
      "learning_rate": 2.754901960784314e-05,
      "loss": 49.6199,
      "step": 14641
    },
    {
      "epoch": 14.66,
      "grad_norm": 3851.5400390625,
      "learning_rate": 2.754385964912281e-05,
      "loss": 54.3901,
      "step": 14642
    },
    {
      "epoch": 14.66,
      "grad_norm": 2801.12744140625,
      "learning_rate": 2.753869969040248e-05,
      "loss": 53.752,
      "step": 14643
    },
    {
      "epoch": 14.66,
      "grad_norm": 2823.720703125,
      "learning_rate": 2.7533539731682144e-05,
      "loss": 52.5141,
      "step": 14644
    },
    {
      "epoch": 14.66,
      "grad_norm": 2672.748779296875,
      "learning_rate": 2.7528379772961815e-05,
      "loss": 44.7686,
      "step": 14645
    },
    {
      "epoch": 14.66,
      "grad_norm": 28557.1953125,
      "learning_rate": 2.7523219814241486e-05,
      "loss": 51.2529,
      "step": 14646
    },
    {
      "epoch": 14.66,
      "grad_norm": 4617.08642578125,
      "learning_rate": 2.7518059855521157e-05,
      "loss": 53.6045,
      "step": 14647
    },
    {
      "epoch": 14.66,
      "grad_norm": 4694.68359375,
      "learning_rate": 2.7512899896800825e-05,
      "loss": 43.0826,
      "step": 14648
    },
    {
      "epoch": 14.66,
      "grad_norm": 17926.76953125,
      "learning_rate": 2.7507739938080496e-05,
      "loss": 21.831,
      "step": 14649
    },
    {
      "epoch": 14.66,
      "grad_norm": 30832.080078125,
      "learning_rate": 2.7502579979360167e-05,
      "loss": 56.8986,
      "step": 14650
    },
    {
      "epoch": 14.67,
      "grad_norm": 8192.935546875,
      "learning_rate": 2.7497420020639835e-05,
      "loss": 56.5603,
      "step": 14651
    },
    {
      "epoch": 14.67,
      "grad_norm": 35282.49609375,
      "learning_rate": 2.7492260061919506e-05,
      "loss": 36.2444,
      "step": 14652
    },
    {
      "epoch": 14.67,
      "grad_norm": 44024.5546875,
      "learning_rate": 2.7487100103199177e-05,
      "loss": 53.738,
      "step": 14653
    },
    {
      "epoch": 14.67,
      "grad_norm": 65517.05859375,
      "learning_rate": 2.7481940144478845e-05,
      "loss": 44.1915,
      "step": 14654
    },
    {
      "epoch": 14.67,
      "grad_norm": 119887.296875,
      "learning_rate": 2.7476780185758516e-05,
      "loss": 44.173,
      "step": 14655
    },
    {
      "epoch": 14.67,
      "grad_norm": 53087.18359375,
      "learning_rate": 2.7471620227038187e-05,
      "loss": 34.5058,
      "step": 14656
    },
    {
      "epoch": 14.67,
      "grad_norm": 114430.78125,
      "learning_rate": 2.7466460268317855e-05,
      "loss": 56.0352,
      "step": 14657
    },
    {
      "epoch": 14.67,
      "grad_norm": 666308.25,
      "learning_rate": 2.7461300309597526e-05,
      "loss": 41.6293,
      "step": 14658
    },
    {
      "epoch": 14.67,
      "grad_norm": 10592.2646484375,
      "learning_rate": 2.7456140350877197e-05,
      "loss": 53.1261,
      "step": 14659
    },
    {
      "epoch": 14.67,
      "grad_norm": 27567.49609375,
      "learning_rate": 2.7450980392156865e-05,
      "loss": 60.6545,
      "step": 14660
    },
    {
      "epoch": 14.68,
      "grad_norm": 3011.059326171875,
      "learning_rate": 2.7445820433436536e-05,
      "loss": 42.3096,
      "step": 14661
    },
    {
      "epoch": 14.68,
      "grad_norm": 5176.240234375,
      "learning_rate": 2.74406604747162e-05,
      "loss": 48.3499,
      "step": 14662
    },
    {
      "epoch": 14.68,
      "grad_norm": 11260.404296875,
      "learning_rate": 2.743550051599587e-05,
      "loss": 50.9983,
      "step": 14663
    },
    {
      "epoch": 14.68,
      "grad_norm": 6338.94384765625,
      "learning_rate": 2.7430340557275543e-05,
      "loss": 44.4425,
      "step": 14664
    },
    {
      "epoch": 14.68,
      "grad_norm": 8382.50390625,
      "learning_rate": 2.742518059855521e-05,
      "loss": 50.7874,
      "step": 14665
    },
    {
      "epoch": 14.68,
      "grad_norm": 5617.9716796875,
      "learning_rate": 2.742002063983488e-05,
      "loss": 54.0293,
      "step": 14666
    },
    {
      "epoch": 14.68,
      "grad_norm": 14648.865234375,
      "learning_rate": 2.7414860681114553e-05,
      "loss": 48.7678,
      "step": 14667
    },
    {
      "epoch": 14.68,
      "grad_norm": 11458.029296875,
      "learning_rate": 2.740970072239422e-05,
      "loss": 57.5606,
      "step": 14668
    },
    {
      "epoch": 14.68,
      "grad_norm": 6315.52001953125,
      "learning_rate": 2.740454076367389e-05,
      "loss": 53.5489,
      "step": 14669
    },
    {
      "epoch": 14.68,
      "grad_norm": 34359.9765625,
      "learning_rate": 2.7399380804953563e-05,
      "loss": 33.5671,
      "step": 14670
    },
    {
      "epoch": 14.69,
      "grad_norm": 4686.8095703125,
      "learning_rate": 2.739422084623323e-05,
      "loss": 46.6042,
      "step": 14671
    },
    {
      "epoch": 14.69,
      "grad_norm": 25975.3515625,
      "learning_rate": 2.73890608875129e-05,
      "loss": 46.6912,
      "step": 14672
    },
    {
      "epoch": 14.69,
      "grad_norm": 96858.3828125,
      "learning_rate": 2.7383900928792573e-05,
      "loss": 49.57,
      "step": 14673
    },
    {
      "epoch": 14.69,
      "grad_norm": 5115.826171875,
      "learning_rate": 2.737874097007224e-05,
      "loss": 47.0805,
      "step": 14674
    },
    {
      "epoch": 14.69,
      "grad_norm": 87994.0703125,
      "learning_rate": 2.737358101135191e-05,
      "loss": 56.9118,
      "step": 14675
    },
    {
      "epoch": 14.69,
      "grad_norm": 15088.771484375,
      "learning_rate": 2.7368421052631583e-05,
      "loss": 49.4713,
      "step": 14676
    },
    {
      "epoch": 14.69,
      "grad_norm": 17034.85546875,
      "learning_rate": 2.7363261093911254e-05,
      "loss": 40.3769,
      "step": 14677
    },
    {
      "epoch": 14.69,
      "grad_norm": 2122.152099609375,
      "learning_rate": 2.735810113519092e-05,
      "loss": 57.1323,
      "step": 14678
    },
    {
      "epoch": 14.69,
      "grad_norm": 62494.02734375,
      "learning_rate": 2.7352941176470593e-05,
      "loss": 41.1036,
      "step": 14679
    },
    {
      "epoch": 14.69,
      "grad_norm": 131020.046875,
      "learning_rate": 2.7347781217750257e-05,
      "loss": 42.7838,
      "step": 14680
    },
    {
      "epoch": 14.7,
      "grad_norm": 5539.64501953125,
      "learning_rate": 2.7342621259029928e-05,
      "loss": 56.0715,
      "step": 14681
    },
    {
      "epoch": 14.7,
      "grad_norm": 5192.91064453125,
      "learning_rate": 2.7337461300309596e-05,
      "loss": 45.09,
      "step": 14682
    },
    {
      "epoch": 14.7,
      "grad_norm": 65396.14453125,
      "learning_rate": 2.7332301341589267e-05,
      "loss": 52.2958,
      "step": 14683
    },
    {
      "epoch": 14.7,
      "grad_norm": 5621.36572265625,
      "learning_rate": 2.7327141382868938e-05,
      "loss": 50.1125,
      "step": 14684
    },
    {
      "epoch": 14.7,
      "grad_norm": 9911.751953125,
      "learning_rate": 2.7321981424148606e-05,
      "loss": 48.833,
      "step": 14685
    },
    {
      "epoch": 14.7,
      "grad_norm": 3635.624755859375,
      "learning_rate": 2.7316821465428277e-05,
      "loss": 48.0721,
      "step": 14686
    },
    {
      "epoch": 14.7,
      "grad_norm": 6531.5732421875,
      "learning_rate": 2.7311661506707948e-05,
      "loss": 31.8183,
      "step": 14687
    },
    {
      "epoch": 14.7,
      "grad_norm": 4028.6376953125,
      "learning_rate": 2.7306501547987616e-05,
      "loss": 55.0993,
      "step": 14688
    },
    {
      "epoch": 14.7,
      "grad_norm": 14933.728515625,
      "learning_rate": 2.7301341589267287e-05,
      "loss": 46.6889,
      "step": 14689
    },
    {
      "epoch": 14.7,
      "grad_norm": 79068.96875,
      "learning_rate": 2.7296181630546958e-05,
      "loss": 46.9479,
      "step": 14690
    },
    {
      "epoch": 14.71,
      "grad_norm": 13454.896484375,
      "learning_rate": 2.729102167182663e-05,
      "loss": 59.061,
      "step": 14691
    },
    {
      "epoch": 14.71,
      "grad_norm": 364954.15625,
      "learning_rate": 2.7285861713106297e-05,
      "loss": 35.8189,
      "step": 14692
    },
    {
      "epoch": 14.71,
      "grad_norm": 5069.115234375,
      "learning_rate": 2.7280701754385968e-05,
      "loss": 50.4111,
      "step": 14693
    },
    {
      "epoch": 14.71,
      "grad_norm": 80452.9375,
      "learning_rate": 2.727554179566564e-05,
      "loss": 52.7193,
      "step": 14694
    },
    {
      "epoch": 14.71,
      "grad_norm": 32084.37109375,
      "learning_rate": 2.7270381836945307e-05,
      "loss": 42.541,
      "step": 14695
    },
    {
      "epoch": 14.71,
      "grad_norm": 60827.66015625,
      "learning_rate": 2.7265221878224978e-05,
      "loss": 51.8574,
      "step": 14696
    },
    {
      "epoch": 14.71,
      "grad_norm": 25180.36328125,
      "learning_rate": 2.7260061919504642e-05,
      "loss": 53.3647,
      "step": 14697
    },
    {
      "epoch": 14.71,
      "grad_norm": 29985.615234375,
      "learning_rate": 2.7254901960784314e-05,
      "loss": 53.4796,
      "step": 14698
    },
    {
      "epoch": 14.71,
      "grad_norm": 4275.8447265625,
      "learning_rate": 2.724974200206398e-05,
      "loss": 52.1385,
      "step": 14699
    },
    {
      "epoch": 14.71,
      "grad_norm": 77147.2734375,
      "learning_rate": 2.7244582043343652e-05,
      "loss": 50.4652,
      "step": 14700
    },
    {
      "epoch": 14.72,
      "grad_norm": 20470.599609375,
      "learning_rate": 2.7239422084623324e-05,
      "loss": 36.7982,
      "step": 14701
    },
    {
      "epoch": 14.72,
      "grad_norm": 5098.17626953125,
      "learning_rate": 2.723426212590299e-05,
      "loss": 58.9677,
      "step": 14702
    },
    {
      "epoch": 14.72,
      "grad_norm": 16452.669921875,
      "learning_rate": 2.7229102167182662e-05,
      "loss": 48.814,
      "step": 14703
    },
    {
      "epoch": 14.72,
      "grad_norm": 3042.185791015625,
      "learning_rate": 2.7223942208462334e-05,
      "loss": 47.8153,
      "step": 14704
    },
    {
      "epoch": 14.72,
      "grad_norm": 9952.361328125,
      "learning_rate": 2.7218782249742005e-05,
      "loss": 44.3999,
      "step": 14705
    },
    {
      "epoch": 14.72,
      "grad_norm": 311374.5,
      "learning_rate": 2.7213622291021672e-05,
      "loss": 57.7813,
      "step": 14706
    },
    {
      "epoch": 14.72,
      "grad_norm": 2303.571533203125,
      "learning_rate": 2.7208462332301344e-05,
      "loss": 36.3474,
      "step": 14707
    },
    {
      "epoch": 14.72,
      "grad_norm": 6296.7744140625,
      "learning_rate": 2.7203302373581015e-05,
      "loss": 40.3131,
      "step": 14708
    },
    {
      "epoch": 14.72,
      "grad_norm": 41265.49609375,
      "learning_rate": 2.7198142414860682e-05,
      "loss": 46.8435,
      "step": 14709
    },
    {
      "epoch": 14.72,
      "grad_norm": 59144.3125,
      "learning_rate": 2.7192982456140354e-05,
      "loss": 43.5347,
      "step": 14710
    },
    {
      "epoch": 14.73,
      "grad_norm": 1925.5618896484375,
      "learning_rate": 2.7187822497420025e-05,
      "loss": 46.4697,
      "step": 14711
    },
    {
      "epoch": 14.73,
      "grad_norm": 1473.4921875,
      "learning_rate": 2.7182662538699692e-05,
      "loss": 53.4663,
      "step": 14712
    },
    {
      "epoch": 14.73,
      "grad_norm": 5453.8984375,
      "learning_rate": 2.7177502579979364e-05,
      "loss": 50.651,
      "step": 14713
    },
    {
      "epoch": 14.73,
      "grad_norm": 19800.509765625,
      "learning_rate": 2.7172342621259035e-05,
      "loss": 44.6294,
      "step": 14714
    },
    {
      "epoch": 14.73,
      "grad_norm": 3448.40185546875,
      "learning_rate": 2.71671826625387e-05,
      "loss": 48.3323,
      "step": 14715
    },
    {
      "epoch": 14.73,
      "grad_norm": 2681.853515625,
      "learning_rate": 2.7162022703818367e-05,
      "loss": 50.9569,
      "step": 14716
    },
    {
      "epoch": 14.73,
      "grad_norm": 9337.2548828125,
      "learning_rate": 2.7156862745098038e-05,
      "loss": 44.471,
      "step": 14717
    },
    {
      "epoch": 14.73,
      "grad_norm": 6325.51708984375,
      "learning_rate": 2.715170278637771e-05,
      "loss": 54.89,
      "step": 14718
    },
    {
      "epoch": 14.73,
      "grad_norm": 9873.453125,
      "learning_rate": 2.714654282765738e-05,
      "loss": 60.6671,
      "step": 14719
    },
    {
      "epoch": 14.73,
      "grad_norm": 8087.068359375,
      "learning_rate": 2.7141382868937048e-05,
      "loss": 40.2134,
      "step": 14720
    },
    {
      "epoch": 14.74,
      "grad_norm": 17772.685546875,
      "learning_rate": 2.713622291021672e-05,
      "loss": 28.0681,
      "step": 14721
    },
    {
      "epoch": 14.74,
      "grad_norm": 21292.78515625,
      "learning_rate": 2.713106295149639e-05,
      "loss": 47.6808,
      "step": 14722
    },
    {
      "epoch": 14.74,
      "grad_norm": 4179.28955078125,
      "learning_rate": 2.7125902992776058e-05,
      "loss": 42.1416,
      "step": 14723
    },
    {
      "epoch": 14.74,
      "grad_norm": 11245.7158203125,
      "learning_rate": 2.712074303405573e-05,
      "loss": 54.7305,
      "step": 14724
    },
    {
      "epoch": 14.74,
      "grad_norm": 26074.1875,
      "learning_rate": 2.71155830753354e-05,
      "loss": 35.2061,
      "step": 14725
    },
    {
      "epoch": 14.74,
      "grad_norm": 3715.42333984375,
      "learning_rate": 2.7110423116615068e-05,
      "loss": 47.477,
      "step": 14726
    },
    {
      "epoch": 14.74,
      "grad_norm": 2361.140625,
      "learning_rate": 2.710526315789474e-05,
      "loss": 45.6101,
      "step": 14727
    },
    {
      "epoch": 14.74,
      "grad_norm": 60952.4140625,
      "learning_rate": 2.710010319917441e-05,
      "loss": 58.1653,
      "step": 14728
    },
    {
      "epoch": 14.74,
      "grad_norm": 8783.634765625,
      "learning_rate": 2.7094943240454078e-05,
      "loss": 47.946,
      "step": 14729
    },
    {
      "epoch": 14.74,
      "grad_norm": 19580.572265625,
      "learning_rate": 2.708978328173375e-05,
      "loss": 48.1452,
      "step": 14730
    },
    {
      "epoch": 14.75,
      "grad_norm": 6887.708984375,
      "learning_rate": 2.708462332301342e-05,
      "loss": 34.9525,
      "step": 14731
    },
    {
      "epoch": 14.75,
      "grad_norm": 43521.86328125,
      "learning_rate": 2.7079463364293088e-05,
      "loss": 57.1975,
      "step": 14732
    },
    {
      "epoch": 14.75,
      "grad_norm": 12788.68359375,
      "learning_rate": 2.7074303405572752e-05,
      "loss": 47.8082,
      "step": 14733
    },
    {
      "epoch": 14.75,
      "grad_norm": 12388.263671875,
      "learning_rate": 2.7069143446852423e-05,
      "loss": 53.3018,
      "step": 14734
    },
    {
      "epoch": 14.75,
      "grad_norm": 23089.509765625,
      "learning_rate": 2.7063983488132095e-05,
      "loss": 53.8791,
      "step": 14735
    },
    {
      "epoch": 14.75,
      "grad_norm": 60641.89453125,
      "learning_rate": 2.7058823529411766e-05,
      "loss": 49.5924,
      "step": 14736
    },
    {
      "epoch": 14.75,
      "grad_norm": 58906.578125,
      "learning_rate": 2.7053663570691433e-05,
      "loss": 50.6855,
      "step": 14737
    },
    {
      "epoch": 14.75,
      "grad_norm": 55254.3515625,
      "learning_rate": 2.7048503611971105e-05,
      "loss": 44.0058,
      "step": 14738
    },
    {
      "epoch": 14.75,
      "grad_norm": 16300.86328125,
      "learning_rate": 2.7043343653250776e-05,
      "loss": 44.4278,
      "step": 14739
    },
    {
      "epoch": 14.75,
      "grad_norm": 1738.6639404296875,
      "learning_rate": 2.7038183694530443e-05,
      "loss": 52.3113,
      "step": 14740
    },
    {
      "epoch": 14.76,
      "grad_norm": 13035.73828125,
      "learning_rate": 2.7033023735810115e-05,
      "loss": 45.9156,
      "step": 14741
    },
    {
      "epoch": 14.76,
      "grad_norm": 29311.75390625,
      "learning_rate": 2.7027863777089786e-05,
      "loss": 54.8512,
      "step": 14742
    },
    {
      "epoch": 14.76,
      "grad_norm": 3346.831298828125,
      "learning_rate": 2.7022703818369453e-05,
      "loss": 51.5016,
      "step": 14743
    },
    {
      "epoch": 14.76,
      "grad_norm": 7227.09765625,
      "learning_rate": 2.7017543859649125e-05,
      "loss": 36.928,
      "step": 14744
    },
    {
      "epoch": 14.76,
      "grad_norm": 5801.14453125,
      "learning_rate": 2.7012383900928796e-05,
      "loss": 39.3196,
      "step": 14745
    },
    {
      "epoch": 14.76,
      "grad_norm": 8237.658203125,
      "learning_rate": 2.7007223942208463e-05,
      "loss": 37.9678,
      "step": 14746
    },
    {
      "epoch": 14.76,
      "grad_norm": 9165.0048828125,
      "learning_rate": 2.7002063983488135e-05,
      "loss": 47.72,
      "step": 14747
    },
    {
      "epoch": 14.76,
      "grad_norm": 4722.65283203125,
      "learning_rate": 2.6996904024767806e-05,
      "loss": 59.5543,
      "step": 14748
    },
    {
      "epoch": 14.76,
      "grad_norm": 7008.40576171875,
      "learning_rate": 2.6991744066047477e-05,
      "loss": 54.0708,
      "step": 14749
    },
    {
      "epoch": 14.76,
      "grad_norm": 5227.05224609375,
      "learning_rate": 2.6986584107327145e-05,
      "loss": 33.8031,
      "step": 14750
    },
    {
      "epoch": 14.77,
      "grad_norm": 136729.15625,
      "learning_rate": 2.698142414860681e-05,
      "loss": 39.6162,
      "step": 14751
    },
    {
      "epoch": 14.77,
      "grad_norm": 8911.7236328125,
      "learning_rate": 2.697626418988648e-05,
      "loss": 32.6054,
      "step": 14752
    },
    {
      "epoch": 14.77,
      "grad_norm": 4724.9208984375,
      "learning_rate": 2.697110423116615e-05,
      "loss": 47.9865,
      "step": 14753
    },
    {
      "epoch": 14.77,
      "grad_norm": 91193.5390625,
      "learning_rate": 2.696594427244582e-05,
      "loss": 51.0561,
      "step": 14754
    },
    {
      "epoch": 14.77,
      "grad_norm": 9084.7802734375,
      "learning_rate": 2.696078431372549e-05,
      "loss": 52.9849,
      "step": 14755
    },
    {
      "epoch": 14.77,
      "grad_norm": 24668.193359375,
      "learning_rate": 2.695562435500516e-05,
      "loss": 30.313,
      "step": 14756
    },
    {
      "epoch": 14.77,
      "grad_norm": 14568.525390625,
      "learning_rate": 2.695046439628483e-05,
      "loss": 57.0299,
      "step": 14757
    },
    {
      "epoch": 14.77,
      "grad_norm": 15333.5546875,
      "learning_rate": 2.69453044375645e-05,
      "loss": 44.4723,
      "step": 14758
    },
    {
      "epoch": 14.77,
      "grad_norm": 16522.498046875,
      "learning_rate": 2.694014447884417e-05,
      "loss": 48.2096,
      "step": 14759
    },
    {
      "epoch": 14.77,
      "grad_norm": 19307.20703125,
      "learning_rate": 2.693498452012384e-05,
      "loss": 58.0842,
      "step": 14760
    },
    {
      "epoch": 14.78,
      "grad_norm": 4050.13916015625,
      "learning_rate": 2.692982456140351e-05,
      "loss": 54.0653,
      "step": 14761
    },
    {
      "epoch": 14.78,
      "grad_norm": 4470.1357421875,
      "learning_rate": 2.692466460268318e-05,
      "loss": 49.7309,
      "step": 14762
    },
    {
      "epoch": 14.78,
      "grad_norm": 17393.9140625,
      "learning_rate": 2.691950464396285e-05,
      "loss": 52.3479,
      "step": 14763
    },
    {
      "epoch": 14.78,
      "grad_norm": 26541.349609375,
      "learning_rate": 2.691434468524252e-05,
      "loss": 50.2535,
      "step": 14764
    },
    {
      "epoch": 14.78,
      "grad_norm": 59668.01953125,
      "learning_rate": 2.690918472652219e-05,
      "loss": 53.1257,
      "step": 14765
    },
    {
      "epoch": 14.78,
      "grad_norm": 8301.1826171875,
      "learning_rate": 2.6904024767801862e-05,
      "loss": 44.6227,
      "step": 14766
    },
    {
      "epoch": 14.78,
      "grad_norm": 6704.96728515625,
      "learning_rate": 2.689886480908153e-05,
      "loss": 46.8245,
      "step": 14767
    },
    {
      "epoch": 14.78,
      "grad_norm": 13547.076171875,
      "learning_rate": 2.68937048503612e-05,
      "loss": 51.5381,
      "step": 14768
    },
    {
      "epoch": 14.78,
      "grad_norm": 4745.3662109375,
      "learning_rate": 2.6888544891640866e-05,
      "loss": 44.8005,
      "step": 14769
    },
    {
      "epoch": 14.78,
      "grad_norm": 23310.6171875,
      "learning_rate": 2.6883384932920537e-05,
      "loss": 42.2427,
      "step": 14770
    },
    {
      "epoch": 14.79,
      "grad_norm": 9831.265625,
      "learning_rate": 2.6878224974200204e-05,
      "loss": 52.932,
      "step": 14771
    },
    {
      "epoch": 14.79,
      "grad_norm": 11585.66796875,
      "learning_rate": 2.6873065015479876e-05,
      "loss": 50.9306,
      "step": 14772
    },
    {
      "epoch": 14.79,
      "grad_norm": 35461.7578125,
      "learning_rate": 2.6867905056759547e-05,
      "loss": 14.6316,
      "step": 14773
    },
    {
      "epoch": 14.79,
      "grad_norm": 5088.359375,
      "learning_rate": 2.6862745098039214e-05,
      "loss": 59.5417,
      "step": 14774
    },
    {
      "epoch": 14.79,
      "grad_norm": 6490.8876953125,
      "learning_rate": 2.6857585139318886e-05,
      "loss": 49.691,
      "step": 14775
    },
    {
      "epoch": 14.79,
      "grad_norm": 11795.4169921875,
      "learning_rate": 2.6852425180598557e-05,
      "loss": 44.4981,
      "step": 14776
    },
    {
      "epoch": 14.79,
      "grad_norm": 31416.986328125,
      "learning_rate": 2.6847265221878224e-05,
      "loss": 48.4768,
      "step": 14777
    },
    {
      "epoch": 14.79,
      "grad_norm": 39968.34765625,
      "learning_rate": 2.6842105263157896e-05,
      "loss": 52.7948,
      "step": 14778
    },
    {
      "epoch": 14.79,
      "grad_norm": 8862.3916015625,
      "learning_rate": 2.6836945304437567e-05,
      "loss": 21.5276,
      "step": 14779
    },
    {
      "epoch": 14.79,
      "grad_norm": 1325.509033203125,
      "learning_rate": 2.6831785345717238e-05,
      "loss": 48.0085,
      "step": 14780
    },
    {
      "epoch": 14.8,
      "grad_norm": 9808.4208984375,
      "learning_rate": 2.6826625386996906e-05,
      "loss": 37.2773,
      "step": 14781
    },
    {
      "epoch": 14.8,
      "grad_norm": 10200.6904296875,
      "learning_rate": 2.6821465428276577e-05,
      "loss": 39.4845,
      "step": 14782
    },
    {
      "epoch": 14.8,
      "grad_norm": 11257.6748046875,
      "learning_rate": 2.6816305469556248e-05,
      "loss": 37.841,
      "step": 14783
    },
    {
      "epoch": 14.8,
      "grad_norm": 19032.501953125,
      "learning_rate": 2.6811145510835916e-05,
      "loss": 49.4083,
      "step": 14784
    },
    {
      "epoch": 14.8,
      "grad_norm": 6531.0498046875,
      "learning_rate": 2.6805985552115587e-05,
      "loss": 45.9826,
      "step": 14785
    },
    {
      "epoch": 14.8,
      "grad_norm": 49795.4921875,
      "learning_rate": 2.6800825593395258e-05,
      "loss": 45.6917,
      "step": 14786
    },
    {
      "epoch": 14.8,
      "grad_norm": 4486.021484375,
      "learning_rate": 2.6795665634674922e-05,
      "loss": 57.4162,
      "step": 14787
    },
    {
      "epoch": 14.8,
      "grad_norm": 3739.761962890625,
      "learning_rate": 2.679050567595459e-05,
      "loss": 48.7508,
      "step": 14788
    },
    {
      "epoch": 14.8,
      "grad_norm": 15018.9501953125,
      "learning_rate": 2.678534571723426e-05,
      "loss": 58.1995,
      "step": 14789
    },
    {
      "epoch": 14.8,
      "grad_norm": 9880.1484375,
      "learning_rate": 2.6780185758513932e-05,
      "loss": 38.3799,
      "step": 14790
    },
    {
      "epoch": 14.81,
      "grad_norm": 15294.9150390625,
      "learning_rate": 2.67750257997936e-05,
      "loss": 49.9267,
      "step": 14791
    },
    {
      "epoch": 14.81,
      "grad_norm": 4854.04345703125,
      "learning_rate": 2.676986584107327e-05,
      "loss": 51.5158,
      "step": 14792
    },
    {
      "epoch": 14.81,
      "grad_norm": 61920.94140625,
      "learning_rate": 2.6764705882352942e-05,
      "loss": 28.314,
      "step": 14793
    },
    {
      "epoch": 14.81,
      "grad_norm": 19865.666015625,
      "learning_rate": 2.6759545923632613e-05,
      "loss": 50.7203,
      "step": 14794
    },
    {
      "epoch": 14.81,
      "grad_norm": 56901.390625,
      "learning_rate": 2.675438596491228e-05,
      "loss": 49.4812,
      "step": 14795
    },
    {
      "epoch": 14.81,
      "grad_norm": 12406.83203125,
      "learning_rate": 2.6749226006191952e-05,
      "loss": 58.6109,
      "step": 14796
    },
    {
      "epoch": 14.81,
      "grad_norm": 53805.28515625,
      "learning_rate": 2.6744066047471623e-05,
      "loss": 33.5313,
      "step": 14797
    },
    {
      "epoch": 14.81,
      "grad_norm": 23121.5859375,
      "learning_rate": 2.673890608875129e-05,
      "loss": 47.3074,
      "step": 14798
    },
    {
      "epoch": 14.81,
      "grad_norm": 112224.625,
      "learning_rate": 2.6733746130030962e-05,
      "loss": 44.9247,
      "step": 14799
    },
    {
      "epoch": 14.81,
      "grad_norm": 32874.63671875,
      "learning_rate": 2.6728586171310633e-05,
      "loss": 39.6309,
      "step": 14800
    },
    {
      "epoch": 14.82,
      "grad_norm": 6041.34912109375,
      "learning_rate": 2.67234262125903e-05,
      "loss": 44.0398,
      "step": 14801
    },
    {
      "epoch": 14.82,
      "grad_norm": 4080.63623046875,
      "learning_rate": 2.6718266253869972e-05,
      "loss": 48.1238,
      "step": 14802
    },
    {
      "epoch": 14.82,
      "grad_norm": 14367.62890625,
      "learning_rate": 2.6713106295149643e-05,
      "loss": 43.3647,
      "step": 14803
    },
    {
      "epoch": 14.82,
      "grad_norm": 9564.724609375,
      "learning_rate": 2.670794633642931e-05,
      "loss": 46.2442,
      "step": 14804
    },
    {
      "epoch": 14.82,
      "grad_norm": 22370.84765625,
      "learning_rate": 2.6702786377708975e-05,
      "loss": 36.0405,
      "step": 14805
    },
    {
      "epoch": 14.82,
      "grad_norm": 3208.106689453125,
      "learning_rate": 2.6697626418988647e-05,
      "loss": 53.2532,
      "step": 14806
    },
    {
      "epoch": 14.82,
      "grad_norm": 2772.46533203125,
      "learning_rate": 2.6692466460268318e-05,
      "loss": 53.0613,
      "step": 14807
    },
    {
      "epoch": 14.82,
      "grad_norm": 4149.02490234375,
      "learning_rate": 2.668730650154799e-05,
      "loss": 55.1568,
      "step": 14808
    },
    {
      "epoch": 14.82,
      "grad_norm": 8198.248046875,
      "learning_rate": 2.6682146542827657e-05,
      "loss": 46.5412,
      "step": 14809
    },
    {
      "epoch": 14.82,
      "grad_norm": 5426.95703125,
      "learning_rate": 2.6676986584107328e-05,
      "loss": 55.3726,
      "step": 14810
    },
    {
      "epoch": 14.83,
      "grad_norm": 20540.484375,
      "learning_rate": 2.6671826625387e-05,
      "loss": 23.206,
      "step": 14811
    },
    {
      "epoch": 14.83,
      "grad_norm": 22959.611328125,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 43.0885,
      "step": 14812
    },
    {
      "epoch": 14.83,
      "grad_norm": 9603.0400390625,
      "learning_rate": 2.6661506707946338e-05,
      "loss": 59.1676,
      "step": 14813
    },
    {
      "epoch": 14.83,
      "grad_norm": 3757.0302734375,
      "learning_rate": 2.665634674922601e-05,
      "loss": 55.9953,
      "step": 14814
    },
    {
      "epoch": 14.83,
      "grad_norm": 9378.5927734375,
      "learning_rate": 2.6651186790505677e-05,
      "loss": 55.634,
      "step": 14815
    },
    {
      "epoch": 14.83,
      "grad_norm": 61945.23046875,
      "learning_rate": 2.6646026831785348e-05,
      "loss": 40.3127,
      "step": 14816
    },
    {
      "epoch": 14.83,
      "grad_norm": 10299.564453125,
      "learning_rate": 2.664086687306502e-05,
      "loss": 49.161,
      "step": 14817
    },
    {
      "epoch": 14.83,
      "grad_norm": 72399.8984375,
      "learning_rate": 2.6635706914344687e-05,
      "loss": 55.9906,
      "step": 14818
    },
    {
      "epoch": 14.83,
      "grad_norm": 4901.49072265625,
      "learning_rate": 2.6630546955624358e-05,
      "loss": 51.7588,
      "step": 14819
    },
    {
      "epoch": 14.83,
      "grad_norm": 6322.33203125,
      "learning_rate": 2.662538699690403e-05,
      "loss": 57.8755,
      "step": 14820
    },
    {
      "epoch": 14.84,
      "grad_norm": 20457.224609375,
      "learning_rate": 2.6620227038183697e-05,
      "loss": 43.9781,
      "step": 14821
    },
    {
      "epoch": 14.84,
      "grad_norm": 8176.7919921875,
      "learning_rate": 2.6615067079463368e-05,
      "loss": 46.9826,
      "step": 14822
    },
    {
      "epoch": 14.84,
      "grad_norm": 14804.849609375,
      "learning_rate": 2.6609907120743032e-05,
      "loss": 50.6457,
      "step": 14823
    },
    {
      "epoch": 14.84,
      "grad_norm": 4007.64208984375,
      "learning_rate": 2.6604747162022703e-05,
      "loss": 48.9232,
      "step": 14824
    },
    {
      "epoch": 14.84,
      "grad_norm": 4506.79833984375,
      "learning_rate": 2.6599587203302374e-05,
      "loss": 51.6181,
      "step": 14825
    },
    {
      "epoch": 14.84,
      "grad_norm": 114570.8984375,
      "learning_rate": 2.6594427244582042e-05,
      "loss": 39.1766,
      "step": 14826
    },
    {
      "epoch": 14.84,
      "grad_norm": 20645.275390625,
      "learning_rate": 2.6589267285861713e-05,
      "loss": 55.3046,
      "step": 14827
    },
    {
      "epoch": 14.84,
      "grad_norm": 38810.7890625,
      "learning_rate": 2.6584107327141384e-05,
      "loss": 45.4313,
      "step": 14828
    },
    {
      "epoch": 14.84,
      "grad_norm": 5933.13330078125,
      "learning_rate": 2.6578947368421052e-05,
      "loss": 52.3519,
      "step": 14829
    },
    {
      "epoch": 14.84,
      "grad_norm": 18171.29296875,
      "learning_rate": 2.6573787409700723e-05,
      "loss": 57.043,
      "step": 14830
    },
    {
      "epoch": 14.85,
      "grad_norm": 31642.515625,
      "learning_rate": 2.6568627450980394e-05,
      "loss": 58.9034,
      "step": 14831
    },
    {
      "epoch": 14.85,
      "grad_norm": 6028.64990234375,
      "learning_rate": 2.6563467492260062e-05,
      "loss": 41.5571,
      "step": 14832
    },
    {
      "epoch": 14.85,
      "grad_norm": 3449.9931640625,
      "learning_rate": 2.6558307533539733e-05,
      "loss": 47.7193,
      "step": 14833
    },
    {
      "epoch": 14.85,
      "grad_norm": 8997.6455078125,
      "learning_rate": 2.6553147574819404e-05,
      "loss": 29.6153,
      "step": 14834
    },
    {
      "epoch": 14.85,
      "grad_norm": 5636.84765625,
      "learning_rate": 2.6547987616099072e-05,
      "loss": 43.3825,
      "step": 14835
    },
    {
      "epoch": 14.85,
      "grad_norm": 11327.24609375,
      "learning_rate": 2.6542827657378743e-05,
      "loss": 60.5832,
      "step": 14836
    },
    {
      "epoch": 14.85,
      "grad_norm": 3313.504150390625,
      "learning_rate": 2.6537667698658414e-05,
      "loss": 55.3006,
      "step": 14837
    },
    {
      "epoch": 14.85,
      "grad_norm": 57681.19921875,
      "learning_rate": 2.6532507739938085e-05,
      "loss": 56.5957,
      "step": 14838
    },
    {
      "epoch": 14.85,
      "grad_norm": 10297.2255859375,
      "learning_rate": 2.6527347781217753e-05,
      "loss": 47.8782,
      "step": 14839
    },
    {
      "epoch": 14.85,
      "grad_norm": 31674.72265625,
      "learning_rate": 2.6522187822497424e-05,
      "loss": 27.682,
      "step": 14840
    },
    {
      "epoch": 14.86,
      "grad_norm": 5454.23779296875,
      "learning_rate": 2.651702786377709e-05,
      "loss": 50.633,
      "step": 14841
    },
    {
      "epoch": 14.86,
      "grad_norm": 34890.70703125,
      "learning_rate": 2.651186790505676e-05,
      "loss": 52.4031,
      "step": 14842
    },
    {
      "epoch": 14.86,
      "grad_norm": 15998.1611328125,
      "learning_rate": 2.6506707946336428e-05,
      "loss": 44.5532,
      "step": 14843
    },
    {
      "epoch": 14.86,
      "grad_norm": 51682.0234375,
      "learning_rate": 2.65015479876161e-05,
      "loss": 43.1174,
      "step": 14844
    },
    {
      "epoch": 14.86,
      "grad_norm": 24761.896484375,
      "learning_rate": 2.649638802889577e-05,
      "loss": 42.2161,
      "step": 14845
    },
    {
      "epoch": 14.86,
      "grad_norm": 68395.140625,
      "learning_rate": 2.6491228070175438e-05,
      "loss": 62.1543,
      "step": 14846
    },
    {
      "epoch": 14.86,
      "grad_norm": 4261.94482421875,
      "learning_rate": 2.648606811145511e-05,
      "loss": 58.7562,
      "step": 14847
    },
    {
      "epoch": 14.86,
      "grad_norm": 4153.9375,
      "learning_rate": 2.648090815273478e-05,
      "loss": 54.7217,
      "step": 14848
    },
    {
      "epoch": 14.86,
      "grad_norm": 11716.63671875,
      "learning_rate": 2.6475748194014448e-05,
      "loss": 45.6959,
      "step": 14849
    },
    {
      "epoch": 14.86,
      "grad_norm": 83233.3828125,
      "learning_rate": 2.647058823529412e-05,
      "loss": 14.8156,
      "step": 14850
    },
    {
      "epoch": 14.87,
      "grad_norm": 3667.8310546875,
      "learning_rate": 2.646542827657379e-05,
      "loss": 51.9097,
      "step": 14851
    },
    {
      "epoch": 14.87,
      "grad_norm": 8567.2880859375,
      "learning_rate": 2.646026831785346e-05,
      "loss": 52.7739,
      "step": 14852
    },
    {
      "epoch": 14.87,
      "grad_norm": 5158.7705078125,
      "learning_rate": 2.645510835913313e-05,
      "loss": 52.4012,
      "step": 14853
    },
    {
      "epoch": 14.87,
      "grad_norm": 37754.76171875,
      "learning_rate": 2.64499484004128e-05,
      "loss": 42.5146,
      "step": 14854
    },
    {
      "epoch": 14.87,
      "grad_norm": 67189.0703125,
      "learning_rate": 2.644478844169247e-05,
      "loss": 32.7968,
      "step": 14855
    },
    {
      "epoch": 14.87,
      "grad_norm": 6839.4111328125,
      "learning_rate": 2.643962848297214e-05,
      "loss": 53.853,
      "step": 14856
    },
    {
      "epoch": 14.87,
      "grad_norm": 86735.09375,
      "learning_rate": 2.643446852425181e-05,
      "loss": 33.3178,
      "step": 14857
    },
    {
      "epoch": 14.87,
      "grad_norm": 18141.693359375,
      "learning_rate": 2.6429308565531474e-05,
      "loss": 49.4249,
      "step": 14858
    },
    {
      "epoch": 14.87,
      "grad_norm": 18013.48046875,
      "learning_rate": 2.6424148606811145e-05,
      "loss": 41.0946,
      "step": 14859
    },
    {
      "epoch": 14.87,
      "grad_norm": 5337.78564453125,
      "learning_rate": 2.6418988648090813e-05,
      "loss": 47.2796,
      "step": 14860
    },
    {
      "epoch": 14.88,
      "grad_norm": 12583.0556640625,
      "learning_rate": 2.6413828689370484e-05,
      "loss": 43.8891,
      "step": 14861
    },
    {
      "epoch": 14.88,
      "grad_norm": 473582.59375,
      "learning_rate": 2.6408668730650155e-05,
      "loss": 49.8331,
      "step": 14862
    },
    {
      "epoch": 14.88,
      "grad_norm": 8452.7431640625,
      "learning_rate": 2.6403508771929823e-05,
      "loss": 59.9533,
      "step": 14863
    },
    {
      "epoch": 14.88,
      "grad_norm": 6799.22607421875,
      "learning_rate": 2.6398348813209494e-05,
      "loss": 42.1366,
      "step": 14864
    },
    {
      "epoch": 14.88,
      "grad_norm": 39516.92578125,
      "learning_rate": 2.6393188854489165e-05,
      "loss": 29.353,
      "step": 14865
    },
    {
      "epoch": 14.88,
      "grad_norm": 27796.27734375,
      "learning_rate": 2.6388028895768836e-05,
      "loss": 53.7335,
      "step": 14866
    },
    {
      "epoch": 14.88,
      "grad_norm": 23303.119140625,
      "learning_rate": 2.6382868937048504e-05,
      "loss": 53.4254,
      "step": 14867
    },
    {
      "epoch": 14.88,
      "grad_norm": 6068.36181640625,
      "learning_rate": 2.6377708978328175e-05,
      "loss": 50.9584,
      "step": 14868
    },
    {
      "epoch": 14.88,
      "grad_norm": 32049.802734375,
      "learning_rate": 2.6372549019607846e-05,
      "loss": 55.2241,
      "step": 14869
    },
    {
      "epoch": 14.88,
      "grad_norm": 117412.0703125,
      "learning_rate": 2.6367389060887514e-05,
      "loss": 51.7708,
      "step": 14870
    },
    {
      "epoch": 14.89,
      "grad_norm": 4914.33251953125,
      "learning_rate": 2.6362229102167185e-05,
      "loss": 38.8204,
      "step": 14871
    },
    {
      "epoch": 14.89,
      "grad_norm": 10263.7275390625,
      "learning_rate": 2.6357069143446856e-05,
      "loss": 52.6138,
      "step": 14872
    },
    {
      "epoch": 14.89,
      "grad_norm": 21301.177734375,
      "learning_rate": 2.6351909184726524e-05,
      "loss": 53.7834,
      "step": 14873
    },
    {
      "epoch": 14.89,
      "grad_norm": 6124.17626953125,
      "learning_rate": 2.6346749226006195e-05,
      "loss": 48.2871,
      "step": 14874
    },
    {
      "epoch": 14.89,
      "grad_norm": 11192.8544921875,
      "learning_rate": 2.6341589267285866e-05,
      "loss": 47.6803,
      "step": 14875
    },
    {
      "epoch": 14.89,
      "grad_norm": 6574.666015625,
      "learning_rate": 2.633642930856553e-05,
      "loss": 54.7788,
      "step": 14876
    },
    {
      "epoch": 14.89,
      "grad_norm": 16274.45703125,
      "learning_rate": 2.63312693498452e-05,
      "loss": 58.5459,
      "step": 14877
    },
    {
      "epoch": 14.89,
      "grad_norm": 45018.23828125,
      "learning_rate": 2.632610939112487e-05,
      "loss": 50.6769,
      "step": 14878
    },
    {
      "epoch": 14.89,
      "grad_norm": 819982.8125,
      "learning_rate": 2.632094943240454e-05,
      "loss": 51.6218,
      "step": 14879
    },
    {
      "epoch": 14.89,
      "grad_norm": 11959.228515625,
      "learning_rate": 2.6315789473684212e-05,
      "loss": 54.5349,
      "step": 14880
    },
    {
      "epoch": 14.9,
      "grad_norm": 36575.23046875,
      "learning_rate": 2.631062951496388e-05,
      "loss": 50.1453,
      "step": 14881
    },
    {
      "epoch": 14.9,
      "grad_norm": 102541.2890625,
      "learning_rate": 2.630546955624355e-05,
      "loss": 39.747,
      "step": 14882
    },
    {
      "epoch": 14.9,
      "grad_norm": 6809.064453125,
      "learning_rate": 2.6300309597523222e-05,
      "loss": 45.5326,
      "step": 14883
    },
    {
      "epoch": 14.9,
      "grad_norm": 10243.5615234375,
      "learning_rate": 2.629514963880289e-05,
      "loss": 37.5456,
      "step": 14884
    },
    {
      "epoch": 14.9,
      "grad_norm": 51945.5078125,
      "learning_rate": 2.628998968008256e-05,
      "loss": 51.4966,
      "step": 14885
    },
    {
      "epoch": 14.9,
      "grad_norm": 27118.8828125,
      "learning_rate": 2.6284829721362232e-05,
      "loss": 53.1287,
      "step": 14886
    },
    {
      "epoch": 14.9,
      "grad_norm": 14703.6826171875,
      "learning_rate": 2.62796697626419e-05,
      "loss": 46.3374,
      "step": 14887
    },
    {
      "epoch": 14.9,
      "grad_norm": 23545.0078125,
      "learning_rate": 2.627450980392157e-05,
      "loss": 54.5436,
      "step": 14888
    },
    {
      "epoch": 14.9,
      "grad_norm": 18706.3828125,
      "learning_rate": 2.6269349845201242e-05,
      "loss": 56.8578,
      "step": 14889
    },
    {
      "epoch": 14.9,
      "grad_norm": 48184.05078125,
      "learning_rate": 2.626418988648091e-05,
      "loss": 56.2139,
      "step": 14890
    },
    {
      "epoch": 14.91,
      "grad_norm": 71300.828125,
      "learning_rate": 2.625902992776058e-05,
      "loss": 40.9766,
      "step": 14891
    },
    {
      "epoch": 14.91,
      "grad_norm": 8222.3515625,
      "learning_rate": 2.6253869969040252e-05,
      "loss": 37.4746,
      "step": 14892
    },
    {
      "epoch": 14.91,
      "grad_norm": 2827.08544921875,
      "learning_rate": 2.624871001031992e-05,
      "loss": 59.4516,
      "step": 14893
    },
    {
      "epoch": 14.91,
      "grad_norm": 75129.0,
      "learning_rate": 2.6243550051599587e-05,
      "loss": 23.1598,
      "step": 14894
    },
    {
      "epoch": 14.91,
      "grad_norm": 2441.931884765625,
      "learning_rate": 2.6238390092879255e-05,
      "loss": 52.3153,
      "step": 14895
    },
    {
      "epoch": 14.91,
      "grad_norm": 26677.8125,
      "learning_rate": 2.6233230134158926e-05,
      "loss": 56.4891,
      "step": 14896
    },
    {
      "epoch": 14.91,
      "grad_norm": 12753.4501953125,
      "learning_rate": 2.6228070175438597e-05,
      "loss": 47.1337,
      "step": 14897
    },
    {
      "epoch": 14.91,
      "grad_norm": 4294.7255859375,
      "learning_rate": 2.6222910216718265e-05,
      "loss": 57.5023,
      "step": 14898
    },
    {
      "epoch": 14.91,
      "grad_norm": 20535.833984375,
      "learning_rate": 2.6217750257997936e-05,
      "loss": 53.7312,
      "step": 14899
    },
    {
      "epoch": 14.91,
      "grad_norm": 73706.3515625,
      "learning_rate": 2.6212590299277607e-05,
      "loss": 47.8878,
      "step": 14900
    },
    {
      "epoch": 14.92,
      "grad_norm": 159257.875,
      "learning_rate": 2.6207430340557275e-05,
      "loss": 42.011,
      "step": 14901
    },
    {
      "epoch": 14.92,
      "grad_norm": 5639.25390625,
      "learning_rate": 2.6202270381836946e-05,
      "loss": 45.6949,
      "step": 14902
    },
    {
      "epoch": 14.92,
      "grad_norm": 5041.748046875,
      "learning_rate": 2.6197110423116617e-05,
      "loss": 54.8174,
      "step": 14903
    },
    {
      "epoch": 14.92,
      "grad_norm": 7694.06201171875,
      "learning_rate": 2.6191950464396285e-05,
      "loss": 39.2678,
      "step": 14904
    },
    {
      "epoch": 14.92,
      "grad_norm": 46638.03125,
      "learning_rate": 2.6186790505675956e-05,
      "loss": 54.2519,
      "step": 14905
    },
    {
      "epoch": 14.92,
      "grad_norm": 2853.468505859375,
      "learning_rate": 2.6181630546955627e-05,
      "loss": 56.8386,
      "step": 14906
    },
    {
      "epoch": 14.92,
      "grad_norm": 33599.015625,
      "learning_rate": 2.6176470588235295e-05,
      "loss": 27.5037,
      "step": 14907
    },
    {
      "epoch": 14.92,
      "grad_norm": 6921.38916015625,
      "learning_rate": 2.6171310629514966e-05,
      "loss": 51.7712,
      "step": 14908
    },
    {
      "epoch": 14.92,
      "grad_norm": 6331.849609375,
      "learning_rate": 2.6166150670794637e-05,
      "loss": 58.7442,
      "step": 14909
    },
    {
      "epoch": 14.92,
      "grad_norm": 13509.4931640625,
      "learning_rate": 2.616099071207431e-05,
      "loss": 51.2036,
      "step": 14910
    },
    {
      "epoch": 14.93,
      "grad_norm": 13157.375,
      "learning_rate": 2.6155830753353976e-05,
      "loss": 62.0686,
      "step": 14911
    },
    {
      "epoch": 14.93,
      "grad_norm": 117084.578125,
      "learning_rate": 2.615067079463364e-05,
      "loss": 42.3837,
      "step": 14912
    },
    {
      "epoch": 14.93,
      "grad_norm": 14949.490234375,
      "learning_rate": 2.6145510835913312e-05,
      "loss": 56.8836,
      "step": 14913
    },
    {
      "epoch": 14.93,
      "grad_norm": 26527.90625,
      "learning_rate": 2.6140350877192983e-05,
      "loss": 46.0293,
      "step": 14914
    },
    {
      "epoch": 14.93,
      "grad_norm": 7645.916015625,
      "learning_rate": 2.613519091847265e-05,
      "loss": 53.3573,
      "step": 14915
    },
    {
      "epoch": 14.93,
      "grad_norm": 10637.4287109375,
      "learning_rate": 2.6130030959752322e-05,
      "loss": 40.1237,
      "step": 14916
    },
    {
      "epoch": 14.93,
      "grad_norm": 9643.9326171875,
      "learning_rate": 2.6124871001031993e-05,
      "loss": 52.3099,
      "step": 14917
    },
    {
      "epoch": 14.93,
      "grad_norm": 18538.22265625,
      "learning_rate": 2.611971104231166e-05,
      "loss": 42.7184,
      "step": 14918
    },
    {
      "epoch": 14.93,
      "grad_norm": 15264.400390625,
      "learning_rate": 2.6114551083591332e-05,
      "loss": 64.5187,
      "step": 14919
    },
    {
      "epoch": 14.93,
      "grad_norm": 4676.736328125,
      "learning_rate": 2.6109391124871003e-05,
      "loss": 48.8634,
      "step": 14920
    },
    {
      "epoch": 14.94,
      "grad_norm": 9048.1474609375,
      "learning_rate": 2.610423116615067e-05,
      "loss": 43.5454,
      "step": 14921
    },
    {
      "epoch": 14.94,
      "grad_norm": 23943.8359375,
      "learning_rate": 2.6099071207430342e-05,
      "loss": 46.4073,
      "step": 14922
    },
    {
      "epoch": 14.94,
      "grad_norm": 13157.794921875,
      "learning_rate": 2.6093911248710013e-05,
      "loss": 59.0137,
      "step": 14923
    },
    {
      "epoch": 14.94,
      "grad_norm": 5170.8564453125,
      "learning_rate": 2.6088751289989684e-05,
      "loss": 55.7033,
      "step": 14924
    },
    {
      "epoch": 14.94,
      "grad_norm": 1246.402587890625,
      "learning_rate": 2.6083591331269352e-05,
      "loss": 59.4786,
      "step": 14925
    },
    {
      "epoch": 14.94,
      "grad_norm": 182887.65625,
      "learning_rate": 2.6078431372549023e-05,
      "loss": 49.4136,
      "step": 14926
    },
    {
      "epoch": 14.94,
      "grad_norm": 61534.59765625,
      "learning_rate": 2.6073271413828694e-05,
      "loss": 50.8977,
      "step": 14927
    },
    {
      "epoch": 14.94,
      "grad_norm": 65060.94921875,
      "learning_rate": 2.6068111455108362e-05,
      "loss": 51.3024,
      "step": 14928
    },
    {
      "epoch": 14.94,
      "grad_norm": 15972.46875,
      "learning_rate": 2.6062951496388033e-05,
      "loss": 54.6757,
      "step": 14929
    },
    {
      "epoch": 14.94,
      "grad_norm": 3254.908203125,
      "learning_rate": 2.6057791537667697e-05,
      "loss": 46.5077,
      "step": 14930
    },
    {
      "epoch": 14.95,
      "grad_norm": 2981.359130859375,
      "learning_rate": 2.605263157894737e-05,
      "loss": 47.396,
      "step": 14931
    },
    {
      "epoch": 14.95,
      "grad_norm": 15670.77734375,
      "learning_rate": 2.6047471620227036e-05,
      "loss": 46.4068,
      "step": 14932
    },
    {
      "epoch": 14.95,
      "grad_norm": 4768.818359375,
      "learning_rate": 2.6042311661506707e-05,
      "loss": 48.2367,
      "step": 14933
    },
    {
      "epoch": 14.95,
      "grad_norm": 15116.458984375,
      "learning_rate": 2.603715170278638e-05,
      "loss": 55.0531,
      "step": 14934
    },
    {
      "epoch": 14.95,
      "grad_norm": 7508.68408203125,
      "learning_rate": 2.6031991744066046e-05,
      "loss": 46.4458,
      "step": 14935
    },
    {
      "epoch": 14.95,
      "grad_norm": 11071.9560546875,
      "learning_rate": 2.6026831785345717e-05,
      "loss": 50.6215,
      "step": 14936
    },
    {
      "epoch": 14.95,
      "grad_norm": 12884.1875,
      "learning_rate": 2.602167182662539e-05,
      "loss": 51.2508,
      "step": 14937
    },
    {
      "epoch": 14.95,
      "grad_norm": 9798.1240234375,
      "learning_rate": 2.601651186790506e-05,
      "loss": 32.9451,
      "step": 14938
    },
    {
      "epoch": 14.95,
      "grad_norm": 4264.11083984375,
      "learning_rate": 2.6011351909184727e-05,
      "loss": 52.8953,
      "step": 14939
    },
    {
      "epoch": 14.95,
      "grad_norm": 9352.591796875,
      "learning_rate": 2.60061919504644e-05,
      "loss": 39.5875,
      "step": 14940
    },
    {
      "epoch": 14.96,
      "grad_norm": 38203.7890625,
      "learning_rate": 2.600103199174407e-05,
      "loss": 40.81,
      "step": 14941
    },
    {
      "epoch": 14.96,
      "grad_norm": 6070.35791015625,
      "learning_rate": 2.5995872033023737e-05,
      "loss": 50.2563,
      "step": 14942
    },
    {
      "epoch": 14.96,
      "grad_norm": 26782.466796875,
      "learning_rate": 2.599071207430341e-05,
      "loss": 55.2769,
      "step": 14943
    },
    {
      "epoch": 14.96,
      "grad_norm": 20507.2734375,
      "learning_rate": 2.598555211558308e-05,
      "loss": 53.9126,
      "step": 14944
    },
    {
      "epoch": 14.96,
      "grad_norm": 38384.296875,
      "learning_rate": 2.5980392156862747e-05,
      "loss": 48.9162,
      "step": 14945
    },
    {
      "epoch": 14.96,
      "grad_norm": 18423.8046875,
      "learning_rate": 2.597523219814242e-05,
      "loss": 36.0198,
      "step": 14946
    },
    {
      "epoch": 14.96,
      "grad_norm": 8504.7734375,
      "learning_rate": 2.597007223942209e-05,
      "loss": 44.2516,
      "step": 14947
    },
    {
      "epoch": 14.96,
      "grad_norm": 6738.02294921875,
      "learning_rate": 2.5964912280701754e-05,
      "loss": 59.694,
      "step": 14948
    },
    {
      "epoch": 14.96,
      "grad_norm": 8696.48828125,
      "learning_rate": 2.5959752321981422e-05,
      "loss": 55.5358,
      "step": 14949
    },
    {
      "epoch": 14.96,
      "grad_norm": 6612.955078125,
      "learning_rate": 2.5954592363261093e-05,
      "loss": 50.2177,
      "step": 14950
    },
    {
      "epoch": 14.97,
      "grad_norm": 15200.6923828125,
      "learning_rate": 2.5949432404540764e-05,
      "loss": 49.0299,
      "step": 14951
    },
    {
      "epoch": 14.97,
      "grad_norm": 22039.771484375,
      "learning_rate": 2.5944272445820432e-05,
      "loss": 39.3178,
      "step": 14952
    },
    {
      "epoch": 14.97,
      "grad_norm": 8527.2724609375,
      "learning_rate": 2.5939112487100103e-05,
      "loss": 39.9534,
      "step": 14953
    },
    {
      "epoch": 14.97,
      "grad_norm": 11064.9541015625,
      "learning_rate": 2.5933952528379774e-05,
      "loss": 54.9689,
      "step": 14954
    },
    {
      "epoch": 14.97,
      "grad_norm": 7314.4794921875,
      "learning_rate": 2.5928792569659445e-05,
      "loss": 56.0226,
      "step": 14955
    },
    {
      "epoch": 14.97,
      "grad_norm": 4612.2275390625,
      "learning_rate": 2.5923632610939113e-05,
      "loss": 42.4258,
      "step": 14956
    },
    {
      "epoch": 14.97,
      "grad_norm": 14774.765625,
      "learning_rate": 2.5918472652218784e-05,
      "loss": 54.6105,
      "step": 14957
    },
    {
      "epoch": 14.97,
      "grad_norm": 30895.666015625,
      "learning_rate": 2.5913312693498455e-05,
      "loss": 41.4869,
      "step": 14958
    },
    {
      "epoch": 14.97,
      "grad_norm": 63786.859375,
      "learning_rate": 2.5908152734778123e-05,
      "loss": 47.0957,
      "step": 14959
    },
    {
      "epoch": 14.97,
      "grad_norm": 11079.681640625,
      "learning_rate": 2.5902992776057794e-05,
      "loss": 25.4736,
      "step": 14960
    },
    {
      "epoch": 14.98,
      "grad_norm": 9510.501953125,
      "learning_rate": 2.5897832817337465e-05,
      "loss": 40.5612,
      "step": 14961
    },
    {
      "epoch": 14.98,
      "grad_norm": 8486.947265625,
      "learning_rate": 2.5892672858617133e-05,
      "loss": 55.7747,
      "step": 14962
    },
    {
      "epoch": 14.98,
      "grad_norm": 11893.021484375,
      "learning_rate": 2.5887512899896804e-05,
      "loss": 44.561,
      "step": 14963
    },
    {
      "epoch": 14.98,
      "grad_norm": 2159.744873046875,
      "learning_rate": 2.5882352941176475e-05,
      "loss": 50.9399,
      "step": 14964
    },
    {
      "epoch": 14.98,
      "grad_norm": 43987.15234375,
      "learning_rate": 2.5877192982456143e-05,
      "loss": 51.1873,
      "step": 14965
    },
    {
      "epoch": 14.98,
      "grad_norm": 29915.88671875,
      "learning_rate": 2.5872033023735807e-05,
      "loss": 35.859,
      "step": 14966
    },
    {
      "epoch": 14.98,
      "grad_norm": 3641.434814453125,
      "learning_rate": 2.586687306501548e-05,
      "loss": 40.5693,
      "step": 14967
    },
    {
      "epoch": 14.98,
      "grad_norm": 21633.005859375,
      "learning_rate": 2.586171310629515e-05,
      "loss": 33.273,
      "step": 14968
    },
    {
      "epoch": 14.98,
      "grad_norm": 39447.68359375,
      "learning_rate": 2.585655314757482e-05,
      "loss": 53.924,
      "step": 14969
    },
    {
      "epoch": 14.98,
      "grad_norm": 1660.8763427734375,
      "learning_rate": 2.585139318885449e-05,
      "loss": 54.6648,
      "step": 14970
    },
    {
      "epoch": 14.99,
      "grad_norm": 91811.671875,
      "learning_rate": 2.584623323013416e-05,
      "loss": 55.3551,
      "step": 14971
    },
    {
      "epoch": 14.99,
      "grad_norm": 17122.255859375,
      "learning_rate": 2.584107327141383e-05,
      "loss": 49.0363,
      "step": 14972
    },
    {
      "epoch": 14.99,
      "grad_norm": 21547.484375,
      "learning_rate": 2.58359133126935e-05,
      "loss": 48.7252,
      "step": 14973
    },
    {
      "epoch": 14.99,
      "grad_norm": 16179.5029296875,
      "learning_rate": 2.583075335397317e-05,
      "loss": 45.6432,
      "step": 14974
    },
    {
      "epoch": 14.99,
      "grad_norm": 1251.9854736328125,
      "learning_rate": 2.582559339525284e-05,
      "loss": 53.3536,
      "step": 14975
    },
    {
      "epoch": 14.99,
      "grad_norm": 41481.44921875,
      "learning_rate": 2.582043343653251e-05,
      "loss": 43.8511,
      "step": 14976
    },
    {
      "epoch": 14.99,
      "grad_norm": 14160.5380859375,
      "learning_rate": 2.581527347781218e-05,
      "loss": 52.7987,
      "step": 14977
    },
    {
      "epoch": 14.99,
      "grad_norm": 4289.939453125,
      "learning_rate": 2.581011351909185e-05,
      "loss": 53.8501,
      "step": 14978
    },
    {
      "epoch": 14.99,
      "grad_norm": 4700.0732421875,
      "learning_rate": 2.580495356037152e-05,
      "loss": 51.3303,
      "step": 14979
    },
    {
      "epoch": 14.99,
      "grad_norm": 11649.41015625,
      "learning_rate": 2.579979360165119e-05,
      "loss": 55.5473,
      "step": 14980
    },
    {
      "epoch": 15.0,
      "grad_norm": 10063.4697265625,
      "learning_rate": 2.579463364293086e-05,
      "loss": 50.8362,
      "step": 14981
    },
    {
      "epoch": 15.0,
      "grad_norm": 90595.765625,
      "learning_rate": 2.578947368421053e-05,
      "loss": 51.4816,
      "step": 14982
    },
    {
      "epoch": 15.0,
      "grad_norm": 4730.1865234375,
      "learning_rate": 2.57843137254902e-05,
      "loss": 52.9969,
      "step": 14983
    },
    {
      "epoch": 15.0,
      "grad_norm": 4673.720703125,
      "learning_rate": 2.5779153766769864e-05,
      "loss": 50.7079,
      "step": 14984
    },
    {
      "epoch": 15.0,
      "grad_norm": 184654.328125,
      "learning_rate": 2.5773993808049535e-05,
      "loss": 49.7244,
      "step": 14985
    },
    {
      "epoch": 15.0,
      "grad_norm": 7552.8466796875,
      "learning_rate": 2.5768833849329206e-05,
      "loss": 58.8214,
      "step": 14986
    },
    {
      "epoch": 15.0,
      "grad_norm": 9382.7607421875,
      "learning_rate": 2.5763673890608874e-05,
      "loss": 49.0302,
      "step": 14987
    },
    {
      "epoch": 15.0,
      "grad_norm": 22011.50390625,
      "learning_rate": 2.5758513931888545e-05,
      "loss": 45.9343,
      "step": 14988
    },
    {
      "epoch": 15.0,
      "grad_norm": 7686.841796875,
      "learning_rate": 2.5753353973168216e-05,
      "loss": 51.9575,
      "step": 14989
    },
    {
      "epoch": 15.01,
      "grad_norm": 4234.42822265625,
      "learning_rate": 2.5748194014447884e-05,
      "loss": 47.4774,
      "step": 14990
    },
    {
      "epoch": 15.01,
      "grad_norm": 9172.876953125,
      "learning_rate": 2.5743034055727555e-05,
      "loss": 39.1009,
      "step": 14991
    },
    {
      "epoch": 15.01,
      "grad_norm": 6871.2666015625,
      "learning_rate": 2.5737874097007226e-05,
      "loss": 43.4609,
      "step": 14992
    },
    {
      "epoch": 15.01,
      "grad_norm": 10690.607421875,
      "learning_rate": 2.5732714138286894e-05,
      "loss": 55.637,
      "step": 14993
    },
    {
      "epoch": 15.01,
      "grad_norm": 80633.375,
      "learning_rate": 2.5727554179566565e-05,
      "loss": 45.3573,
      "step": 14994
    },
    {
      "epoch": 15.01,
      "grad_norm": 68813.9140625,
      "learning_rate": 2.5722394220846236e-05,
      "loss": 54.7072,
      "step": 14995
    },
    {
      "epoch": 15.01,
      "grad_norm": 6863.29638671875,
      "learning_rate": 2.5717234262125904e-05,
      "loss": 57.4989,
      "step": 14996
    },
    {
      "epoch": 15.01,
      "grad_norm": 14649.525390625,
      "learning_rate": 2.5712074303405575e-05,
      "loss": 59.3605,
      "step": 14997
    },
    {
      "epoch": 15.01,
      "grad_norm": 6965.6318359375,
      "learning_rate": 2.5706914344685246e-05,
      "loss": 56.2045,
      "step": 14998
    },
    {
      "epoch": 15.01,
      "grad_norm": 8859.595703125,
      "learning_rate": 2.5701754385964917e-05,
      "loss": 50.5496,
      "step": 14999
    },
    {
      "epoch": 15.02,
      "grad_norm": 36840.02734375,
      "learning_rate": 2.5696594427244585e-05,
      "loss": 50.0365,
      "step": 15000
    },
    {
      "epoch": 15.02,
      "grad_norm": 3199.458984375,
      "learning_rate": 2.569143446852425e-05,
      "loss": 43.5221,
      "step": 15001
    },
    {
      "epoch": 15.02,
      "grad_norm": 20073.630859375,
      "learning_rate": 2.568627450980392e-05,
      "loss": 55.8309,
      "step": 15002
    },
    {
      "epoch": 15.02,
      "grad_norm": 12700.4853515625,
      "learning_rate": 2.568111455108359e-05,
      "loss": 55.4769,
      "step": 15003
    },
    {
      "epoch": 15.02,
      "grad_norm": 11084.08203125,
      "learning_rate": 2.567595459236326e-05,
      "loss": 56.1425,
      "step": 15004
    },
    {
      "epoch": 15.02,
      "grad_norm": 36164.17578125,
      "learning_rate": 2.567079463364293e-05,
      "loss": 50.688,
      "step": 15005
    },
    {
      "epoch": 15.02,
      "grad_norm": 67729.1640625,
      "learning_rate": 2.56656346749226e-05,
      "loss": 59.0452,
      "step": 15006
    },
    {
      "epoch": 15.02,
      "grad_norm": 4523.20556640625,
      "learning_rate": 2.566047471620227e-05,
      "loss": 56.7364,
      "step": 15007
    },
    {
      "epoch": 15.02,
      "grad_norm": 7447.47119140625,
      "learning_rate": 2.565531475748194e-05,
      "loss": 38.788,
      "step": 15008
    },
    {
      "epoch": 15.02,
      "grad_norm": 23238.759765625,
      "learning_rate": 2.565015479876161e-05,
      "loss": 51.5513,
      "step": 15009
    },
    {
      "epoch": 15.03,
      "grad_norm": 3613.40673828125,
      "learning_rate": 2.564499484004128e-05,
      "loss": 51.6649,
      "step": 15010
    },
    {
      "epoch": 15.03,
      "grad_norm": 26566.6171875,
      "learning_rate": 2.563983488132095e-05,
      "loss": 57.136,
      "step": 15011
    },
    {
      "epoch": 15.03,
      "grad_norm": 5728.08984375,
      "learning_rate": 2.563467492260062e-05,
      "loss": 54.2311,
      "step": 15012
    },
    {
      "epoch": 15.03,
      "grad_norm": 6088.654296875,
      "learning_rate": 2.5629514963880293e-05,
      "loss": 51.9668,
      "step": 15013
    },
    {
      "epoch": 15.03,
      "grad_norm": 3201.189697265625,
      "learning_rate": 2.562435500515996e-05,
      "loss": 52.9794,
      "step": 15014
    },
    {
      "epoch": 15.03,
      "grad_norm": 33566.36328125,
      "learning_rate": 2.561919504643963e-05,
      "loss": 39.7236,
      "step": 15015
    },
    {
      "epoch": 15.03,
      "grad_norm": 23337.875,
      "learning_rate": 2.5614035087719303e-05,
      "loss": 56.4466,
      "step": 15016
    },
    {
      "epoch": 15.03,
      "grad_norm": 9733.640625,
      "learning_rate": 2.560887512899897e-05,
      "loss": 52.4611,
      "step": 15017
    },
    {
      "epoch": 15.03,
      "grad_norm": 67129.84375,
      "learning_rate": 2.560371517027864e-05,
      "loss": 43.3961,
      "step": 15018
    },
    {
      "epoch": 15.03,
      "grad_norm": 51893.55078125,
      "learning_rate": 2.5598555211558306e-05,
      "loss": 43.7281,
      "step": 15019
    },
    {
      "epoch": 15.04,
      "grad_norm": 13805.275390625,
      "learning_rate": 2.5593395252837977e-05,
      "loss": 55.756,
      "step": 15020
    },
    {
      "epoch": 15.04,
      "grad_norm": 13294.720703125,
      "learning_rate": 2.5588235294117645e-05,
      "loss": 38.7371,
      "step": 15021
    },
    {
      "epoch": 15.04,
      "grad_norm": 8387.20703125,
      "learning_rate": 2.5583075335397316e-05,
      "loss": 51.7155,
      "step": 15022
    },
    {
      "epoch": 15.04,
      "grad_norm": 13385.4443359375,
      "learning_rate": 2.5577915376676987e-05,
      "loss": 51.7209,
      "step": 15023
    },
    {
      "epoch": 15.04,
      "grad_norm": 39061.640625,
      "learning_rate": 2.5572755417956655e-05,
      "loss": 43.0055,
      "step": 15024
    },
    {
      "epoch": 15.04,
      "grad_norm": 4361.287109375,
      "learning_rate": 2.5567595459236326e-05,
      "loss": 45.3131,
      "step": 15025
    },
    {
      "epoch": 15.04,
      "grad_norm": 10742.5400390625,
      "learning_rate": 2.5562435500515997e-05,
      "loss": 58.7405,
      "step": 15026
    },
    {
      "epoch": 15.04,
      "grad_norm": 3744.0634765625,
      "learning_rate": 2.5557275541795668e-05,
      "loss": 56.8577,
      "step": 15027
    },
    {
      "epoch": 15.04,
      "grad_norm": 7196.59228515625,
      "learning_rate": 2.5552115583075336e-05,
      "loss": 56.847,
      "step": 15028
    },
    {
      "epoch": 15.04,
      "grad_norm": 493257.15625,
      "learning_rate": 2.5546955624355007e-05,
      "loss": 44.8477,
      "step": 15029
    },
    {
      "epoch": 15.05,
      "grad_norm": 44834.671875,
      "learning_rate": 2.5541795665634678e-05,
      "loss": 43.6137,
      "step": 15030
    },
    {
      "epoch": 15.05,
      "grad_norm": 31137.578125,
      "learning_rate": 2.5536635706914346e-05,
      "loss": 48.2569,
      "step": 15031
    },
    {
      "epoch": 15.05,
      "grad_norm": 31535.234375,
      "learning_rate": 2.5531475748194017e-05,
      "loss": 55.7958,
      "step": 15032
    },
    {
      "epoch": 15.05,
      "grad_norm": 24854.294921875,
      "learning_rate": 2.5526315789473688e-05,
      "loss": 38.7136,
      "step": 15033
    },
    {
      "epoch": 15.05,
      "grad_norm": 5575.28857421875,
      "learning_rate": 2.5521155830753356e-05,
      "loss": 39.7666,
      "step": 15034
    },
    {
      "epoch": 15.05,
      "grad_norm": 4039.130859375,
      "learning_rate": 2.5515995872033027e-05,
      "loss": 34.5071,
      "step": 15035
    },
    {
      "epoch": 15.05,
      "grad_norm": 8848.6201171875,
      "learning_rate": 2.5510835913312698e-05,
      "loss": 49.6561,
      "step": 15036
    },
    {
      "epoch": 15.05,
      "grad_norm": 9609.39453125,
      "learning_rate": 2.5505675954592363e-05,
      "loss": 27.0488,
      "step": 15037
    },
    {
      "epoch": 15.05,
      "grad_norm": 61684.17578125,
      "learning_rate": 2.550051599587203e-05,
      "loss": 38.1836,
      "step": 15038
    },
    {
      "epoch": 15.05,
      "grad_norm": 4543.4111328125,
      "learning_rate": 2.54953560371517e-05,
      "loss": 45.5187,
      "step": 15039
    },
    {
      "epoch": 15.06,
      "grad_norm": 17575.078125,
      "learning_rate": 2.5490196078431373e-05,
      "loss": 49.3999,
      "step": 15040
    },
    {
      "epoch": 15.06,
      "grad_norm": 84058.4765625,
      "learning_rate": 2.5485036119711044e-05,
      "loss": 41.4439,
      "step": 15041
    },
    {
      "epoch": 15.06,
      "grad_norm": 19950.37109375,
      "learning_rate": 2.547987616099071e-05,
      "loss": 50.946,
      "step": 15042
    },
    {
      "epoch": 15.06,
      "grad_norm": 310319.03125,
      "learning_rate": 2.5474716202270383e-05,
      "loss": 29.5435,
      "step": 15043
    },
    {
      "epoch": 15.06,
      "grad_norm": 51292.28515625,
      "learning_rate": 2.5469556243550054e-05,
      "loss": 58.2537,
      "step": 15044
    },
    {
      "epoch": 15.06,
      "grad_norm": 18209.880859375,
      "learning_rate": 2.546439628482972e-05,
      "loss": 45.7009,
      "step": 15045
    },
    {
      "epoch": 15.06,
      "grad_norm": 40177.19921875,
      "learning_rate": 2.5459236326109393e-05,
      "loss": 55.72,
      "step": 15046
    },
    {
      "epoch": 15.06,
      "grad_norm": 5366.4599609375,
      "learning_rate": 2.5454076367389064e-05,
      "loss": 55.748,
      "step": 15047
    },
    {
      "epoch": 15.06,
      "grad_norm": 6962.3388671875,
      "learning_rate": 2.544891640866873e-05,
      "loss": 45.3457,
      "step": 15048
    },
    {
      "epoch": 15.06,
      "grad_norm": 3607.355712890625,
      "learning_rate": 2.5443756449948403e-05,
      "loss": 47.7218,
      "step": 15049
    },
    {
      "epoch": 15.07,
      "grad_norm": 69545.5078125,
      "learning_rate": 2.5438596491228074e-05,
      "loss": 40.0907,
      "step": 15050
    },
    {
      "epoch": 15.07,
      "grad_norm": 20681.01171875,
      "learning_rate": 2.543343653250774e-05,
      "loss": 50.1557,
      "step": 15051
    },
    {
      "epoch": 15.07,
      "grad_norm": 16747.42578125,
      "learning_rate": 2.5428276573787413e-05,
      "loss": 49.2653,
      "step": 15052
    },
    {
      "epoch": 15.07,
      "grad_norm": 7878.87646484375,
      "learning_rate": 2.5423116615067084e-05,
      "loss": 49.2946,
      "step": 15053
    },
    {
      "epoch": 15.07,
      "grad_norm": 10229.357421875,
      "learning_rate": 2.541795665634675e-05,
      "loss": 56.6667,
      "step": 15054
    },
    {
      "epoch": 15.07,
      "grad_norm": 12463.7353515625,
      "learning_rate": 2.541279669762642e-05,
      "loss": 54.5576,
      "step": 15055
    },
    {
      "epoch": 15.07,
      "grad_norm": 14771.6484375,
      "learning_rate": 2.5407636738906087e-05,
      "loss": 50.8484,
      "step": 15056
    },
    {
      "epoch": 15.07,
      "grad_norm": 9278.55078125,
      "learning_rate": 2.5402476780185758e-05,
      "loss": 46.4117,
      "step": 15057
    },
    {
      "epoch": 15.07,
      "grad_norm": 13024.0458984375,
      "learning_rate": 2.539731682146543e-05,
      "loss": 47.1081,
      "step": 15058
    },
    {
      "epoch": 15.07,
      "grad_norm": 9496.791015625,
      "learning_rate": 2.5392156862745097e-05,
      "loss": 46.8809,
      "step": 15059
    },
    {
      "epoch": 15.08,
      "grad_norm": 7883.63525390625,
      "learning_rate": 2.5386996904024768e-05,
      "loss": 55.6881,
      "step": 15060
    },
    {
      "epoch": 15.08,
      "grad_norm": 20595.607421875,
      "learning_rate": 2.538183694530444e-05,
      "loss": 34.127,
      "step": 15061
    },
    {
      "epoch": 15.08,
      "grad_norm": 47312.125,
      "learning_rate": 2.5376676986584107e-05,
      "loss": 57.2174,
      "step": 15062
    },
    {
      "epoch": 15.08,
      "grad_norm": 16889.353515625,
      "learning_rate": 2.5371517027863778e-05,
      "loss": 57.6618,
      "step": 15063
    },
    {
      "epoch": 15.08,
      "grad_norm": 8425.205078125,
      "learning_rate": 2.536635706914345e-05,
      "loss": 59.5482,
      "step": 15064
    },
    {
      "epoch": 15.08,
      "grad_norm": 7155.400390625,
      "learning_rate": 2.5361197110423117e-05,
      "loss": 55.0733,
      "step": 15065
    },
    {
      "epoch": 15.08,
      "grad_norm": 14735.8330078125,
      "learning_rate": 2.5356037151702788e-05,
      "loss": 42.0532,
      "step": 15066
    },
    {
      "epoch": 15.08,
      "grad_norm": 3332.712158203125,
      "learning_rate": 2.535087719298246e-05,
      "loss": 42.3155,
      "step": 15067
    },
    {
      "epoch": 15.08,
      "grad_norm": 18158.302734375,
      "learning_rate": 2.5345717234262127e-05,
      "loss": 43.3747,
      "step": 15068
    },
    {
      "epoch": 15.08,
      "grad_norm": 2564.845947265625,
      "learning_rate": 2.5340557275541798e-05,
      "loss": 56.727,
      "step": 15069
    },
    {
      "epoch": 15.09,
      "grad_norm": 6370.4833984375,
      "learning_rate": 2.533539731682147e-05,
      "loss": 43.9009,
      "step": 15070
    },
    {
      "epoch": 15.09,
      "grad_norm": 26714.908203125,
      "learning_rate": 2.533023735810114e-05,
      "loss": 47.7457,
      "step": 15071
    },
    {
      "epoch": 15.09,
      "grad_norm": 20450.748046875,
      "learning_rate": 2.5325077399380808e-05,
      "loss": 39.7256,
      "step": 15072
    },
    {
      "epoch": 15.09,
      "grad_norm": 18171.810546875,
      "learning_rate": 2.5319917440660472e-05,
      "loss": 22.3032,
      "step": 15073
    },
    {
      "epoch": 15.09,
      "grad_norm": 7063.5771484375,
      "learning_rate": 2.5314757481940144e-05,
      "loss": 53.3721,
      "step": 15074
    },
    {
      "epoch": 15.09,
      "grad_norm": 12364.5576171875,
      "learning_rate": 2.5309597523219815e-05,
      "loss": 52.285,
      "step": 15075
    },
    {
      "epoch": 15.09,
      "grad_norm": 1059.3209228515625,
      "learning_rate": 2.5304437564499482e-05,
      "loss": 49.4634,
      "step": 15076
    },
    {
      "epoch": 15.09,
      "grad_norm": 1631.73876953125,
      "learning_rate": 2.5299277605779154e-05,
      "loss": 46.2003,
      "step": 15077
    },
    {
      "epoch": 15.09,
      "grad_norm": 18861.380859375,
      "learning_rate": 2.5294117647058825e-05,
      "loss": 48.8563,
      "step": 15078
    },
    {
      "epoch": 15.09,
      "grad_norm": 39122.75390625,
      "learning_rate": 2.5288957688338492e-05,
      "loss": 35.7157,
      "step": 15079
    },
    {
      "epoch": 15.1,
      "grad_norm": 41723.87890625,
      "learning_rate": 2.5283797729618164e-05,
      "loss": 50.9065,
      "step": 15080
    },
    {
      "epoch": 15.1,
      "grad_norm": 4702.99267578125,
      "learning_rate": 2.5278637770897835e-05,
      "loss": 57.2389,
      "step": 15081
    },
    {
      "epoch": 15.1,
      "grad_norm": 18184.107421875,
      "learning_rate": 2.5273477812177502e-05,
      "loss": 57.5181,
      "step": 15082
    },
    {
      "epoch": 15.1,
      "grad_norm": 5657.87890625,
      "learning_rate": 2.5268317853457174e-05,
      "loss": 62.6749,
      "step": 15083
    },
    {
      "epoch": 15.1,
      "grad_norm": 88639.0703125,
      "learning_rate": 2.5263157894736845e-05,
      "loss": 40.5561,
      "step": 15084
    },
    {
      "epoch": 15.1,
      "grad_norm": 4900.56787109375,
      "learning_rate": 2.5257997936016516e-05,
      "loss": 37.5636,
      "step": 15085
    },
    {
      "epoch": 15.1,
      "grad_norm": 15398.453125,
      "learning_rate": 2.5252837977296184e-05,
      "loss": 54.067,
      "step": 15086
    },
    {
      "epoch": 15.1,
      "grad_norm": 10781.609375,
      "learning_rate": 2.5247678018575855e-05,
      "loss": 56.3215,
      "step": 15087
    },
    {
      "epoch": 15.1,
      "grad_norm": 17996.580078125,
      "learning_rate": 2.5242518059855526e-05,
      "loss": 63.9404,
      "step": 15088
    },
    {
      "epoch": 15.1,
      "grad_norm": 2416.98681640625,
      "learning_rate": 2.5237358101135194e-05,
      "loss": 56.9817,
      "step": 15089
    },
    {
      "epoch": 15.11,
      "grad_norm": 7319.97119140625,
      "learning_rate": 2.5232198142414865e-05,
      "loss": 51.2699,
      "step": 15090
    },
    {
      "epoch": 15.11,
      "grad_norm": 7094.837890625,
      "learning_rate": 2.522703818369453e-05,
      "loss": 50.9567,
      "step": 15091
    },
    {
      "epoch": 15.11,
      "grad_norm": 4956.70654296875,
      "learning_rate": 2.52218782249742e-05,
      "loss": 53.2086,
      "step": 15092
    },
    {
      "epoch": 15.11,
      "grad_norm": 52044.3125,
      "learning_rate": 2.5216718266253868e-05,
      "loss": 55.9353,
      "step": 15093
    },
    {
      "epoch": 15.11,
      "grad_norm": 36086.22265625,
      "learning_rate": 2.521155830753354e-05,
      "loss": 41.3469,
      "step": 15094
    },
    {
      "epoch": 15.11,
      "grad_norm": 26904.80859375,
      "learning_rate": 2.520639834881321e-05,
      "loss": 56.1889,
      "step": 15095
    },
    {
      "epoch": 15.11,
      "grad_norm": 16058.013671875,
      "learning_rate": 2.5201238390092878e-05,
      "loss": 52.4086,
      "step": 15096
    },
    {
      "epoch": 15.11,
      "grad_norm": 4644.53271484375,
      "learning_rate": 2.519607843137255e-05,
      "loss": 59.7103,
      "step": 15097
    },
    {
      "epoch": 15.11,
      "grad_norm": 28226.3359375,
      "learning_rate": 2.519091847265222e-05,
      "loss": 22.7362,
      "step": 15098
    },
    {
      "epoch": 15.11,
      "grad_norm": 9766.55078125,
      "learning_rate": 2.518575851393189e-05,
      "loss": 54.0228,
      "step": 15099
    },
    {
      "epoch": 15.12,
      "grad_norm": 1936.30322265625,
      "learning_rate": 2.518059855521156e-05,
      "loss": 51.1392,
      "step": 15100
    },
    {
      "epoch": 15.12,
      "grad_norm": 5551.31884765625,
      "learning_rate": 2.517543859649123e-05,
      "loss": 44.2523,
      "step": 15101
    },
    {
      "epoch": 15.12,
      "grad_norm": 6795.77685546875,
      "learning_rate": 2.51702786377709e-05,
      "loss": 41.6561,
      "step": 15102
    },
    {
      "epoch": 15.12,
      "grad_norm": 6838.5625,
      "learning_rate": 2.516511867905057e-05,
      "loss": 36.3299,
      "step": 15103
    },
    {
      "epoch": 15.12,
      "grad_norm": 20573.91796875,
      "learning_rate": 2.515995872033024e-05,
      "loss": 42.7752,
      "step": 15104
    },
    {
      "epoch": 15.12,
      "grad_norm": 4870.8720703125,
      "learning_rate": 2.515479876160991e-05,
      "loss": 47.1258,
      "step": 15105
    },
    {
      "epoch": 15.12,
      "grad_norm": 1290.5704345703125,
      "learning_rate": 2.514963880288958e-05,
      "loss": 50.0375,
      "step": 15106
    },
    {
      "epoch": 15.12,
      "grad_norm": 2048.72900390625,
      "learning_rate": 2.514447884416925e-05,
      "loss": 56.2617,
      "step": 15107
    },
    {
      "epoch": 15.12,
      "grad_norm": 10238.2607421875,
      "learning_rate": 2.513931888544892e-05,
      "loss": 31.7607,
      "step": 15108
    },
    {
      "epoch": 15.12,
      "grad_norm": 8173.73486328125,
      "learning_rate": 2.5134158926728586e-05,
      "loss": 56.1744,
      "step": 15109
    },
    {
      "epoch": 15.13,
      "grad_norm": 4320.62353515625,
      "learning_rate": 2.5128998968008253e-05,
      "loss": 56.4691,
      "step": 15110
    },
    {
      "epoch": 15.13,
      "grad_norm": 13715.205078125,
      "learning_rate": 2.5123839009287925e-05,
      "loss": 46.2143,
      "step": 15111
    },
    {
      "epoch": 15.13,
      "grad_norm": 9450.0400390625,
      "learning_rate": 2.5118679050567596e-05,
      "loss": 55.7827,
      "step": 15112
    },
    {
      "epoch": 15.13,
      "grad_norm": 126481.0546875,
      "learning_rate": 2.5113519091847267e-05,
      "loss": 58.5178,
      "step": 15113
    },
    {
      "epoch": 15.13,
      "grad_norm": 9900.5419921875,
      "learning_rate": 2.5108359133126935e-05,
      "loss": 47.7545,
      "step": 15114
    },
    {
      "epoch": 15.13,
      "grad_norm": 16541.982421875,
      "learning_rate": 2.5103199174406606e-05,
      "loss": 58.8921,
      "step": 15115
    },
    {
      "epoch": 15.13,
      "grad_norm": 20881.30078125,
      "learning_rate": 2.5098039215686277e-05,
      "loss": 57.0798,
      "step": 15116
    },
    {
      "epoch": 15.13,
      "grad_norm": 6822.55322265625,
      "learning_rate": 2.5092879256965945e-05,
      "loss": 59.3252,
      "step": 15117
    },
    {
      "epoch": 15.13,
      "grad_norm": 31885.07421875,
      "learning_rate": 2.5087719298245616e-05,
      "loss": 56.8964,
      "step": 15118
    },
    {
      "epoch": 15.13,
      "grad_norm": 8716.1396484375,
      "learning_rate": 2.5082559339525287e-05,
      "loss": 40.7734,
      "step": 15119
    },
    {
      "epoch": 15.14,
      "grad_norm": 39259.97265625,
      "learning_rate": 2.5077399380804955e-05,
      "loss": 59.0794,
      "step": 15120
    },
    {
      "epoch": 15.14,
      "grad_norm": 17698.466796875,
      "learning_rate": 2.5072239422084626e-05,
      "loss": 31.8155,
      "step": 15121
    },
    {
      "epoch": 15.14,
      "grad_norm": 23970.859375,
      "learning_rate": 2.5067079463364297e-05,
      "loss": 49.5691,
      "step": 15122
    },
    {
      "epoch": 15.14,
      "grad_norm": 46643.546875,
      "learning_rate": 2.5061919504643965e-05,
      "loss": 59.3823,
      "step": 15123
    },
    {
      "epoch": 15.14,
      "grad_norm": 23340.0234375,
      "learning_rate": 2.5056759545923636e-05,
      "loss": 48.7822,
      "step": 15124
    },
    {
      "epoch": 15.14,
      "grad_norm": 3759.004638671875,
      "learning_rate": 2.5051599587203307e-05,
      "loss": 51.5449,
      "step": 15125
    },
    {
      "epoch": 15.14,
      "grad_norm": 11042.06640625,
      "learning_rate": 2.5046439628482975e-05,
      "loss": 49.0583,
      "step": 15126
    },
    {
      "epoch": 15.14,
      "grad_norm": 53371.82421875,
      "learning_rate": 2.504127966976264e-05,
      "loss": 56.0476,
      "step": 15127
    },
    {
      "epoch": 15.14,
      "grad_norm": 17327.353515625,
      "learning_rate": 2.503611971104231e-05,
      "loss": 41.6025,
      "step": 15128
    },
    {
      "epoch": 15.14,
      "grad_norm": 6317.64501953125,
      "learning_rate": 2.503095975232198e-05,
      "loss": 45.5629,
      "step": 15129
    },
    {
      "epoch": 15.15,
      "grad_norm": 47665.015625,
      "learning_rate": 2.5025799793601652e-05,
      "loss": 29.1451,
      "step": 15130
    },
    {
      "epoch": 15.15,
      "grad_norm": 4351.37451171875,
      "learning_rate": 2.502063983488132e-05,
      "loss": 53.6019,
      "step": 15131
    },
    {
      "epoch": 15.15,
      "grad_norm": 5980.1875,
      "learning_rate": 2.501547987616099e-05,
      "loss": 56.6458,
      "step": 15132
    },
    {
      "epoch": 15.15,
      "grad_norm": 7338.001953125,
      "learning_rate": 2.5010319917440662e-05,
      "loss": 53.5624,
      "step": 15133
    },
    {
      "epoch": 15.15,
      "grad_norm": 3761.155517578125,
      "learning_rate": 2.500515995872033e-05,
      "loss": 38.8345,
      "step": 15134
    },
    {
      "epoch": 15.15,
      "grad_norm": 9257.7490234375,
      "learning_rate": 2.5e-05,
      "loss": 55.1057,
      "step": 15135
    },
    {
      "epoch": 15.15,
      "grad_norm": 4263.1083984375,
      "learning_rate": 2.4994840041279672e-05,
      "loss": 49.9711,
      "step": 15136
    },
    {
      "epoch": 15.15,
      "grad_norm": 78988.0546875,
      "learning_rate": 2.498968008255934e-05,
      "loss": 43.7228,
      "step": 15137
    },
    {
      "epoch": 15.15,
      "grad_norm": 27436.1640625,
      "learning_rate": 2.498452012383901e-05,
      "loss": 51.1613,
      "step": 15138
    },
    {
      "epoch": 15.15,
      "grad_norm": 16399.453125,
      "learning_rate": 2.4979360165118682e-05,
      "loss": 35.3976,
      "step": 15139
    },
    {
      "epoch": 15.16,
      "grad_norm": 3215.205322265625,
      "learning_rate": 2.497420020639835e-05,
      "loss": 52.1197,
      "step": 15140
    },
    {
      "epoch": 15.16,
      "grad_norm": 15141.419921875,
      "learning_rate": 2.4969040247678018e-05,
      "loss": 54.9667,
      "step": 15141
    },
    {
      "epoch": 15.16,
      "grad_norm": 68707.0546875,
      "learning_rate": 2.496388028895769e-05,
      "loss": 57.0172,
      "step": 15142
    },
    {
      "epoch": 15.16,
      "grad_norm": 14715.7529296875,
      "learning_rate": 2.495872033023736e-05,
      "loss": 44.1358,
      "step": 15143
    },
    {
      "epoch": 15.16,
      "grad_norm": 2878.329345703125,
      "learning_rate": 2.4953560371517028e-05,
      "loss": 23.3605,
      "step": 15144
    },
    {
      "epoch": 15.16,
      "grad_norm": 4690.97998046875,
      "learning_rate": 2.49484004127967e-05,
      "loss": 48.7557,
      "step": 15145
    },
    {
      "epoch": 15.16,
      "grad_norm": 16648.294921875,
      "learning_rate": 2.494324045407637e-05,
      "loss": 52.5232,
      "step": 15146
    },
    {
      "epoch": 15.16,
      "grad_norm": 2768.727294921875,
      "learning_rate": 2.4938080495356038e-05,
      "loss": 47.1231,
      "step": 15147
    },
    {
      "epoch": 15.16,
      "grad_norm": 9237.2177734375,
      "learning_rate": 2.493292053663571e-05,
      "loss": 51.8343,
      "step": 15148
    },
    {
      "epoch": 15.16,
      "grad_norm": 21570.9921875,
      "learning_rate": 2.4927760577915377e-05,
      "loss": 39.2942,
      "step": 15149
    },
    {
      "epoch": 15.17,
      "grad_norm": 137119.84375,
      "learning_rate": 2.4922600619195048e-05,
      "loss": 24.6014,
      "step": 15150
    },
    {
      "epoch": 15.17,
      "grad_norm": 1132.3978271484375,
      "learning_rate": 2.4917440660474716e-05,
      "loss": 53.8694,
      "step": 15151
    },
    {
      "epoch": 15.17,
      "grad_norm": 13211.1728515625,
      "learning_rate": 2.4912280701754387e-05,
      "loss": 29.1544,
      "step": 15152
    },
    {
      "epoch": 15.17,
      "grad_norm": 12089.65625,
      "learning_rate": 2.4907120743034058e-05,
      "loss": 21.1408,
      "step": 15153
    },
    {
      "epoch": 15.17,
      "grad_norm": 85977.046875,
      "learning_rate": 2.4901960784313726e-05,
      "loss": 40.5987,
      "step": 15154
    },
    {
      "epoch": 15.17,
      "grad_norm": 4560.59716796875,
      "learning_rate": 2.4896800825593397e-05,
      "loss": 42.9238,
      "step": 15155
    },
    {
      "epoch": 15.17,
      "grad_norm": 5154.60107421875,
      "learning_rate": 2.4891640866873068e-05,
      "loss": 46.3318,
      "step": 15156
    },
    {
      "epoch": 15.17,
      "grad_norm": 14199.08984375,
      "learning_rate": 2.4886480908152736e-05,
      "loss": 49.6583,
      "step": 15157
    },
    {
      "epoch": 15.17,
      "grad_norm": 15263.388671875,
      "learning_rate": 2.4881320949432403e-05,
      "loss": 51.8754,
      "step": 15158
    },
    {
      "epoch": 15.17,
      "grad_norm": 35686.3828125,
      "learning_rate": 2.4876160990712074e-05,
      "loss": 51.8078,
      "step": 15159
    },
    {
      "epoch": 15.18,
      "grad_norm": 28889.294921875,
      "learning_rate": 2.4871001031991746e-05,
      "loss": 43.1873,
      "step": 15160
    },
    {
      "epoch": 15.18,
      "grad_norm": 7504.45703125,
      "learning_rate": 2.4865841073271413e-05,
      "loss": 53.7373,
      "step": 15161
    },
    {
      "epoch": 15.18,
      "grad_norm": 24823.255859375,
      "learning_rate": 2.4860681114551084e-05,
      "loss": 52.7315,
      "step": 15162
    },
    {
      "epoch": 15.18,
      "grad_norm": 14882.537109375,
      "learning_rate": 2.4855521155830756e-05,
      "loss": 47.6219,
      "step": 15163
    },
    {
      "epoch": 15.18,
      "grad_norm": 12074.984375,
      "learning_rate": 2.4850361197110423e-05,
      "loss": 44.2217,
      "step": 15164
    },
    {
      "epoch": 15.18,
      "grad_norm": 2880.175537109375,
      "learning_rate": 2.4845201238390094e-05,
      "loss": 52.565,
      "step": 15165
    },
    {
      "epoch": 15.18,
      "grad_norm": 6860.08203125,
      "learning_rate": 2.4840041279669766e-05,
      "loss": 53.7865,
      "step": 15166
    },
    {
      "epoch": 15.18,
      "grad_norm": 142261.125,
      "learning_rate": 2.4834881320949433e-05,
      "loss": 56.7449,
      "step": 15167
    },
    {
      "epoch": 15.18,
      "grad_norm": 52107.66796875,
      "learning_rate": 2.48297213622291e-05,
      "loss": 44.7617,
      "step": 15168
    },
    {
      "epoch": 15.18,
      "grad_norm": 5868.68408203125,
      "learning_rate": 2.4824561403508772e-05,
      "loss": 61.8158,
      "step": 15169
    },
    {
      "epoch": 15.19,
      "grad_norm": 16614.25390625,
      "learning_rate": 2.4819401444788443e-05,
      "loss": 50.4971,
      "step": 15170
    },
    {
      "epoch": 15.19,
      "grad_norm": 88919.3046875,
      "learning_rate": 2.481424148606811e-05,
      "loss": 26.8314,
      "step": 15171
    },
    {
      "epoch": 15.19,
      "grad_norm": 6820.28759765625,
      "learning_rate": 2.4809081527347782e-05,
      "loss": 54.4523,
      "step": 15172
    },
    {
      "epoch": 15.19,
      "grad_norm": 12659.4013671875,
      "learning_rate": 2.4803921568627453e-05,
      "loss": 58.7951,
      "step": 15173
    },
    {
      "epoch": 15.19,
      "grad_norm": 11131.3203125,
      "learning_rate": 2.4798761609907124e-05,
      "loss": 51.2885,
      "step": 15174
    },
    {
      "epoch": 15.19,
      "grad_norm": 11517.0146484375,
      "learning_rate": 2.4793601651186792e-05,
      "loss": 47.9601,
      "step": 15175
    },
    {
      "epoch": 15.19,
      "grad_norm": 362844.3125,
      "learning_rate": 2.478844169246646e-05,
      "loss": 52.8297,
      "step": 15176
    },
    {
      "epoch": 15.19,
      "grad_norm": 62670.6875,
      "learning_rate": 2.478328173374613e-05,
      "loss": 54.0476,
      "step": 15177
    },
    {
      "epoch": 15.19,
      "grad_norm": 3443.360595703125,
      "learning_rate": 2.47781217750258e-05,
      "loss": 61.9809,
      "step": 15178
    },
    {
      "epoch": 15.19,
      "grad_norm": 17426.236328125,
      "learning_rate": 2.477296181630547e-05,
      "loss": 44.0321,
      "step": 15179
    },
    {
      "epoch": 15.2,
      "grad_norm": 4537.14697265625,
      "learning_rate": 2.476780185758514e-05,
      "loss": 47.1794,
      "step": 15180
    },
    {
      "epoch": 15.2,
      "grad_norm": 9214.2548828125,
      "learning_rate": 2.4762641898864812e-05,
      "loss": 48.8457,
      "step": 15181
    },
    {
      "epoch": 15.2,
      "grad_norm": 12591.4248046875,
      "learning_rate": 2.475748194014448e-05,
      "loss": 48.3098,
      "step": 15182
    },
    {
      "epoch": 15.2,
      "grad_norm": 4855.87841796875,
      "learning_rate": 2.475232198142415e-05,
      "loss": 35.5066,
      "step": 15183
    },
    {
      "epoch": 15.2,
      "grad_norm": 13485.2841796875,
      "learning_rate": 2.4747162022703822e-05,
      "loss": 52.3277,
      "step": 15184
    },
    {
      "epoch": 15.2,
      "grad_norm": 7806.30615234375,
      "learning_rate": 2.4742002063983487e-05,
      "loss": 52.4762,
      "step": 15185
    },
    {
      "epoch": 15.2,
      "grad_norm": 39351.9453125,
      "learning_rate": 2.4736842105263158e-05,
      "loss": 53.8171,
      "step": 15186
    },
    {
      "epoch": 15.2,
      "grad_norm": 12321.0615234375,
      "learning_rate": 2.473168214654283e-05,
      "loss": 50.7288,
      "step": 15187
    },
    {
      "epoch": 15.2,
      "grad_norm": 13254.9111328125,
      "learning_rate": 2.47265221878225e-05,
      "loss": 35.072,
      "step": 15188
    },
    {
      "epoch": 15.2,
      "grad_norm": 2710.231201171875,
      "learning_rate": 2.4721362229102168e-05,
      "loss": 50.1113,
      "step": 15189
    },
    {
      "epoch": 15.21,
      "grad_norm": 29831.173828125,
      "learning_rate": 2.471620227038184e-05,
      "loss": 55.2448,
      "step": 15190
    },
    {
      "epoch": 15.21,
      "grad_norm": 72450.4375,
      "learning_rate": 2.471104231166151e-05,
      "loss": 47.4627,
      "step": 15191
    },
    {
      "epoch": 15.21,
      "grad_norm": 6210.42041015625,
      "learning_rate": 2.4705882352941178e-05,
      "loss": 45.4281,
      "step": 15192
    },
    {
      "epoch": 15.21,
      "grad_norm": 2909.123291015625,
      "learning_rate": 2.470072239422085e-05,
      "loss": 39.8897,
      "step": 15193
    },
    {
      "epoch": 15.21,
      "grad_norm": 127470.5703125,
      "learning_rate": 2.4695562435500517e-05,
      "loss": 62.9159,
      "step": 15194
    },
    {
      "epoch": 15.21,
      "grad_norm": 11725.4716796875,
      "learning_rate": 2.4690402476780188e-05,
      "loss": 45.1441,
      "step": 15195
    },
    {
      "epoch": 15.21,
      "grad_norm": 5339.49951171875,
      "learning_rate": 2.4685242518059855e-05,
      "loss": 43.4341,
      "step": 15196
    },
    {
      "epoch": 15.21,
      "grad_norm": 732.1058349609375,
      "learning_rate": 2.4680082559339527e-05,
      "loss": 59.7166,
      "step": 15197
    },
    {
      "epoch": 15.21,
      "grad_norm": 7588.32080078125,
      "learning_rate": 2.4674922600619198e-05,
      "loss": 46.6275,
      "step": 15198
    },
    {
      "epoch": 15.21,
      "grad_norm": 8939.662109375,
      "learning_rate": 2.4669762641898865e-05,
      "loss": 43.5555,
      "step": 15199
    },
    {
      "epoch": 15.22,
      "grad_norm": 41800.02734375,
      "learning_rate": 2.4664602683178537e-05,
      "loss": 55.6792,
      "step": 15200
    },
    {
      "epoch": 15.22,
      "grad_norm": 22773.708984375,
      "learning_rate": 2.4659442724458208e-05,
      "loss": 55.4519,
      "step": 15201
    },
    {
      "epoch": 15.22,
      "grad_norm": 72004.25,
      "learning_rate": 2.4654282765737875e-05,
      "loss": 46.2993,
      "step": 15202
    },
    {
      "epoch": 15.22,
      "grad_norm": 25114.7109375,
      "learning_rate": 2.4649122807017543e-05,
      "loss": 35.3275,
      "step": 15203
    },
    {
      "epoch": 15.22,
      "grad_norm": 68705.234375,
      "learning_rate": 2.4643962848297214e-05,
      "loss": 53.5721,
      "step": 15204
    },
    {
      "epoch": 15.22,
      "grad_norm": 24548.89453125,
      "learning_rate": 2.4638802889576885e-05,
      "loss": 30.4708,
      "step": 15205
    },
    {
      "epoch": 15.22,
      "grad_norm": 5631.9501953125,
      "learning_rate": 2.4633642930856553e-05,
      "loss": 51.2062,
      "step": 15206
    },
    {
      "epoch": 15.22,
      "grad_norm": 36386.21875,
      "learning_rate": 2.4628482972136224e-05,
      "loss": 48.5135,
      "step": 15207
    },
    {
      "epoch": 15.22,
      "grad_norm": 8712.927734375,
      "learning_rate": 2.4623323013415895e-05,
      "loss": 48.564,
      "step": 15208
    },
    {
      "epoch": 15.22,
      "grad_norm": 4046.24169921875,
      "learning_rate": 2.4618163054695563e-05,
      "loss": 53.3758,
      "step": 15209
    },
    {
      "epoch": 15.23,
      "grad_norm": 26107.84375,
      "learning_rate": 2.4613003095975234e-05,
      "loss": 40.1973,
      "step": 15210
    },
    {
      "epoch": 15.23,
      "grad_norm": 152420.96875,
      "learning_rate": 2.4607843137254902e-05,
      "loss": 52.0484,
      "step": 15211
    },
    {
      "epoch": 15.23,
      "grad_norm": 22875.828125,
      "learning_rate": 2.4602683178534573e-05,
      "loss": 49.5253,
      "step": 15212
    },
    {
      "epoch": 15.23,
      "grad_norm": 3278.656494140625,
      "learning_rate": 2.459752321981424e-05,
      "loss": 56.0746,
      "step": 15213
    },
    {
      "epoch": 15.23,
      "grad_norm": 16155.1728515625,
      "learning_rate": 2.4592363261093912e-05,
      "loss": 48.829,
      "step": 15214
    },
    {
      "epoch": 15.23,
      "grad_norm": 9630.1435546875,
      "learning_rate": 2.4587203302373583e-05,
      "loss": 44.5192,
      "step": 15215
    },
    {
      "epoch": 15.23,
      "grad_norm": 8516.4716796875,
      "learning_rate": 2.458204334365325e-05,
      "loss": 41.8391,
      "step": 15216
    },
    {
      "epoch": 15.23,
      "grad_norm": 6114.75537109375,
      "learning_rate": 2.4576883384932922e-05,
      "loss": 42.0343,
      "step": 15217
    },
    {
      "epoch": 15.23,
      "grad_norm": 4330.3759765625,
      "learning_rate": 2.4571723426212593e-05,
      "loss": 53.5805,
      "step": 15218
    },
    {
      "epoch": 15.23,
      "grad_norm": 1830.4481201171875,
      "learning_rate": 2.456656346749226e-05,
      "loss": 52.9277,
      "step": 15219
    },
    {
      "epoch": 15.24,
      "grad_norm": 5659.4736328125,
      "learning_rate": 2.456140350877193e-05,
      "loss": 54.4838,
      "step": 15220
    },
    {
      "epoch": 15.24,
      "grad_norm": 57910.16015625,
      "learning_rate": 2.45562435500516e-05,
      "loss": 62.6755,
      "step": 15221
    },
    {
      "epoch": 15.24,
      "grad_norm": 12626.36328125,
      "learning_rate": 2.455108359133127e-05,
      "loss": 65.9492,
      "step": 15222
    },
    {
      "epoch": 15.24,
      "grad_norm": 17627.501953125,
      "learning_rate": 2.454592363261094e-05,
      "loss": 56.149,
      "step": 15223
    },
    {
      "epoch": 15.24,
      "grad_norm": 25951.837890625,
      "learning_rate": 2.454076367389061e-05,
      "loss": 51.2595,
      "step": 15224
    },
    {
      "epoch": 15.24,
      "grad_norm": 15978.6083984375,
      "learning_rate": 2.453560371517028e-05,
      "loss": 45.6005,
      "step": 15225
    },
    {
      "epoch": 15.24,
      "grad_norm": 3602.6591796875,
      "learning_rate": 2.453044375644995e-05,
      "loss": 57.3595,
      "step": 15226
    },
    {
      "epoch": 15.24,
      "grad_norm": 63168.53125,
      "learning_rate": 2.452528379772962e-05,
      "loss": 52.7252,
      "step": 15227
    },
    {
      "epoch": 15.24,
      "grad_norm": 24550.474609375,
      "learning_rate": 2.452012383900929e-05,
      "loss": 49.7513,
      "step": 15228
    },
    {
      "epoch": 15.24,
      "grad_norm": 29217.482421875,
      "learning_rate": 2.451496388028896e-05,
      "loss": 54.4063,
      "step": 15229
    },
    {
      "epoch": 15.25,
      "grad_norm": 44944.70703125,
      "learning_rate": 2.4509803921568626e-05,
      "loss": 54.3171,
      "step": 15230
    },
    {
      "epoch": 15.25,
      "grad_norm": 9232.8505859375,
      "learning_rate": 2.4504643962848298e-05,
      "loss": 52.0897,
      "step": 15231
    },
    {
      "epoch": 15.25,
      "grad_norm": 12852.333984375,
      "learning_rate": 2.449948400412797e-05,
      "loss": 30.8273,
      "step": 15232
    },
    {
      "epoch": 15.25,
      "grad_norm": 11676.7158203125,
      "learning_rate": 2.4494324045407636e-05,
      "loss": 53.4576,
      "step": 15233
    },
    {
      "epoch": 15.25,
      "grad_norm": 44006.14453125,
      "learning_rate": 2.4489164086687308e-05,
      "loss": 52.7571,
      "step": 15234
    },
    {
      "epoch": 15.25,
      "grad_norm": 15817.5947265625,
      "learning_rate": 2.448400412796698e-05,
      "loss": 58.3036,
      "step": 15235
    },
    {
      "epoch": 15.25,
      "grad_norm": 9293.8603515625,
      "learning_rate": 2.4478844169246646e-05,
      "loss": 56.4811,
      "step": 15236
    },
    {
      "epoch": 15.25,
      "grad_norm": 9788.4296875,
      "learning_rate": 2.4473684210526318e-05,
      "loss": 23.1714,
      "step": 15237
    },
    {
      "epoch": 15.25,
      "grad_norm": 3626.62158203125,
      "learning_rate": 2.4468524251805985e-05,
      "loss": 55.4932,
      "step": 15238
    },
    {
      "epoch": 15.25,
      "grad_norm": 13838.923828125,
      "learning_rate": 2.4463364293085656e-05,
      "loss": 51.1174,
      "step": 15239
    },
    {
      "epoch": 15.26,
      "grad_norm": 29794.8046875,
      "learning_rate": 2.4458204334365324e-05,
      "loss": 48.2054,
      "step": 15240
    },
    {
      "epoch": 15.26,
      "grad_norm": 22517.265625,
      "learning_rate": 2.4453044375644995e-05,
      "loss": 52.329,
      "step": 15241
    },
    {
      "epoch": 15.26,
      "grad_norm": 7431.212890625,
      "learning_rate": 2.4447884416924666e-05,
      "loss": 51.6856,
      "step": 15242
    },
    {
      "epoch": 15.26,
      "grad_norm": 14886.53125,
      "learning_rate": 2.4442724458204334e-05,
      "loss": 58.6784,
      "step": 15243
    },
    {
      "epoch": 15.26,
      "grad_norm": 2276.388916015625,
      "learning_rate": 2.4437564499484005e-05,
      "loss": 45.4121,
      "step": 15244
    },
    {
      "epoch": 15.26,
      "grad_norm": 10892.3037109375,
      "learning_rate": 2.4432404540763676e-05,
      "loss": 48.1156,
      "step": 15245
    },
    {
      "epoch": 15.26,
      "grad_norm": 6748.00390625,
      "learning_rate": 2.4427244582043348e-05,
      "loss": 54.5762,
      "step": 15246
    },
    {
      "epoch": 15.26,
      "grad_norm": 5443.404296875,
      "learning_rate": 2.4422084623323012e-05,
      "loss": 49.1261,
      "step": 15247
    },
    {
      "epoch": 15.26,
      "grad_norm": 25664.623046875,
      "learning_rate": 2.4416924664602683e-05,
      "loss": 51.4872,
      "step": 15248
    },
    {
      "epoch": 15.26,
      "grad_norm": 47327.4453125,
      "learning_rate": 2.4411764705882354e-05,
      "loss": 40.5855,
      "step": 15249
    },
    {
      "epoch": 15.27,
      "grad_norm": 11727.6689453125,
      "learning_rate": 2.4406604747162022e-05,
      "loss": 31.3032,
      "step": 15250
    },
    {
      "epoch": 15.27,
      "grad_norm": 2469.626220703125,
      "learning_rate": 2.4401444788441693e-05,
      "loss": 61.1633,
      "step": 15251
    },
    {
      "epoch": 15.27,
      "grad_norm": 5410.84033203125,
      "learning_rate": 2.4396284829721364e-05,
      "loss": 49.9303,
      "step": 15252
    },
    {
      "epoch": 15.27,
      "grad_norm": 63436.90234375,
      "learning_rate": 2.4391124871001035e-05,
      "loss": 47.6735,
      "step": 15253
    },
    {
      "epoch": 15.27,
      "grad_norm": 16075.1201171875,
      "learning_rate": 2.4385964912280703e-05,
      "loss": 54.6051,
      "step": 15254
    },
    {
      "epoch": 15.27,
      "grad_norm": 8408.685546875,
      "learning_rate": 2.4380804953560374e-05,
      "loss": 56.0992,
      "step": 15255
    },
    {
      "epoch": 15.27,
      "grad_norm": 49090.90625,
      "learning_rate": 2.4375644994840042e-05,
      "loss": 58.3199,
      "step": 15256
    },
    {
      "epoch": 15.27,
      "grad_norm": 5386.990234375,
      "learning_rate": 2.437048503611971e-05,
      "loss": 51.1081,
      "step": 15257
    },
    {
      "epoch": 15.27,
      "grad_norm": 21073.177734375,
      "learning_rate": 2.436532507739938e-05,
      "loss": 56.221,
      "step": 15258
    },
    {
      "epoch": 15.27,
      "grad_norm": 2155169.75,
      "learning_rate": 2.4360165118679052e-05,
      "loss": 52.2609,
      "step": 15259
    },
    {
      "epoch": 15.28,
      "grad_norm": 7764.4150390625,
      "learning_rate": 2.4355005159958723e-05,
      "loss": 48.8036,
      "step": 15260
    },
    {
      "epoch": 15.28,
      "grad_norm": 12350.38671875,
      "learning_rate": 2.434984520123839e-05,
      "loss": 53.2384,
      "step": 15261
    },
    {
      "epoch": 15.28,
      "grad_norm": 18357.15625,
      "learning_rate": 2.4344685242518062e-05,
      "loss": 30.213,
      "step": 15262
    },
    {
      "epoch": 15.28,
      "grad_norm": 4355.88330078125,
      "learning_rate": 2.4339525283797733e-05,
      "loss": 45.2995,
      "step": 15263
    },
    {
      "epoch": 15.28,
      "grad_norm": 14658.2998046875,
      "learning_rate": 2.43343653250774e-05,
      "loss": 49.427,
      "step": 15264
    },
    {
      "epoch": 15.28,
      "grad_norm": 149344.078125,
      "learning_rate": 2.432920536635707e-05,
      "loss": 19.2547,
      "step": 15265
    },
    {
      "epoch": 15.28,
      "grad_norm": 20432.37109375,
      "learning_rate": 2.432404540763674e-05,
      "loss": 58.1561,
      "step": 15266
    },
    {
      "epoch": 15.28,
      "grad_norm": 2338.49462890625,
      "learning_rate": 2.431888544891641e-05,
      "loss": 57.4342,
      "step": 15267
    },
    {
      "epoch": 15.28,
      "grad_norm": 3436.186279296875,
      "learning_rate": 2.431372549019608e-05,
      "loss": 54.752,
      "step": 15268
    },
    {
      "epoch": 15.28,
      "grad_norm": 3458.714111328125,
      "learning_rate": 2.430856553147575e-05,
      "loss": 58.7062,
      "step": 15269
    },
    {
      "epoch": 15.29,
      "grad_norm": 18716.775390625,
      "learning_rate": 2.430340557275542e-05,
      "loss": 45.8251,
      "step": 15270
    },
    {
      "epoch": 15.29,
      "grad_norm": 33441.49609375,
      "learning_rate": 2.429824561403509e-05,
      "loss": 59.2094,
      "step": 15271
    },
    {
      "epoch": 15.29,
      "grad_norm": 9371.6259765625,
      "learning_rate": 2.429308565531476e-05,
      "loss": 40.0232,
      "step": 15272
    },
    {
      "epoch": 15.29,
      "grad_norm": 4906.19189453125,
      "learning_rate": 2.428792569659443e-05,
      "loss": 53.2117,
      "step": 15273
    },
    {
      "epoch": 15.29,
      "grad_norm": 1693.514892578125,
      "learning_rate": 2.42827657378741e-05,
      "loss": 46.7126,
      "step": 15274
    },
    {
      "epoch": 15.29,
      "grad_norm": 24343.755859375,
      "learning_rate": 2.4277605779153766e-05,
      "loss": 59.2346,
      "step": 15275
    },
    {
      "epoch": 15.29,
      "grad_norm": 29551.853515625,
      "learning_rate": 2.4272445820433437e-05,
      "loss": 31.4036,
      "step": 15276
    },
    {
      "epoch": 15.29,
      "grad_norm": 75896.859375,
      "learning_rate": 2.426728586171311e-05,
      "loss": 54.9166,
      "step": 15277
    },
    {
      "epoch": 15.29,
      "grad_norm": 59472.15234375,
      "learning_rate": 2.4262125902992776e-05,
      "loss": 31.2104,
      "step": 15278
    },
    {
      "epoch": 15.29,
      "grad_norm": 23267.01171875,
      "learning_rate": 2.4256965944272447e-05,
      "loss": 58.277,
      "step": 15279
    },
    {
      "epoch": 15.3,
      "grad_norm": 44811.59765625,
      "learning_rate": 2.425180598555212e-05,
      "loss": 27.559,
      "step": 15280
    },
    {
      "epoch": 15.3,
      "grad_norm": 28506.25390625,
      "learning_rate": 2.4246646026831786e-05,
      "loss": 42.2752,
      "step": 15281
    },
    {
      "epoch": 15.3,
      "grad_norm": 203811.359375,
      "learning_rate": 2.4241486068111457e-05,
      "loss": 43.8569,
      "step": 15282
    },
    {
      "epoch": 15.3,
      "grad_norm": 39123.109375,
      "learning_rate": 2.4236326109391125e-05,
      "loss": 53.6213,
      "step": 15283
    },
    {
      "epoch": 15.3,
      "grad_norm": 46071.83984375,
      "learning_rate": 2.4231166150670796e-05,
      "loss": 47.0856,
      "step": 15284
    },
    {
      "epoch": 15.3,
      "grad_norm": 37587.34765625,
      "learning_rate": 2.4226006191950464e-05,
      "loss": 54.3468,
      "step": 15285
    },
    {
      "epoch": 15.3,
      "grad_norm": 129050.1328125,
      "learning_rate": 2.4220846233230135e-05,
      "loss": 54.5308,
      "step": 15286
    },
    {
      "epoch": 15.3,
      "grad_norm": 15112.001953125,
      "learning_rate": 2.4215686274509806e-05,
      "loss": 50.6036,
      "step": 15287
    },
    {
      "epoch": 15.3,
      "grad_norm": 21725.9140625,
      "learning_rate": 2.4210526315789474e-05,
      "loss": 55.3493,
      "step": 15288
    },
    {
      "epoch": 15.3,
      "grad_norm": 6079.53271484375,
      "learning_rate": 2.4205366357069145e-05,
      "loss": 53.2524,
      "step": 15289
    },
    {
      "epoch": 15.31,
      "grad_norm": 15192.0556640625,
      "learning_rate": 2.4200206398348816e-05,
      "loss": 53.8879,
      "step": 15290
    },
    {
      "epoch": 15.31,
      "grad_norm": 6694.90185546875,
      "learning_rate": 2.4195046439628484e-05,
      "loss": 52.8323,
      "step": 15291
    },
    {
      "epoch": 15.31,
      "grad_norm": 22676.580078125,
      "learning_rate": 2.4189886480908152e-05,
      "loss": 58.3765,
      "step": 15292
    },
    {
      "epoch": 15.31,
      "grad_norm": 5621.115234375,
      "learning_rate": 2.4184726522187823e-05,
      "loss": 36.342,
      "step": 15293
    },
    {
      "epoch": 15.31,
      "grad_norm": 5912.3681640625,
      "learning_rate": 2.4179566563467494e-05,
      "loss": 54.7817,
      "step": 15294
    },
    {
      "epoch": 15.31,
      "grad_norm": 8538.767578125,
      "learning_rate": 2.4174406604747162e-05,
      "loss": 55.142,
      "step": 15295
    },
    {
      "epoch": 15.31,
      "grad_norm": 35901.4765625,
      "learning_rate": 2.4169246646026833e-05,
      "loss": 53.5526,
      "step": 15296
    },
    {
      "epoch": 15.31,
      "grad_norm": 5866.42578125,
      "learning_rate": 2.4164086687306504e-05,
      "loss": 38.8147,
      "step": 15297
    },
    {
      "epoch": 15.31,
      "grad_norm": 4569.91259765625,
      "learning_rate": 2.4158926728586172e-05,
      "loss": 50.6703,
      "step": 15298
    },
    {
      "epoch": 15.31,
      "grad_norm": 4525.599609375,
      "learning_rate": 2.4153766769865843e-05,
      "loss": 59.7005,
      "step": 15299
    },
    {
      "epoch": 15.32,
      "grad_norm": 18679.40625,
      "learning_rate": 2.4148606811145514e-05,
      "loss": 54.2952,
      "step": 15300
    },
    {
      "epoch": 15.32,
      "grad_norm": 6714.99560546875,
      "learning_rate": 2.4143446852425182e-05,
      "loss": 50.8204,
      "step": 15301
    },
    {
      "epoch": 15.32,
      "grad_norm": 15478.189453125,
      "learning_rate": 2.413828689370485e-05,
      "loss": 48.9493,
      "step": 15302
    },
    {
      "epoch": 15.32,
      "grad_norm": 2898.537109375,
      "learning_rate": 2.413312693498452e-05,
      "loss": 59.1423,
      "step": 15303
    },
    {
      "epoch": 15.32,
      "grad_norm": 3410.048583984375,
      "learning_rate": 2.4127966976264192e-05,
      "loss": 56.6933,
      "step": 15304
    },
    {
      "epoch": 15.32,
      "grad_norm": 3917.185302734375,
      "learning_rate": 2.412280701754386e-05,
      "loss": 45.2753,
      "step": 15305
    },
    {
      "epoch": 15.32,
      "grad_norm": 21302.271484375,
      "learning_rate": 2.411764705882353e-05,
      "loss": 59.1821,
      "step": 15306
    },
    {
      "epoch": 15.32,
      "grad_norm": 2051.663330078125,
      "learning_rate": 2.4112487100103202e-05,
      "loss": 43.6938,
      "step": 15307
    },
    {
      "epoch": 15.32,
      "grad_norm": 2964.751220703125,
      "learning_rate": 2.410732714138287e-05,
      "loss": 48.7469,
      "step": 15308
    },
    {
      "epoch": 15.32,
      "grad_norm": 58689.53515625,
      "learning_rate": 2.410216718266254e-05,
      "loss": 56.6331,
      "step": 15309
    },
    {
      "epoch": 15.33,
      "grad_norm": 6354.8623046875,
      "learning_rate": 2.409700722394221e-05,
      "loss": 42.377,
      "step": 15310
    },
    {
      "epoch": 15.33,
      "grad_norm": 24971.84375,
      "learning_rate": 2.409184726522188e-05,
      "loss": 51.1624,
      "step": 15311
    },
    {
      "epoch": 15.33,
      "grad_norm": 5374.76123046875,
      "learning_rate": 2.4086687306501547e-05,
      "loss": 52.2983,
      "step": 15312
    },
    {
      "epoch": 15.33,
      "grad_norm": 5293.931640625,
      "learning_rate": 2.408152734778122e-05,
      "loss": 44.3584,
      "step": 15313
    },
    {
      "epoch": 15.33,
      "grad_norm": 10354.4697265625,
      "learning_rate": 2.407636738906089e-05,
      "loss": 46.3816,
      "step": 15314
    },
    {
      "epoch": 15.33,
      "grad_norm": 9906.7861328125,
      "learning_rate": 2.4071207430340557e-05,
      "loss": 48.5739,
      "step": 15315
    },
    {
      "epoch": 15.33,
      "grad_norm": 20345.01953125,
      "learning_rate": 2.406604747162023e-05,
      "loss": 44.9083,
      "step": 15316
    },
    {
      "epoch": 15.33,
      "grad_norm": 175953.40625,
      "learning_rate": 2.40608875128999e-05,
      "loss": 54.7713,
      "step": 15317
    },
    {
      "epoch": 15.33,
      "grad_norm": 7151.37255859375,
      "learning_rate": 2.405572755417957e-05,
      "loss": 51.9465,
      "step": 15318
    },
    {
      "epoch": 15.33,
      "grad_norm": 18530.25390625,
      "learning_rate": 2.4050567595459235e-05,
      "loss": 58.7627,
      "step": 15319
    },
    {
      "epoch": 15.34,
      "grad_norm": 14294.6318359375,
      "learning_rate": 2.4045407636738906e-05,
      "loss": 48.2337,
      "step": 15320
    },
    {
      "epoch": 15.34,
      "grad_norm": 8010.26611328125,
      "learning_rate": 2.4040247678018577e-05,
      "loss": 61.3414,
      "step": 15321
    },
    {
      "epoch": 15.34,
      "grad_norm": 6744.10009765625,
      "learning_rate": 2.4035087719298245e-05,
      "loss": 52.8366,
      "step": 15322
    },
    {
      "epoch": 15.34,
      "grad_norm": 43929.96875,
      "learning_rate": 2.4029927760577916e-05,
      "loss": 31.41,
      "step": 15323
    },
    {
      "epoch": 15.34,
      "grad_norm": 3310.928955078125,
      "learning_rate": 2.4024767801857587e-05,
      "loss": 54.4037,
      "step": 15324
    },
    {
      "epoch": 15.34,
      "grad_norm": 3573.110595703125,
      "learning_rate": 2.401960784313726e-05,
      "loss": 49.2694,
      "step": 15325
    },
    {
      "epoch": 15.34,
      "grad_norm": 2642.125244140625,
      "learning_rate": 2.4014447884416926e-05,
      "loss": 48.4638,
      "step": 15326
    },
    {
      "epoch": 15.34,
      "grad_norm": 7905.12451171875,
      "learning_rate": 2.4009287925696597e-05,
      "loss": 53.8452,
      "step": 15327
    },
    {
      "epoch": 15.34,
      "grad_norm": 26033.681640625,
      "learning_rate": 2.4004127966976265e-05,
      "loss": 44.646,
      "step": 15328
    },
    {
      "epoch": 15.34,
      "grad_norm": 14284.9658203125,
      "learning_rate": 2.3998968008255933e-05,
      "loss": 60.3575,
      "step": 15329
    },
    {
      "epoch": 15.35,
      "grad_norm": 6073.17431640625,
      "learning_rate": 2.3993808049535604e-05,
      "loss": 53.787,
      "step": 15330
    },
    {
      "epoch": 15.35,
      "grad_norm": 29135.328125,
      "learning_rate": 2.3988648090815275e-05,
      "loss": 22.6333,
      "step": 15331
    },
    {
      "epoch": 15.35,
      "grad_norm": 10717.26953125,
      "learning_rate": 2.3983488132094943e-05,
      "loss": 51.2394,
      "step": 15332
    },
    {
      "epoch": 15.35,
      "grad_norm": 5681.9453125,
      "learning_rate": 2.3978328173374614e-05,
      "loss": 53.7223,
      "step": 15333
    },
    {
      "epoch": 15.35,
      "grad_norm": 105664.734375,
      "learning_rate": 2.3973168214654285e-05,
      "loss": 32.5107,
      "step": 15334
    },
    {
      "epoch": 15.35,
      "grad_norm": 3179.97265625,
      "learning_rate": 2.3968008255933956e-05,
      "loss": 49.5146,
      "step": 15335
    },
    {
      "epoch": 15.35,
      "grad_norm": 59121.984375,
      "learning_rate": 2.3962848297213624e-05,
      "loss": 48.6875,
      "step": 15336
    },
    {
      "epoch": 15.35,
      "grad_norm": 145882.078125,
      "learning_rate": 2.395768833849329e-05,
      "loss": 48.8909,
      "step": 15337
    },
    {
      "epoch": 15.35,
      "grad_norm": 107607.1875,
      "learning_rate": 2.3952528379772963e-05,
      "loss": 40.0358,
      "step": 15338
    },
    {
      "epoch": 15.35,
      "grad_norm": 9673.3642578125,
      "learning_rate": 2.394736842105263e-05,
      "loss": 46.4031,
      "step": 15339
    },
    {
      "epoch": 15.36,
      "grad_norm": 8511.6259765625,
      "learning_rate": 2.39422084623323e-05,
      "loss": 45.9254,
      "step": 15340
    },
    {
      "epoch": 15.36,
      "grad_norm": 7058.1357421875,
      "learning_rate": 2.3937048503611973e-05,
      "loss": 53.1192,
      "step": 15341
    },
    {
      "epoch": 15.36,
      "grad_norm": 39548.55859375,
      "learning_rate": 2.3931888544891644e-05,
      "loss": 41.6704,
      "step": 15342
    },
    {
      "epoch": 15.36,
      "grad_norm": 34661.52734375,
      "learning_rate": 2.392672858617131e-05,
      "loss": 46.4304,
      "step": 15343
    },
    {
      "epoch": 15.36,
      "grad_norm": 64843.05859375,
      "learning_rate": 2.3921568627450983e-05,
      "loss": 47.4272,
      "step": 15344
    },
    {
      "epoch": 15.36,
      "grad_norm": 2271.36865234375,
      "learning_rate": 2.3916408668730654e-05,
      "loss": 51.3816,
      "step": 15345
    },
    {
      "epoch": 15.36,
      "grad_norm": 5335.86572265625,
      "learning_rate": 2.3911248710010318e-05,
      "loss": 41.4486,
      "step": 15346
    },
    {
      "epoch": 15.36,
      "grad_norm": 4691.02685546875,
      "learning_rate": 2.390608875128999e-05,
      "loss": 46.574,
      "step": 15347
    },
    {
      "epoch": 15.36,
      "grad_norm": 6960.0380859375,
      "learning_rate": 2.390092879256966e-05,
      "loss": 43.1963,
      "step": 15348
    },
    {
      "epoch": 15.36,
      "grad_norm": 5988.30078125,
      "learning_rate": 2.389576883384933e-05,
      "loss": 49.1447,
      "step": 15349
    },
    {
      "epoch": 15.37,
      "grad_norm": 16719.041015625,
      "learning_rate": 2.3890608875129e-05,
      "loss": 27.6815,
      "step": 15350
    },
    {
      "epoch": 15.37,
      "grad_norm": 12874.7548828125,
      "learning_rate": 2.388544891640867e-05,
      "loss": 46.9431,
      "step": 15351
    },
    {
      "epoch": 15.37,
      "grad_norm": 59842.7890625,
      "learning_rate": 2.388028895768834e-05,
      "loss": 45.3456,
      "step": 15352
    },
    {
      "epoch": 15.37,
      "grad_norm": 4960.60302734375,
      "learning_rate": 2.387512899896801e-05,
      "loss": 43.4088,
      "step": 15353
    },
    {
      "epoch": 15.37,
      "grad_norm": 2740.730712890625,
      "learning_rate": 2.386996904024768e-05,
      "loss": 57.4856,
      "step": 15354
    },
    {
      "epoch": 15.37,
      "grad_norm": 22979.623046875,
      "learning_rate": 2.386480908152735e-05,
      "loss": 46.6786,
      "step": 15355
    },
    {
      "epoch": 15.37,
      "grad_norm": 22394.158203125,
      "learning_rate": 2.385964912280702e-05,
      "loss": 46.4312,
      "step": 15356
    },
    {
      "epoch": 15.37,
      "grad_norm": 4071.538818359375,
      "learning_rate": 2.3854489164086687e-05,
      "loss": 58.0391,
      "step": 15357
    },
    {
      "epoch": 15.37,
      "grad_norm": 3301.28271484375,
      "learning_rate": 2.384932920536636e-05,
      "loss": 45.5018,
      "step": 15358
    },
    {
      "epoch": 15.37,
      "grad_norm": 3828.712890625,
      "learning_rate": 2.384416924664603e-05,
      "loss": 52.0355,
      "step": 15359
    },
    {
      "epoch": 15.38,
      "grad_norm": 11510.5771484375,
      "learning_rate": 2.3839009287925697e-05,
      "loss": 59.076,
      "step": 15360
    },
    {
      "epoch": 15.38,
      "grad_norm": 6093.646484375,
      "learning_rate": 2.383384932920537e-05,
      "loss": 65.0816,
      "step": 15361
    },
    {
      "epoch": 15.38,
      "grad_norm": 20693.939453125,
      "learning_rate": 2.382868937048504e-05,
      "loss": 45.1646,
      "step": 15362
    },
    {
      "epoch": 15.38,
      "grad_norm": 5953.9501953125,
      "learning_rate": 2.3823529411764707e-05,
      "loss": 56.6317,
      "step": 15363
    },
    {
      "epoch": 15.38,
      "grad_norm": 69211.9921875,
      "learning_rate": 2.3818369453044375e-05,
      "loss": 55.1974,
      "step": 15364
    },
    {
      "epoch": 15.38,
      "grad_norm": 13222.654296875,
      "learning_rate": 2.3813209494324046e-05,
      "loss": 49.3992,
      "step": 15365
    },
    {
      "epoch": 15.38,
      "grad_norm": 26258.009765625,
      "learning_rate": 2.3808049535603717e-05,
      "loss": 47.0668,
      "step": 15366
    },
    {
      "epoch": 15.38,
      "grad_norm": 3117.0048828125,
      "learning_rate": 2.3802889576883385e-05,
      "loss": 59.079,
      "step": 15367
    },
    {
      "epoch": 15.38,
      "grad_norm": 6181.21435546875,
      "learning_rate": 2.3797729618163056e-05,
      "loss": 36.973,
      "step": 15368
    },
    {
      "epoch": 15.38,
      "grad_norm": 173339.015625,
      "learning_rate": 2.3792569659442727e-05,
      "loss": 34.2148,
      "step": 15369
    },
    {
      "epoch": 15.39,
      "grad_norm": 6441.37109375,
      "learning_rate": 2.3787409700722395e-05,
      "loss": 48.9331,
      "step": 15370
    },
    {
      "epoch": 15.39,
      "grad_norm": 59941.30859375,
      "learning_rate": 2.3782249742002066e-05,
      "loss": 51.4883,
      "step": 15371
    },
    {
      "epoch": 15.39,
      "grad_norm": 11762.513671875,
      "learning_rate": 2.3777089783281734e-05,
      "loss": 34.9732,
      "step": 15372
    },
    {
      "epoch": 15.39,
      "grad_norm": 15262.255859375,
      "learning_rate": 2.3771929824561405e-05,
      "loss": 36.9092,
      "step": 15373
    },
    {
      "epoch": 15.39,
      "grad_norm": 25530.783203125,
      "learning_rate": 2.3766769865841073e-05,
      "loss": 59.1295,
      "step": 15374
    },
    {
      "epoch": 15.39,
      "grad_norm": 7887.2529296875,
      "learning_rate": 2.3761609907120744e-05,
      "loss": 52.8729,
      "step": 15375
    },
    {
      "epoch": 15.39,
      "grad_norm": 9849.119140625,
      "learning_rate": 2.3756449948400415e-05,
      "loss": 53.2892,
      "step": 15376
    },
    {
      "epoch": 15.39,
      "grad_norm": 13306.5517578125,
      "learning_rate": 2.3751289989680083e-05,
      "loss": 55.3225,
      "step": 15377
    },
    {
      "epoch": 15.39,
      "grad_norm": 7023.44189453125,
      "learning_rate": 2.3746130030959754e-05,
      "loss": 57.71,
      "step": 15378
    },
    {
      "epoch": 15.39,
      "grad_norm": 13453.935546875,
      "learning_rate": 2.3740970072239425e-05,
      "loss": 54.9178,
      "step": 15379
    },
    {
      "epoch": 15.4,
      "grad_norm": 55170.41015625,
      "learning_rate": 2.3735810113519093e-05,
      "loss": 24.3226,
      "step": 15380
    },
    {
      "epoch": 15.4,
      "grad_norm": 84867.4375,
      "learning_rate": 2.373065015479876e-05,
      "loss": 58.1845,
      "step": 15381
    },
    {
      "epoch": 15.4,
      "grad_norm": 2339.43310546875,
      "learning_rate": 2.372549019607843e-05,
      "loss": 56.9106,
      "step": 15382
    },
    {
      "epoch": 15.4,
      "grad_norm": 5821.662109375,
      "learning_rate": 2.3720330237358103e-05,
      "loss": 47.0665,
      "step": 15383
    },
    {
      "epoch": 15.4,
      "grad_norm": 14587.8359375,
      "learning_rate": 2.371517027863777e-05,
      "loss": 43.009,
      "step": 15384
    },
    {
      "epoch": 15.4,
      "grad_norm": 4405.96728515625,
      "learning_rate": 2.371001031991744e-05,
      "loss": 52.0233,
      "step": 15385
    },
    {
      "epoch": 15.4,
      "grad_norm": 2731.24658203125,
      "learning_rate": 2.3704850361197113e-05,
      "loss": 41.332,
      "step": 15386
    },
    {
      "epoch": 15.4,
      "grad_norm": 7749.95703125,
      "learning_rate": 2.369969040247678e-05,
      "loss": 53.8934,
      "step": 15387
    },
    {
      "epoch": 15.4,
      "grad_norm": 24039.578125,
      "learning_rate": 2.369453044375645e-05,
      "loss": 47.3873,
      "step": 15388
    },
    {
      "epoch": 15.4,
      "grad_norm": 34408.625,
      "learning_rate": 2.3689370485036123e-05,
      "loss": 54.9487,
      "step": 15389
    },
    {
      "epoch": 15.41,
      "grad_norm": 5739.28271484375,
      "learning_rate": 2.368421052631579e-05,
      "loss": 56.9885,
      "step": 15390
    },
    {
      "epoch": 15.41,
      "grad_norm": 1658.88427734375,
      "learning_rate": 2.3679050567595458e-05,
      "loss": 60.8468,
      "step": 15391
    },
    {
      "epoch": 15.41,
      "grad_norm": 5482.6826171875,
      "learning_rate": 2.367389060887513e-05,
      "loss": 47.1332,
      "step": 15392
    },
    {
      "epoch": 15.41,
      "grad_norm": 8414.2412109375,
      "learning_rate": 2.36687306501548e-05,
      "loss": 51.5439,
      "step": 15393
    },
    {
      "epoch": 15.41,
      "grad_norm": 3448.917724609375,
      "learning_rate": 2.3663570691434468e-05,
      "loss": 58.4625,
      "step": 15394
    },
    {
      "epoch": 15.41,
      "grad_norm": 27999.048828125,
      "learning_rate": 2.365841073271414e-05,
      "loss": 50.4035,
      "step": 15395
    },
    {
      "epoch": 15.41,
      "grad_norm": 5885.0068359375,
      "learning_rate": 2.365325077399381e-05,
      "loss": 59.0606,
      "step": 15396
    },
    {
      "epoch": 15.41,
      "grad_norm": 52141.703125,
      "learning_rate": 2.3648090815273478e-05,
      "loss": 55.9742,
      "step": 15397
    },
    {
      "epoch": 15.41,
      "grad_norm": 4094.976806640625,
      "learning_rate": 2.364293085655315e-05,
      "loss": 60.1906,
      "step": 15398
    },
    {
      "epoch": 15.41,
      "grad_norm": 13315.0712890625,
      "learning_rate": 2.3637770897832817e-05,
      "loss": 51.232,
      "step": 15399
    },
    {
      "epoch": 15.42,
      "grad_norm": 37981.5,
      "learning_rate": 2.3632610939112488e-05,
      "loss": 57.2502,
      "step": 15400
    },
    {
      "epoch": 15.42,
      "grad_norm": 91762.5,
      "learning_rate": 2.3627450980392156e-05,
      "loss": 36.5109,
      "step": 15401
    },
    {
      "epoch": 15.42,
      "grad_norm": 77795.015625,
      "learning_rate": 2.3622291021671827e-05,
      "loss": 58.5214,
      "step": 15402
    },
    {
      "epoch": 15.42,
      "grad_norm": 13446.9775390625,
      "learning_rate": 2.3617131062951498e-05,
      "loss": 53.4443,
      "step": 15403
    },
    {
      "epoch": 15.42,
      "grad_norm": 14694.56640625,
      "learning_rate": 2.3611971104231166e-05,
      "loss": 56.0755,
      "step": 15404
    },
    {
      "epoch": 15.42,
      "grad_norm": 63288.44921875,
      "learning_rate": 2.3606811145510837e-05,
      "loss": 42.4838,
      "step": 15405
    },
    {
      "epoch": 15.42,
      "grad_norm": 10572.931640625,
      "learning_rate": 2.3601651186790508e-05,
      "loss": 46.3305,
      "step": 15406
    },
    {
      "epoch": 15.42,
      "grad_norm": 9005.67578125,
      "learning_rate": 2.359649122807018e-05,
      "loss": 57.3784,
      "step": 15407
    },
    {
      "epoch": 15.42,
      "grad_norm": 18473.046875,
      "learning_rate": 2.3591331269349844e-05,
      "loss": 48.0342,
      "step": 15408
    },
    {
      "epoch": 15.42,
      "grad_norm": 6085.64453125,
      "learning_rate": 2.3586171310629515e-05,
      "loss": 49.6477,
      "step": 15409
    },
    {
      "epoch": 15.43,
      "grad_norm": 8280.294921875,
      "learning_rate": 2.3581011351909186e-05,
      "loss": 43.0558,
      "step": 15410
    },
    {
      "epoch": 15.43,
      "grad_norm": 25407.591796875,
      "learning_rate": 2.3575851393188854e-05,
      "loss": 22.6353,
      "step": 15411
    },
    {
      "epoch": 15.43,
      "grad_norm": 10892.072265625,
      "learning_rate": 2.3570691434468525e-05,
      "loss": 51.3481,
      "step": 15412
    },
    {
      "epoch": 15.43,
      "grad_norm": 13025.939453125,
      "learning_rate": 2.3565531475748196e-05,
      "loss": 52.2001,
      "step": 15413
    },
    {
      "epoch": 15.43,
      "grad_norm": 6642.9541015625,
      "learning_rate": 2.3560371517027867e-05,
      "loss": 52.9763,
      "step": 15414
    },
    {
      "epoch": 15.43,
      "grad_norm": 21526.697265625,
      "learning_rate": 2.3555211558307535e-05,
      "loss": 52.057,
      "step": 15415
    },
    {
      "epoch": 15.43,
      "grad_norm": 7208.15087890625,
      "learning_rate": 2.3550051599587206e-05,
      "loss": 60.4504,
      "step": 15416
    },
    {
      "epoch": 15.43,
      "grad_norm": 7587.31884765625,
      "learning_rate": 2.3544891640866874e-05,
      "loss": 51.8251,
      "step": 15417
    },
    {
      "epoch": 15.43,
      "grad_norm": 24873.4921875,
      "learning_rate": 2.353973168214654e-05,
      "loss": 50.5995,
      "step": 15418
    },
    {
      "epoch": 15.43,
      "grad_norm": 4591.95166015625,
      "learning_rate": 2.3534571723426213e-05,
      "loss": 49.8095,
      "step": 15419
    },
    {
      "epoch": 15.44,
      "grad_norm": 5673.0947265625,
      "learning_rate": 2.3529411764705884e-05,
      "loss": 50.0838,
      "step": 15420
    },
    {
      "epoch": 15.44,
      "grad_norm": 22320.1640625,
      "learning_rate": 2.3524251805985555e-05,
      "loss": 36.1892,
      "step": 15421
    },
    {
      "epoch": 15.44,
      "grad_norm": 45526.23828125,
      "learning_rate": 2.3519091847265223e-05,
      "loss": 53.3922,
      "step": 15422
    },
    {
      "epoch": 15.44,
      "grad_norm": 842.71337890625,
      "learning_rate": 2.3513931888544894e-05,
      "loss": 56.9651,
      "step": 15423
    },
    {
      "epoch": 15.44,
      "grad_norm": 9988.890625,
      "learning_rate": 2.3508771929824565e-05,
      "loss": 49.9319,
      "step": 15424
    },
    {
      "epoch": 15.44,
      "grad_norm": 5280.48974609375,
      "learning_rate": 2.3503611971104233e-05,
      "loss": 59.78,
      "step": 15425
    },
    {
      "epoch": 15.44,
      "grad_norm": 10264.87109375,
      "learning_rate": 2.34984520123839e-05,
      "loss": 47.2031,
      "step": 15426
    },
    {
      "epoch": 15.44,
      "grad_norm": 4534.93017578125,
      "learning_rate": 2.349329205366357e-05,
      "loss": 49.2941,
      "step": 15427
    },
    {
      "epoch": 15.44,
      "grad_norm": 7198.5908203125,
      "learning_rate": 2.3488132094943243e-05,
      "loss": 46.1392,
      "step": 15428
    },
    {
      "epoch": 15.44,
      "grad_norm": 4492.22216796875,
      "learning_rate": 2.348297213622291e-05,
      "loss": 51.9675,
      "step": 15429
    },
    {
      "epoch": 15.45,
      "grad_norm": 9445.8271484375,
      "learning_rate": 2.347781217750258e-05,
      "loss": 56.6832,
      "step": 15430
    },
    {
      "epoch": 15.45,
      "grad_norm": 2690.0029296875,
      "learning_rate": 2.3472652218782253e-05,
      "loss": 54.1675,
      "step": 15431
    },
    {
      "epoch": 15.45,
      "grad_norm": 18892.162109375,
      "learning_rate": 2.346749226006192e-05,
      "loss": 57.8324,
      "step": 15432
    },
    {
      "epoch": 15.45,
      "grad_norm": 4626.537109375,
      "learning_rate": 2.346233230134159e-05,
      "loss": 32.2947,
      "step": 15433
    },
    {
      "epoch": 15.45,
      "grad_norm": 185417.140625,
      "learning_rate": 2.3457172342621263e-05,
      "loss": 51.4603,
      "step": 15434
    },
    {
      "epoch": 15.45,
      "grad_norm": 26441.775390625,
      "learning_rate": 2.345201238390093e-05,
      "loss": 60.0423,
      "step": 15435
    },
    {
      "epoch": 15.45,
      "grad_norm": 13976.818359375,
      "learning_rate": 2.3446852425180598e-05,
      "loss": 61.9533,
      "step": 15436
    },
    {
      "epoch": 15.45,
      "grad_norm": 18458.138671875,
      "learning_rate": 2.344169246646027e-05,
      "loss": 39.0041,
      "step": 15437
    },
    {
      "epoch": 15.45,
      "grad_norm": 18778.033203125,
      "learning_rate": 2.343653250773994e-05,
      "loss": 42.2773,
      "step": 15438
    },
    {
      "epoch": 15.45,
      "grad_norm": 15824.1748046875,
      "learning_rate": 2.3431372549019608e-05,
      "loss": 54.565,
      "step": 15439
    },
    {
      "epoch": 15.46,
      "grad_norm": 10225.5380859375,
      "learning_rate": 2.342621259029928e-05,
      "loss": 55.6138,
      "step": 15440
    },
    {
      "epoch": 15.46,
      "grad_norm": 31595.48046875,
      "learning_rate": 2.342105263157895e-05,
      "loss": 52.7701,
      "step": 15441
    },
    {
      "epoch": 15.46,
      "grad_norm": 71984.21875,
      "learning_rate": 2.3415892672858618e-05,
      "loss": 51.8864,
      "step": 15442
    },
    {
      "epoch": 15.46,
      "grad_norm": 5852.3056640625,
      "learning_rate": 2.341073271413829e-05,
      "loss": 57.609,
      "step": 15443
    },
    {
      "epoch": 15.46,
      "grad_norm": 100041.7109375,
      "learning_rate": 2.3405572755417957e-05,
      "loss": 50.1351,
      "step": 15444
    },
    {
      "epoch": 15.46,
      "grad_norm": 4825.11572265625,
      "learning_rate": 2.3400412796697628e-05,
      "loss": 43.7615,
      "step": 15445
    },
    {
      "epoch": 15.46,
      "grad_norm": 11648.501953125,
      "learning_rate": 2.3395252837977296e-05,
      "loss": 54.6486,
      "step": 15446
    },
    {
      "epoch": 15.46,
      "grad_norm": 1681.3277587890625,
      "learning_rate": 2.3390092879256967e-05,
      "loss": 53.7461,
      "step": 15447
    },
    {
      "epoch": 15.46,
      "grad_norm": 2584.271240234375,
      "learning_rate": 2.3384932920536638e-05,
      "loss": 62.1776,
      "step": 15448
    },
    {
      "epoch": 15.46,
      "grad_norm": 15614.9697265625,
      "learning_rate": 2.3379772961816306e-05,
      "loss": 52.5705,
      "step": 15449
    },
    {
      "epoch": 15.47,
      "grad_norm": 4230.3857421875,
      "learning_rate": 2.3374613003095977e-05,
      "loss": 53.2677,
      "step": 15450
    },
    {
      "epoch": 15.47,
      "grad_norm": 1402.311767578125,
      "learning_rate": 2.3369453044375648e-05,
      "loss": 52.6853,
      "step": 15451
    },
    {
      "epoch": 15.47,
      "grad_norm": 1519.3646240234375,
      "learning_rate": 2.3364293085655316e-05,
      "loss": 55.4528,
      "step": 15452
    },
    {
      "epoch": 15.47,
      "grad_norm": 12089.2802734375,
      "learning_rate": 2.3359133126934984e-05,
      "loss": 52.7085,
      "step": 15453
    },
    {
      "epoch": 15.47,
      "grad_norm": 4990.07470703125,
      "learning_rate": 2.3353973168214655e-05,
      "loss": 48.2688,
      "step": 15454
    },
    {
      "epoch": 15.47,
      "grad_norm": 11701.0615234375,
      "learning_rate": 2.3348813209494326e-05,
      "loss": 51.6596,
      "step": 15455
    },
    {
      "epoch": 15.47,
      "grad_norm": 20074.505859375,
      "learning_rate": 2.3343653250773994e-05,
      "loss": 52.3763,
      "step": 15456
    },
    {
      "epoch": 15.47,
      "grad_norm": 5300.2724609375,
      "learning_rate": 2.3338493292053665e-05,
      "loss": 56.2166,
      "step": 15457
    },
    {
      "epoch": 15.47,
      "grad_norm": 1885.514892578125,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 49.6826,
      "step": 15458
    },
    {
      "epoch": 15.47,
      "grad_norm": 6698.4638671875,
      "learning_rate": 2.3328173374613004e-05,
      "loss": 57.7249,
      "step": 15459
    },
    {
      "epoch": 15.48,
      "grad_norm": 28422.845703125,
      "learning_rate": 2.3323013415892675e-05,
      "loss": 61.9467,
      "step": 15460
    },
    {
      "epoch": 15.48,
      "grad_norm": 22319.1015625,
      "learning_rate": 2.3317853457172346e-05,
      "loss": 58.7083,
      "step": 15461
    },
    {
      "epoch": 15.48,
      "grad_norm": 10388.546875,
      "learning_rate": 2.3312693498452014e-05,
      "loss": 53.1448,
      "step": 15462
    },
    {
      "epoch": 15.48,
      "grad_norm": 15933.73046875,
      "learning_rate": 2.330753353973168e-05,
      "loss": 52.8909,
      "step": 15463
    },
    {
      "epoch": 15.48,
      "grad_norm": 17361.146484375,
      "learning_rate": 2.3302373581011352e-05,
      "loss": 56.7127,
      "step": 15464
    },
    {
      "epoch": 15.48,
      "grad_norm": 21705.4140625,
      "learning_rate": 2.3297213622291024e-05,
      "loss": 53.3156,
      "step": 15465
    },
    {
      "epoch": 15.48,
      "grad_norm": 10239.287109375,
      "learning_rate": 2.329205366357069e-05,
      "loss": 37.8543,
      "step": 15466
    },
    {
      "epoch": 15.48,
      "grad_norm": 43743.3359375,
      "learning_rate": 2.3286893704850362e-05,
      "loss": 62.068,
      "step": 15467
    },
    {
      "epoch": 15.48,
      "grad_norm": 8113.8671875,
      "learning_rate": 2.3281733746130034e-05,
      "loss": 58.3223,
      "step": 15468
    },
    {
      "epoch": 15.48,
      "grad_norm": 41808.4375,
      "learning_rate": 2.32765737874097e-05,
      "loss": 18.6153,
      "step": 15469
    },
    {
      "epoch": 15.49,
      "grad_norm": 11344.2978515625,
      "learning_rate": 2.3271413828689372e-05,
      "loss": 51.2077,
      "step": 15470
    },
    {
      "epoch": 15.49,
      "grad_norm": 7531.1806640625,
      "learning_rate": 2.326625386996904e-05,
      "loss": 52.1371,
      "step": 15471
    },
    {
      "epoch": 15.49,
      "grad_norm": 11894.4609375,
      "learning_rate": 2.326109391124871e-05,
      "loss": 48.0183,
      "step": 15472
    },
    {
      "epoch": 15.49,
      "grad_norm": 7746.77294921875,
      "learning_rate": 2.325593395252838e-05,
      "loss": 56.1819,
      "step": 15473
    },
    {
      "epoch": 15.49,
      "grad_norm": 3990.159912109375,
      "learning_rate": 2.325077399380805e-05,
      "loss": 53.4259,
      "step": 15474
    },
    {
      "epoch": 15.49,
      "grad_norm": 6797.017578125,
      "learning_rate": 2.324561403508772e-05,
      "loss": 61.5511,
      "step": 15475
    },
    {
      "epoch": 15.49,
      "grad_norm": 3046.56689453125,
      "learning_rate": 2.324045407636739e-05,
      "loss": 61.2377,
      "step": 15476
    },
    {
      "epoch": 15.49,
      "grad_norm": 14622.53515625,
      "learning_rate": 2.323529411764706e-05,
      "loss": 53.4432,
      "step": 15477
    },
    {
      "epoch": 15.49,
      "grad_norm": 2379.255615234375,
      "learning_rate": 2.323013415892673e-05,
      "loss": 52.5838,
      "step": 15478
    },
    {
      "epoch": 15.49,
      "grad_norm": 51013.44140625,
      "learning_rate": 2.3224974200206402e-05,
      "loss": 57.7693,
      "step": 15479
    },
    {
      "epoch": 15.5,
      "grad_norm": 20845.228515625,
      "learning_rate": 2.3219814241486067e-05,
      "loss": 56.4031,
      "step": 15480
    },
    {
      "epoch": 15.5,
      "grad_norm": 20406.69140625,
      "learning_rate": 2.3214654282765738e-05,
      "loss": 52.8438,
      "step": 15481
    },
    {
      "epoch": 15.5,
      "grad_norm": 31747.98046875,
      "learning_rate": 2.320949432404541e-05,
      "loss": 40.4863,
      "step": 15482
    },
    {
      "epoch": 15.5,
      "grad_norm": 29105.12109375,
      "learning_rate": 2.3204334365325077e-05,
      "loss": 57.6929,
      "step": 15483
    },
    {
      "epoch": 15.5,
      "grad_norm": 7237.66162109375,
      "learning_rate": 2.3199174406604748e-05,
      "loss": 58.7221,
      "step": 15484
    },
    {
      "epoch": 15.5,
      "grad_norm": 4226.70458984375,
      "learning_rate": 2.319401444788442e-05,
      "loss": 54.5963,
      "step": 15485
    },
    {
      "epoch": 15.5,
      "grad_norm": 12734.287109375,
      "learning_rate": 2.318885448916409e-05,
      "loss": 52.4696,
      "step": 15486
    },
    {
      "epoch": 15.5,
      "grad_norm": 1913.181396484375,
      "learning_rate": 2.3183694530443758e-05,
      "loss": 53.5311,
      "step": 15487
    },
    {
      "epoch": 15.5,
      "grad_norm": 18461.57421875,
      "learning_rate": 2.317853457172343e-05,
      "loss": 53.7369,
      "step": 15488
    },
    {
      "epoch": 15.5,
      "grad_norm": 7902.52001953125,
      "learning_rate": 2.3173374613003097e-05,
      "loss": 53.4367,
      "step": 15489
    },
    {
      "epoch": 15.51,
      "grad_norm": 19007.962890625,
      "learning_rate": 2.3168214654282765e-05,
      "loss": 47.26,
      "step": 15490
    },
    {
      "epoch": 15.51,
      "grad_norm": 3269.75634765625,
      "learning_rate": 2.3163054695562436e-05,
      "loss": 55.4028,
      "step": 15491
    },
    {
      "epoch": 15.51,
      "grad_norm": 40108.79296875,
      "learning_rate": 2.3157894736842107e-05,
      "loss": 25.1148,
      "step": 15492
    },
    {
      "epoch": 15.51,
      "grad_norm": 1753.701171875,
      "learning_rate": 2.3152734778121778e-05,
      "loss": 56.3054,
      "step": 15493
    },
    {
      "epoch": 15.51,
      "grad_norm": 8137.3349609375,
      "learning_rate": 2.3147574819401446e-05,
      "loss": 51.663,
      "step": 15494
    },
    {
      "epoch": 15.51,
      "grad_norm": 3811.710205078125,
      "learning_rate": 2.3142414860681117e-05,
      "loss": 46.1816,
      "step": 15495
    },
    {
      "epoch": 15.51,
      "grad_norm": 4634.95166015625,
      "learning_rate": 2.3137254901960788e-05,
      "loss": 52.9408,
      "step": 15496
    },
    {
      "epoch": 15.51,
      "grad_norm": 4186.0810546875,
      "learning_rate": 2.3132094943240456e-05,
      "loss": 60.0034,
      "step": 15497
    },
    {
      "epoch": 15.51,
      "grad_norm": 8634.890625,
      "learning_rate": 2.3126934984520123e-05,
      "loss": 46.9018,
      "step": 15498
    },
    {
      "epoch": 15.51,
      "grad_norm": 106777.9765625,
      "learning_rate": 2.3121775025799795e-05,
      "loss": 53.4363,
      "step": 15499
    },
    {
      "epoch": 15.52,
      "grad_norm": 1282.1422119140625,
      "learning_rate": 2.3116615067079466e-05,
      "loss": 52.9215,
      "step": 15500
    },
    {
      "epoch": 15.52,
      "grad_norm": 2347.75537109375,
      "learning_rate": 2.3111455108359133e-05,
      "loss": 56.5107,
      "step": 15501
    },
    {
      "epoch": 15.52,
      "grad_norm": 218673.59375,
      "learning_rate": 2.3106295149638805e-05,
      "loss": 45.1466,
      "step": 15502
    },
    {
      "epoch": 15.52,
      "grad_norm": 1578.3258056640625,
      "learning_rate": 2.3101135190918476e-05,
      "loss": 55.203,
      "step": 15503
    },
    {
      "epoch": 15.52,
      "grad_norm": 5545.6279296875,
      "learning_rate": 2.3095975232198143e-05,
      "loss": 61.1373,
      "step": 15504
    },
    {
      "epoch": 15.52,
      "grad_norm": 9852.1875,
      "learning_rate": 2.3090815273477815e-05,
      "loss": 56.8841,
      "step": 15505
    },
    {
      "epoch": 15.52,
      "grad_norm": 2974.768798828125,
      "learning_rate": 2.3085655314757482e-05,
      "loss": 59.7071,
      "step": 15506
    },
    {
      "epoch": 15.52,
      "grad_norm": 3476.385009765625,
      "learning_rate": 2.3080495356037153e-05,
      "loss": 48.1784,
      "step": 15507
    },
    {
      "epoch": 15.52,
      "grad_norm": 3861.14990234375,
      "learning_rate": 2.307533539731682e-05,
      "loss": 61.527,
      "step": 15508
    },
    {
      "epoch": 15.52,
      "grad_norm": 18613.91796875,
      "learning_rate": 2.3070175438596492e-05,
      "loss": 54.8795,
      "step": 15509
    },
    {
      "epoch": 15.53,
      "grad_norm": 37170.6640625,
      "learning_rate": 2.3065015479876163e-05,
      "loss": 32.3764,
      "step": 15510
    },
    {
      "epoch": 15.53,
      "grad_norm": 4228.9345703125,
      "learning_rate": 2.305985552115583e-05,
      "loss": 62.0509,
      "step": 15511
    },
    {
      "epoch": 15.53,
      "grad_norm": 39256.19140625,
      "learning_rate": 2.3054695562435502e-05,
      "loss": 53.7247,
      "step": 15512
    },
    {
      "epoch": 15.53,
      "grad_norm": 20613.048828125,
      "learning_rate": 2.3049535603715173e-05,
      "loss": 52.8839,
      "step": 15513
    },
    {
      "epoch": 15.53,
      "grad_norm": 85799.46875,
      "learning_rate": 2.304437564499484e-05,
      "loss": 46.2209,
      "step": 15514
    },
    {
      "epoch": 15.53,
      "grad_norm": 21234.986328125,
      "learning_rate": 2.303921568627451e-05,
      "loss": 50.3593,
      "step": 15515
    },
    {
      "epoch": 15.53,
      "grad_norm": 86896.5078125,
      "learning_rate": 2.303405572755418e-05,
      "loss": 26.8521,
      "step": 15516
    },
    {
      "epoch": 15.53,
      "grad_norm": 1601.537353515625,
      "learning_rate": 2.302889576883385e-05,
      "loss": 55.1329,
      "step": 15517
    },
    {
      "epoch": 15.53,
      "grad_norm": 5170.23046875,
      "learning_rate": 2.302373581011352e-05,
      "loss": 51.2979,
      "step": 15518
    },
    {
      "epoch": 15.53,
      "grad_norm": 2626.84326171875,
      "learning_rate": 2.301857585139319e-05,
      "loss": 54.8775,
      "step": 15519
    },
    {
      "epoch": 15.54,
      "grad_norm": 8453.490234375,
      "learning_rate": 2.301341589267286e-05,
      "loss": 49.72,
      "step": 15520
    },
    {
      "epoch": 15.54,
      "grad_norm": 11901.88671875,
      "learning_rate": 2.300825593395253e-05,
      "loss": 44.3066,
      "step": 15521
    },
    {
      "epoch": 15.54,
      "grad_norm": 6406.423828125,
      "learning_rate": 2.30030959752322e-05,
      "loss": 61.4151,
      "step": 15522
    },
    {
      "epoch": 15.54,
      "grad_norm": 14242.8857421875,
      "learning_rate": 2.299793601651187e-05,
      "loss": 57.3526,
      "step": 15523
    },
    {
      "epoch": 15.54,
      "grad_norm": 11964.0341796875,
      "learning_rate": 2.299277605779154e-05,
      "loss": 54.8109,
      "step": 15524
    },
    {
      "epoch": 15.54,
      "grad_norm": 15379.48046875,
      "learning_rate": 2.2987616099071207e-05,
      "loss": 53.6615,
      "step": 15525
    },
    {
      "epoch": 15.54,
      "grad_norm": 7587.89111328125,
      "learning_rate": 2.2982456140350878e-05,
      "loss": 61.7234,
      "step": 15526
    },
    {
      "epoch": 15.54,
      "grad_norm": 9402.955078125,
      "learning_rate": 2.297729618163055e-05,
      "loss": 63.431,
      "step": 15527
    },
    {
      "epoch": 15.54,
      "grad_norm": 5928.9501953125,
      "learning_rate": 2.2972136222910217e-05,
      "loss": 53.1539,
      "step": 15528
    },
    {
      "epoch": 15.54,
      "grad_norm": 6041.26953125,
      "learning_rate": 2.2966976264189888e-05,
      "loss": 54.0125,
      "step": 15529
    },
    {
      "epoch": 15.55,
      "grad_norm": 34509.7734375,
      "learning_rate": 2.296181630546956e-05,
      "loss": 56.6934,
      "step": 15530
    },
    {
      "epoch": 15.55,
      "grad_norm": 3884.66064453125,
      "learning_rate": 2.2956656346749227e-05,
      "loss": 47.0008,
      "step": 15531
    },
    {
      "epoch": 15.55,
      "grad_norm": 5949.75146484375,
      "learning_rate": 2.2951496388028898e-05,
      "loss": 61.0945,
      "step": 15532
    },
    {
      "epoch": 15.55,
      "grad_norm": 7546.95361328125,
      "learning_rate": 2.2946336429308566e-05,
      "loss": 48.4936,
      "step": 15533
    },
    {
      "epoch": 15.55,
      "grad_norm": 45680.71875,
      "learning_rate": 2.2941176470588237e-05,
      "loss": 51.5009,
      "step": 15534
    },
    {
      "epoch": 15.55,
      "grad_norm": 9647.87109375,
      "learning_rate": 2.2936016511867904e-05,
      "loss": 53.1795,
      "step": 15535
    },
    {
      "epoch": 15.55,
      "grad_norm": 3885.664306640625,
      "learning_rate": 2.2930856553147576e-05,
      "loss": 55.2502,
      "step": 15536
    },
    {
      "epoch": 15.55,
      "grad_norm": 8001.3212890625,
      "learning_rate": 2.2925696594427247e-05,
      "loss": 50.6889,
      "step": 15537
    },
    {
      "epoch": 15.55,
      "grad_norm": 28836.271484375,
      "learning_rate": 2.2920536635706914e-05,
      "loss": 30.0002,
      "step": 15538
    },
    {
      "epoch": 15.55,
      "grad_norm": 12625.48828125,
      "learning_rate": 2.2915376676986586e-05,
      "loss": 43.7112,
      "step": 15539
    },
    {
      "epoch": 15.56,
      "grad_norm": 2611.485595703125,
      "learning_rate": 2.2910216718266257e-05,
      "loss": 62.4152,
      "step": 15540
    },
    {
      "epoch": 15.56,
      "grad_norm": 11116.7333984375,
      "learning_rate": 2.2905056759545924e-05,
      "loss": 61.9839,
      "step": 15541
    },
    {
      "epoch": 15.56,
      "grad_norm": 5252.2138671875,
      "learning_rate": 2.2899896800825592e-05,
      "loss": 52.6064,
      "step": 15542
    },
    {
      "epoch": 15.56,
      "grad_norm": 14882.7470703125,
      "learning_rate": 2.2894736842105263e-05,
      "loss": 44.9153,
      "step": 15543
    },
    {
      "epoch": 15.56,
      "grad_norm": 15725.3544921875,
      "learning_rate": 2.2889576883384934e-05,
      "loss": 31.51,
      "step": 15544
    },
    {
      "epoch": 15.56,
      "grad_norm": 11404.2158203125,
      "learning_rate": 2.2884416924664602e-05,
      "loss": 59.6784,
      "step": 15545
    },
    {
      "epoch": 15.56,
      "grad_norm": 3483.06787109375,
      "learning_rate": 2.2879256965944273e-05,
      "loss": 61.2637,
      "step": 15546
    },
    {
      "epoch": 15.56,
      "grad_norm": 7478.92041015625,
      "learning_rate": 2.2874097007223944e-05,
      "loss": 55.0556,
      "step": 15547
    },
    {
      "epoch": 15.56,
      "grad_norm": 11402.9140625,
      "learning_rate": 2.2868937048503612e-05,
      "loss": 48.3247,
      "step": 15548
    },
    {
      "epoch": 15.56,
      "grad_norm": 2127.435302734375,
      "learning_rate": 2.2863777089783283e-05,
      "loss": 60.6139,
      "step": 15549
    },
    {
      "epoch": 15.57,
      "grad_norm": 3751.22021484375,
      "learning_rate": 2.2858617131062954e-05,
      "loss": 54.4394,
      "step": 15550
    },
    {
      "epoch": 15.57,
      "grad_norm": 16773.294921875,
      "learning_rate": 2.2853457172342622e-05,
      "loss": 26.4451,
      "step": 15551
    },
    {
      "epoch": 15.57,
      "grad_norm": 5060.19873046875,
      "learning_rate": 2.284829721362229e-05,
      "loss": 50.3437,
      "step": 15552
    },
    {
      "epoch": 15.57,
      "grad_norm": 26019.49609375,
      "learning_rate": 2.284313725490196e-05,
      "loss": 54.0287,
      "step": 15553
    },
    {
      "epoch": 15.57,
      "grad_norm": 63399.0234375,
      "learning_rate": 2.2837977296181632e-05,
      "loss": 49.3972,
      "step": 15554
    },
    {
      "epoch": 15.57,
      "grad_norm": 3319.4501953125,
      "learning_rate": 2.28328173374613e-05,
      "loss": 46.6578,
      "step": 15555
    },
    {
      "epoch": 15.57,
      "grad_norm": 11083.123046875,
      "learning_rate": 2.282765737874097e-05,
      "loss": 49.2056,
      "step": 15556
    },
    {
      "epoch": 15.57,
      "grad_norm": 2831.486572265625,
      "learning_rate": 2.2822497420020642e-05,
      "loss": 60.4705,
      "step": 15557
    },
    {
      "epoch": 15.57,
      "grad_norm": 9289.6796875,
      "learning_rate": 2.281733746130031e-05,
      "loss": 51.2048,
      "step": 15558
    },
    {
      "epoch": 15.57,
      "grad_norm": 6858.71875,
      "learning_rate": 2.281217750257998e-05,
      "loss": 54.1481,
      "step": 15559
    },
    {
      "epoch": 15.58,
      "grad_norm": 7384.46923828125,
      "learning_rate": 2.280701754385965e-05,
      "loss": 38.1363,
      "step": 15560
    },
    {
      "epoch": 15.58,
      "grad_norm": 20234.94140625,
      "learning_rate": 2.280185758513932e-05,
      "loss": 49.1859,
      "step": 15561
    },
    {
      "epoch": 15.58,
      "grad_norm": 15151.8154296875,
      "learning_rate": 2.2796697626418988e-05,
      "loss": 55.8219,
      "step": 15562
    },
    {
      "epoch": 15.58,
      "grad_norm": 5157.14306640625,
      "learning_rate": 2.279153766769866e-05,
      "loss": 56.2317,
      "step": 15563
    },
    {
      "epoch": 15.58,
      "grad_norm": 1638.228515625,
      "learning_rate": 2.278637770897833e-05,
      "loss": 48.8575,
      "step": 15564
    },
    {
      "epoch": 15.58,
      "grad_norm": 9918.4716796875,
      "learning_rate": 2.2781217750257998e-05,
      "loss": 51.213,
      "step": 15565
    },
    {
      "epoch": 15.58,
      "grad_norm": 30435.705078125,
      "learning_rate": 2.277605779153767e-05,
      "loss": 39.4415,
      "step": 15566
    },
    {
      "epoch": 15.58,
      "grad_norm": 28053.115234375,
      "learning_rate": 2.277089783281734e-05,
      "loss": 52.6585,
      "step": 15567
    },
    {
      "epoch": 15.58,
      "grad_norm": 5098.16162109375,
      "learning_rate": 2.276573787409701e-05,
      "loss": 40.4489,
      "step": 15568
    },
    {
      "epoch": 15.58,
      "grad_norm": 83608.2890625,
      "learning_rate": 2.2760577915376675e-05,
      "loss": 52.3948,
      "step": 15569
    },
    {
      "epoch": 15.59,
      "grad_norm": 5278.93505859375,
      "learning_rate": 2.2755417956656347e-05,
      "loss": 46.5874,
      "step": 15570
    },
    {
      "epoch": 15.59,
      "grad_norm": 2324.533935546875,
      "learning_rate": 2.2750257997936018e-05,
      "loss": 57.6699,
      "step": 15571
    },
    {
      "epoch": 15.59,
      "grad_norm": 4228.65087890625,
      "learning_rate": 2.2745098039215685e-05,
      "loss": 49.6353,
      "step": 15572
    },
    {
      "epoch": 15.59,
      "grad_norm": 2135.959716796875,
      "learning_rate": 2.2739938080495357e-05,
      "loss": 53.5991,
      "step": 15573
    },
    {
      "epoch": 15.59,
      "grad_norm": 22350.28125,
      "learning_rate": 2.2734778121775028e-05,
      "loss": 57.5401,
      "step": 15574
    },
    {
      "epoch": 15.59,
      "grad_norm": 6404.15283203125,
      "learning_rate": 2.27296181630547e-05,
      "loss": 56.5362,
      "step": 15575
    },
    {
      "epoch": 15.59,
      "grad_norm": 5686.111328125,
      "learning_rate": 2.2724458204334367e-05,
      "loss": 40.8855,
      "step": 15576
    },
    {
      "epoch": 15.59,
      "grad_norm": 4260.14208984375,
      "learning_rate": 2.2719298245614038e-05,
      "loss": 50.3137,
      "step": 15577
    },
    {
      "epoch": 15.59,
      "grad_norm": 14535.888671875,
      "learning_rate": 2.2714138286893705e-05,
      "loss": 46.4717,
      "step": 15578
    },
    {
      "epoch": 15.59,
      "grad_norm": 14310.443359375,
      "learning_rate": 2.2708978328173373e-05,
      "loss": 64.9334,
      "step": 15579
    },
    {
      "epoch": 15.6,
      "grad_norm": 12196.6064453125,
      "learning_rate": 2.2703818369453044e-05,
      "loss": 39.129,
      "step": 15580
    },
    {
      "epoch": 15.6,
      "grad_norm": 19660.7265625,
      "learning_rate": 2.2698658410732715e-05,
      "loss": 52.0823,
      "step": 15581
    },
    {
      "epoch": 15.6,
      "grad_norm": 3341.836669921875,
      "learning_rate": 2.2693498452012387e-05,
      "loss": 42.5162,
      "step": 15582
    },
    {
      "epoch": 15.6,
      "grad_norm": 7185.619140625,
      "learning_rate": 2.2688338493292054e-05,
      "loss": 51.0913,
      "step": 15583
    },
    {
      "epoch": 15.6,
      "grad_norm": 12922.99609375,
      "learning_rate": 2.2683178534571725e-05,
      "loss": 55.6634,
      "step": 15584
    },
    {
      "epoch": 15.6,
      "grad_norm": 5233.154296875,
      "learning_rate": 2.2678018575851397e-05,
      "loss": 54.8479,
      "step": 15585
    },
    {
      "epoch": 15.6,
      "grad_norm": 7062.95703125,
      "learning_rate": 2.2672858617131064e-05,
      "loss": 35.3325,
      "step": 15586
    },
    {
      "epoch": 15.6,
      "grad_norm": 31960.80859375,
      "learning_rate": 2.2667698658410732e-05,
      "loss": 45.1151,
      "step": 15587
    },
    {
      "epoch": 15.6,
      "grad_norm": 19698.962890625,
      "learning_rate": 2.2662538699690403e-05,
      "loss": 60.7356,
      "step": 15588
    },
    {
      "epoch": 15.6,
      "grad_norm": 186803.3125,
      "learning_rate": 2.2657378740970074e-05,
      "loss": 46.8944,
      "step": 15589
    },
    {
      "epoch": 15.61,
      "grad_norm": 7591.7490234375,
      "learning_rate": 2.2652218782249742e-05,
      "loss": 44.3967,
      "step": 15590
    },
    {
      "epoch": 15.61,
      "grad_norm": 8336.8779296875,
      "learning_rate": 2.2647058823529413e-05,
      "loss": 54.9486,
      "step": 15591
    },
    {
      "epoch": 15.61,
      "grad_norm": 22667.15625,
      "learning_rate": 2.2641898864809084e-05,
      "loss": 56.3139,
      "step": 15592
    },
    {
      "epoch": 15.61,
      "grad_norm": 2979.200439453125,
      "learning_rate": 2.2636738906088752e-05,
      "loss": 53.866,
      "step": 15593
    },
    {
      "epoch": 15.61,
      "grad_norm": 1597.59228515625,
      "learning_rate": 2.2631578947368423e-05,
      "loss": 60.1955,
      "step": 15594
    },
    {
      "epoch": 15.61,
      "grad_norm": 47314.203125,
      "learning_rate": 2.2626418988648094e-05,
      "loss": 55.4026,
      "step": 15595
    },
    {
      "epoch": 15.61,
      "grad_norm": 22752.73828125,
      "learning_rate": 2.2621259029927762e-05,
      "loss": 29.1581,
      "step": 15596
    },
    {
      "epoch": 15.61,
      "grad_norm": 8234.19140625,
      "learning_rate": 2.261609907120743e-05,
      "loss": 61.9659,
      "step": 15597
    },
    {
      "epoch": 15.61,
      "grad_norm": 9198.3193359375,
      "learning_rate": 2.26109391124871e-05,
      "loss": 52.3346,
      "step": 15598
    },
    {
      "epoch": 15.61,
      "grad_norm": 7356.07666015625,
      "learning_rate": 2.2605779153766772e-05,
      "loss": 59.7759,
      "step": 15599
    },
    {
      "epoch": 15.62,
      "grad_norm": 5772.228515625,
      "learning_rate": 2.260061919504644e-05,
      "loss": 57.8967,
      "step": 15600
    },
    {
      "epoch": 15.62,
      "grad_norm": 17013.564453125,
      "learning_rate": 2.259545923632611e-05,
      "loss": 59.6847,
      "step": 15601
    },
    {
      "epoch": 15.62,
      "grad_norm": 7222.45654296875,
      "learning_rate": 2.2590299277605782e-05,
      "loss": 33.8398,
      "step": 15602
    },
    {
      "epoch": 15.62,
      "grad_norm": 11623.5654296875,
      "learning_rate": 2.258513931888545e-05,
      "loss": 60.178,
      "step": 15603
    },
    {
      "epoch": 15.62,
      "grad_norm": 574111.3125,
      "learning_rate": 2.257997936016512e-05,
      "loss": 61.1051,
      "step": 15604
    },
    {
      "epoch": 15.62,
      "grad_norm": 49877.98046875,
      "learning_rate": 2.257481940144479e-05,
      "loss": 46.0466,
      "step": 15605
    },
    {
      "epoch": 15.62,
      "grad_norm": 35408.38671875,
      "learning_rate": 2.256965944272446e-05,
      "loss": 54.5558,
      "step": 15606
    },
    {
      "epoch": 15.62,
      "grad_norm": 39308.38671875,
      "learning_rate": 2.2564499484004128e-05,
      "loss": 46.4003,
      "step": 15607
    },
    {
      "epoch": 15.62,
      "grad_norm": 6897.3193359375,
      "learning_rate": 2.25593395252838e-05,
      "loss": 44.8245,
      "step": 15608
    },
    {
      "epoch": 15.62,
      "grad_norm": 3767.1474609375,
      "learning_rate": 2.255417956656347e-05,
      "loss": 57.7605,
      "step": 15609
    },
    {
      "epoch": 15.63,
      "grad_norm": 37672.8125,
      "learning_rate": 2.2549019607843138e-05,
      "loss": 33.1368,
      "step": 15610
    },
    {
      "epoch": 15.63,
      "grad_norm": 13219.228515625,
      "learning_rate": 2.254385964912281e-05,
      "loss": 54.2881,
      "step": 15611
    },
    {
      "epoch": 15.63,
      "grad_norm": 8360.189453125,
      "learning_rate": 2.253869969040248e-05,
      "loss": 48.7116,
      "step": 15612
    },
    {
      "epoch": 15.63,
      "grad_norm": 12661.3671875,
      "learning_rate": 2.2533539731682148e-05,
      "loss": 57.7337,
      "step": 15613
    },
    {
      "epoch": 15.63,
      "grad_norm": 6633.64599609375,
      "learning_rate": 2.2528379772961815e-05,
      "loss": 38.4517,
      "step": 15614
    },
    {
      "epoch": 15.63,
      "grad_norm": 167589.203125,
      "learning_rate": 2.2523219814241486e-05,
      "loss": 47.8491,
      "step": 15615
    },
    {
      "epoch": 15.63,
      "grad_norm": 13318.9404296875,
      "learning_rate": 2.2518059855521158e-05,
      "loss": 34.9669,
      "step": 15616
    },
    {
      "epoch": 15.63,
      "grad_norm": 14559.419921875,
      "learning_rate": 2.2512899896800825e-05,
      "loss": 45.3702,
      "step": 15617
    },
    {
      "epoch": 15.63,
      "grad_norm": 78829.3671875,
      "learning_rate": 2.2507739938080496e-05,
      "loss": 45.7015,
      "step": 15618
    },
    {
      "epoch": 15.63,
      "grad_norm": 32147.474609375,
      "learning_rate": 2.2502579979360168e-05,
      "loss": 45.1492,
      "step": 15619
    },
    {
      "epoch": 15.64,
      "grad_norm": 3206.47216796875,
      "learning_rate": 2.2497420020639835e-05,
      "loss": 58.2653,
      "step": 15620
    },
    {
      "epoch": 15.64,
      "grad_norm": 4770.39697265625,
      "learning_rate": 2.2492260061919506e-05,
      "loss": 51.1675,
      "step": 15621
    },
    {
      "epoch": 15.64,
      "grad_norm": 7045.7177734375,
      "learning_rate": 2.2487100103199178e-05,
      "loss": 52.8942,
      "step": 15622
    },
    {
      "epoch": 15.64,
      "grad_norm": 4401.5322265625,
      "learning_rate": 2.2481940144478845e-05,
      "loss": 57.3513,
      "step": 15623
    },
    {
      "epoch": 15.64,
      "grad_norm": 5321.74658203125,
      "learning_rate": 2.2476780185758513e-05,
      "loss": 59.3429,
      "step": 15624
    },
    {
      "epoch": 15.64,
      "grad_norm": 4489.18359375,
      "learning_rate": 2.2471620227038184e-05,
      "loss": 56.4832,
      "step": 15625
    },
    {
      "epoch": 15.64,
      "grad_norm": 1176.3023681640625,
      "learning_rate": 2.2466460268317855e-05,
      "loss": 61.863,
      "step": 15626
    },
    {
      "epoch": 15.64,
      "grad_norm": 30983.451171875,
      "learning_rate": 2.2461300309597523e-05,
      "loss": 44.3335,
      "step": 15627
    },
    {
      "epoch": 15.64,
      "grad_norm": 10038.73828125,
      "learning_rate": 2.2456140350877194e-05,
      "loss": 51.8602,
      "step": 15628
    },
    {
      "epoch": 15.64,
      "grad_norm": 10668.8740234375,
      "learning_rate": 2.2450980392156865e-05,
      "loss": 44.7887,
      "step": 15629
    },
    {
      "epoch": 15.65,
      "grad_norm": 9435.1337890625,
      "learning_rate": 2.2445820433436533e-05,
      "loss": 49.2277,
      "step": 15630
    },
    {
      "epoch": 15.65,
      "grad_norm": 3449.65869140625,
      "learning_rate": 2.2440660474716204e-05,
      "loss": 47.5301,
      "step": 15631
    },
    {
      "epoch": 15.65,
      "grad_norm": 7494.5654296875,
      "learning_rate": 2.2435500515995872e-05,
      "loss": 57.8479,
      "step": 15632
    },
    {
      "epoch": 15.65,
      "grad_norm": 1846.407958984375,
      "learning_rate": 2.2430340557275543e-05,
      "loss": 62.0931,
      "step": 15633
    },
    {
      "epoch": 15.65,
      "grad_norm": 41191.77734375,
      "learning_rate": 2.242518059855521e-05,
      "loss": 54.4676,
      "step": 15634
    },
    {
      "epoch": 15.65,
      "grad_norm": 16479.046875,
      "learning_rate": 2.2420020639834882e-05,
      "loss": 57.0102,
      "step": 15635
    },
    {
      "epoch": 15.65,
      "grad_norm": 5574.82666015625,
      "learning_rate": 2.2414860681114553e-05,
      "loss": 58.7889,
      "step": 15636
    },
    {
      "epoch": 15.65,
      "grad_norm": 2105.355224609375,
      "learning_rate": 2.240970072239422e-05,
      "loss": 54.0046,
      "step": 15637
    },
    {
      "epoch": 15.65,
      "grad_norm": 40749.15234375,
      "learning_rate": 2.2404540763673892e-05,
      "loss": 50.0525,
      "step": 15638
    },
    {
      "epoch": 15.65,
      "grad_norm": 126469.0703125,
      "learning_rate": 2.2399380804953563e-05,
      "loss": 50.8191,
      "step": 15639
    },
    {
      "epoch": 15.66,
      "grad_norm": 65602.734375,
      "learning_rate": 2.2394220846233234e-05,
      "loss": 44.2233,
      "step": 15640
    },
    {
      "epoch": 15.66,
      "grad_norm": 7674.12890625,
      "learning_rate": 2.23890608875129e-05,
      "loss": 58.3571,
      "step": 15641
    },
    {
      "epoch": 15.66,
      "grad_norm": 10214.64453125,
      "learning_rate": 2.238390092879257e-05,
      "loss": 51.9674,
      "step": 15642
    },
    {
      "epoch": 15.66,
      "grad_norm": 5590.00146484375,
      "learning_rate": 2.237874097007224e-05,
      "loss": 48.4278,
      "step": 15643
    },
    {
      "epoch": 15.66,
      "grad_norm": 29073.009765625,
      "learning_rate": 2.237358101135191e-05,
      "loss": 42.8233,
      "step": 15644
    },
    {
      "epoch": 15.66,
      "grad_norm": 7429.458984375,
      "learning_rate": 2.236842105263158e-05,
      "loss": 55.326,
      "step": 15645
    },
    {
      "epoch": 15.66,
      "grad_norm": 11640.4931640625,
      "learning_rate": 2.236326109391125e-05,
      "loss": 57.7404,
      "step": 15646
    },
    {
      "epoch": 15.66,
      "grad_norm": 10177.58203125,
      "learning_rate": 2.2358101135190922e-05,
      "loss": 33.0077,
      "step": 15647
    },
    {
      "epoch": 15.66,
      "grad_norm": 32861.1953125,
      "learning_rate": 2.235294117647059e-05,
      "loss": 60.4853,
      "step": 15648
    },
    {
      "epoch": 15.66,
      "grad_norm": 16202.5556640625,
      "learning_rate": 2.2347781217750257e-05,
      "loss": 34.1346,
      "step": 15649
    },
    {
      "epoch": 15.67,
      "grad_norm": 28888.73046875,
      "learning_rate": 2.234262125902993e-05,
      "loss": 60.667,
      "step": 15650
    },
    {
      "epoch": 15.67,
      "grad_norm": 3700.36767578125,
      "learning_rate": 2.2337461300309596e-05,
      "loss": 60.514,
      "step": 15651
    },
    {
      "epoch": 15.67,
      "grad_norm": 1950.66796875,
      "learning_rate": 2.2332301341589267e-05,
      "loss": 54.1744,
      "step": 15652
    },
    {
      "epoch": 15.67,
      "grad_norm": 35256.1484375,
      "learning_rate": 2.232714138286894e-05,
      "loss": 52.7065,
      "step": 15653
    },
    {
      "epoch": 15.67,
      "grad_norm": 3700.507568359375,
      "learning_rate": 2.232198142414861e-05,
      "loss": 55.146,
      "step": 15654
    },
    {
      "epoch": 15.67,
      "grad_norm": 4682.46142578125,
      "learning_rate": 2.2316821465428277e-05,
      "loss": 58.0302,
      "step": 15655
    },
    {
      "epoch": 15.67,
      "grad_norm": 3576.095947265625,
      "learning_rate": 2.231166150670795e-05,
      "loss": 32.0379,
      "step": 15656
    },
    {
      "epoch": 15.67,
      "grad_norm": 2745.8662109375,
      "learning_rate": 2.230650154798762e-05,
      "loss": 56.9359,
      "step": 15657
    },
    {
      "epoch": 15.67,
      "grad_norm": 2621.430908203125,
      "learning_rate": 2.2301341589267284e-05,
      "loss": 46.7761,
      "step": 15658
    },
    {
      "epoch": 15.67,
      "grad_norm": 16341.611328125,
      "learning_rate": 2.2296181630546955e-05,
      "loss": 46.6269,
      "step": 15659
    },
    {
      "epoch": 15.68,
      "grad_norm": 7457.18994140625,
      "learning_rate": 2.2291021671826626e-05,
      "loss": 56.1635,
      "step": 15660
    },
    {
      "epoch": 15.68,
      "grad_norm": 4328.27783203125,
      "learning_rate": 2.2285861713106297e-05,
      "loss": 34.7278,
      "step": 15661
    },
    {
      "epoch": 15.68,
      "grad_norm": 2228.71142578125,
      "learning_rate": 2.2280701754385965e-05,
      "loss": 44.6095,
      "step": 15662
    },
    {
      "epoch": 15.68,
      "grad_norm": 1373.810546875,
      "learning_rate": 2.2275541795665636e-05,
      "loss": 50.0356,
      "step": 15663
    },
    {
      "epoch": 15.68,
      "grad_norm": 33537.921875,
      "learning_rate": 2.2270381836945307e-05,
      "loss": 55.5215,
      "step": 15664
    },
    {
      "epoch": 15.68,
      "grad_norm": 33613.54296875,
      "learning_rate": 2.2265221878224975e-05,
      "loss": 44.1571,
      "step": 15665
    },
    {
      "epoch": 15.68,
      "grad_norm": 19175.541015625,
      "learning_rate": 2.2260061919504646e-05,
      "loss": 28.879,
      "step": 15666
    },
    {
      "epoch": 15.68,
      "grad_norm": 9485.546875,
      "learning_rate": 2.2254901960784314e-05,
      "loss": 45.7326,
      "step": 15667
    },
    {
      "epoch": 15.68,
      "grad_norm": 2374.692626953125,
      "learning_rate": 2.2249742002063985e-05,
      "loss": 61.5307,
      "step": 15668
    },
    {
      "epoch": 15.68,
      "grad_norm": 6750.166015625,
      "learning_rate": 2.2244582043343653e-05,
      "loss": 57.2086,
      "step": 15669
    },
    {
      "epoch": 15.69,
      "grad_norm": 19153.6953125,
      "learning_rate": 2.2239422084623324e-05,
      "loss": 52.8314,
      "step": 15670
    },
    {
      "epoch": 15.69,
      "grad_norm": 4683.55615234375,
      "learning_rate": 2.2234262125902995e-05,
      "loss": 55.1131,
      "step": 15671
    },
    {
      "epoch": 15.69,
      "grad_norm": 7524.54248046875,
      "learning_rate": 2.2229102167182663e-05,
      "loss": 57.6168,
      "step": 15672
    },
    {
      "epoch": 15.69,
      "grad_norm": 5411.517578125,
      "learning_rate": 2.2223942208462334e-05,
      "loss": 43.4052,
      "step": 15673
    },
    {
      "epoch": 15.69,
      "grad_norm": 23525.998046875,
      "learning_rate": 2.2218782249742005e-05,
      "loss": 61.2906,
      "step": 15674
    },
    {
      "epoch": 15.69,
      "grad_norm": 8531.3173828125,
      "learning_rate": 2.2213622291021673e-05,
      "loss": 50.5503,
      "step": 15675
    },
    {
      "epoch": 15.69,
      "grad_norm": 6135.6884765625,
      "learning_rate": 2.220846233230134e-05,
      "loss": 49.5774,
      "step": 15676
    },
    {
      "epoch": 15.69,
      "grad_norm": 12214.6728515625,
      "learning_rate": 2.2203302373581012e-05,
      "loss": 53.9444,
      "step": 15677
    },
    {
      "epoch": 15.69,
      "grad_norm": 36243.9609375,
      "learning_rate": 2.2198142414860683e-05,
      "loss": 58.7914,
      "step": 15678
    },
    {
      "epoch": 15.69,
      "grad_norm": 3749.037109375,
      "learning_rate": 2.219298245614035e-05,
      "loss": 37.085,
      "step": 15679
    },
    {
      "epoch": 15.7,
      "grad_norm": 24783.28515625,
      "learning_rate": 2.2187822497420022e-05,
      "loss": 67.1141,
      "step": 15680
    },
    {
      "epoch": 15.7,
      "grad_norm": 16017.9970703125,
      "learning_rate": 2.2182662538699693e-05,
      "loss": 55.5005,
      "step": 15681
    },
    {
      "epoch": 15.7,
      "grad_norm": 368880.15625,
      "learning_rate": 2.217750257997936e-05,
      "loss": 58.7672,
      "step": 15682
    },
    {
      "epoch": 15.7,
      "grad_norm": 5196.98388671875,
      "learning_rate": 2.2172342621259032e-05,
      "loss": 48.8277,
      "step": 15683
    },
    {
      "epoch": 15.7,
      "grad_norm": 3650.521484375,
      "learning_rate": 2.2167182662538703e-05,
      "loss": 50.6671,
      "step": 15684
    },
    {
      "epoch": 15.7,
      "grad_norm": 1832.4322509765625,
      "learning_rate": 2.216202270381837e-05,
      "loss": 56.3231,
      "step": 15685
    },
    {
      "epoch": 15.7,
      "grad_norm": 16238.91015625,
      "learning_rate": 2.215686274509804e-05,
      "loss": 52.0257,
      "step": 15686
    },
    {
      "epoch": 15.7,
      "grad_norm": 9501.6796875,
      "learning_rate": 2.215170278637771e-05,
      "loss": 57.0461,
      "step": 15687
    },
    {
      "epoch": 15.7,
      "grad_norm": 10941.7333984375,
      "learning_rate": 2.214654282765738e-05,
      "loss": 51.3465,
      "step": 15688
    },
    {
      "epoch": 15.7,
      "grad_norm": 12451.8212890625,
      "learning_rate": 2.214138286893705e-05,
      "loss": 53.4208,
      "step": 15689
    },
    {
      "epoch": 15.71,
      "grad_norm": 38752.96484375,
      "learning_rate": 2.213622291021672e-05,
      "loss": 17.5103,
      "step": 15690
    },
    {
      "epoch": 15.71,
      "grad_norm": 4584.7294921875,
      "learning_rate": 2.213106295149639e-05,
      "loss": 54.3321,
      "step": 15691
    },
    {
      "epoch": 15.71,
      "grad_norm": 111676.859375,
      "learning_rate": 2.212590299277606e-05,
      "loss": 53.6946,
      "step": 15692
    },
    {
      "epoch": 15.71,
      "grad_norm": 12011.6767578125,
      "learning_rate": 2.212074303405573e-05,
      "loss": 54.3226,
      "step": 15693
    },
    {
      "epoch": 15.71,
      "grad_norm": 1587.1141357421875,
      "learning_rate": 2.2115583075335397e-05,
      "loss": 62.3719,
      "step": 15694
    },
    {
      "epoch": 15.71,
      "grad_norm": 14088.974609375,
      "learning_rate": 2.211042311661507e-05,
      "loss": 62.8964,
      "step": 15695
    },
    {
      "epoch": 15.71,
      "grad_norm": 51097.6171875,
      "learning_rate": 2.2105263157894736e-05,
      "loss": 51.3924,
      "step": 15696
    },
    {
      "epoch": 15.71,
      "grad_norm": 2118.48046875,
      "learning_rate": 2.2100103199174407e-05,
      "loss": 58.8321,
      "step": 15697
    },
    {
      "epoch": 15.71,
      "grad_norm": 126212.7890625,
      "learning_rate": 2.209494324045408e-05,
      "loss": 38.5086,
      "step": 15698
    },
    {
      "epoch": 15.71,
      "grad_norm": 12695.7265625,
      "learning_rate": 2.2089783281733746e-05,
      "loss": 57.0923,
      "step": 15699
    },
    {
      "epoch": 15.72,
      "grad_norm": 2916.715576171875,
      "learning_rate": 2.2084623323013417e-05,
      "loss": 49.5897,
      "step": 15700
    },
    {
      "epoch": 15.72,
      "grad_norm": 3809.836669921875,
      "learning_rate": 2.207946336429309e-05,
      "loss": 53.6505,
      "step": 15701
    },
    {
      "epoch": 15.72,
      "grad_norm": 2733.519775390625,
      "learning_rate": 2.2074303405572756e-05,
      "loss": 53.7375,
      "step": 15702
    },
    {
      "epoch": 15.72,
      "grad_norm": 6777.3134765625,
      "learning_rate": 2.2069143446852424e-05,
      "loss": 53.8275,
      "step": 15703
    },
    {
      "epoch": 15.72,
      "grad_norm": 1420.7742919921875,
      "learning_rate": 2.2063983488132095e-05,
      "loss": 49.3285,
      "step": 15704
    },
    {
      "epoch": 15.72,
      "grad_norm": 15635.2900390625,
      "learning_rate": 2.2058823529411766e-05,
      "loss": 46.2965,
      "step": 15705
    },
    {
      "epoch": 15.72,
      "grad_norm": 12461.9541015625,
      "learning_rate": 2.2053663570691434e-05,
      "loss": 36.3314,
      "step": 15706
    },
    {
      "epoch": 15.72,
      "grad_norm": 9472.44921875,
      "learning_rate": 2.2048503611971105e-05,
      "loss": 38.1185,
      "step": 15707
    },
    {
      "epoch": 15.72,
      "grad_norm": 8523.4501953125,
      "learning_rate": 2.2043343653250776e-05,
      "loss": 50.7492,
      "step": 15708
    },
    {
      "epoch": 15.72,
      "grad_norm": 2298.50341796875,
      "learning_rate": 2.2038183694530444e-05,
      "loss": 53.0677,
      "step": 15709
    },
    {
      "epoch": 15.73,
      "grad_norm": 2014.370361328125,
      "learning_rate": 2.2033023735810115e-05,
      "loss": 58.4702,
      "step": 15710
    },
    {
      "epoch": 15.73,
      "grad_norm": 3146.989501953125,
      "learning_rate": 2.2027863777089786e-05,
      "loss": 61.5091,
      "step": 15711
    },
    {
      "epoch": 15.73,
      "grad_norm": 8685.310546875,
      "learning_rate": 2.2022703818369454e-05,
      "loss": 53.3053,
      "step": 15712
    },
    {
      "epoch": 15.73,
      "grad_norm": 35218.015625,
      "learning_rate": 2.201754385964912e-05,
      "loss": 58.7568,
      "step": 15713
    },
    {
      "epoch": 15.73,
      "grad_norm": 9300.8232421875,
      "learning_rate": 2.2012383900928793e-05,
      "loss": 55.6321,
      "step": 15714
    },
    {
      "epoch": 15.73,
      "grad_norm": 42076.94921875,
      "learning_rate": 2.2007223942208464e-05,
      "loss": 54.3312,
      "step": 15715
    },
    {
      "epoch": 15.73,
      "grad_norm": 46214.828125,
      "learning_rate": 2.200206398348813e-05,
      "loss": 55.3417,
      "step": 15716
    },
    {
      "epoch": 15.73,
      "grad_norm": 55833.359375,
      "learning_rate": 2.1996904024767803e-05,
      "loss": 49.9182,
      "step": 15717
    },
    {
      "epoch": 15.73,
      "grad_norm": 18456.6328125,
      "learning_rate": 2.1991744066047474e-05,
      "loss": 29.0297,
      "step": 15718
    },
    {
      "epoch": 15.73,
      "grad_norm": 6955.53369140625,
      "learning_rate": 2.1986584107327145e-05,
      "loss": 59.2991,
      "step": 15719
    },
    {
      "epoch": 15.74,
      "grad_norm": 18864.822265625,
      "learning_rate": 2.1981424148606813e-05,
      "loss": 28.5702,
      "step": 15720
    },
    {
      "epoch": 15.74,
      "grad_norm": 4946.26953125,
      "learning_rate": 2.197626418988648e-05,
      "loss": 58.7159,
      "step": 15721
    },
    {
      "epoch": 15.74,
      "grad_norm": 4833.8076171875,
      "learning_rate": 2.197110423116615e-05,
      "loss": 60.7831,
      "step": 15722
    },
    {
      "epoch": 15.74,
      "grad_norm": 24300.267578125,
      "learning_rate": 2.196594427244582e-05,
      "loss": 32.496,
      "step": 15723
    },
    {
      "epoch": 15.74,
      "grad_norm": 31865.2578125,
      "learning_rate": 2.196078431372549e-05,
      "loss": 41.8482,
      "step": 15724
    },
    {
      "epoch": 15.74,
      "grad_norm": 1107.803955078125,
      "learning_rate": 2.195562435500516e-05,
      "loss": 60.9172,
      "step": 15725
    },
    {
      "epoch": 15.74,
      "grad_norm": 250391.671875,
      "learning_rate": 2.195046439628483e-05,
      "loss": 43.7002,
      "step": 15726
    },
    {
      "epoch": 15.74,
      "grad_norm": 93693.9765625,
      "learning_rate": 2.19453044375645e-05,
      "loss": 29.3172,
      "step": 15727
    },
    {
      "epoch": 15.74,
      "grad_norm": 54506.6171875,
      "learning_rate": 2.194014447884417e-05,
      "loss": 52.6834,
      "step": 15728
    },
    {
      "epoch": 15.74,
      "grad_norm": 7263.41015625,
      "learning_rate": 2.1934984520123843e-05,
      "loss": 45.0222,
      "step": 15729
    },
    {
      "epoch": 15.75,
      "grad_norm": 4716.4638671875,
      "learning_rate": 2.1929824561403507e-05,
      "loss": 47.525,
      "step": 15730
    },
    {
      "epoch": 15.75,
      "grad_norm": 69211.28125,
      "learning_rate": 2.1924664602683178e-05,
      "loss": 51.2687,
      "step": 15731
    },
    {
      "epoch": 15.75,
      "grad_norm": 18239.064453125,
      "learning_rate": 2.191950464396285e-05,
      "loss": 60.8631,
      "step": 15732
    },
    {
      "epoch": 15.75,
      "grad_norm": 5765.29541015625,
      "learning_rate": 2.1914344685242517e-05,
      "loss": 53.5585,
      "step": 15733
    },
    {
      "epoch": 15.75,
      "grad_norm": 3153.2900390625,
      "learning_rate": 2.1909184726522188e-05,
      "loss": 57.2655,
      "step": 15734
    },
    {
      "epoch": 15.75,
      "grad_norm": 6927.94873046875,
      "learning_rate": 2.190402476780186e-05,
      "loss": 49.1316,
      "step": 15735
    },
    {
      "epoch": 15.75,
      "grad_norm": 6276.2890625,
      "learning_rate": 2.189886480908153e-05,
      "loss": 62.7375,
      "step": 15736
    },
    {
      "epoch": 15.75,
      "grad_norm": 2014.95654296875,
      "learning_rate": 2.1893704850361198e-05,
      "loss": 57.4121,
      "step": 15737
    },
    {
      "epoch": 15.75,
      "grad_norm": 11655.0517578125,
      "learning_rate": 2.188854489164087e-05,
      "loss": 42.3647,
      "step": 15738
    },
    {
      "epoch": 15.75,
      "grad_norm": 4272.61279296875,
      "learning_rate": 2.1883384932920537e-05,
      "loss": 59.1145,
      "step": 15739
    },
    {
      "epoch": 15.76,
      "grad_norm": 11012.455078125,
      "learning_rate": 2.1878224974200205e-05,
      "loss": 59.3809,
      "step": 15740
    },
    {
      "epoch": 15.76,
      "grad_norm": 3770.96533203125,
      "learning_rate": 2.1873065015479876e-05,
      "loss": 40.2183,
      "step": 15741
    },
    {
      "epoch": 15.76,
      "grad_norm": 11198.005859375,
      "learning_rate": 2.1867905056759547e-05,
      "loss": 57.1266,
      "step": 15742
    },
    {
      "epoch": 15.76,
      "grad_norm": 38585.078125,
      "learning_rate": 2.1862745098039218e-05,
      "loss": 60.1948,
      "step": 15743
    },
    {
      "epoch": 15.76,
      "grad_norm": 14832.83984375,
      "learning_rate": 2.1857585139318886e-05,
      "loss": 28.2675,
      "step": 15744
    },
    {
      "epoch": 15.76,
      "grad_norm": 4516.7958984375,
      "learning_rate": 2.1852425180598557e-05,
      "loss": 60.2775,
      "step": 15745
    },
    {
      "epoch": 15.76,
      "grad_norm": 7297.224609375,
      "learning_rate": 2.1847265221878228e-05,
      "loss": 55.8308,
      "step": 15746
    },
    {
      "epoch": 15.76,
      "grad_norm": 1235.200927734375,
      "learning_rate": 2.1842105263157896e-05,
      "loss": 35.0355,
      "step": 15747
    },
    {
      "epoch": 15.76,
      "grad_norm": 9398.6318359375,
      "learning_rate": 2.1836945304437564e-05,
      "loss": 54.5838,
      "step": 15748
    },
    {
      "epoch": 15.76,
      "grad_norm": 5004.60791015625,
      "learning_rate": 2.1831785345717235e-05,
      "loss": 53.3169,
      "step": 15749
    },
    {
      "epoch": 15.77,
      "grad_norm": 1981.5926513671875,
      "learning_rate": 2.1826625386996906e-05,
      "loss": 59.5367,
      "step": 15750
    },
    {
      "epoch": 15.77,
      "grad_norm": 3183.89501953125,
      "learning_rate": 2.1821465428276574e-05,
      "loss": 50.7105,
      "step": 15751
    },
    {
      "epoch": 15.77,
      "grad_norm": 18175.27734375,
      "learning_rate": 2.1816305469556245e-05,
      "loss": 39.8679,
      "step": 15752
    },
    {
      "epoch": 15.77,
      "grad_norm": 29859.01953125,
      "learning_rate": 2.1811145510835916e-05,
      "loss": 56.3839,
      "step": 15753
    },
    {
      "epoch": 15.77,
      "grad_norm": 7108.45068359375,
      "learning_rate": 2.1805985552115584e-05,
      "loss": 59.2348,
      "step": 15754
    },
    {
      "epoch": 15.77,
      "grad_norm": 3500.77880859375,
      "learning_rate": 2.1800825593395255e-05,
      "loss": 58.1151,
      "step": 15755
    },
    {
      "epoch": 15.77,
      "grad_norm": 3244.829345703125,
      "learning_rate": 2.1795665634674926e-05,
      "loss": 55.5204,
      "step": 15756
    },
    {
      "epoch": 15.77,
      "grad_norm": 19177.11328125,
      "learning_rate": 2.1790505675954594e-05,
      "loss": 38.5028,
      "step": 15757
    },
    {
      "epoch": 15.77,
      "grad_norm": 48926.92578125,
      "learning_rate": 2.178534571723426e-05,
      "loss": 49.2828,
      "step": 15758
    },
    {
      "epoch": 15.77,
      "grad_norm": 7232.951171875,
      "learning_rate": 2.1780185758513933e-05,
      "loss": 57.1394,
      "step": 15759
    },
    {
      "epoch": 15.78,
      "grad_norm": 11144.1298828125,
      "learning_rate": 2.1775025799793604e-05,
      "loss": 56.6342,
      "step": 15760
    },
    {
      "epoch": 15.78,
      "grad_norm": 1894.1326904296875,
      "learning_rate": 2.176986584107327e-05,
      "loss": 59.8941,
      "step": 15761
    },
    {
      "epoch": 15.78,
      "grad_norm": 7500.90380859375,
      "learning_rate": 2.1764705882352943e-05,
      "loss": 45.7283,
      "step": 15762
    },
    {
      "epoch": 15.78,
      "grad_norm": 19735.3515625,
      "learning_rate": 2.1759545923632614e-05,
      "loss": 51.9696,
      "step": 15763
    },
    {
      "epoch": 15.78,
      "grad_norm": 24506.748046875,
      "learning_rate": 2.175438596491228e-05,
      "loss": 56.3871,
      "step": 15764
    },
    {
      "epoch": 15.78,
      "grad_norm": 15266.9345703125,
      "learning_rate": 2.1749226006191953e-05,
      "loss": 59.3809,
      "step": 15765
    },
    {
      "epoch": 15.78,
      "grad_norm": 1133.968505859375,
      "learning_rate": 2.174406604747162e-05,
      "loss": 61.0598,
      "step": 15766
    },
    {
      "epoch": 15.78,
      "grad_norm": 2083.36962890625,
      "learning_rate": 2.173890608875129e-05,
      "loss": 57.3187,
      "step": 15767
    },
    {
      "epoch": 15.78,
      "grad_norm": 1903.9532470703125,
      "learning_rate": 2.173374613003096e-05,
      "loss": 62.9477,
      "step": 15768
    },
    {
      "epoch": 15.78,
      "grad_norm": 8377.5380859375,
      "learning_rate": 2.172858617131063e-05,
      "loss": 58.0714,
      "step": 15769
    },
    {
      "epoch": 15.79,
      "grad_norm": 6469.8134765625,
      "learning_rate": 2.17234262125903e-05,
      "loss": 51.8338,
      "step": 15770
    },
    {
      "epoch": 15.79,
      "grad_norm": 21087.830078125,
      "learning_rate": 2.171826625386997e-05,
      "loss": 57.7768,
      "step": 15771
    },
    {
      "epoch": 15.79,
      "grad_norm": 20194.7890625,
      "learning_rate": 2.171310629514964e-05,
      "loss": 50.2866,
      "step": 15772
    },
    {
      "epoch": 15.79,
      "grad_norm": 12109.2724609375,
      "learning_rate": 2.170794633642931e-05,
      "loss": 54.7863,
      "step": 15773
    },
    {
      "epoch": 15.79,
      "grad_norm": 16020.5146484375,
      "learning_rate": 2.170278637770898e-05,
      "loss": 60.3631,
      "step": 15774
    },
    {
      "epoch": 15.79,
      "grad_norm": 4457.73095703125,
      "learning_rate": 2.1697626418988647e-05,
      "loss": 49.654,
      "step": 15775
    },
    {
      "epoch": 15.79,
      "grad_norm": 21290.87109375,
      "learning_rate": 2.1692466460268318e-05,
      "loss": 47.8949,
      "step": 15776
    },
    {
      "epoch": 15.79,
      "grad_norm": 31815.3984375,
      "learning_rate": 2.168730650154799e-05,
      "loss": 60.4017,
      "step": 15777
    },
    {
      "epoch": 15.79,
      "grad_norm": 15112.2109375,
      "learning_rate": 2.1682146542827657e-05,
      "loss": 49.3985,
      "step": 15778
    },
    {
      "epoch": 15.79,
      "grad_norm": 2327.3125,
      "learning_rate": 2.1676986584107328e-05,
      "loss": 60.3303,
      "step": 15779
    },
    {
      "epoch": 15.8,
      "grad_norm": 8149.91455078125,
      "learning_rate": 2.1671826625387e-05,
      "loss": 53.446,
      "step": 15780
    },
    {
      "epoch": 15.8,
      "grad_norm": 7321.93359375,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 59.5135,
      "step": 15781
    },
    {
      "epoch": 15.8,
      "grad_norm": 4458.5634765625,
      "learning_rate": 2.1661506707946338e-05,
      "loss": 53.1622,
      "step": 15782
    },
    {
      "epoch": 15.8,
      "grad_norm": 10314.9453125,
      "learning_rate": 2.165634674922601e-05,
      "loss": 48.7857,
      "step": 15783
    },
    {
      "epoch": 15.8,
      "grad_norm": 10952.1826171875,
      "learning_rate": 2.1651186790505677e-05,
      "loss": 22.8344,
      "step": 15784
    },
    {
      "epoch": 15.8,
      "grad_norm": 6665.60107421875,
      "learning_rate": 2.1646026831785345e-05,
      "loss": 49.1163,
      "step": 15785
    },
    {
      "epoch": 15.8,
      "grad_norm": 6482.41357421875,
      "learning_rate": 2.1640866873065016e-05,
      "loss": 59.0929,
      "step": 15786
    },
    {
      "epoch": 15.8,
      "grad_norm": 1595.1190185546875,
      "learning_rate": 2.1635706914344687e-05,
      "loss": 60.3872,
      "step": 15787
    },
    {
      "epoch": 15.8,
      "grad_norm": 51061.15234375,
      "learning_rate": 2.1630546955624355e-05,
      "loss": 53.4449,
      "step": 15788
    },
    {
      "epoch": 15.8,
      "grad_norm": 41944.07421875,
      "learning_rate": 2.1625386996904026e-05,
      "loss": 61.6713,
      "step": 15789
    },
    {
      "epoch": 15.81,
      "grad_norm": 4695.79541015625,
      "learning_rate": 2.1620227038183697e-05,
      "loss": 47.2842,
      "step": 15790
    },
    {
      "epoch": 15.81,
      "grad_norm": 12280.8154296875,
      "learning_rate": 2.1615067079463365e-05,
      "loss": 58.0709,
      "step": 15791
    },
    {
      "epoch": 15.81,
      "grad_norm": 22646.4765625,
      "learning_rate": 2.1609907120743036e-05,
      "loss": 51.5108,
      "step": 15792
    },
    {
      "epoch": 15.81,
      "grad_norm": 48682.15625,
      "learning_rate": 2.1604747162022704e-05,
      "loss": 43.0177,
      "step": 15793
    },
    {
      "epoch": 15.81,
      "grad_norm": 9582.4697265625,
      "learning_rate": 2.1599587203302375e-05,
      "loss": 55.8861,
      "step": 15794
    },
    {
      "epoch": 15.81,
      "grad_norm": 33649.24609375,
      "learning_rate": 2.1594427244582043e-05,
      "loss": 42.4397,
      "step": 15795
    },
    {
      "epoch": 15.81,
      "grad_norm": 6175.2548828125,
      "learning_rate": 2.1589267285861714e-05,
      "loss": 60.0346,
      "step": 15796
    },
    {
      "epoch": 15.81,
      "grad_norm": 18047.451171875,
      "learning_rate": 2.1584107327141385e-05,
      "loss": 54.0799,
      "step": 15797
    },
    {
      "epoch": 15.81,
      "grad_norm": 3342.2763671875,
      "learning_rate": 2.1578947368421053e-05,
      "loss": 53.7703,
      "step": 15798
    },
    {
      "epoch": 15.81,
      "grad_norm": 3111.49853515625,
      "learning_rate": 2.1573787409700724e-05,
      "loss": 55.0928,
      "step": 15799
    },
    {
      "epoch": 15.82,
      "grad_norm": 48913.2578125,
      "learning_rate": 2.1568627450980395e-05,
      "loss": 51.3123,
      "step": 15800
    },
    {
      "epoch": 15.82,
      "grad_norm": 7523.24755859375,
      "learning_rate": 2.1563467492260063e-05,
      "loss": 54.9624,
      "step": 15801
    },
    {
      "epoch": 15.82,
      "grad_norm": 5384.73828125,
      "learning_rate": 2.155830753353973e-05,
      "loss": 56.8369,
      "step": 15802
    },
    {
      "epoch": 15.82,
      "grad_norm": 17730.328125,
      "learning_rate": 2.15531475748194e-05,
      "loss": 43.4755,
      "step": 15803
    },
    {
      "epoch": 15.82,
      "grad_norm": 24748.90234375,
      "learning_rate": 2.1547987616099073e-05,
      "loss": 56.8463,
      "step": 15804
    },
    {
      "epoch": 15.82,
      "grad_norm": 7134.6689453125,
      "learning_rate": 2.154282765737874e-05,
      "loss": 62.0935,
      "step": 15805
    },
    {
      "epoch": 15.82,
      "grad_norm": 9149.0185546875,
      "learning_rate": 2.153766769865841e-05,
      "loss": 51.2472,
      "step": 15806
    },
    {
      "epoch": 15.82,
      "grad_norm": 7947.5546875,
      "learning_rate": 2.1532507739938083e-05,
      "loss": 51.4609,
      "step": 15807
    },
    {
      "epoch": 15.82,
      "grad_norm": 2961.5478515625,
      "learning_rate": 2.1527347781217754e-05,
      "loss": 57.8716,
      "step": 15808
    },
    {
      "epoch": 15.82,
      "grad_norm": 8731.5126953125,
      "learning_rate": 2.152218782249742e-05,
      "loss": 46.9008,
      "step": 15809
    },
    {
      "epoch": 15.83,
      "grad_norm": 37462.31640625,
      "learning_rate": 2.151702786377709e-05,
      "loss": 50.7613,
      "step": 15810
    },
    {
      "epoch": 15.83,
      "grad_norm": 13442.40234375,
      "learning_rate": 2.151186790505676e-05,
      "loss": 42.9279,
      "step": 15811
    },
    {
      "epoch": 15.83,
      "grad_norm": 10891.775390625,
      "learning_rate": 2.1506707946336428e-05,
      "loss": 47.5849,
      "step": 15812
    },
    {
      "epoch": 15.83,
      "grad_norm": 4587.37744140625,
      "learning_rate": 2.15015479876161e-05,
      "loss": 60.0805,
      "step": 15813
    },
    {
      "epoch": 15.83,
      "grad_norm": 28126.146484375,
      "learning_rate": 2.149638802889577e-05,
      "loss": 58.8456,
      "step": 15814
    },
    {
      "epoch": 15.83,
      "grad_norm": 4744.62158203125,
      "learning_rate": 2.149122807017544e-05,
      "loss": 58.1734,
      "step": 15815
    },
    {
      "epoch": 15.83,
      "grad_norm": 4497.98681640625,
      "learning_rate": 2.148606811145511e-05,
      "loss": 43.2733,
      "step": 15816
    },
    {
      "epoch": 15.83,
      "grad_norm": 4879.49462890625,
      "learning_rate": 2.148090815273478e-05,
      "loss": 43.3218,
      "step": 15817
    },
    {
      "epoch": 15.83,
      "grad_norm": 6219.8330078125,
      "learning_rate": 2.147574819401445e-05,
      "loss": 61.1576,
      "step": 15818
    },
    {
      "epoch": 15.83,
      "grad_norm": 12168.0234375,
      "learning_rate": 2.1470588235294116e-05,
      "loss": 35.5919,
      "step": 15819
    },
    {
      "epoch": 15.84,
      "grad_norm": 16110.6865234375,
      "learning_rate": 2.1465428276573787e-05,
      "loss": 61.1714,
      "step": 15820
    },
    {
      "epoch": 15.84,
      "grad_norm": 1980.4324951171875,
      "learning_rate": 2.1460268317853458e-05,
      "loss": 58.2566,
      "step": 15821
    },
    {
      "epoch": 15.84,
      "grad_norm": 34920.05859375,
      "learning_rate": 2.145510835913313e-05,
      "loss": 37.6317,
      "step": 15822
    },
    {
      "epoch": 15.84,
      "grad_norm": 4846.33349609375,
      "learning_rate": 2.1449948400412797e-05,
      "loss": 30.1226,
      "step": 15823
    },
    {
      "epoch": 15.84,
      "grad_norm": 45739.15625,
      "learning_rate": 2.1444788441692468e-05,
      "loss": 54.2421,
      "step": 15824
    },
    {
      "epoch": 15.84,
      "grad_norm": 6656.68603515625,
      "learning_rate": 2.143962848297214e-05,
      "loss": 59.2632,
      "step": 15825
    },
    {
      "epoch": 15.84,
      "grad_norm": 2385.669921875,
      "learning_rate": 2.1434468524251807e-05,
      "loss": 63.5963,
      "step": 15826
    },
    {
      "epoch": 15.84,
      "grad_norm": 4420.99169921875,
      "learning_rate": 2.1429308565531478e-05,
      "loss": 65.9616,
      "step": 15827
    },
    {
      "epoch": 15.84,
      "grad_norm": 6443.47802734375,
      "learning_rate": 2.1424148606811146e-05,
      "loss": 51.3405,
      "step": 15828
    },
    {
      "epoch": 15.84,
      "grad_norm": 2054.238037109375,
      "learning_rate": 2.1418988648090817e-05,
      "loss": 58.4552,
      "step": 15829
    },
    {
      "epoch": 15.85,
      "grad_norm": 2550.496337890625,
      "learning_rate": 2.1413828689370485e-05,
      "loss": 44.4576,
      "step": 15830
    },
    {
      "epoch": 15.85,
      "grad_norm": 5357.69384765625,
      "learning_rate": 2.1408668730650156e-05,
      "loss": 41.5762,
      "step": 15831
    },
    {
      "epoch": 15.85,
      "grad_norm": 1628.8729248046875,
      "learning_rate": 2.1403508771929827e-05,
      "loss": 54.4727,
      "step": 15832
    },
    {
      "epoch": 15.85,
      "grad_norm": 38165.06640625,
      "learning_rate": 2.1398348813209495e-05,
      "loss": 56.6408,
      "step": 15833
    },
    {
      "epoch": 15.85,
      "grad_norm": 15718.9560546875,
      "learning_rate": 2.1393188854489166e-05,
      "loss": 50.9961,
      "step": 15834
    },
    {
      "epoch": 15.85,
      "grad_norm": 7243.36962890625,
      "learning_rate": 2.1388028895768837e-05,
      "loss": 46.4567,
      "step": 15835
    },
    {
      "epoch": 15.85,
      "grad_norm": 71314.8984375,
      "learning_rate": 2.1382868937048505e-05,
      "loss": 55.8048,
      "step": 15836
    },
    {
      "epoch": 15.85,
      "grad_norm": 15924.1611328125,
      "learning_rate": 2.1377708978328172e-05,
      "loss": 59.6225,
      "step": 15837
    },
    {
      "epoch": 15.85,
      "grad_norm": 22740.02734375,
      "learning_rate": 2.1372549019607844e-05,
      "loss": 63.1463,
      "step": 15838
    },
    {
      "epoch": 15.85,
      "grad_norm": 19549.060546875,
      "learning_rate": 2.1367389060887515e-05,
      "loss": 58.3363,
      "step": 15839
    },
    {
      "epoch": 15.86,
      "grad_norm": 33154.0703125,
      "learning_rate": 2.1362229102167182e-05,
      "loss": 56.7305,
      "step": 15840
    },
    {
      "epoch": 15.86,
      "grad_norm": 44664.37109375,
      "learning_rate": 2.1357069143446854e-05,
      "loss": 44.1074,
      "step": 15841
    },
    {
      "epoch": 15.86,
      "grad_norm": 23904.50390625,
      "learning_rate": 2.1351909184726525e-05,
      "loss": 57.912,
      "step": 15842
    },
    {
      "epoch": 15.86,
      "grad_norm": 11918.12109375,
      "learning_rate": 2.1346749226006192e-05,
      "loss": 34.6044,
      "step": 15843
    },
    {
      "epoch": 15.86,
      "grad_norm": 7234.1923828125,
      "learning_rate": 2.1341589267285864e-05,
      "loss": 54.5844,
      "step": 15844
    },
    {
      "epoch": 15.86,
      "grad_norm": 22604.287109375,
      "learning_rate": 2.1336429308565535e-05,
      "loss": 56.7948,
      "step": 15845
    },
    {
      "epoch": 15.86,
      "grad_norm": 4666.984375,
      "learning_rate": 2.1331269349845202e-05,
      "loss": 43.5748,
      "step": 15846
    },
    {
      "epoch": 15.86,
      "grad_norm": 13769.7314453125,
      "learning_rate": 2.132610939112487e-05,
      "loss": 51.8291,
      "step": 15847
    },
    {
      "epoch": 15.86,
      "grad_norm": 34240.11328125,
      "learning_rate": 2.132094943240454e-05,
      "loss": 56.8747,
      "step": 15848
    },
    {
      "epoch": 15.86,
      "grad_norm": 9468.9853515625,
      "learning_rate": 2.1315789473684212e-05,
      "loss": 49.9075,
      "step": 15849
    },
    {
      "epoch": 15.87,
      "grad_norm": 8659.7001953125,
      "learning_rate": 2.131062951496388e-05,
      "loss": 49.2987,
      "step": 15850
    },
    {
      "epoch": 15.87,
      "grad_norm": 31271.3671875,
      "learning_rate": 2.130546955624355e-05,
      "loss": 59.4358,
      "step": 15851
    },
    {
      "epoch": 15.87,
      "grad_norm": 11743.6455078125,
      "learning_rate": 2.1300309597523222e-05,
      "loss": 59.7101,
      "step": 15852
    },
    {
      "epoch": 15.87,
      "grad_norm": 14861.02734375,
      "learning_rate": 2.129514963880289e-05,
      "loss": 36.798,
      "step": 15853
    },
    {
      "epoch": 15.87,
      "grad_norm": 71941.0703125,
      "learning_rate": 2.128998968008256e-05,
      "loss": 55.0549,
      "step": 15854
    },
    {
      "epoch": 15.87,
      "grad_norm": 15581.7412109375,
      "learning_rate": 2.128482972136223e-05,
      "loss": 61.2067,
      "step": 15855
    },
    {
      "epoch": 15.87,
      "grad_norm": 5067.7685546875,
      "learning_rate": 2.12796697626419e-05,
      "loss": 59.89,
      "step": 15856
    },
    {
      "epoch": 15.87,
      "grad_norm": 11162.1484375,
      "learning_rate": 2.1274509803921568e-05,
      "loss": 47.2884,
      "step": 15857
    },
    {
      "epoch": 15.87,
      "grad_norm": 2642.8095703125,
      "learning_rate": 2.126934984520124e-05,
      "loss": 61.4122,
      "step": 15858
    },
    {
      "epoch": 15.87,
      "grad_norm": 21071.45703125,
      "learning_rate": 2.126418988648091e-05,
      "loss": 56.7383,
      "step": 15859
    },
    {
      "epoch": 15.88,
      "grad_norm": 6748.35986328125,
      "learning_rate": 2.1259029927760578e-05,
      "loss": 51.7001,
      "step": 15860
    },
    {
      "epoch": 15.88,
      "grad_norm": 247850.28125,
      "learning_rate": 2.125386996904025e-05,
      "loss": 58.2874,
      "step": 15861
    },
    {
      "epoch": 15.88,
      "grad_norm": 11279.76171875,
      "learning_rate": 2.124871001031992e-05,
      "loss": 51.4901,
      "step": 15862
    },
    {
      "epoch": 15.88,
      "grad_norm": 4384.333984375,
      "learning_rate": 2.1243550051599588e-05,
      "loss": 58.0582,
      "step": 15863
    },
    {
      "epoch": 15.88,
      "grad_norm": 29932.986328125,
      "learning_rate": 2.1238390092879256e-05,
      "loss": 59.8513,
      "step": 15864
    },
    {
      "epoch": 15.88,
      "grad_norm": 4402.673828125,
      "learning_rate": 2.1233230134158927e-05,
      "loss": 66.6998,
      "step": 15865
    },
    {
      "epoch": 15.88,
      "grad_norm": 6529.32666015625,
      "learning_rate": 2.1228070175438598e-05,
      "loss": 58.2028,
      "step": 15866
    },
    {
      "epoch": 15.88,
      "grad_norm": 6591.5791015625,
      "learning_rate": 2.1222910216718266e-05,
      "loss": 57.5706,
      "step": 15867
    },
    {
      "epoch": 15.88,
      "grad_norm": 3130.00048828125,
      "learning_rate": 2.1217750257997937e-05,
      "loss": 52.266,
      "step": 15868
    },
    {
      "epoch": 15.88,
      "grad_norm": 3386.691650390625,
      "learning_rate": 2.1212590299277608e-05,
      "loss": 58.6751,
      "step": 15869
    },
    {
      "epoch": 15.89,
      "grad_norm": 36683.78125,
      "learning_rate": 2.1207430340557276e-05,
      "loss": 52.7834,
      "step": 15870
    },
    {
      "epoch": 15.89,
      "grad_norm": 1662.188720703125,
      "learning_rate": 2.1202270381836947e-05,
      "loss": 64.4667,
      "step": 15871
    },
    {
      "epoch": 15.89,
      "grad_norm": 1574.3953857421875,
      "learning_rate": 2.1197110423116618e-05,
      "loss": 61.8365,
      "step": 15872
    },
    {
      "epoch": 15.89,
      "grad_norm": 2783.976318359375,
      "learning_rate": 2.1191950464396286e-05,
      "loss": 50.0877,
      "step": 15873
    },
    {
      "epoch": 15.89,
      "grad_norm": 9366.0498046875,
      "learning_rate": 2.1186790505675953e-05,
      "loss": 50.7046,
      "step": 15874
    },
    {
      "epoch": 15.89,
      "grad_norm": 51320.37890625,
      "learning_rate": 2.1181630546955625e-05,
      "loss": 56.8754,
      "step": 15875
    },
    {
      "epoch": 15.89,
      "grad_norm": 4587.66162109375,
      "learning_rate": 2.1176470588235296e-05,
      "loss": 41.2824,
      "step": 15876
    },
    {
      "epoch": 15.89,
      "grad_norm": 21456.78515625,
      "learning_rate": 2.1171310629514963e-05,
      "loss": 49.1354,
      "step": 15877
    },
    {
      "epoch": 15.89,
      "grad_norm": 11633.0380859375,
      "learning_rate": 2.1166150670794635e-05,
      "loss": 50.2979,
      "step": 15878
    },
    {
      "epoch": 15.89,
      "grad_norm": 18225.1640625,
      "learning_rate": 2.1160990712074306e-05,
      "loss": 44.9015,
      "step": 15879
    },
    {
      "epoch": 15.9,
      "grad_norm": 52542.26171875,
      "learning_rate": 2.1155830753353977e-05,
      "loss": 40.1303,
      "step": 15880
    },
    {
      "epoch": 15.9,
      "grad_norm": 24896.1015625,
      "learning_rate": 2.1150670794633645e-05,
      "loss": 61.1469,
      "step": 15881
    },
    {
      "epoch": 15.9,
      "grad_norm": 11601.2109375,
      "learning_rate": 2.1145510835913312e-05,
      "loss": 46.1517,
      "step": 15882
    },
    {
      "epoch": 15.9,
      "grad_norm": 13427.6220703125,
      "learning_rate": 2.1140350877192983e-05,
      "loss": 43.4053,
      "step": 15883
    },
    {
      "epoch": 15.9,
      "grad_norm": 2985.037353515625,
      "learning_rate": 2.113519091847265e-05,
      "loss": 62.1616,
      "step": 15884
    },
    {
      "epoch": 15.9,
      "grad_norm": 33617.08203125,
      "learning_rate": 2.1130030959752322e-05,
      "loss": 57.9801,
      "step": 15885
    },
    {
      "epoch": 15.9,
      "grad_norm": 3567.09033203125,
      "learning_rate": 2.1124871001031993e-05,
      "loss": 55.4856,
      "step": 15886
    },
    {
      "epoch": 15.9,
      "grad_norm": 3056.72509765625,
      "learning_rate": 2.1119711042311665e-05,
      "loss": 60.8105,
      "step": 15887
    },
    {
      "epoch": 15.9,
      "grad_norm": 3359.46533203125,
      "learning_rate": 2.1114551083591332e-05,
      "loss": 53.6871,
      "step": 15888
    },
    {
      "epoch": 15.9,
      "grad_norm": 83618.03125,
      "learning_rate": 2.1109391124871003e-05,
      "loss": 58.382,
      "step": 15889
    },
    {
      "epoch": 15.91,
      "grad_norm": 3819.743408203125,
      "learning_rate": 2.1104231166150675e-05,
      "loss": 55.1156,
      "step": 15890
    },
    {
      "epoch": 15.91,
      "grad_norm": 6553.6142578125,
      "learning_rate": 2.109907120743034e-05,
      "loss": 46.5069,
      "step": 15891
    },
    {
      "epoch": 15.91,
      "grad_norm": 8006.33935546875,
      "learning_rate": 2.109391124871001e-05,
      "loss": 36.9528,
      "step": 15892
    },
    {
      "epoch": 15.91,
      "grad_norm": 20349.9765625,
      "learning_rate": 2.108875128998968e-05,
      "loss": 37.564,
      "step": 15893
    },
    {
      "epoch": 15.91,
      "grad_norm": 25100.404296875,
      "learning_rate": 2.1083591331269352e-05,
      "loss": 50.2472,
      "step": 15894
    },
    {
      "epoch": 15.91,
      "grad_norm": 5498.958984375,
      "learning_rate": 2.107843137254902e-05,
      "loss": 53.7616,
      "step": 15895
    },
    {
      "epoch": 15.91,
      "grad_norm": 6789.0966796875,
      "learning_rate": 2.107327141382869e-05,
      "loss": 50.2685,
      "step": 15896
    },
    {
      "epoch": 15.91,
      "grad_norm": 105452.296875,
      "learning_rate": 2.1068111455108362e-05,
      "loss": 38.7846,
      "step": 15897
    },
    {
      "epoch": 15.91,
      "grad_norm": 12182.28125,
      "learning_rate": 2.106295149638803e-05,
      "loss": 52.2016,
      "step": 15898
    },
    {
      "epoch": 15.91,
      "grad_norm": 19400.318359375,
      "learning_rate": 2.10577915376677e-05,
      "loss": 54.7277,
      "step": 15899
    },
    {
      "epoch": 15.92,
      "grad_norm": 4088.305419921875,
      "learning_rate": 2.105263157894737e-05,
      "loss": 51.5996,
      "step": 15900
    },
    {
      "epoch": 15.92,
      "grad_norm": 77654.015625,
      "learning_rate": 2.104747162022704e-05,
      "loss": 34.4846,
      "step": 15901
    },
    {
      "epoch": 15.92,
      "grad_norm": 3419.610595703125,
      "learning_rate": 2.1042311661506708e-05,
      "loss": 59.8144,
      "step": 15902
    },
    {
      "epoch": 15.92,
      "grad_norm": 8129.12939453125,
      "learning_rate": 2.103715170278638e-05,
      "loss": 30.1069,
      "step": 15903
    },
    {
      "epoch": 15.92,
      "grad_norm": 32872.76953125,
      "learning_rate": 2.103199174406605e-05,
      "loss": 49.0861,
      "step": 15904
    },
    {
      "epoch": 15.92,
      "grad_norm": 11984.880859375,
      "learning_rate": 2.1026831785345718e-05,
      "loss": 57.9558,
      "step": 15905
    },
    {
      "epoch": 15.92,
      "grad_norm": 14813.15625,
      "learning_rate": 2.102167182662539e-05,
      "loss": 39.0929,
      "step": 15906
    },
    {
      "epoch": 15.92,
      "grad_norm": 19730.341796875,
      "learning_rate": 2.101651186790506e-05,
      "loss": 56.9849,
      "step": 15907
    },
    {
      "epoch": 15.92,
      "grad_norm": 2896.486083984375,
      "learning_rate": 2.1011351909184728e-05,
      "loss": 57.3131,
      "step": 15908
    },
    {
      "epoch": 15.92,
      "grad_norm": 18540.630859375,
      "learning_rate": 2.1006191950464396e-05,
      "loss": 58.4652,
      "step": 15909
    },
    {
      "epoch": 15.93,
      "grad_norm": 30712.7578125,
      "learning_rate": 2.1001031991744067e-05,
      "loss": 49.4937,
      "step": 15910
    },
    {
      "epoch": 15.93,
      "grad_norm": 228012.890625,
      "learning_rate": 2.0995872033023738e-05,
      "loss": 61.1054,
      "step": 15911
    },
    {
      "epoch": 15.93,
      "grad_norm": 11360.3779296875,
      "learning_rate": 2.0990712074303406e-05,
      "loss": 57.793,
      "step": 15912
    },
    {
      "epoch": 15.93,
      "grad_norm": 10861.966796875,
      "learning_rate": 2.0985552115583077e-05,
      "loss": 63.2965,
      "step": 15913
    },
    {
      "epoch": 15.93,
      "grad_norm": 10419.224609375,
      "learning_rate": 2.0980392156862748e-05,
      "loss": 49.0194,
      "step": 15914
    },
    {
      "epoch": 15.93,
      "grad_norm": 49268.39453125,
      "learning_rate": 2.0975232198142416e-05,
      "loss": 45.7608,
      "step": 15915
    },
    {
      "epoch": 15.93,
      "grad_norm": 119753.6953125,
      "learning_rate": 2.0970072239422087e-05,
      "loss": 53.1589,
      "step": 15916
    },
    {
      "epoch": 15.93,
      "grad_norm": 3589.76416015625,
      "learning_rate": 2.0964912280701758e-05,
      "loss": 40.8332,
      "step": 15917
    },
    {
      "epoch": 15.93,
      "grad_norm": 9815.82421875,
      "learning_rate": 2.0959752321981426e-05,
      "loss": 56.6942,
      "step": 15918
    },
    {
      "epoch": 15.93,
      "grad_norm": 3115.308349609375,
      "learning_rate": 2.0954592363261093e-05,
      "loss": 53.3853,
      "step": 15919
    },
    {
      "epoch": 15.94,
      "grad_norm": 8114.27880859375,
      "learning_rate": 2.0949432404540764e-05,
      "loss": 53.0957,
      "step": 15920
    },
    {
      "epoch": 15.94,
      "grad_norm": 7973.10986328125,
      "learning_rate": 2.0944272445820436e-05,
      "loss": 55.1165,
      "step": 15921
    },
    {
      "epoch": 15.94,
      "grad_norm": 5438.69384765625,
      "learning_rate": 2.0939112487100103e-05,
      "loss": 55.6721,
      "step": 15922
    },
    {
      "epoch": 15.94,
      "grad_norm": 9156.4140625,
      "learning_rate": 2.0933952528379774e-05,
      "loss": 43.5633,
      "step": 15923
    },
    {
      "epoch": 15.94,
      "grad_norm": 6270.6875,
      "learning_rate": 2.0928792569659446e-05,
      "loss": 57.4766,
      "step": 15924
    },
    {
      "epoch": 15.94,
      "grad_norm": 96102.7265625,
      "learning_rate": 2.0923632610939113e-05,
      "loss": 57.9848,
      "step": 15925
    },
    {
      "epoch": 15.94,
      "grad_norm": 3889.83837890625,
      "learning_rate": 2.0918472652218784e-05,
      "loss": 56.2824,
      "step": 15926
    },
    {
      "epoch": 15.94,
      "grad_norm": 10428.3662109375,
      "learning_rate": 2.0913312693498452e-05,
      "loss": 55.4249,
      "step": 15927
    },
    {
      "epoch": 15.94,
      "grad_norm": 1392.1324462890625,
      "learning_rate": 2.0908152734778123e-05,
      "loss": 51.9803,
      "step": 15928
    },
    {
      "epoch": 15.94,
      "grad_norm": 22601.1171875,
      "learning_rate": 2.090299277605779e-05,
      "loss": 35.2018,
      "step": 15929
    },
    {
      "epoch": 15.95,
      "grad_norm": 16204.4736328125,
      "learning_rate": 2.0897832817337462e-05,
      "loss": 39.9357,
      "step": 15930
    },
    {
      "epoch": 15.95,
      "grad_norm": 11517.31640625,
      "learning_rate": 2.0892672858617133e-05,
      "loss": 59.0537,
      "step": 15931
    },
    {
      "epoch": 15.95,
      "grad_norm": 6387.42724609375,
      "learning_rate": 2.08875128998968e-05,
      "loss": 47.3215,
      "step": 15932
    },
    {
      "epoch": 15.95,
      "grad_norm": 66995.265625,
      "learning_rate": 2.0882352941176472e-05,
      "loss": 64.7251,
      "step": 15933
    },
    {
      "epoch": 15.95,
      "grad_norm": 3135.807861328125,
      "learning_rate": 2.0877192982456143e-05,
      "loss": 56.4123,
      "step": 15934
    },
    {
      "epoch": 15.95,
      "grad_norm": 7874.54833984375,
      "learning_rate": 2.087203302373581e-05,
      "loss": 50.3698,
      "step": 15935
    },
    {
      "epoch": 15.95,
      "grad_norm": 159430.78125,
      "learning_rate": 2.086687306501548e-05,
      "loss": 32.8397,
      "step": 15936
    },
    {
      "epoch": 15.95,
      "grad_norm": 1888.906005859375,
      "learning_rate": 2.086171310629515e-05,
      "loss": 55.0425,
      "step": 15937
    },
    {
      "epoch": 15.95,
      "grad_norm": 18498.12109375,
      "learning_rate": 2.085655314757482e-05,
      "loss": 54.0631,
      "step": 15938
    },
    {
      "epoch": 15.95,
      "grad_norm": 12168.087890625,
      "learning_rate": 2.085139318885449e-05,
      "loss": 56.7647,
      "step": 15939
    },
    {
      "epoch": 15.96,
      "grad_norm": 21048.875,
      "learning_rate": 2.084623323013416e-05,
      "loss": 51.596,
      "step": 15940
    },
    {
      "epoch": 15.96,
      "grad_norm": 26719.796875,
      "learning_rate": 2.084107327141383e-05,
      "loss": 31.4894,
      "step": 15941
    },
    {
      "epoch": 15.96,
      "grad_norm": 4988.82421875,
      "learning_rate": 2.08359133126935e-05,
      "loss": 39.8857,
      "step": 15942
    },
    {
      "epoch": 15.96,
      "grad_norm": 119707.25,
      "learning_rate": 2.083075335397317e-05,
      "loss": 34.2399,
      "step": 15943
    },
    {
      "epoch": 15.96,
      "grad_norm": 7409.92431640625,
      "learning_rate": 2.0825593395252838e-05,
      "loss": 48.9065,
      "step": 15944
    },
    {
      "epoch": 15.96,
      "grad_norm": 6929.57958984375,
      "learning_rate": 2.082043343653251e-05,
      "loss": 57.3532,
      "step": 15945
    },
    {
      "epoch": 15.96,
      "grad_norm": 7789.4453125,
      "learning_rate": 2.0815273477812177e-05,
      "loss": 51.5646,
      "step": 15946
    },
    {
      "epoch": 15.96,
      "grad_norm": 2837.90869140625,
      "learning_rate": 2.0810113519091848e-05,
      "loss": 47.3415,
      "step": 15947
    },
    {
      "epoch": 15.96,
      "grad_norm": 15819.1591796875,
      "learning_rate": 2.080495356037152e-05,
      "loss": 65.3118,
      "step": 15948
    },
    {
      "epoch": 15.96,
      "grad_norm": 5071.4609375,
      "learning_rate": 2.0799793601651187e-05,
      "loss": 60.7167,
      "step": 15949
    },
    {
      "epoch": 15.97,
      "grad_norm": 16122.11328125,
      "learning_rate": 2.0794633642930858e-05,
      "loss": 53.2642,
      "step": 15950
    },
    {
      "epoch": 15.97,
      "grad_norm": 12478.001953125,
      "learning_rate": 2.078947368421053e-05,
      "loss": 51.0447,
      "step": 15951
    },
    {
      "epoch": 15.97,
      "grad_norm": 10606.8330078125,
      "learning_rate": 2.0784313725490197e-05,
      "loss": 52.1604,
      "step": 15952
    },
    {
      "epoch": 15.97,
      "grad_norm": 5327.6787109375,
      "learning_rate": 2.0779153766769864e-05,
      "loss": 56.6635,
      "step": 15953
    },
    {
      "epoch": 15.97,
      "grad_norm": 23480.60546875,
      "learning_rate": 2.0773993808049535e-05,
      "loss": 52.5862,
      "step": 15954
    },
    {
      "epoch": 15.97,
      "grad_norm": 75098.8359375,
      "learning_rate": 2.0768833849329207e-05,
      "loss": 49.3835,
      "step": 15955
    },
    {
      "epoch": 15.97,
      "grad_norm": 17190.056640625,
      "learning_rate": 2.0763673890608874e-05,
      "loss": 57.1747,
      "step": 15956
    },
    {
      "epoch": 15.97,
      "grad_norm": 3531.662109375,
      "learning_rate": 2.0758513931888545e-05,
      "loss": 64.4265,
      "step": 15957
    },
    {
      "epoch": 15.97,
      "grad_norm": 5421.275390625,
      "learning_rate": 2.0753353973168217e-05,
      "loss": 49.6997,
      "step": 15958
    },
    {
      "epoch": 15.97,
      "grad_norm": 6720.677734375,
      "learning_rate": 2.0748194014447884e-05,
      "loss": 43.7601,
      "step": 15959
    },
    {
      "epoch": 15.98,
      "grad_norm": 35471.5078125,
      "learning_rate": 2.0743034055727555e-05,
      "loss": 42.511,
      "step": 15960
    },
    {
      "epoch": 15.98,
      "grad_norm": 2059.955322265625,
      "learning_rate": 2.0737874097007227e-05,
      "loss": 56.5264,
      "step": 15961
    },
    {
      "epoch": 15.98,
      "grad_norm": 9741.0556640625,
      "learning_rate": 2.0732714138286894e-05,
      "loss": 51.9146,
      "step": 15962
    },
    {
      "epoch": 15.98,
      "grad_norm": 5855.2998046875,
      "learning_rate": 2.0727554179566562e-05,
      "loss": 56.1978,
      "step": 15963
    },
    {
      "epoch": 15.98,
      "grad_norm": 12864.5537109375,
      "learning_rate": 2.0722394220846233e-05,
      "loss": 54.7727,
      "step": 15964
    },
    {
      "epoch": 15.98,
      "grad_norm": 19525.943359375,
      "learning_rate": 2.0717234262125904e-05,
      "loss": 61.8215,
      "step": 15965
    },
    {
      "epoch": 15.98,
      "grad_norm": 12111.62890625,
      "learning_rate": 2.0712074303405572e-05,
      "loss": 64.7058,
      "step": 15966
    },
    {
      "epoch": 15.98,
      "grad_norm": 50571.1953125,
      "learning_rate": 2.0706914344685243e-05,
      "loss": 26.1493,
      "step": 15967
    },
    {
      "epoch": 15.98,
      "grad_norm": 87095.890625,
      "learning_rate": 2.0701754385964914e-05,
      "loss": 43.2953,
      "step": 15968
    },
    {
      "epoch": 15.98,
      "grad_norm": 8746.541015625,
      "learning_rate": 2.0696594427244585e-05,
      "loss": 57.3534,
      "step": 15969
    },
    {
      "epoch": 15.99,
      "grad_norm": 7705.64013671875,
      "learning_rate": 2.0691434468524253e-05,
      "loss": 49.357,
      "step": 15970
    },
    {
      "epoch": 15.99,
      "grad_norm": 2354.41845703125,
      "learning_rate": 2.068627450980392e-05,
      "loss": 60.6896,
      "step": 15971
    },
    {
      "epoch": 15.99,
      "grad_norm": 5332.357421875,
      "learning_rate": 2.0681114551083592e-05,
      "loss": 51.8357,
      "step": 15972
    },
    {
      "epoch": 15.99,
      "grad_norm": 17667.208984375,
      "learning_rate": 2.067595459236326e-05,
      "loss": 35.5181,
      "step": 15973
    },
    {
      "epoch": 15.99,
      "grad_norm": 2858.656982421875,
      "learning_rate": 2.067079463364293e-05,
      "loss": 50.9874,
      "step": 15974
    },
    {
      "epoch": 15.99,
      "grad_norm": 19761.990234375,
      "learning_rate": 2.0665634674922602e-05,
      "loss": 56.8256,
      "step": 15975
    },
    {
      "epoch": 15.99,
      "grad_norm": 20501.962890625,
      "learning_rate": 2.0660474716202273e-05,
      "loss": 56.0925,
      "step": 15976
    },
    {
      "epoch": 15.99,
      "grad_norm": 10572.8427734375,
      "learning_rate": 2.065531475748194e-05,
      "loss": 64.9065,
      "step": 15977
    },
    {
      "epoch": 15.99,
      "grad_norm": 5227.78173828125,
      "learning_rate": 2.0650154798761612e-05,
      "loss": 54.7915,
      "step": 15978
    },
    {
      "epoch": 15.99,
      "grad_norm": 2460.30615234375,
      "learning_rate": 2.0644994840041283e-05,
      "loss": 53.3421,
      "step": 15979
    },
    {
      "epoch": 16.0,
      "grad_norm": 11056.8369140625,
      "learning_rate": 2.0639834881320948e-05,
      "loss": 57.4673,
      "step": 15980
    },
    {
      "epoch": 16.0,
      "grad_norm": 17870.8984375,
      "learning_rate": 2.063467492260062e-05,
      "loss": 59.4491,
      "step": 15981
    },
    {
      "epoch": 16.0,
      "grad_norm": 2926.809814453125,
      "learning_rate": 2.062951496388029e-05,
      "loss": 48.4506,
      "step": 15982
    },
    {
      "epoch": 16.0,
      "grad_norm": 1650.9720458984375,
      "learning_rate": 2.062435500515996e-05,
      "loss": 57.8949,
      "step": 15983
    },
    {
      "epoch": 16.0,
      "grad_norm": 55399.6875,
      "learning_rate": 2.061919504643963e-05,
      "loss": 58.3043,
      "step": 15984
    },
    {
      "epoch": 16.0,
      "grad_norm": 8161.29296875,
      "learning_rate": 2.06140350877193e-05,
      "loss": 54.922,
      "step": 15985
    },
    {
      "epoch": 16.0,
      "grad_norm": 13081.2626953125,
      "learning_rate": 2.060887512899897e-05,
      "loss": 54.6441,
      "step": 15986
    },
    {
      "epoch": 16.0,
      "grad_norm": 4763.53369140625,
      "learning_rate": 2.060371517027864e-05,
      "loss": 53.6931,
      "step": 15987
    },
    {
      "epoch": 16.0,
      "grad_norm": 5692.8349609375,
      "learning_rate": 2.059855521155831e-05,
      "loss": 48.2786,
      "step": 15988
    },
    {
      "epoch": 16.01,
      "grad_norm": 6490.58837890625,
      "learning_rate": 2.0593395252837978e-05,
      "loss": 64.9867,
      "step": 15989
    },
    {
      "epoch": 16.01,
      "grad_norm": 16149.7236328125,
      "learning_rate": 2.058823529411765e-05,
      "loss": 41.9211,
      "step": 15990
    },
    {
      "epoch": 16.01,
      "grad_norm": 50871.3203125,
      "learning_rate": 2.0583075335397316e-05,
      "loss": 44.7555,
      "step": 15991
    },
    {
      "epoch": 16.01,
      "grad_norm": 14226.22265625,
      "learning_rate": 2.0577915376676988e-05,
      "loss": 44.8823,
      "step": 15992
    },
    {
      "epoch": 16.01,
      "grad_norm": 12386.5009765625,
      "learning_rate": 2.057275541795666e-05,
      "loss": 53.244,
      "step": 15993
    },
    {
      "epoch": 16.01,
      "grad_norm": 11946.4072265625,
      "learning_rate": 2.0567595459236326e-05,
      "loss": 58.0708,
      "step": 15994
    },
    {
      "epoch": 16.01,
      "grad_norm": 8754.5517578125,
      "learning_rate": 2.0562435500515998e-05,
      "loss": 43.6038,
      "step": 15995
    },
    {
      "epoch": 16.01,
      "grad_norm": 6694.197265625,
      "learning_rate": 2.055727554179567e-05,
      "loss": 56.8955,
      "step": 15996
    },
    {
      "epoch": 16.01,
      "grad_norm": 11157.66015625,
      "learning_rate": 2.0552115583075336e-05,
      "loss": 51.5298,
      "step": 15997
    },
    {
      "epoch": 16.01,
      "grad_norm": 11982.78125,
      "learning_rate": 2.0546955624355004e-05,
      "loss": 61.2559,
      "step": 15998
    },
    {
      "epoch": 16.02,
      "grad_norm": 8201.869140625,
      "learning_rate": 2.0541795665634675e-05,
      "loss": 53.1479,
      "step": 15999
    },
    {
      "epoch": 16.02,
      "grad_norm": 4441.4453125,
      "learning_rate": 2.0536635706914346e-05,
      "loss": 55.6185,
      "step": 16000
    },
    {
      "epoch": 16.02,
      "grad_norm": 21406.818359375,
      "learning_rate": 2.0531475748194014e-05,
      "loss": 48.6692,
      "step": 16001
    },
    {
      "epoch": 16.02,
      "grad_norm": 10019.11328125,
      "learning_rate": 2.0526315789473685e-05,
      "loss": 52.849,
      "step": 16002
    },
    {
      "epoch": 16.02,
      "grad_norm": 14101.5048828125,
      "learning_rate": 2.0521155830753356e-05,
      "loss": 49.6125,
      "step": 16003
    },
    {
      "epoch": 16.02,
      "grad_norm": 3892.744384765625,
      "learning_rate": 2.0515995872033024e-05,
      "loss": 55.9024,
      "step": 16004
    },
    {
      "epoch": 16.02,
      "grad_norm": 3896.1943359375,
      "learning_rate": 2.0510835913312695e-05,
      "loss": 53.1477,
      "step": 16005
    },
    {
      "epoch": 16.02,
      "grad_norm": 37950.29296875,
      "learning_rate": 2.0505675954592366e-05,
      "loss": 22.799,
      "step": 16006
    },
    {
      "epoch": 16.02,
      "grad_norm": 71790.7421875,
      "learning_rate": 2.0500515995872034e-05,
      "loss": 45.0561,
      "step": 16007
    },
    {
      "epoch": 16.02,
      "grad_norm": 8495.5791015625,
      "learning_rate": 2.0495356037151702e-05,
      "loss": 45.5375,
      "step": 16008
    },
    {
      "epoch": 16.03,
      "grad_norm": 4083.69775390625,
      "learning_rate": 2.0490196078431373e-05,
      "loss": 58.6438,
      "step": 16009
    },
    {
      "epoch": 16.03,
      "grad_norm": 3134.29296875,
      "learning_rate": 2.0485036119711044e-05,
      "loss": 52.6573,
      "step": 16010
    },
    {
      "epoch": 16.03,
      "grad_norm": 20608.408203125,
      "learning_rate": 2.0479876160990712e-05,
      "loss": 50.229,
      "step": 16011
    },
    {
      "epoch": 16.03,
      "grad_norm": 50285.234375,
      "learning_rate": 2.0474716202270383e-05,
      "loss": 46.3487,
      "step": 16012
    },
    {
      "epoch": 16.03,
      "grad_norm": 23216.078125,
      "learning_rate": 2.0469556243550054e-05,
      "loss": 51.7212,
      "step": 16013
    },
    {
      "epoch": 16.03,
      "grad_norm": 6982.16064453125,
      "learning_rate": 2.0464396284829722e-05,
      "loss": 60.4516,
      "step": 16014
    },
    {
      "epoch": 16.03,
      "grad_norm": 10681.6474609375,
      "learning_rate": 2.0459236326109393e-05,
      "loss": 43.2913,
      "step": 16015
    },
    {
      "epoch": 16.03,
      "grad_norm": 14473.919921875,
      "learning_rate": 2.045407636738906e-05,
      "loss": 40.5688,
      "step": 16016
    },
    {
      "epoch": 16.03,
      "grad_norm": 27278.84375,
      "learning_rate": 2.0448916408668732e-05,
      "loss": 46.5555,
      "step": 16017
    },
    {
      "epoch": 16.03,
      "grad_norm": 27658.33984375,
      "learning_rate": 2.04437564499484e-05,
      "loss": 60.2985,
      "step": 16018
    },
    {
      "epoch": 16.04,
      "grad_norm": 3524.861328125,
      "learning_rate": 2.043859649122807e-05,
      "loss": 62.9787,
      "step": 16019
    },
    {
      "epoch": 16.04,
      "grad_norm": 12206.0283203125,
      "learning_rate": 2.0433436532507742e-05,
      "loss": 46.1652,
      "step": 16020
    },
    {
      "epoch": 16.04,
      "grad_norm": 2323.610107421875,
      "learning_rate": 2.042827657378741e-05,
      "loss": 63.7934,
      "step": 16021
    },
    {
      "epoch": 16.04,
      "grad_norm": 108804.609375,
      "learning_rate": 2.042311661506708e-05,
      "loss": 57.2611,
      "step": 16022
    },
    {
      "epoch": 16.04,
      "grad_norm": 12410.189453125,
      "learning_rate": 2.0417956656346752e-05,
      "loss": 47.2696,
      "step": 16023
    },
    {
      "epoch": 16.04,
      "grad_norm": 4161.65869140625,
      "learning_rate": 2.041279669762642e-05,
      "loss": 46.6405,
      "step": 16024
    },
    {
      "epoch": 16.04,
      "grad_norm": 2558.4033203125,
      "learning_rate": 2.0407636738906087e-05,
      "loss": 58.8742,
      "step": 16025
    },
    {
      "epoch": 16.04,
      "grad_norm": 8246.4013671875,
      "learning_rate": 2.040247678018576e-05,
      "loss": 43.6536,
      "step": 16026
    },
    {
      "epoch": 16.04,
      "grad_norm": 10862.994140625,
      "learning_rate": 2.039731682146543e-05,
      "loss": 61.5299,
      "step": 16027
    },
    {
      "epoch": 16.04,
      "grad_norm": 4145.97412109375,
      "learning_rate": 2.0392156862745097e-05,
      "loss": 56.2083,
      "step": 16028
    },
    {
      "epoch": 16.05,
      "grad_norm": 27474.63671875,
      "learning_rate": 2.038699690402477e-05,
      "loss": 60.5346,
      "step": 16029
    },
    {
      "epoch": 16.05,
      "grad_norm": 5974.23291015625,
      "learning_rate": 2.038183694530444e-05,
      "loss": 59.0789,
      "step": 16030
    },
    {
      "epoch": 16.05,
      "grad_norm": 27857.6796875,
      "learning_rate": 2.0376676986584107e-05,
      "loss": 57.0996,
      "step": 16031
    },
    {
      "epoch": 16.05,
      "grad_norm": 935.917236328125,
      "learning_rate": 2.037151702786378e-05,
      "loss": 57.5176,
      "step": 16032
    },
    {
      "epoch": 16.05,
      "grad_norm": 21339.875,
      "learning_rate": 2.036635706914345e-05,
      "loss": 36.613,
      "step": 16033
    },
    {
      "epoch": 16.05,
      "grad_norm": 1766.7296142578125,
      "learning_rate": 2.0361197110423117e-05,
      "loss": 63.5327,
      "step": 16034
    },
    {
      "epoch": 16.05,
      "grad_norm": 7551.74462890625,
      "learning_rate": 2.0356037151702785e-05,
      "loss": 54.4557,
      "step": 16035
    },
    {
      "epoch": 16.05,
      "grad_norm": 7790.82080078125,
      "learning_rate": 2.0350877192982456e-05,
      "loss": 57.7744,
      "step": 16036
    },
    {
      "epoch": 16.05,
      "grad_norm": 3209.1005859375,
      "learning_rate": 2.0345717234262127e-05,
      "loss": 59.0128,
      "step": 16037
    },
    {
      "epoch": 16.05,
      "grad_norm": 8361.728515625,
      "learning_rate": 2.0340557275541795e-05,
      "loss": 51.58,
      "step": 16038
    },
    {
      "epoch": 16.06,
      "grad_norm": 13408.2626953125,
      "learning_rate": 2.0335397316821466e-05,
      "loss": 51.1955,
      "step": 16039
    },
    {
      "epoch": 16.06,
      "grad_norm": 24510.083984375,
      "learning_rate": 2.0330237358101137e-05,
      "loss": 43.7732,
      "step": 16040
    },
    {
      "epoch": 16.06,
      "grad_norm": 3149.7841796875,
      "learning_rate": 2.032507739938081e-05,
      "loss": 45.2395,
      "step": 16041
    },
    {
      "epoch": 16.06,
      "grad_norm": 5995.021484375,
      "learning_rate": 2.0319917440660476e-05,
      "loss": 56.6407,
      "step": 16042
    },
    {
      "epoch": 16.06,
      "grad_norm": 13341.556640625,
      "learning_rate": 2.0314757481940144e-05,
      "loss": 39.8059,
      "step": 16043
    },
    {
      "epoch": 16.06,
      "grad_norm": 4276.62060546875,
      "learning_rate": 2.0309597523219815e-05,
      "loss": 58.0755,
      "step": 16044
    },
    {
      "epoch": 16.06,
      "grad_norm": 10037.90625,
      "learning_rate": 2.0304437564499483e-05,
      "loss": 58.5518,
      "step": 16045
    },
    {
      "epoch": 16.06,
      "grad_norm": 7009.15966796875,
      "learning_rate": 2.0299277605779154e-05,
      "loss": 56.3188,
      "step": 16046
    },
    {
      "epoch": 16.06,
      "grad_norm": 11379.76171875,
      "learning_rate": 2.0294117647058825e-05,
      "loss": 53.0035,
      "step": 16047
    },
    {
      "epoch": 16.06,
      "grad_norm": 21654.75,
      "learning_rate": 2.0288957688338496e-05,
      "loss": 41.4632,
      "step": 16048
    },
    {
      "epoch": 16.07,
      "grad_norm": 4251.37548828125,
      "learning_rate": 2.0283797729618164e-05,
      "loss": 59.0416,
      "step": 16049
    },
    {
      "epoch": 16.07,
      "grad_norm": 10490.1943359375,
      "learning_rate": 2.0278637770897835e-05,
      "loss": 60.3573,
      "step": 16050
    },
    {
      "epoch": 16.07,
      "grad_norm": 172619.453125,
      "learning_rate": 2.0273477812177506e-05,
      "loss": 53.9997,
      "step": 16051
    },
    {
      "epoch": 16.07,
      "grad_norm": 2940.87646484375,
      "learning_rate": 2.026831785345717e-05,
      "loss": 58.1306,
      "step": 16052
    },
    {
      "epoch": 16.07,
      "grad_norm": 10200.1484375,
      "learning_rate": 2.0263157894736842e-05,
      "loss": 57.9334,
      "step": 16053
    },
    {
      "epoch": 16.07,
      "grad_norm": 9369.2685546875,
      "learning_rate": 2.0257997936016513e-05,
      "loss": 64.3068,
      "step": 16054
    },
    {
      "epoch": 16.07,
      "grad_norm": 5598.43310546875,
      "learning_rate": 2.0252837977296184e-05,
      "loss": 53.6707,
      "step": 16055
    },
    {
      "epoch": 16.07,
      "grad_norm": 11063.9228515625,
      "learning_rate": 2.0247678018575852e-05,
      "loss": 51.8529,
      "step": 16056
    },
    {
      "epoch": 16.07,
      "grad_norm": 1158.3675537109375,
      "learning_rate": 2.0242518059855523e-05,
      "loss": 52.6347,
      "step": 16057
    },
    {
      "epoch": 16.07,
      "grad_norm": 2320.9541015625,
      "learning_rate": 2.0237358101135194e-05,
      "loss": 63.01,
      "step": 16058
    },
    {
      "epoch": 16.08,
      "grad_norm": 2619.4482421875,
      "learning_rate": 2.0232198142414862e-05,
      "loss": 45.626,
      "step": 16059
    },
    {
      "epoch": 16.08,
      "grad_norm": 26292.716796875,
      "learning_rate": 2.0227038183694533e-05,
      "loss": 57.1471,
      "step": 16060
    },
    {
      "epoch": 16.08,
      "grad_norm": 3830.157470703125,
      "learning_rate": 2.02218782249742e-05,
      "loss": 58.4239,
      "step": 16061
    },
    {
      "epoch": 16.08,
      "grad_norm": 34766.16796875,
      "learning_rate": 2.0216718266253872e-05,
      "loss": 58.2417,
      "step": 16062
    },
    {
      "epoch": 16.08,
      "grad_norm": 80936.7734375,
      "learning_rate": 2.021155830753354e-05,
      "loss": 49.9022,
      "step": 16063
    },
    {
      "epoch": 16.08,
      "grad_norm": 11699.9033203125,
      "learning_rate": 2.020639834881321e-05,
      "loss": 54.0749,
      "step": 16064
    },
    {
      "epoch": 16.08,
      "grad_norm": 9450.9072265625,
      "learning_rate": 2.0201238390092882e-05,
      "loss": 57.3272,
      "step": 16065
    },
    {
      "epoch": 16.08,
      "grad_norm": 13167.51953125,
      "learning_rate": 2.019607843137255e-05,
      "loss": 39.6939,
      "step": 16066
    },
    {
      "epoch": 16.08,
      "grad_norm": 2308.9541015625,
      "learning_rate": 2.019091847265222e-05,
      "loss": 61.3426,
      "step": 16067
    },
    {
      "epoch": 16.08,
      "grad_norm": 21823.080078125,
      "learning_rate": 2.0185758513931892e-05,
      "loss": 56.6136,
      "step": 16068
    },
    {
      "epoch": 16.09,
      "grad_norm": 26754.05078125,
      "learning_rate": 2.018059855521156e-05,
      "loss": 29.3805,
      "step": 16069
    },
    {
      "epoch": 16.09,
      "grad_norm": 4605.9208984375,
      "learning_rate": 2.0175438596491227e-05,
      "loss": 54.6653,
      "step": 16070
    },
    {
      "epoch": 16.09,
      "grad_norm": 62791.7265625,
      "learning_rate": 2.01702786377709e-05,
      "loss": 39.1741,
      "step": 16071
    },
    {
      "epoch": 16.09,
      "grad_norm": 10645.064453125,
      "learning_rate": 2.016511867905057e-05,
      "loss": 57.9659,
      "step": 16072
    },
    {
      "epoch": 16.09,
      "grad_norm": 4426.3427734375,
      "learning_rate": 2.0159958720330237e-05,
      "loss": 58.8576,
      "step": 16073
    },
    {
      "epoch": 16.09,
      "grad_norm": 13436.794921875,
      "learning_rate": 2.015479876160991e-05,
      "loss": 58.9584,
      "step": 16074
    },
    {
      "epoch": 16.09,
      "grad_norm": 9719.6689453125,
      "learning_rate": 2.014963880288958e-05,
      "loss": 57.2897,
      "step": 16075
    },
    {
      "epoch": 16.09,
      "grad_norm": 1960.6669921875,
      "learning_rate": 2.0144478844169247e-05,
      "loss": 48.2149,
      "step": 16076
    },
    {
      "epoch": 16.09,
      "grad_norm": 27275.236328125,
      "learning_rate": 2.013931888544892e-05,
      "loss": 71.5603,
      "step": 16077
    },
    {
      "epoch": 16.09,
      "grad_norm": 26822.205078125,
      "learning_rate": 2.013415892672859e-05,
      "loss": 45.2783,
      "step": 16078
    },
    {
      "epoch": 16.1,
      "grad_norm": 15796.62109375,
      "learning_rate": 2.0128998968008257e-05,
      "loss": 56.9301,
      "step": 16079
    },
    {
      "epoch": 16.1,
      "grad_norm": 1988.0391845703125,
      "learning_rate": 2.0123839009287925e-05,
      "loss": 54.0968,
      "step": 16080
    },
    {
      "epoch": 16.1,
      "grad_norm": 14866.0107421875,
      "learning_rate": 2.0118679050567596e-05,
      "loss": 54.9595,
      "step": 16081
    },
    {
      "epoch": 16.1,
      "grad_norm": 6098.93017578125,
      "learning_rate": 2.0113519091847267e-05,
      "loss": 56.6108,
      "step": 16082
    },
    {
      "epoch": 16.1,
      "grad_norm": 40782.9921875,
      "learning_rate": 2.0108359133126935e-05,
      "loss": 27.2114,
      "step": 16083
    },
    {
      "epoch": 16.1,
      "grad_norm": 31605.76953125,
      "learning_rate": 2.0103199174406606e-05,
      "loss": 30.0839,
      "step": 16084
    },
    {
      "epoch": 16.1,
      "grad_norm": 24184.501953125,
      "learning_rate": 2.0098039215686277e-05,
      "loss": 57.5764,
      "step": 16085
    },
    {
      "epoch": 16.1,
      "grad_norm": 4868.9990234375,
      "learning_rate": 2.0092879256965945e-05,
      "loss": 56.417,
      "step": 16086
    },
    {
      "epoch": 16.1,
      "grad_norm": 5203.435546875,
      "learning_rate": 2.0087719298245616e-05,
      "loss": 35.1416,
      "step": 16087
    },
    {
      "epoch": 16.1,
      "grad_norm": 5605.2822265625,
      "learning_rate": 2.0082559339525284e-05,
      "loss": 62.6278,
      "step": 16088
    },
    {
      "epoch": 16.11,
      "grad_norm": 16329.0634765625,
      "learning_rate": 2.0077399380804955e-05,
      "loss": 44.016,
      "step": 16089
    },
    {
      "epoch": 16.11,
      "grad_norm": 1788.9501953125,
      "learning_rate": 2.0072239422084623e-05,
      "loss": 66.3092,
      "step": 16090
    },
    {
      "epoch": 16.11,
      "grad_norm": 26549.80859375,
      "learning_rate": 2.0067079463364294e-05,
      "loss": 46.6674,
      "step": 16091
    },
    {
      "epoch": 16.11,
      "grad_norm": 2178.63916015625,
      "learning_rate": 2.0061919504643965e-05,
      "loss": 60.0995,
      "step": 16092
    },
    {
      "epoch": 16.11,
      "grad_norm": 3678.971923828125,
      "learning_rate": 2.0056759545923633e-05,
      "loss": 47.6239,
      "step": 16093
    },
    {
      "epoch": 16.11,
      "grad_norm": 7557.1513671875,
      "learning_rate": 2.0051599587203304e-05,
      "loss": 37.2475,
      "step": 16094
    },
    {
      "epoch": 16.11,
      "grad_norm": 3877.3251953125,
      "learning_rate": 2.0046439628482975e-05,
      "loss": 54.8678,
      "step": 16095
    },
    {
      "epoch": 16.11,
      "grad_norm": 13657.482421875,
      "learning_rate": 2.0041279669762643e-05,
      "loss": 56.1924,
      "step": 16096
    },
    {
      "epoch": 16.11,
      "grad_norm": 10692.4619140625,
      "learning_rate": 2.003611971104231e-05,
      "loss": 57.4096,
      "step": 16097
    },
    {
      "epoch": 16.11,
      "grad_norm": 30650.81640625,
      "learning_rate": 2.003095975232198e-05,
      "loss": 43.4905,
      "step": 16098
    },
    {
      "epoch": 16.12,
      "grad_norm": 9192.2490234375,
      "learning_rate": 2.0025799793601653e-05,
      "loss": 45.0161,
      "step": 16099
    },
    {
      "epoch": 16.12,
      "grad_norm": 10345.287109375,
      "learning_rate": 2.002063983488132e-05,
      "loss": 32.7244,
      "step": 16100
    },
    {
      "epoch": 16.12,
      "grad_norm": 25672.748046875,
      "learning_rate": 2.001547987616099e-05,
      "loss": 58.8307,
      "step": 16101
    },
    {
      "epoch": 16.12,
      "grad_norm": 5922.0693359375,
      "learning_rate": 2.0010319917440663e-05,
      "loss": 50.8776,
      "step": 16102
    },
    {
      "epoch": 16.12,
      "grad_norm": 5848.26904296875,
      "learning_rate": 2.000515995872033e-05,
      "loss": 62.1478,
      "step": 16103
    },
    {
      "epoch": 16.12,
      "grad_norm": 5511.19189453125,
      "learning_rate": 2e-05,
      "loss": 50.4116,
      "step": 16104
    },
    {
      "epoch": 16.12,
      "grad_norm": 14865.0625,
      "learning_rate": 1.999484004127967e-05,
      "loss": 46.7313,
      "step": 16105
    },
    {
      "epoch": 16.12,
      "grad_norm": 37313.40234375,
      "learning_rate": 1.998968008255934e-05,
      "loss": 55.8227,
      "step": 16106
    },
    {
      "epoch": 16.12,
      "grad_norm": 3404.48388671875,
      "learning_rate": 1.9984520123839008e-05,
      "loss": 54.9231,
      "step": 16107
    },
    {
      "epoch": 16.12,
      "grad_norm": 14370.939453125,
      "learning_rate": 1.997936016511868e-05,
      "loss": 45.9055,
      "step": 16108
    },
    {
      "epoch": 16.13,
      "grad_norm": 29809.76171875,
      "learning_rate": 1.997420020639835e-05,
      "loss": 58.324,
      "step": 16109
    },
    {
      "epoch": 16.13,
      "grad_norm": 92414.5390625,
      "learning_rate": 1.9969040247678018e-05,
      "loss": 53.3906,
      "step": 16110
    },
    {
      "epoch": 16.13,
      "grad_norm": 17630.8515625,
      "learning_rate": 1.996388028895769e-05,
      "loss": 64.9325,
      "step": 16111
    },
    {
      "epoch": 16.13,
      "grad_norm": 14435.15625,
      "learning_rate": 1.995872033023736e-05,
      "loss": 60.1766,
      "step": 16112
    },
    {
      "epoch": 16.13,
      "grad_norm": 99436.6875,
      "learning_rate": 1.9953560371517028e-05,
      "loss": 38.7589,
      "step": 16113
    },
    {
      "epoch": 16.13,
      "grad_norm": 11611.705078125,
      "learning_rate": 1.9948400412796696e-05,
      "loss": 56.2522,
      "step": 16114
    },
    {
      "epoch": 16.13,
      "grad_norm": 6090.79541015625,
      "learning_rate": 1.9943240454076367e-05,
      "loss": 52.8463,
      "step": 16115
    },
    {
      "epoch": 16.13,
      "grad_norm": 4865.26416015625,
      "learning_rate": 1.9938080495356038e-05,
      "loss": 60.3337,
      "step": 16116
    },
    {
      "epoch": 16.13,
      "grad_norm": 4902.640625,
      "learning_rate": 1.9932920536635706e-05,
      "loss": 52.4633,
      "step": 16117
    },
    {
      "epoch": 16.13,
      "grad_norm": 10613.953125,
      "learning_rate": 1.9927760577915377e-05,
      "loss": 48.4928,
      "step": 16118
    },
    {
      "epoch": 16.14,
      "grad_norm": 7468.17919921875,
      "learning_rate": 1.9922600619195048e-05,
      "loss": 58.8713,
      "step": 16119
    },
    {
      "epoch": 16.14,
      "grad_norm": 27301.83203125,
      "learning_rate": 1.9917440660474716e-05,
      "loss": 50.3642,
      "step": 16120
    },
    {
      "epoch": 16.14,
      "grad_norm": 1053.9388427734375,
      "learning_rate": 1.9912280701754387e-05,
      "loss": 54.4071,
      "step": 16121
    },
    {
      "epoch": 16.14,
      "grad_norm": 3109.98193359375,
      "learning_rate": 1.9907120743034058e-05,
      "loss": 56.5751,
      "step": 16122
    },
    {
      "epoch": 16.14,
      "grad_norm": 4152.38134765625,
      "learning_rate": 1.9901960784313726e-05,
      "loss": 59.8459,
      "step": 16123
    },
    {
      "epoch": 16.14,
      "grad_norm": 37508.765625,
      "learning_rate": 1.9896800825593394e-05,
      "loss": 36.1073,
      "step": 16124
    },
    {
      "epoch": 16.14,
      "grad_norm": 4585.8828125,
      "learning_rate": 1.9891640866873065e-05,
      "loss": 53.789,
      "step": 16125
    },
    {
      "epoch": 16.14,
      "grad_norm": 8671.505859375,
      "learning_rate": 1.9886480908152736e-05,
      "loss": 63.734,
      "step": 16126
    },
    {
      "epoch": 16.14,
      "grad_norm": 7146.58349609375,
      "learning_rate": 1.9881320949432404e-05,
      "loss": 53.0382,
      "step": 16127
    },
    {
      "epoch": 16.14,
      "grad_norm": 8450.80078125,
      "learning_rate": 1.9876160990712075e-05,
      "loss": 40.7441,
      "step": 16128
    },
    {
      "epoch": 16.15,
      "grad_norm": 2683.617919921875,
      "learning_rate": 1.9871001031991746e-05,
      "loss": 62.2879,
      "step": 16129
    },
    {
      "epoch": 16.15,
      "grad_norm": 21567.63671875,
      "learning_rate": 1.9865841073271417e-05,
      "loss": 52.6884,
      "step": 16130
    },
    {
      "epoch": 16.15,
      "grad_norm": 3517.600830078125,
      "learning_rate": 1.9860681114551085e-05,
      "loss": 50.682,
      "step": 16131
    },
    {
      "epoch": 16.15,
      "grad_norm": 15329.4365234375,
      "learning_rate": 1.9855521155830753e-05,
      "loss": 46.8444,
      "step": 16132
    },
    {
      "epoch": 16.15,
      "grad_norm": 22497.45703125,
      "learning_rate": 1.9850361197110424e-05,
      "loss": 55.7223,
      "step": 16133
    },
    {
      "epoch": 16.15,
      "grad_norm": 29108.994140625,
      "learning_rate": 1.984520123839009e-05,
      "loss": 49.8553,
      "step": 16134
    },
    {
      "epoch": 16.15,
      "grad_norm": 20207.75390625,
      "learning_rate": 1.9840041279669763e-05,
      "loss": 53.4462,
      "step": 16135
    },
    {
      "epoch": 16.15,
      "grad_norm": 3212.585205078125,
      "learning_rate": 1.9834881320949434e-05,
      "loss": 55.3072,
      "step": 16136
    },
    {
      "epoch": 16.15,
      "grad_norm": 11193.970703125,
      "learning_rate": 1.9829721362229105e-05,
      "loss": 56.4873,
      "step": 16137
    },
    {
      "epoch": 16.15,
      "grad_norm": 13867.8515625,
      "learning_rate": 1.9824561403508773e-05,
      "loss": 51.99,
      "step": 16138
    },
    {
      "epoch": 16.16,
      "grad_norm": 4325.27734375,
      "learning_rate": 1.9819401444788444e-05,
      "loss": 47.0311,
      "step": 16139
    },
    {
      "epoch": 16.16,
      "grad_norm": 7007.4306640625,
      "learning_rate": 1.9814241486068115e-05,
      "loss": 46.5471,
      "step": 16140
    },
    {
      "epoch": 16.16,
      "grad_norm": 15786.755859375,
      "learning_rate": 1.980908152734778e-05,
      "loss": 58.1269,
      "step": 16141
    },
    {
      "epoch": 16.16,
      "grad_norm": 1839.6416015625,
      "learning_rate": 1.980392156862745e-05,
      "loss": 52.8663,
      "step": 16142
    },
    {
      "epoch": 16.16,
      "grad_norm": 104950.421875,
      "learning_rate": 1.979876160990712e-05,
      "loss": 59.3277,
      "step": 16143
    },
    {
      "epoch": 16.16,
      "grad_norm": 9392.7802734375,
      "learning_rate": 1.9793601651186793e-05,
      "loss": 35.9684,
      "step": 16144
    },
    {
      "epoch": 16.16,
      "grad_norm": 252099.046875,
      "learning_rate": 1.978844169246646e-05,
      "loss": 39.7643,
      "step": 16145
    },
    {
      "epoch": 16.16,
      "grad_norm": 5367.03662109375,
      "learning_rate": 1.978328173374613e-05,
      "loss": 57.3404,
      "step": 16146
    },
    {
      "epoch": 16.16,
      "grad_norm": 4479.89501953125,
      "learning_rate": 1.9778121775025803e-05,
      "loss": 54.5202,
      "step": 16147
    },
    {
      "epoch": 16.16,
      "grad_norm": 21775.216796875,
      "learning_rate": 1.977296181630547e-05,
      "loss": 46.5084,
      "step": 16148
    },
    {
      "epoch": 16.17,
      "grad_norm": 5092.57861328125,
      "learning_rate": 1.976780185758514e-05,
      "loss": 57.349,
      "step": 16149
    },
    {
      "epoch": 16.17,
      "grad_norm": 24763.515625,
      "learning_rate": 1.976264189886481e-05,
      "loss": 51.9539,
      "step": 16150
    },
    {
      "epoch": 16.17,
      "grad_norm": 3521.21630859375,
      "learning_rate": 1.975748194014448e-05,
      "loss": 67.0818,
      "step": 16151
    },
    {
      "epoch": 16.17,
      "grad_norm": 23357.765625,
      "learning_rate": 1.9752321981424148e-05,
      "loss": 53.3325,
      "step": 16152
    },
    {
      "epoch": 16.17,
      "grad_norm": 49564.0703125,
      "learning_rate": 1.974716202270382e-05,
      "loss": 43.6263,
      "step": 16153
    },
    {
      "epoch": 16.17,
      "grad_norm": 12917.751953125,
      "learning_rate": 1.974200206398349e-05,
      "loss": 44.165,
      "step": 16154
    },
    {
      "epoch": 16.17,
      "grad_norm": 11540.0732421875,
      "learning_rate": 1.9736842105263158e-05,
      "loss": 52.0057,
      "step": 16155
    },
    {
      "epoch": 16.17,
      "grad_norm": 10836.703125,
      "learning_rate": 1.973168214654283e-05,
      "loss": 60.7103,
      "step": 16156
    },
    {
      "epoch": 16.17,
      "grad_norm": 9562.8349609375,
      "learning_rate": 1.97265221878225e-05,
      "loss": 57.0341,
      "step": 16157
    },
    {
      "epoch": 16.17,
      "grad_norm": 4044.455322265625,
      "learning_rate": 1.9721362229102168e-05,
      "loss": 45.3387,
      "step": 16158
    },
    {
      "epoch": 16.18,
      "grad_norm": 12926.2939453125,
      "learning_rate": 1.9716202270381836e-05,
      "loss": 61.1916,
      "step": 16159
    },
    {
      "epoch": 16.18,
      "grad_norm": 34435.47265625,
      "learning_rate": 1.9711042311661507e-05,
      "loss": 55.6233,
      "step": 16160
    },
    {
      "epoch": 16.18,
      "grad_norm": 13026.654296875,
      "learning_rate": 1.9705882352941178e-05,
      "loss": 61.6444,
      "step": 16161
    },
    {
      "epoch": 16.18,
      "grad_norm": 26721.66015625,
      "learning_rate": 1.9700722394220846e-05,
      "loss": 65.8669,
      "step": 16162
    },
    {
      "epoch": 16.18,
      "grad_norm": 7652.95703125,
      "learning_rate": 1.9695562435500517e-05,
      "loss": 43.6061,
      "step": 16163
    },
    {
      "epoch": 16.18,
      "grad_norm": 6527.86279296875,
      "learning_rate": 1.9690402476780188e-05,
      "loss": 44.1222,
      "step": 16164
    },
    {
      "epoch": 16.18,
      "grad_norm": 4438.43896484375,
      "learning_rate": 1.9685242518059856e-05,
      "loss": 58.7754,
      "step": 16165
    },
    {
      "epoch": 16.18,
      "grad_norm": 34080.2109375,
      "learning_rate": 1.9680082559339527e-05,
      "loss": 59.4002,
      "step": 16166
    },
    {
      "epoch": 16.18,
      "grad_norm": 453463.1875,
      "learning_rate": 1.9674922600619198e-05,
      "loss": 58.9539,
      "step": 16167
    },
    {
      "epoch": 16.18,
      "grad_norm": 26634.359375,
      "learning_rate": 1.9669762641898866e-05,
      "loss": 52.7933,
      "step": 16168
    },
    {
      "epoch": 16.19,
      "grad_norm": 8781.701171875,
      "learning_rate": 1.9664602683178534e-05,
      "loss": 63.6866,
      "step": 16169
    },
    {
      "epoch": 16.19,
      "grad_norm": 19525.6953125,
      "learning_rate": 1.9659442724458205e-05,
      "loss": 53.3836,
      "step": 16170
    },
    {
      "epoch": 16.19,
      "grad_norm": 12364.2529296875,
      "learning_rate": 1.9654282765737876e-05,
      "loss": 57.358,
      "step": 16171
    },
    {
      "epoch": 16.19,
      "grad_norm": 30589.734375,
      "learning_rate": 1.9649122807017544e-05,
      "loss": 49.7538,
      "step": 16172
    },
    {
      "epoch": 16.19,
      "grad_norm": 3396.844970703125,
      "learning_rate": 1.9643962848297215e-05,
      "loss": 52.06,
      "step": 16173
    },
    {
      "epoch": 16.19,
      "grad_norm": 13866.3623046875,
      "learning_rate": 1.9638802889576886e-05,
      "loss": 52.8141,
      "step": 16174
    },
    {
      "epoch": 16.19,
      "grad_norm": 23411.787109375,
      "learning_rate": 1.9633642930856554e-05,
      "loss": 64.1129,
      "step": 16175
    },
    {
      "epoch": 16.19,
      "grad_norm": 4030.523681640625,
      "learning_rate": 1.9628482972136225e-05,
      "loss": 57.9072,
      "step": 16176
    },
    {
      "epoch": 16.19,
      "grad_norm": 3018.75537109375,
      "learning_rate": 1.9623323013415892e-05,
      "loss": 55.8378,
      "step": 16177
    },
    {
      "epoch": 16.19,
      "grad_norm": 1240.9256591796875,
      "learning_rate": 1.9618163054695564e-05,
      "loss": 58.7523,
      "step": 16178
    },
    {
      "epoch": 16.2,
      "grad_norm": 23595.06640625,
      "learning_rate": 1.961300309597523e-05,
      "loss": 51.4067,
      "step": 16179
    },
    {
      "epoch": 16.2,
      "grad_norm": 7295.7939453125,
      "learning_rate": 1.9607843137254903e-05,
      "loss": 65.8682,
      "step": 16180
    },
    {
      "epoch": 16.2,
      "grad_norm": 43549.1328125,
      "learning_rate": 1.9602683178534574e-05,
      "loss": 56.2795,
      "step": 16181
    },
    {
      "epoch": 16.2,
      "grad_norm": 13778.8291015625,
      "learning_rate": 1.959752321981424e-05,
      "loss": 22.0629,
      "step": 16182
    },
    {
      "epoch": 16.2,
      "grad_norm": 7056.81103515625,
      "learning_rate": 1.9592363261093913e-05,
      "loss": 52.461,
      "step": 16183
    },
    {
      "epoch": 16.2,
      "grad_norm": 54218.55078125,
      "learning_rate": 1.9587203302373584e-05,
      "loss": 43.4051,
      "step": 16184
    },
    {
      "epoch": 16.2,
      "grad_norm": 18798.97265625,
      "learning_rate": 1.958204334365325e-05,
      "loss": 57.2486,
      "step": 16185
    },
    {
      "epoch": 16.2,
      "grad_norm": 13694.568359375,
      "learning_rate": 1.957688338493292e-05,
      "loss": 44.5191,
      "step": 16186
    },
    {
      "epoch": 16.2,
      "grad_norm": 15451.267578125,
      "learning_rate": 1.957172342621259e-05,
      "loss": 39.2372,
      "step": 16187
    },
    {
      "epoch": 16.2,
      "grad_norm": 14808.3017578125,
      "learning_rate": 1.956656346749226e-05,
      "loss": 59.2164,
      "step": 16188
    },
    {
      "epoch": 16.21,
      "grad_norm": 16907.462890625,
      "learning_rate": 1.956140350877193e-05,
      "loss": 63.7125,
      "step": 16189
    },
    {
      "epoch": 16.21,
      "grad_norm": 59664.44921875,
      "learning_rate": 1.95562435500516e-05,
      "loss": 57.9821,
      "step": 16190
    },
    {
      "epoch": 16.21,
      "grad_norm": 9746.6015625,
      "learning_rate": 1.955108359133127e-05,
      "loss": 57.7419,
      "step": 16191
    },
    {
      "epoch": 16.21,
      "grad_norm": 8681.451171875,
      "learning_rate": 1.954592363261094e-05,
      "loss": 56.8218,
      "step": 16192
    },
    {
      "epoch": 16.21,
      "grad_norm": 19669.650390625,
      "learning_rate": 1.954076367389061e-05,
      "loss": 41.9317,
      "step": 16193
    },
    {
      "epoch": 16.21,
      "grad_norm": 28880.81640625,
      "learning_rate": 1.953560371517028e-05,
      "loss": 60.7714,
      "step": 16194
    },
    {
      "epoch": 16.21,
      "grad_norm": 124318.078125,
      "learning_rate": 1.953044375644995e-05,
      "loss": 42.8823,
      "step": 16195
    },
    {
      "epoch": 16.21,
      "grad_norm": 3833.456298828125,
      "learning_rate": 1.9525283797729617e-05,
      "loss": 50.4086,
      "step": 16196
    },
    {
      "epoch": 16.21,
      "grad_norm": 31266.23828125,
      "learning_rate": 1.9520123839009288e-05,
      "loss": 60.9366,
      "step": 16197
    },
    {
      "epoch": 16.21,
      "grad_norm": 1834.2420654296875,
      "learning_rate": 1.951496388028896e-05,
      "loss": 57.6665,
      "step": 16198
    },
    {
      "epoch": 16.22,
      "grad_norm": 11776.0126953125,
      "learning_rate": 1.9509803921568627e-05,
      "loss": 58.3454,
      "step": 16199
    },
    {
      "epoch": 16.22,
      "grad_norm": 7363.359375,
      "learning_rate": 1.9504643962848298e-05,
      "loss": 52.4937,
      "step": 16200
    },
    {
      "epoch": 16.22,
      "grad_norm": 71798.1015625,
      "learning_rate": 1.949948400412797e-05,
      "loss": 63.6412,
      "step": 16201
    },
    {
      "epoch": 16.22,
      "grad_norm": 16355.513671875,
      "learning_rate": 1.949432404540764e-05,
      "loss": 57.9108,
      "step": 16202
    },
    {
      "epoch": 16.22,
      "grad_norm": 2505.285888671875,
      "learning_rate": 1.9489164086687308e-05,
      "loss": 58.7126,
      "step": 16203
    },
    {
      "epoch": 16.22,
      "grad_norm": 5182.9453125,
      "learning_rate": 1.9484004127966976e-05,
      "loss": 47.289,
      "step": 16204
    },
    {
      "epoch": 16.22,
      "grad_norm": 22891.2890625,
      "learning_rate": 1.9478844169246647e-05,
      "loss": 58.3151,
      "step": 16205
    },
    {
      "epoch": 16.22,
      "grad_norm": 8382.0615234375,
      "learning_rate": 1.9473684210526315e-05,
      "loss": 55.6786,
      "step": 16206
    },
    {
      "epoch": 16.22,
      "grad_norm": 3153.7607421875,
      "learning_rate": 1.9468524251805986e-05,
      "loss": 50.1402,
      "step": 16207
    },
    {
      "epoch": 16.22,
      "grad_norm": 4272.21875,
      "learning_rate": 1.9463364293085657e-05,
      "loss": 49.9654,
      "step": 16208
    },
    {
      "epoch": 16.23,
      "grad_norm": 6162.509765625,
      "learning_rate": 1.9458204334365328e-05,
      "loss": 48.9269,
      "step": 16209
    },
    {
      "epoch": 16.23,
      "grad_norm": 58866.8125,
      "learning_rate": 1.9453044375644996e-05,
      "loss": 45.5056,
      "step": 16210
    },
    {
      "epoch": 16.23,
      "grad_norm": 21934.818359375,
      "learning_rate": 1.9447884416924667e-05,
      "loss": 46.9151,
      "step": 16211
    },
    {
      "epoch": 16.23,
      "grad_norm": 7092.2861328125,
      "learning_rate": 1.9442724458204338e-05,
      "loss": 31.4764,
      "step": 16212
    },
    {
      "epoch": 16.23,
      "grad_norm": 1180.8145751953125,
      "learning_rate": 1.9437564499484002e-05,
      "loss": 54.6257,
      "step": 16213
    },
    {
      "epoch": 16.23,
      "grad_norm": 17649.330078125,
      "learning_rate": 1.9432404540763673e-05,
      "loss": 54.649,
      "step": 16214
    },
    {
      "epoch": 16.23,
      "grad_norm": 21419.255859375,
      "learning_rate": 1.9427244582043345e-05,
      "loss": 56.3335,
      "step": 16215
    },
    {
      "epoch": 16.23,
      "grad_norm": 7783.953125,
      "learning_rate": 1.9422084623323016e-05,
      "loss": 55.0051,
      "step": 16216
    },
    {
      "epoch": 16.23,
      "grad_norm": 24515.099609375,
      "learning_rate": 1.9416924664602683e-05,
      "loss": 38.2776,
      "step": 16217
    },
    {
      "epoch": 16.23,
      "grad_norm": 1957.608154296875,
      "learning_rate": 1.9411764705882355e-05,
      "loss": 58.738,
      "step": 16218
    },
    {
      "epoch": 16.24,
      "grad_norm": 1560.9879150390625,
      "learning_rate": 1.9406604747162026e-05,
      "loss": 42.2672,
      "step": 16219
    },
    {
      "epoch": 16.24,
      "grad_norm": 7806.98193359375,
      "learning_rate": 1.9401444788441694e-05,
      "loss": 60.9833,
      "step": 16220
    },
    {
      "epoch": 16.24,
      "grad_norm": 8885.74609375,
      "learning_rate": 1.9396284829721365e-05,
      "loss": 65.5597,
      "step": 16221
    },
    {
      "epoch": 16.24,
      "grad_norm": 15434.513671875,
      "learning_rate": 1.9391124871001032e-05,
      "loss": 53.8832,
      "step": 16222
    },
    {
      "epoch": 16.24,
      "grad_norm": 27903.7109375,
      "learning_rate": 1.9385964912280704e-05,
      "loss": 49.6365,
      "step": 16223
    },
    {
      "epoch": 16.24,
      "grad_norm": 63260.0625,
      "learning_rate": 1.938080495356037e-05,
      "loss": 49.8779,
      "step": 16224
    },
    {
      "epoch": 16.24,
      "grad_norm": 1441.526123046875,
      "learning_rate": 1.9375644994840042e-05,
      "loss": 55.103,
      "step": 16225
    },
    {
      "epoch": 16.24,
      "grad_norm": 46473.671875,
      "learning_rate": 1.9370485036119714e-05,
      "loss": 38.2604,
      "step": 16226
    },
    {
      "epoch": 16.24,
      "grad_norm": 14782.13671875,
      "learning_rate": 1.936532507739938e-05,
      "loss": 56.7878,
      "step": 16227
    },
    {
      "epoch": 16.24,
      "grad_norm": 28208.0390625,
      "learning_rate": 1.9360165118679052e-05,
      "loss": 60.9683,
      "step": 16228
    },
    {
      "epoch": 16.25,
      "grad_norm": 22798.224609375,
      "learning_rate": 1.9355005159958724e-05,
      "loss": 61.3563,
      "step": 16229
    },
    {
      "epoch": 16.25,
      "grad_norm": 219696.71875,
      "learning_rate": 1.934984520123839e-05,
      "loss": 46.9812,
      "step": 16230
    },
    {
      "epoch": 16.25,
      "grad_norm": 16488.91015625,
      "learning_rate": 1.934468524251806e-05,
      "loss": 58.2602,
      "step": 16231
    },
    {
      "epoch": 16.25,
      "grad_norm": 11379.8955078125,
      "learning_rate": 1.933952528379773e-05,
      "loss": 57.7,
      "step": 16232
    },
    {
      "epoch": 16.25,
      "grad_norm": 10086.98828125,
      "learning_rate": 1.93343653250774e-05,
      "loss": 48.94,
      "step": 16233
    },
    {
      "epoch": 16.25,
      "grad_norm": 56655.953125,
      "learning_rate": 1.932920536635707e-05,
      "loss": 38.6478,
      "step": 16234
    },
    {
      "epoch": 16.25,
      "grad_norm": 8432.396484375,
      "learning_rate": 1.932404540763674e-05,
      "loss": 43.5695,
      "step": 16235
    },
    {
      "epoch": 16.25,
      "grad_norm": 23106.1171875,
      "learning_rate": 1.931888544891641e-05,
      "loss": 34.2514,
      "step": 16236
    },
    {
      "epoch": 16.25,
      "grad_norm": 28403.6796875,
      "learning_rate": 1.931372549019608e-05,
      "loss": 58.4636,
      "step": 16237
    },
    {
      "epoch": 16.25,
      "grad_norm": 2899.6123046875,
      "learning_rate": 1.930856553147575e-05,
      "loss": 65.3344,
      "step": 16238
    },
    {
      "epoch": 16.26,
      "grad_norm": 10366.4921875,
      "learning_rate": 1.9303405572755418e-05,
      "loss": 58.9827,
      "step": 16239
    },
    {
      "epoch": 16.26,
      "grad_norm": 18612.375,
      "learning_rate": 1.929824561403509e-05,
      "loss": 38.0412,
      "step": 16240
    },
    {
      "epoch": 16.26,
      "grad_norm": 37775.41796875,
      "learning_rate": 1.9293085655314757e-05,
      "loss": 47.5492,
      "step": 16241
    },
    {
      "epoch": 16.26,
      "grad_norm": 15071.587890625,
      "learning_rate": 1.9287925696594428e-05,
      "loss": 50.8572,
      "step": 16242
    },
    {
      "epoch": 16.26,
      "grad_norm": 32133.712890625,
      "learning_rate": 1.92827657378741e-05,
      "loss": 52.8984,
      "step": 16243
    },
    {
      "epoch": 16.26,
      "grad_norm": 8290.84765625,
      "learning_rate": 1.9277605779153767e-05,
      "loss": 54.861,
      "step": 16244
    },
    {
      "epoch": 16.26,
      "grad_norm": 5570.3681640625,
      "learning_rate": 1.9272445820433438e-05,
      "loss": 57.0714,
      "step": 16245
    },
    {
      "epoch": 16.26,
      "grad_norm": 6824.2470703125,
      "learning_rate": 1.926728586171311e-05,
      "loss": 60.9835,
      "step": 16246
    },
    {
      "epoch": 16.26,
      "grad_norm": 12656.1171875,
      "learning_rate": 1.9262125902992777e-05,
      "loss": 51.262,
      "step": 16247
    },
    {
      "epoch": 16.26,
      "grad_norm": 1912.058349609375,
      "learning_rate": 1.9256965944272444e-05,
      "loss": 45.5638,
      "step": 16248
    },
    {
      "epoch": 16.27,
      "grad_norm": 59875.26171875,
      "learning_rate": 1.9251805985552116e-05,
      "loss": 51.0365,
      "step": 16249
    },
    {
      "epoch": 16.27,
      "grad_norm": 12910.5595703125,
      "learning_rate": 1.9246646026831787e-05,
      "loss": 59.5311,
      "step": 16250
    },
    {
      "epoch": 16.27,
      "grad_norm": 13254.6181640625,
      "learning_rate": 1.9241486068111454e-05,
      "loss": 56.9558,
      "step": 16251
    },
    {
      "epoch": 16.27,
      "grad_norm": 14152.076171875,
      "learning_rate": 1.9236326109391126e-05,
      "loss": 55.9652,
      "step": 16252
    },
    {
      "epoch": 16.27,
      "grad_norm": 7993.26806640625,
      "learning_rate": 1.9231166150670797e-05,
      "loss": 54.398,
      "step": 16253
    },
    {
      "epoch": 16.27,
      "grad_norm": 3139.081298828125,
      "learning_rate": 1.9226006191950464e-05,
      "loss": 61.0582,
      "step": 16254
    },
    {
      "epoch": 16.27,
      "grad_norm": 6128.05126953125,
      "learning_rate": 1.9220846233230136e-05,
      "loss": 65.7333,
      "step": 16255
    },
    {
      "epoch": 16.27,
      "grad_norm": 34319.98046875,
      "learning_rate": 1.9215686274509807e-05,
      "loss": 60.0735,
      "step": 16256
    },
    {
      "epoch": 16.27,
      "grad_norm": 3962.88427734375,
      "learning_rate": 1.9210526315789474e-05,
      "loss": 54.7133,
      "step": 16257
    },
    {
      "epoch": 16.27,
      "grad_norm": 27615.115234375,
      "learning_rate": 1.9205366357069142e-05,
      "loss": 34.8798,
      "step": 16258
    },
    {
      "epoch": 16.28,
      "grad_norm": 6209.67578125,
      "learning_rate": 1.9200206398348813e-05,
      "loss": 61.1705,
      "step": 16259
    },
    {
      "epoch": 16.28,
      "grad_norm": 2620.447265625,
      "learning_rate": 1.9195046439628485e-05,
      "loss": 64.5136,
      "step": 16260
    },
    {
      "epoch": 16.28,
      "grad_norm": 13143.724609375,
      "learning_rate": 1.9189886480908152e-05,
      "loss": 59.2124,
      "step": 16261
    },
    {
      "epoch": 16.28,
      "grad_norm": 1369.7626953125,
      "learning_rate": 1.9184726522187823e-05,
      "loss": 57.6308,
      "step": 16262
    },
    {
      "epoch": 16.28,
      "grad_norm": 14877.1005859375,
      "learning_rate": 1.9179566563467495e-05,
      "loss": 36.5232,
      "step": 16263
    },
    {
      "epoch": 16.28,
      "grad_norm": 8429.3720703125,
      "learning_rate": 1.9174406604747162e-05,
      "loss": 52.726,
      "step": 16264
    },
    {
      "epoch": 16.28,
      "grad_norm": 29286.2421875,
      "learning_rate": 1.9169246646026833e-05,
      "loss": 57.5493,
      "step": 16265
    },
    {
      "epoch": 16.28,
      "grad_norm": 2465.8486328125,
      "learning_rate": 1.91640866873065e-05,
      "loss": 56.5885,
      "step": 16266
    },
    {
      "epoch": 16.28,
      "grad_norm": 12086.7080078125,
      "learning_rate": 1.9158926728586172e-05,
      "loss": 33.2351,
      "step": 16267
    },
    {
      "epoch": 16.28,
      "grad_norm": 5318.068359375,
      "learning_rate": 1.915376676986584e-05,
      "loss": 57.9745,
      "step": 16268
    },
    {
      "epoch": 16.29,
      "grad_norm": 30874.115234375,
      "learning_rate": 1.914860681114551e-05,
      "loss": 62.9795,
      "step": 16269
    },
    {
      "epoch": 16.29,
      "grad_norm": 14926.8642578125,
      "learning_rate": 1.9143446852425182e-05,
      "loss": 46.5098,
      "step": 16270
    },
    {
      "epoch": 16.29,
      "grad_norm": 82577.03125,
      "learning_rate": 1.913828689370485e-05,
      "loss": 51.5349,
      "step": 16271
    },
    {
      "epoch": 16.29,
      "grad_norm": 19372.375,
      "learning_rate": 1.913312693498452e-05,
      "loss": 33.6925,
      "step": 16272
    },
    {
      "epoch": 16.29,
      "grad_norm": 27987.30859375,
      "learning_rate": 1.9127966976264192e-05,
      "loss": 34.673,
      "step": 16273
    },
    {
      "epoch": 16.29,
      "grad_norm": 1901.9088134765625,
      "learning_rate": 1.9122807017543863e-05,
      "loss": 66.1022,
      "step": 16274
    },
    {
      "epoch": 16.29,
      "grad_norm": 12486.9853515625,
      "learning_rate": 1.9117647058823528e-05,
      "loss": 58.1867,
      "step": 16275
    },
    {
      "epoch": 16.29,
      "grad_norm": 3485.12158203125,
      "learning_rate": 1.91124871001032e-05,
      "loss": 59.0211,
      "step": 16276
    },
    {
      "epoch": 16.29,
      "grad_norm": 5900.93896484375,
      "learning_rate": 1.910732714138287e-05,
      "loss": 61.8551,
      "step": 16277
    },
    {
      "epoch": 16.29,
      "grad_norm": 4154.60546875,
      "learning_rate": 1.9102167182662538e-05,
      "loss": 43.7968,
      "step": 16278
    },
    {
      "epoch": 16.3,
      "grad_norm": 10257.478515625,
      "learning_rate": 1.909700722394221e-05,
      "loss": 37.8298,
      "step": 16279
    },
    {
      "epoch": 16.3,
      "grad_norm": 24254.84375,
      "learning_rate": 1.909184726522188e-05,
      "loss": 46.238,
      "step": 16280
    },
    {
      "epoch": 16.3,
      "grad_norm": 25323.1328125,
      "learning_rate": 1.908668730650155e-05,
      "loss": 53.5338,
      "step": 16281
    },
    {
      "epoch": 16.3,
      "grad_norm": 82729.1015625,
      "learning_rate": 1.908152734778122e-05,
      "loss": 56.9601,
      "step": 16282
    },
    {
      "epoch": 16.3,
      "grad_norm": 20927.310546875,
      "learning_rate": 1.907636738906089e-05,
      "loss": 51.1615,
      "step": 16283
    },
    {
      "epoch": 16.3,
      "grad_norm": 12649.072265625,
      "learning_rate": 1.9071207430340558e-05,
      "loss": 63.4158,
      "step": 16284
    },
    {
      "epoch": 16.3,
      "grad_norm": 18976.6640625,
      "learning_rate": 1.9066047471620225e-05,
      "loss": 56.9529,
      "step": 16285
    },
    {
      "epoch": 16.3,
      "grad_norm": 19888.3984375,
      "learning_rate": 1.9060887512899897e-05,
      "loss": 40.9074,
      "step": 16286
    },
    {
      "epoch": 16.3,
      "grad_norm": 20421.642578125,
      "learning_rate": 1.9055727554179568e-05,
      "loss": 60.182,
      "step": 16287
    },
    {
      "epoch": 16.3,
      "grad_norm": 6770.310546875,
      "learning_rate": 1.905056759545924e-05,
      "loss": 52.1138,
      "step": 16288
    },
    {
      "epoch": 16.31,
      "grad_norm": 3894.5380859375,
      "learning_rate": 1.9045407636738907e-05,
      "loss": 55.8035,
      "step": 16289
    },
    {
      "epoch": 16.31,
      "grad_norm": 175951.953125,
      "learning_rate": 1.9040247678018578e-05,
      "loss": 52.2406,
      "step": 16290
    },
    {
      "epoch": 16.31,
      "grad_norm": 2514.0380859375,
      "learning_rate": 1.903508771929825e-05,
      "loss": 67.5664,
      "step": 16291
    },
    {
      "epoch": 16.31,
      "grad_norm": 125528.8671875,
      "learning_rate": 1.9029927760577917e-05,
      "loss": 34.2758,
      "step": 16292
    },
    {
      "epoch": 16.31,
      "grad_norm": 42928.69140625,
      "learning_rate": 1.9024767801857584e-05,
      "loss": 42.5384,
      "step": 16293
    },
    {
      "epoch": 16.31,
      "grad_norm": 19528.09375,
      "learning_rate": 1.9019607843137255e-05,
      "loss": 49.5946,
      "step": 16294
    },
    {
      "epoch": 16.31,
      "grad_norm": 2505.570068359375,
      "learning_rate": 1.9014447884416923e-05,
      "loss": 60.7401,
      "step": 16295
    },
    {
      "epoch": 16.31,
      "grad_norm": 11367.86328125,
      "learning_rate": 1.9009287925696594e-05,
      "loss": 55.4956,
      "step": 16296
    },
    {
      "epoch": 16.31,
      "grad_norm": 50917.5390625,
      "learning_rate": 1.9004127966976265e-05,
      "loss": 51.3663,
      "step": 16297
    },
    {
      "epoch": 16.31,
      "grad_norm": 9619.5771484375,
      "learning_rate": 1.8998968008255937e-05,
      "loss": 61.8274,
      "step": 16298
    },
    {
      "epoch": 16.32,
      "grad_norm": 6021.90771484375,
      "learning_rate": 1.8993808049535604e-05,
      "loss": 53.7298,
      "step": 16299
    },
    {
      "epoch": 16.32,
      "grad_norm": 5735.55908203125,
      "learning_rate": 1.8988648090815275e-05,
      "loss": 57.7418,
      "step": 16300
    },
    {
      "epoch": 16.32,
      "grad_norm": 7906.24755859375,
      "learning_rate": 1.8983488132094947e-05,
      "loss": 48.174,
      "step": 16301
    },
    {
      "epoch": 16.32,
      "grad_norm": 7156.65673828125,
      "learning_rate": 1.897832817337461e-05,
      "loss": 61.3015,
      "step": 16302
    },
    {
      "epoch": 16.32,
      "grad_norm": 1802.642822265625,
      "learning_rate": 1.8973168214654282e-05,
      "loss": 59.4558,
      "step": 16303
    },
    {
      "epoch": 16.32,
      "grad_norm": 1161.6552734375,
      "learning_rate": 1.8968008255933953e-05,
      "loss": 53.913,
      "step": 16304
    },
    {
      "epoch": 16.32,
      "grad_norm": 20920.423828125,
      "learning_rate": 1.8962848297213624e-05,
      "loss": 45.78,
      "step": 16305
    },
    {
      "epoch": 16.32,
      "grad_norm": 2705.326171875,
      "learning_rate": 1.8957688338493292e-05,
      "loss": 50.6402,
      "step": 16306
    },
    {
      "epoch": 16.32,
      "grad_norm": 25611.10546875,
      "learning_rate": 1.8952528379772963e-05,
      "loss": 44.2675,
      "step": 16307
    },
    {
      "epoch": 16.32,
      "grad_norm": 14644.3974609375,
      "learning_rate": 1.8947368421052634e-05,
      "loss": 43.3688,
      "step": 16308
    },
    {
      "epoch": 16.33,
      "grad_norm": 30476.154296875,
      "learning_rate": 1.8942208462332302e-05,
      "loss": 50.4514,
      "step": 16309
    },
    {
      "epoch": 16.33,
      "grad_norm": 22126.896484375,
      "learning_rate": 1.8937048503611973e-05,
      "loss": 52.8484,
      "step": 16310
    },
    {
      "epoch": 16.33,
      "grad_norm": 41583.2109375,
      "learning_rate": 1.893188854489164e-05,
      "loss": 51.8938,
      "step": 16311
    },
    {
      "epoch": 16.33,
      "grad_norm": 4265.4423828125,
      "learning_rate": 1.8926728586171312e-05,
      "loss": 60.7993,
      "step": 16312
    },
    {
      "epoch": 16.33,
      "grad_norm": 18282.994140625,
      "learning_rate": 1.892156862745098e-05,
      "loss": 57.4529,
      "step": 16313
    },
    {
      "epoch": 16.33,
      "grad_norm": 5045.65234375,
      "learning_rate": 1.891640866873065e-05,
      "loss": 49.9151,
      "step": 16314
    },
    {
      "epoch": 16.33,
      "grad_norm": 13633.7919921875,
      "learning_rate": 1.8911248710010322e-05,
      "loss": 61.6427,
      "step": 16315
    },
    {
      "epoch": 16.33,
      "grad_norm": 39384.765625,
      "learning_rate": 1.890608875128999e-05,
      "loss": 30.5736,
      "step": 16316
    },
    {
      "epoch": 16.33,
      "grad_norm": 1183.063720703125,
      "learning_rate": 1.890092879256966e-05,
      "loss": 52.3033,
      "step": 16317
    },
    {
      "epoch": 16.33,
      "grad_norm": 11942.759765625,
      "learning_rate": 1.8895768833849332e-05,
      "loss": 48.899,
      "step": 16318
    },
    {
      "epoch": 16.34,
      "grad_norm": 5686.56005859375,
      "learning_rate": 1.8890608875129e-05,
      "loss": 60.1405,
      "step": 16319
    },
    {
      "epoch": 16.34,
      "grad_norm": 23131.46875,
      "learning_rate": 1.8885448916408668e-05,
      "loss": 67.9521,
      "step": 16320
    },
    {
      "epoch": 16.34,
      "grad_norm": 5635.84228515625,
      "learning_rate": 1.888028895768834e-05,
      "loss": 57.7321,
      "step": 16321
    },
    {
      "epoch": 16.34,
      "grad_norm": 22978.619140625,
      "learning_rate": 1.887512899896801e-05,
      "loss": 47.5145,
      "step": 16322
    },
    {
      "epoch": 16.34,
      "grad_norm": 12843.0771484375,
      "learning_rate": 1.8869969040247678e-05,
      "loss": 51.4833,
      "step": 16323
    },
    {
      "epoch": 16.34,
      "grad_norm": 5085.9580078125,
      "learning_rate": 1.886480908152735e-05,
      "loss": 54.5131,
      "step": 16324
    },
    {
      "epoch": 16.34,
      "grad_norm": 77547.6796875,
      "learning_rate": 1.885964912280702e-05,
      "loss": 38.4601,
      "step": 16325
    },
    {
      "epoch": 16.34,
      "grad_norm": 7206.236328125,
      "learning_rate": 1.8854489164086688e-05,
      "loss": 52.6647,
      "step": 16326
    },
    {
      "epoch": 16.34,
      "grad_norm": 2191.441650390625,
      "learning_rate": 1.884932920536636e-05,
      "loss": 57.2748,
      "step": 16327
    },
    {
      "epoch": 16.34,
      "grad_norm": 29455.517578125,
      "learning_rate": 1.884416924664603e-05,
      "loss": 24.4817,
      "step": 16328
    },
    {
      "epoch": 16.35,
      "grad_norm": 4446.43017578125,
      "learning_rate": 1.8839009287925698e-05,
      "loss": 59.3284,
      "step": 16329
    },
    {
      "epoch": 16.35,
      "grad_norm": 3678.326171875,
      "learning_rate": 1.8833849329205365e-05,
      "loss": 55.569,
      "step": 16330
    },
    {
      "epoch": 16.35,
      "grad_norm": 9076.056640625,
      "learning_rate": 1.8828689370485036e-05,
      "loss": 53.3164,
      "step": 16331
    },
    {
      "epoch": 16.35,
      "grad_norm": 3582.99072265625,
      "learning_rate": 1.8823529411764708e-05,
      "loss": 44.7895,
      "step": 16332
    },
    {
      "epoch": 16.35,
      "grad_norm": 11142.2646484375,
      "learning_rate": 1.8818369453044375e-05,
      "loss": 43.8097,
      "step": 16333
    },
    {
      "epoch": 16.35,
      "grad_norm": 4999.80810546875,
      "learning_rate": 1.8813209494324046e-05,
      "loss": 65.7915,
      "step": 16334
    },
    {
      "epoch": 16.35,
      "grad_norm": 6046.68359375,
      "learning_rate": 1.8808049535603718e-05,
      "loss": 54.815,
      "step": 16335
    },
    {
      "epoch": 16.35,
      "grad_norm": 10762.8984375,
      "learning_rate": 1.8802889576883385e-05,
      "loss": 62.9524,
      "step": 16336
    },
    {
      "epoch": 16.35,
      "grad_norm": 7943.76171875,
      "learning_rate": 1.8797729618163056e-05,
      "loss": 46.197,
      "step": 16337
    },
    {
      "epoch": 16.35,
      "grad_norm": 3665.1376953125,
      "learning_rate": 1.8792569659442724e-05,
      "loss": 49.7069,
      "step": 16338
    },
    {
      "epoch": 16.36,
      "grad_norm": 197498.09375,
      "learning_rate": 1.8787409700722395e-05,
      "loss": 53.2743,
      "step": 16339
    },
    {
      "epoch": 16.36,
      "grad_norm": 27165.5390625,
      "learning_rate": 1.8782249742002063e-05,
      "loss": 54.6004,
      "step": 16340
    },
    {
      "epoch": 16.36,
      "grad_norm": 51554.65625,
      "learning_rate": 1.8777089783281734e-05,
      "loss": 37.7257,
      "step": 16341
    },
    {
      "epoch": 16.36,
      "grad_norm": 2950.938720703125,
      "learning_rate": 1.8771929824561405e-05,
      "loss": 64.7726,
      "step": 16342
    },
    {
      "epoch": 16.36,
      "grad_norm": 8836.7001953125,
      "learning_rate": 1.8766769865841073e-05,
      "loss": 56.8922,
      "step": 16343
    },
    {
      "epoch": 16.36,
      "grad_norm": 48565.7734375,
      "learning_rate": 1.8761609907120744e-05,
      "loss": 51.8998,
      "step": 16344
    },
    {
      "epoch": 16.36,
      "grad_norm": 23350.578125,
      "learning_rate": 1.8756449948400415e-05,
      "loss": 52.6885,
      "step": 16345
    },
    {
      "epoch": 16.36,
      "grad_norm": 17776.3671875,
      "learning_rate": 1.8751289989680083e-05,
      "loss": 49.7081,
      "step": 16346
    },
    {
      "epoch": 16.36,
      "grad_norm": 2201.781494140625,
      "learning_rate": 1.874613003095975e-05,
      "loss": 51.8796,
      "step": 16347
    },
    {
      "epoch": 16.36,
      "grad_norm": 10788.46875,
      "learning_rate": 1.8740970072239422e-05,
      "loss": 61.4469,
      "step": 16348
    },
    {
      "epoch": 16.37,
      "grad_norm": 12466.9677734375,
      "learning_rate": 1.8735810113519093e-05,
      "loss": 63.0468,
      "step": 16349
    },
    {
      "epoch": 16.37,
      "grad_norm": 16647.49609375,
      "learning_rate": 1.873065015479876e-05,
      "loss": 57.3674,
      "step": 16350
    },
    {
      "epoch": 16.37,
      "grad_norm": 12942.171875,
      "learning_rate": 1.8725490196078432e-05,
      "loss": 52.23,
      "step": 16351
    },
    {
      "epoch": 16.37,
      "grad_norm": 2044.8935546875,
      "learning_rate": 1.8720330237358103e-05,
      "loss": 59.0263,
      "step": 16352
    },
    {
      "epoch": 16.37,
      "grad_norm": 14758.3701171875,
      "learning_rate": 1.871517027863777e-05,
      "loss": 31.5841,
      "step": 16353
    },
    {
      "epoch": 16.37,
      "grad_norm": 30153.93359375,
      "learning_rate": 1.8710010319917442e-05,
      "loss": 49.209,
      "step": 16354
    },
    {
      "epoch": 16.37,
      "grad_norm": 7897.32958984375,
      "learning_rate": 1.8704850361197113e-05,
      "loss": 55.6072,
      "step": 16355
    },
    {
      "epoch": 16.37,
      "grad_norm": 17039.267578125,
      "learning_rate": 1.869969040247678e-05,
      "loss": 38.606,
      "step": 16356
    },
    {
      "epoch": 16.37,
      "grad_norm": 5730.02099609375,
      "learning_rate": 1.869453044375645e-05,
      "loss": 52.1114,
      "step": 16357
    },
    {
      "epoch": 16.37,
      "grad_norm": 19127.955078125,
      "learning_rate": 1.868937048503612e-05,
      "loss": 48.5702,
      "step": 16358
    },
    {
      "epoch": 16.38,
      "grad_norm": 10557.8935546875,
      "learning_rate": 1.868421052631579e-05,
      "loss": 29.7877,
      "step": 16359
    },
    {
      "epoch": 16.38,
      "grad_norm": 1948.5338134765625,
      "learning_rate": 1.867905056759546e-05,
      "loss": 54.6189,
      "step": 16360
    },
    {
      "epoch": 16.38,
      "grad_norm": 21248.2109375,
      "learning_rate": 1.867389060887513e-05,
      "loss": 45.7296,
      "step": 16361
    },
    {
      "epoch": 16.38,
      "grad_norm": 14081.4755859375,
      "learning_rate": 1.86687306501548e-05,
      "loss": 52.9695,
      "step": 16362
    },
    {
      "epoch": 16.38,
      "grad_norm": 6055.5341796875,
      "learning_rate": 1.8663570691434472e-05,
      "loss": 37.5642,
      "step": 16363
    },
    {
      "epoch": 16.38,
      "grad_norm": 8392.912109375,
      "learning_rate": 1.865841073271414e-05,
      "loss": 49.8056,
      "step": 16364
    },
    {
      "epoch": 16.38,
      "grad_norm": 7070.81494140625,
      "learning_rate": 1.8653250773993807e-05,
      "loss": 66.9178,
      "step": 16365
    },
    {
      "epoch": 16.38,
      "grad_norm": 22785.6953125,
      "learning_rate": 1.864809081527348e-05,
      "loss": 34.0397,
      "step": 16366
    },
    {
      "epoch": 16.38,
      "grad_norm": 29591.484375,
      "learning_rate": 1.8642930856553146e-05,
      "loss": 54.6963,
      "step": 16367
    },
    {
      "epoch": 16.38,
      "grad_norm": 6840.62353515625,
      "learning_rate": 1.8637770897832817e-05,
      "loss": 52.0964,
      "step": 16368
    },
    {
      "epoch": 16.39,
      "grad_norm": 20336.556640625,
      "learning_rate": 1.863261093911249e-05,
      "loss": 61.1991,
      "step": 16369
    },
    {
      "epoch": 16.39,
      "grad_norm": 1921.1375732421875,
      "learning_rate": 1.862745098039216e-05,
      "loss": 48.9916,
      "step": 16370
    },
    {
      "epoch": 16.39,
      "grad_norm": 11018.6552734375,
      "learning_rate": 1.8622291021671827e-05,
      "loss": 47.913,
      "step": 16371
    },
    {
      "epoch": 16.39,
      "grad_norm": 12869.3984375,
      "learning_rate": 1.86171310629515e-05,
      "loss": 43.3201,
      "step": 16372
    },
    {
      "epoch": 16.39,
      "grad_norm": 16847.05859375,
      "learning_rate": 1.861197110423117e-05,
      "loss": 55.4429,
      "step": 16373
    },
    {
      "epoch": 16.39,
      "grad_norm": 8435.1552734375,
      "learning_rate": 1.8606811145510834e-05,
      "loss": 52.6548,
      "step": 16374
    },
    {
      "epoch": 16.39,
      "grad_norm": 9625.7802734375,
      "learning_rate": 1.8601651186790505e-05,
      "loss": 56.3258,
      "step": 16375
    },
    {
      "epoch": 16.39,
      "grad_norm": 2000.6580810546875,
      "learning_rate": 1.8596491228070176e-05,
      "loss": 36.856,
      "step": 16376
    },
    {
      "epoch": 16.39,
      "grad_norm": 3004.9990234375,
      "learning_rate": 1.8591331269349847e-05,
      "loss": 57.7054,
      "step": 16377
    },
    {
      "epoch": 16.39,
      "grad_norm": 2602.828369140625,
      "learning_rate": 1.8586171310629515e-05,
      "loss": 50.0006,
      "step": 16378
    },
    {
      "epoch": 16.4,
      "grad_norm": 4591.5029296875,
      "learning_rate": 1.8581011351909186e-05,
      "loss": 53.4037,
      "step": 16379
    },
    {
      "epoch": 16.4,
      "grad_norm": 2153.659912109375,
      "learning_rate": 1.8575851393188857e-05,
      "loss": 56.2325,
      "step": 16380
    },
    {
      "epoch": 16.4,
      "grad_norm": 31231.75390625,
      "learning_rate": 1.8570691434468525e-05,
      "loss": 51.287,
      "step": 16381
    },
    {
      "epoch": 16.4,
      "grad_norm": 11313.0546875,
      "learning_rate": 1.8565531475748196e-05,
      "loss": 59.997,
      "step": 16382
    },
    {
      "epoch": 16.4,
      "grad_norm": 10360.6923828125,
      "learning_rate": 1.8560371517027864e-05,
      "loss": 60.2133,
      "step": 16383
    },
    {
      "epoch": 16.4,
      "grad_norm": 2853.291748046875,
      "learning_rate": 1.8555211558307535e-05,
      "loss": 38.6941,
      "step": 16384
    },
    {
      "epoch": 16.4,
      "grad_norm": 131475.421875,
      "learning_rate": 1.8550051599587203e-05,
      "loss": 52.0201,
      "step": 16385
    },
    {
      "epoch": 16.4,
      "grad_norm": 19028.8203125,
      "learning_rate": 1.8544891640866874e-05,
      "loss": 51.8794,
      "step": 16386
    },
    {
      "epoch": 16.4,
      "grad_norm": 17494.76953125,
      "learning_rate": 1.8539731682146545e-05,
      "loss": 58.4264,
      "step": 16387
    },
    {
      "epoch": 16.4,
      "grad_norm": 2356.885498046875,
      "learning_rate": 1.8534571723426213e-05,
      "loss": 44.3376,
      "step": 16388
    },
    {
      "epoch": 16.41,
      "grad_norm": 2521.104248046875,
      "learning_rate": 1.8529411764705884e-05,
      "loss": 61.5243,
      "step": 16389
    },
    {
      "epoch": 16.41,
      "grad_norm": 2078.373779296875,
      "learning_rate": 1.8524251805985555e-05,
      "loss": 62.3447,
      "step": 16390
    },
    {
      "epoch": 16.41,
      "grad_norm": 4242.3349609375,
      "learning_rate": 1.8519091847265223e-05,
      "loss": 54.2974,
      "step": 16391
    },
    {
      "epoch": 16.41,
      "grad_norm": 8115.0283203125,
      "learning_rate": 1.851393188854489e-05,
      "loss": 49.1538,
      "step": 16392
    },
    {
      "epoch": 16.41,
      "grad_norm": 26795.521484375,
      "learning_rate": 1.8508771929824562e-05,
      "loss": 41.7058,
      "step": 16393
    },
    {
      "epoch": 16.41,
      "grad_norm": 51500.50390625,
      "learning_rate": 1.8503611971104233e-05,
      "loss": 52.1041,
      "step": 16394
    },
    {
      "epoch": 16.41,
      "grad_norm": 6208.16162109375,
      "learning_rate": 1.84984520123839e-05,
      "loss": 62.5159,
      "step": 16395
    },
    {
      "epoch": 16.41,
      "grad_norm": 2330.260498046875,
      "learning_rate": 1.8493292053663572e-05,
      "loss": 57.47,
      "step": 16396
    },
    {
      "epoch": 16.41,
      "grad_norm": 31410.4765625,
      "learning_rate": 1.8488132094943243e-05,
      "loss": 62.5075,
      "step": 16397
    },
    {
      "epoch": 16.41,
      "grad_norm": 2601.747802734375,
      "learning_rate": 1.848297213622291e-05,
      "loss": 51.6128,
      "step": 16398
    },
    {
      "epoch": 16.42,
      "grad_norm": 20401.244140625,
      "learning_rate": 1.8477812177502582e-05,
      "loss": 20.6009,
      "step": 16399
    },
    {
      "epoch": 16.42,
      "grad_norm": 14507.900390625,
      "learning_rate": 1.847265221878225e-05,
      "loss": 58.8338,
      "step": 16400
    },
    {
      "epoch": 16.42,
      "grad_norm": 16497.8828125,
      "learning_rate": 1.846749226006192e-05,
      "loss": 56.5919,
      "step": 16401
    },
    {
      "epoch": 16.42,
      "grad_norm": 7315.38916015625,
      "learning_rate": 1.846233230134159e-05,
      "loss": 36.7549,
      "step": 16402
    },
    {
      "epoch": 16.42,
      "grad_norm": 86530.53125,
      "learning_rate": 1.845717234262126e-05,
      "loss": 50.3774,
      "step": 16403
    },
    {
      "epoch": 16.42,
      "grad_norm": 17941.671875,
      "learning_rate": 1.845201238390093e-05,
      "loss": 52.8937,
      "step": 16404
    },
    {
      "epoch": 16.42,
      "grad_norm": 48838.54296875,
      "learning_rate": 1.84468524251806e-05,
      "loss": 47.2614,
      "step": 16405
    },
    {
      "epoch": 16.42,
      "grad_norm": 4132.716796875,
      "learning_rate": 1.844169246646027e-05,
      "loss": 48.6051,
      "step": 16406
    },
    {
      "epoch": 16.42,
      "grad_norm": 7093.19921875,
      "learning_rate": 1.843653250773994e-05,
      "loss": 44.9169,
      "step": 16407
    },
    {
      "epoch": 16.42,
      "grad_norm": 5287.04345703125,
      "learning_rate": 1.843137254901961e-05,
      "loss": 45.8882,
      "step": 16408
    },
    {
      "epoch": 16.43,
      "grad_norm": 33662.5703125,
      "learning_rate": 1.8426212590299276e-05,
      "loss": 40.7204,
      "step": 16409
    },
    {
      "epoch": 16.43,
      "grad_norm": 9616.85546875,
      "learning_rate": 1.8421052631578947e-05,
      "loss": 54.0519,
      "step": 16410
    },
    {
      "epoch": 16.43,
      "grad_norm": 3818.710205078125,
      "learning_rate": 1.841589267285862e-05,
      "loss": 59.9157,
      "step": 16411
    },
    {
      "epoch": 16.43,
      "grad_norm": 2931.38818359375,
      "learning_rate": 1.8410732714138286e-05,
      "loss": 61.5997,
      "step": 16412
    },
    {
      "epoch": 16.43,
      "grad_norm": 12105.947265625,
      "learning_rate": 1.8405572755417957e-05,
      "loss": 22.6471,
      "step": 16413
    },
    {
      "epoch": 16.43,
      "grad_norm": 12546.0029296875,
      "learning_rate": 1.840041279669763e-05,
      "loss": 63.7537,
      "step": 16414
    },
    {
      "epoch": 16.43,
      "grad_norm": 153866.859375,
      "learning_rate": 1.8395252837977296e-05,
      "loss": 56.2028,
      "step": 16415
    },
    {
      "epoch": 16.43,
      "grad_norm": 54391.60546875,
      "learning_rate": 1.8390092879256967e-05,
      "loss": 57.7966,
      "step": 16416
    },
    {
      "epoch": 16.43,
      "grad_norm": 10212.24609375,
      "learning_rate": 1.838493292053664e-05,
      "loss": 58.244,
      "step": 16417
    },
    {
      "epoch": 16.43,
      "grad_norm": 8809.1982421875,
      "learning_rate": 1.8379772961816306e-05,
      "loss": 51.7042,
      "step": 16418
    },
    {
      "epoch": 16.44,
      "grad_norm": 12218.32421875,
      "learning_rate": 1.8374613003095974e-05,
      "loss": 57.689,
      "step": 16419
    },
    {
      "epoch": 16.44,
      "grad_norm": 48469.17578125,
      "learning_rate": 1.8369453044375645e-05,
      "loss": 57.2456,
      "step": 16420
    },
    {
      "epoch": 16.44,
      "grad_norm": 7478.8681640625,
      "learning_rate": 1.8364293085655316e-05,
      "loss": 56.4529,
      "step": 16421
    },
    {
      "epoch": 16.44,
      "grad_norm": 20520.3125,
      "learning_rate": 1.8359133126934984e-05,
      "loss": 47.323,
      "step": 16422
    },
    {
      "epoch": 16.44,
      "grad_norm": 5653.3203125,
      "learning_rate": 1.8353973168214655e-05,
      "loss": 57.8991,
      "step": 16423
    },
    {
      "epoch": 16.44,
      "grad_norm": 35016.18359375,
      "learning_rate": 1.8348813209494326e-05,
      "loss": 55.5558,
      "step": 16424
    },
    {
      "epoch": 16.44,
      "grad_norm": 6834.9697265625,
      "learning_rate": 1.8343653250773994e-05,
      "loss": 52.589,
      "step": 16425
    },
    {
      "epoch": 16.44,
      "grad_norm": 5119.29150390625,
      "learning_rate": 1.8338493292053665e-05,
      "loss": 57.1797,
      "step": 16426
    },
    {
      "epoch": 16.44,
      "grad_norm": 8052.015625,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 46.4398,
      "step": 16427
    },
    {
      "epoch": 16.44,
      "grad_norm": 16420.41796875,
      "learning_rate": 1.8328173374613004e-05,
      "loss": 45.7509,
      "step": 16428
    },
    {
      "epoch": 16.45,
      "grad_norm": 3045.031005859375,
      "learning_rate": 1.8323013415892672e-05,
      "loss": 52.9418,
      "step": 16429
    },
    {
      "epoch": 16.45,
      "grad_norm": 6221.30078125,
      "learning_rate": 1.8317853457172343e-05,
      "loss": 59.5451,
      "step": 16430
    },
    {
      "epoch": 16.45,
      "grad_norm": 7476.2080078125,
      "learning_rate": 1.8312693498452014e-05,
      "loss": 61.8002,
      "step": 16431
    },
    {
      "epoch": 16.45,
      "grad_norm": 6627.49658203125,
      "learning_rate": 1.8307533539731682e-05,
      "loss": 42.4122,
      "step": 16432
    },
    {
      "epoch": 16.45,
      "grad_norm": 4359.341796875,
      "learning_rate": 1.8302373581011353e-05,
      "loss": 62.3673,
      "step": 16433
    },
    {
      "epoch": 16.45,
      "grad_norm": 3042.014892578125,
      "learning_rate": 1.8297213622291024e-05,
      "loss": 32.7279,
      "step": 16434
    },
    {
      "epoch": 16.45,
      "grad_norm": 4649.93115234375,
      "learning_rate": 1.8292053663570695e-05,
      "loss": 52.5188,
      "step": 16435
    },
    {
      "epoch": 16.45,
      "grad_norm": 4621.7509765625,
      "learning_rate": 1.828689370485036e-05,
      "loss": 58.9655,
      "step": 16436
    },
    {
      "epoch": 16.45,
      "grad_norm": 7734.12451171875,
      "learning_rate": 1.828173374613003e-05,
      "loss": 61.9818,
      "step": 16437
    },
    {
      "epoch": 16.45,
      "grad_norm": 22130.646484375,
      "learning_rate": 1.8276573787409702e-05,
      "loss": 33.3426,
      "step": 16438
    },
    {
      "epoch": 16.46,
      "grad_norm": 4248.59423828125,
      "learning_rate": 1.827141382868937e-05,
      "loss": 61.176,
      "step": 16439
    },
    {
      "epoch": 16.46,
      "grad_norm": 3436.251708984375,
      "learning_rate": 1.826625386996904e-05,
      "loss": 53.1132,
      "step": 16440
    },
    {
      "epoch": 16.46,
      "grad_norm": 10638.9853515625,
      "learning_rate": 1.8261093911248712e-05,
      "loss": 68.7424,
      "step": 16441
    },
    {
      "epoch": 16.46,
      "grad_norm": 4632.34033203125,
      "learning_rate": 1.8255933952528383e-05,
      "loss": 64.6066,
      "step": 16442
    },
    {
      "epoch": 16.46,
      "grad_norm": 4844.34423828125,
      "learning_rate": 1.825077399380805e-05,
      "loss": 68.6158,
      "step": 16443
    },
    {
      "epoch": 16.46,
      "grad_norm": 6812.927734375,
      "learning_rate": 1.8245614035087722e-05,
      "loss": 59.5042,
      "step": 16444
    },
    {
      "epoch": 16.46,
      "grad_norm": 10305.7841796875,
      "learning_rate": 1.824045407636739e-05,
      "loss": 27.5837,
      "step": 16445
    },
    {
      "epoch": 16.46,
      "grad_norm": 5837.16943359375,
      "learning_rate": 1.8235294117647057e-05,
      "loss": 61.9669,
      "step": 16446
    },
    {
      "epoch": 16.46,
      "grad_norm": 17791.15625,
      "learning_rate": 1.823013415892673e-05,
      "loss": 58.5274,
      "step": 16447
    },
    {
      "epoch": 16.46,
      "grad_norm": 16238.453125,
      "learning_rate": 1.82249742002064e-05,
      "loss": 54.8368,
      "step": 16448
    },
    {
      "epoch": 16.47,
      "grad_norm": 9295.1201171875,
      "learning_rate": 1.821981424148607e-05,
      "loss": 56.9866,
      "step": 16449
    },
    {
      "epoch": 16.47,
      "grad_norm": 34054.3671875,
      "learning_rate": 1.821465428276574e-05,
      "loss": 56.4322,
      "step": 16450
    },
    {
      "epoch": 16.47,
      "grad_norm": 1513.5909423828125,
      "learning_rate": 1.820949432404541e-05,
      "loss": 56.8034,
      "step": 16451
    },
    {
      "epoch": 16.47,
      "grad_norm": 10802.0458984375,
      "learning_rate": 1.820433436532508e-05,
      "loss": 48.1738,
      "step": 16452
    },
    {
      "epoch": 16.47,
      "grad_norm": 3830.92236328125,
      "learning_rate": 1.819917440660475e-05,
      "loss": 54.6244,
      "step": 16453
    },
    {
      "epoch": 16.47,
      "grad_norm": 1935.1217041015625,
      "learning_rate": 1.8194014447884416e-05,
      "loss": 52.0628,
      "step": 16454
    },
    {
      "epoch": 16.47,
      "grad_norm": 6603.966796875,
      "learning_rate": 1.8188854489164087e-05,
      "loss": 50.4209,
      "step": 16455
    },
    {
      "epoch": 16.47,
      "grad_norm": 8331.205078125,
      "learning_rate": 1.818369453044376e-05,
      "loss": 51.3662,
      "step": 16456
    },
    {
      "epoch": 16.47,
      "grad_norm": 7577.615234375,
      "learning_rate": 1.8178534571723426e-05,
      "loss": 51.4537,
      "step": 16457
    },
    {
      "epoch": 16.47,
      "grad_norm": 2439.807861328125,
      "learning_rate": 1.8173374613003097e-05,
      "loss": 52.715,
      "step": 16458
    },
    {
      "epoch": 16.48,
      "grad_norm": 12075.529296875,
      "learning_rate": 1.816821465428277e-05,
      "loss": 56.6205,
      "step": 16459
    },
    {
      "epoch": 16.48,
      "grad_norm": 12428.072265625,
      "learning_rate": 1.8163054695562436e-05,
      "loss": 31.9595,
      "step": 16460
    },
    {
      "epoch": 16.48,
      "grad_norm": 109074.1875,
      "learning_rate": 1.8157894736842107e-05,
      "loss": 47.1619,
      "step": 16461
    },
    {
      "epoch": 16.48,
      "grad_norm": 1796.9132080078125,
      "learning_rate": 1.815273477812178e-05,
      "loss": 62.448,
      "step": 16462
    },
    {
      "epoch": 16.48,
      "grad_norm": 35315.48046875,
      "learning_rate": 1.8147574819401446e-05,
      "loss": 61.368,
      "step": 16463
    },
    {
      "epoch": 16.48,
      "grad_norm": 14792.7451171875,
      "learning_rate": 1.8142414860681114e-05,
      "loss": 54.623,
      "step": 16464
    },
    {
      "epoch": 16.48,
      "grad_norm": 12306.8134765625,
      "learning_rate": 1.8137254901960785e-05,
      "loss": 65.6534,
      "step": 16465
    },
    {
      "epoch": 16.48,
      "grad_norm": 6837.83837890625,
      "learning_rate": 1.8132094943240456e-05,
      "loss": 51.2226,
      "step": 16466
    },
    {
      "epoch": 16.48,
      "grad_norm": 715068.375,
      "learning_rate": 1.8126934984520124e-05,
      "loss": 53.0815,
      "step": 16467
    },
    {
      "epoch": 16.48,
      "grad_norm": 8312.208984375,
      "learning_rate": 1.8121775025799795e-05,
      "loss": 48.8032,
      "step": 16468
    },
    {
      "epoch": 16.49,
      "grad_norm": 4582.5283203125,
      "learning_rate": 1.8116615067079466e-05,
      "loss": 52.6999,
      "step": 16469
    },
    {
      "epoch": 16.49,
      "grad_norm": 27575.388671875,
      "learning_rate": 1.8111455108359134e-05,
      "loss": 59.4133,
      "step": 16470
    },
    {
      "epoch": 16.49,
      "grad_norm": 2708.112060546875,
      "learning_rate": 1.8106295149638805e-05,
      "loss": 56.0317,
      "step": 16471
    },
    {
      "epoch": 16.49,
      "grad_norm": 40016.2109375,
      "learning_rate": 1.8101135190918473e-05,
      "loss": 58.6326,
      "step": 16472
    },
    {
      "epoch": 16.49,
      "grad_norm": 2546.310791015625,
      "learning_rate": 1.8095975232198144e-05,
      "loss": 55.9566,
      "step": 16473
    },
    {
      "epoch": 16.49,
      "grad_norm": 3395.311767578125,
      "learning_rate": 1.809081527347781e-05,
      "loss": 56.7026,
      "step": 16474
    },
    {
      "epoch": 16.49,
      "grad_norm": 2236.453857421875,
      "learning_rate": 1.8085655314757483e-05,
      "loss": 62.1269,
      "step": 16475
    },
    {
      "epoch": 16.49,
      "grad_norm": 6156.75830078125,
      "learning_rate": 1.8080495356037154e-05,
      "loss": 45.789,
      "step": 16476
    },
    {
      "epoch": 16.49,
      "grad_norm": 3783.83984375,
      "learning_rate": 1.807533539731682e-05,
      "loss": 61.8068,
      "step": 16477
    },
    {
      "epoch": 16.49,
      "grad_norm": 4642.51025390625,
      "learning_rate": 1.8070175438596493e-05,
      "loss": 53.2507,
      "step": 16478
    },
    {
      "epoch": 16.5,
      "grad_norm": 1772.410888671875,
      "learning_rate": 1.8065015479876164e-05,
      "loss": 58.1581,
      "step": 16479
    },
    {
      "epoch": 16.5,
      "grad_norm": 12677.2412109375,
      "learning_rate": 1.805985552115583e-05,
      "loss": 59.9271,
      "step": 16480
    },
    {
      "epoch": 16.5,
      "grad_norm": 12879.2041015625,
      "learning_rate": 1.80546955624355e-05,
      "loss": 39.3866,
      "step": 16481
    },
    {
      "epoch": 16.5,
      "grad_norm": 6728.47216796875,
      "learning_rate": 1.804953560371517e-05,
      "loss": 60.0598,
      "step": 16482
    },
    {
      "epoch": 16.5,
      "grad_norm": 5237.0400390625,
      "learning_rate": 1.804437564499484e-05,
      "loss": 61.995,
      "step": 16483
    },
    {
      "epoch": 16.5,
      "grad_norm": 5590.64111328125,
      "learning_rate": 1.803921568627451e-05,
      "loss": 61.8696,
      "step": 16484
    },
    {
      "epoch": 16.5,
      "grad_norm": 5030.23046875,
      "learning_rate": 1.803405572755418e-05,
      "loss": 55.6749,
      "step": 16485
    },
    {
      "epoch": 16.5,
      "grad_norm": 15266.806640625,
      "learning_rate": 1.802889576883385e-05,
      "loss": 59.2515,
      "step": 16486
    },
    {
      "epoch": 16.5,
      "grad_norm": 72522.1640625,
      "learning_rate": 1.802373581011352e-05,
      "loss": 55.5392,
      "step": 16487
    },
    {
      "epoch": 16.5,
      "grad_norm": 10885.9501953125,
      "learning_rate": 1.801857585139319e-05,
      "loss": 42.1532,
      "step": 16488
    },
    {
      "epoch": 16.51,
      "grad_norm": 2092.3251953125,
      "learning_rate": 1.801341589267286e-05,
      "loss": 55.733,
      "step": 16489
    },
    {
      "epoch": 16.51,
      "grad_norm": 49894.2890625,
      "learning_rate": 1.800825593395253e-05,
      "loss": 43.7737,
      "step": 16490
    },
    {
      "epoch": 16.51,
      "grad_norm": 3629.446044921875,
      "learning_rate": 1.8003095975232197e-05,
      "loss": 56.2766,
      "step": 16491
    },
    {
      "epoch": 16.51,
      "grad_norm": 36888.6484375,
      "learning_rate": 1.7997936016511868e-05,
      "loss": 55.7831,
      "step": 16492
    },
    {
      "epoch": 16.51,
      "grad_norm": 9101.1416015625,
      "learning_rate": 1.799277605779154e-05,
      "loss": 42.7383,
      "step": 16493
    },
    {
      "epoch": 16.51,
      "grad_norm": 15521.4423828125,
      "learning_rate": 1.7987616099071207e-05,
      "loss": 58.9234,
      "step": 16494
    },
    {
      "epoch": 16.51,
      "grad_norm": 20573.736328125,
      "learning_rate": 1.7982456140350878e-05,
      "loss": 62.2927,
      "step": 16495
    },
    {
      "epoch": 16.51,
      "grad_norm": 17402.169921875,
      "learning_rate": 1.797729618163055e-05,
      "loss": 35.821,
      "step": 16496
    },
    {
      "epoch": 16.51,
      "grad_norm": 1053.426513671875,
      "learning_rate": 1.7972136222910217e-05,
      "loss": 60.4232,
      "step": 16497
    },
    {
      "epoch": 16.51,
      "grad_norm": 2730.403076171875,
      "learning_rate": 1.7966976264189888e-05,
      "loss": 61.5038,
      "step": 16498
    },
    {
      "epoch": 16.52,
      "grad_norm": 5141.20556640625,
      "learning_rate": 1.7961816305469556e-05,
      "loss": 64.7681,
      "step": 16499
    },
    {
      "epoch": 16.52,
      "grad_norm": 28551.603515625,
      "learning_rate": 1.7956656346749227e-05,
      "loss": 61.0872,
      "step": 16500
    },
    {
      "epoch": 16.52,
      "grad_norm": 2223.771240234375,
      "learning_rate": 1.7951496388028895e-05,
      "loss": 60.8534,
      "step": 16501
    },
    {
      "epoch": 16.52,
      "grad_norm": 4096.88330078125,
      "learning_rate": 1.7946336429308566e-05,
      "loss": 52.5545,
      "step": 16502
    },
    {
      "epoch": 16.52,
      "grad_norm": 20367.525390625,
      "learning_rate": 1.7941176470588237e-05,
      "loss": 64.8344,
      "step": 16503
    },
    {
      "epoch": 16.52,
      "grad_norm": 11858.4765625,
      "learning_rate": 1.7936016511867905e-05,
      "loss": 59.7174,
      "step": 16504
    },
    {
      "epoch": 16.52,
      "grad_norm": 34120.45703125,
      "learning_rate": 1.7930856553147576e-05,
      "loss": 46.5293,
      "step": 16505
    },
    {
      "epoch": 16.52,
      "grad_norm": 5948.80517578125,
      "learning_rate": 1.7925696594427247e-05,
      "loss": 54.1108,
      "step": 16506
    },
    {
      "epoch": 16.52,
      "grad_norm": 1717.5777587890625,
      "learning_rate": 1.7920536635706915e-05,
      "loss": 60.3714,
      "step": 16507
    },
    {
      "epoch": 16.52,
      "grad_norm": 4420.15576171875,
      "learning_rate": 1.7915376676986583e-05,
      "loss": 58.5751,
      "step": 16508
    },
    {
      "epoch": 16.53,
      "grad_norm": 1465.3428955078125,
      "learning_rate": 1.7910216718266254e-05,
      "loss": 59.972,
      "step": 16509
    },
    {
      "epoch": 16.53,
      "grad_norm": 7894.2001953125,
      "learning_rate": 1.7905056759545925e-05,
      "loss": 50.8248,
      "step": 16510
    },
    {
      "epoch": 16.53,
      "grad_norm": 2707.106201171875,
      "learning_rate": 1.7899896800825593e-05,
      "loss": 53.3265,
      "step": 16511
    },
    {
      "epoch": 16.53,
      "grad_norm": 2772.488037109375,
      "learning_rate": 1.7894736842105264e-05,
      "loss": 53.5078,
      "step": 16512
    },
    {
      "epoch": 16.53,
      "grad_norm": 17602.61328125,
      "learning_rate": 1.7889576883384935e-05,
      "loss": 57.9412,
      "step": 16513
    },
    {
      "epoch": 16.53,
      "grad_norm": 11792.7080078125,
      "learning_rate": 1.7884416924664603e-05,
      "loss": 52.6689,
      "step": 16514
    },
    {
      "epoch": 16.53,
      "grad_norm": 4978.154296875,
      "learning_rate": 1.7879256965944274e-05,
      "loss": 56.6745,
      "step": 16515
    },
    {
      "epoch": 16.53,
      "grad_norm": 1133.219970703125,
      "learning_rate": 1.7874097007223945e-05,
      "loss": 48.3157,
      "step": 16516
    },
    {
      "epoch": 16.53,
      "grad_norm": 21297.552734375,
      "learning_rate": 1.7868937048503613e-05,
      "loss": 63.1106,
      "step": 16517
    },
    {
      "epoch": 16.53,
      "grad_norm": 3140.497314453125,
      "learning_rate": 1.786377708978328e-05,
      "loss": 54.7492,
      "step": 16518
    },
    {
      "epoch": 16.54,
      "grad_norm": 6122.8046875,
      "learning_rate": 1.785861713106295e-05,
      "loss": 40.3402,
      "step": 16519
    },
    {
      "epoch": 16.54,
      "grad_norm": 7914.3779296875,
      "learning_rate": 1.7853457172342623e-05,
      "loss": 58.0853,
      "step": 16520
    },
    {
      "epoch": 16.54,
      "grad_norm": 3900.54833984375,
      "learning_rate": 1.784829721362229e-05,
      "loss": 59.1899,
      "step": 16521
    },
    {
      "epoch": 16.54,
      "grad_norm": 4492.02685546875,
      "learning_rate": 1.784313725490196e-05,
      "loss": 51.6784,
      "step": 16522
    },
    {
      "epoch": 16.54,
      "grad_norm": 22686.568359375,
      "learning_rate": 1.7837977296181633e-05,
      "loss": 40.6811,
      "step": 16523
    },
    {
      "epoch": 16.54,
      "grad_norm": 6595.677734375,
      "learning_rate": 1.7832817337461304e-05,
      "loss": 52.9772,
      "step": 16524
    },
    {
      "epoch": 16.54,
      "grad_norm": 108708.203125,
      "learning_rate": 1.782765737874097e-05,
      "loss": 48.1255,
      "step": 16525
    },
    {
      "epoch": 16.54,
      "grad_norm": 1063.7977294921875,
      "learning_rate": 1.782249742002064e-05,
      "loss": 52.6423,
      "step": 16526
    },
    {
      "epoch": 16.54,
      "grad_norm": 4837.73974609375,
      "learning_rate": 1.781733746130031e-05,
      "loss": 60.4606,
      "step": 16527
    },
    {
      "epoch": 16.54,
      "grad_norm": 1116.48095703125,
      "learning_rate": 1.7812177502579978e-05,
      "loss": 56.2147,
      "step": 16528
    },
    {
      "epoch": 16.55,
      "grad_norm": 5556.212890625,
      "learning_rate": 1.780701754385965e-05,
      "loss": 47.2558,
      "step": 16529
    },
    {
      "epoch": 16.55,
      "grad_norm": 1140.258544921875,
      "learning_rate": 1.780185758513932e-05,
      "loss": 55.4123,
      "step": 16530
    },
    {
      "epoch": 16.55,
      "grad_norm": 26105.48046875,
      "learning_rate": 1.779669762641899e-05,
      "loss": 60.1377,
      "step": 16531
    },
    {
      "epoch": 16.55,
      "grad_norm": 2590.088134765625,
      "learning_rate": 1.779153766769866e-05,
      "loss": 60.2541,
      "step": 16532
    },
    {
      "epoch": 16.55,
      "grad_norm": 16349.40625,
      "learning_rate": 1.778637770897833e-05,
      "loss": 41.2148,
      "step": 16533
    },
    {
      "epoch": 16.55,
      "grad_norm": 7728.60107421875,
      "learning_rate": 1.7781217750257998e-05,
      "loss": 53.2009,
      "step": 16534
    },
    {
      "epoch": 16.55,
      "grad_norm": 1828.8148193359375,
      "learning_rate": 1.7776057791537666e-05,
      "loss": 54.4797,
      "step": 16535
    },
    {
      "epoch": 16.55,
      "grad_norm": 3036.97314453125,
      "learning_rate": 1.7770897832817337e-05,
      "loss": 59.7704,
      "step": 16536
    },
    {
      "epoch": 16.55,
      "grad_norm": 17171.6640625,
      "learning_rate": 1.7765737874097008e-05,
      "loss": 55.5878,
      "step": 16537
    },
    {
      "epoch": 16.55,
      "grad_norm": 20358.220703125,
      "learning_rate": 1.776057791537668e-05,
      "loss": 42.4376,
      "step": 16538
    },
    {
      "epoch": 16.56,
      "grad_norm": 30508.470703125,
      "learning_rate": 1.7755417956656347e-05,
      "loss": 47.2608,
      "step": 16539
    },
    {
      "epoch": 16.56,
      "grad_norm": 3824.489013671875,
      "learning_rate": 1.7750257997936018e-05,
      "loss": 51.1819,
      "step": 16540
    },
    {
      "epoch": 16.56,
      "grad_norm": 6432.94384765625,
      "learning_rate": 1.774509803921569e-05,
      "loss": 45.0684,
      "step": 16541
    },
    {
      "epoch": 16.56,
      "grad_norm": 20111.564453125,
      "learning_rate": 1.7739938080495357e-05,
      "loss": 33.7213,
      "step": 16542
    },
    {
      "epoch": 16.56,
      "grad_norm": 3353.974365234375,
      "learning_rate": 1.7734778121775025e-05,
      "loss": 55.3863,
      "step": 16543
    },
    {
      "epoch": 16.56,
      "grad_norm": 39039.28125,
      "learning_rate": 1.7729618163054696e-05,
      "loss": 52.2053,
      "step": 16544
    },
    {
      "epoch": 16.56,
      "grad_norm": 11944.54296875,
      "learning_rate": 1.7724458204334367e-05,
      "loss": 57.5934,
      "step": 16545
    },
    {
      "epoch": 16.56,
      "grad_norm": 2580.172119140625,
      "learning_rate": 1.7719298245614035e-05,
      "loss": 57.4374,
      "step": 16546
    },
    {
      "epoch": 16.56,
      "grad_norm": 25417.04296875,
      "learning_rate": 1.7714138286893706e-05,
      "loss": 32.2934,
      "step": 16547
    },
    {
      "epoch": 16.56,
      "grad_norm": 6898.6240234375,
      "learning_rate": 1.7708978328173377e-05,
      "loss": 52.5667,
      "step": 16548
    },
    {
      "epoch": 16.57,
      "grad_norm": 13318.4619140625,
      "learning_rate": 1.7703818369453045e-05,
      "loss": 54.856,
      "step": 16549
    },
    {
      "epoch": 16.57,
      "grad_norm": 8924.9794921875,
      "learning_rate": 1.7698658410732716e-05,
      "loss": 55.9785,
      "step": 16550
    },
    {
      "epoch": 16.57,
      "grad_norm": 11111.6064453125,
      "learning_rate": 1.7693498452012387e-05,
      "loss": 56.8673,
      "step": 16551
    },
    {
      "epoch": 16.57,
      "grad_norm": 9368.7744140625,
      "learning_rate": 1.7688338493292055e-05,
      "loss": 56.8971,
      "step": 16552
    },
    {
      "epoch": 16.57,
      "grad_norm": 9157.072265625,
      "learning_rate": 1.7683178534571722e-05,
      "loss": 49.193,
      "step": 16553
    },
    {
      "epoch": 16.57,
      "grad_norm": 3143.070556640625,
      "learning_rate": 1.7678018575851394e-05,
      "loss": 53.0965,
      "step": 16554
    },
    {
      "epoch": 16.57,
      "grad_norm": 11077.4453125,
      "learning_rate": 1.7672858617131065e-05,
      "loss": 59.21,
      "step": 16555
    },
    {
      "epoch": 16.57,
      "grad_norm": 56790.26171875,
      "learning_rate": 1.7667698658410732e-05,
      "loss": 39.4497,
      "step": 16556
    },
    {
      "epoch": 16.57,
      "grad_norm": 2505.392333984375,
      "learning_rate": 1.7662538699690404e-05,
      "loss": 57.817,
      "step": 16557
    },
    {
      "epoch": 16.57,
      "grad_norm": 9443.359375,
      "learning_rate": 1.7657378740970075e-05,
      "loss": 44.8186,
      "step": 16558
    },
    {
      "epoch": 16.58,
      "grad_norm": 11758.1748046875,
      "learning_rate": 1.7652218782249742e-05,
      "loss": 52.2031,
      "step": 16559
    },
    {
      "epoch": 16.58,
      "grad_norm": 21463.94921875,
      "learning_rate": 1.7647058823529414e-05,
      "loss": 45.3389,
      "step": 16560
    },
    {
      "epoch": 16.58,
      "grad_norm": 15997.333984375,
      "learning_rate": 1.764189886480908e-05,
      "loss": 58.5601,
      "step": 16561
    },
    {
      "epoch": 16.58,
      "grad_norm": 2501.494140625,
      "learning_rate": 1.7636738906088752e-05,
      "loss": 52.5089,
      "step": 16562
    },
    {
      "epoch": 16.58,
      "grad_norm": 12913.78125,
      "learning_rate": 1.763157894736842e-05,
      "loss": 50.7357,
      "step": 16563
    },
    {
      "epoch": 16.58,
      "grad_norm": 2539.082275390625,
      "learning_rate": 1.762641898864809e-05,
      "loss": 60.0105,
      "step": 16564
    },
    {
      "epoch": 16.58,
      "grad_norm": 18404.591796875,
      "learning_rate": 1.7621259029927762e-05,
      "loss": 58.9845,
      "step": 16565
    },
    {
      "epoch": 16.58,
      "grad_norm": 3931.986083984375,
      "learning_rate": 1.761609907120743e-05,
      "loss": 53.1998,
      "step": 16566
    },
    {
      "epoch": 16.58,
      "grad_norm": 4668.4326171875,
      "learning_rate": 1.76109391124871e-05,
      "loss": 61.2928,
      "step": 16567
    },
    {
      "epoch": 16.58,
      "grad_norm": 5299.70947265625,
      "learning_rate": 1.7605779153766772e-05,
      "loss": 58.8895,
      "step": 16568
    },
    {
      "epoch": 16.59,
      "grad_norm": 28777.017578125,
      "learning_rate": 1.760061919504644e-05,
      "loss": 56.715,
      "step": 16569
    },
    {
      "epoch": 16.59,
      "grad_norm": 2039.5908203125,
      "learning_rate": 1.7595459236326108e-05,
      "loss": 59.907,
      "step": 16570
    },
    {
      "epoch": 16.59,
      "grad_norm": 14977.0595703125,
      "learning_rate": 1.759029927760578e-05,
      "loss": 61.9755,
      "step": 16571
    },
    {
      "epoch": 16.59,
      "grad_norm": 201969.921875,
      "learning_rate": 1.758513931888545e-05,
      "loss": 31.1761,
      "step": 16572
    },
    {
      "epoch": 16.59,
      "grad_norm": 9317.9619140625,
      "learning_rate": 1.7579979360165118e-05,
      "loss": 50.871,
      "step": 16573
    },
    {
      "epoch": 16.59,
      "grad_norm": 15865.1708984375,
      "learning_rate": 1.757481940144479e-05,
      "loss": 60.5631,
      "step": 16574
    },
    {
      "epoch": 16.59,
      "grad_norm": 23192.552734375,
      "learning_rate": 1.756965944272446e-05,
      "loss": 60.5572,
      "step": 16575
    },
    {
      "epoch": 16.59,
      "grad_norm": 5609.0,
      "learning_rate": 1.7564499484004128e-05,
      "loss": 61.0952,
      "step": 16576
    },
    {
      "epoch": 16.59,
      "grad_norm": 8548.9130859375,
      "learning_rate": 1.75593395252838e-05,
      "loss": 56.5149,
      "step": 16577
    },
    {
      "epoch": 16.59,
      "grad_norm": 41294.68359375,
      "learning_rate": 1.755417956656347e-05,
      "loss": 51.6299,
      "step": 16578
    },
    {
      "epoch": 16.6,
      "grad_norm": 14367.267578125,
      "learning_rate": 1.7549019607843138e-05,
      "loss": 57.4026,
      "step": 16579
    },
    {
      "epoch": 16.6,
      "grad_norm": 15807.841796875,
      "learning_rate": 1.7543859649122806e-05,
      "loss": 62.9796,
      "step": 16580
    },
    {
      "epoch": 16.6,
      "grad_norm": 5296.89794921875,
      "learning_rate": 1.7538699690402477e-05,
      "loss": 38.9616,
      "step": 16581
    },
    {
      "epoch": 16.6,
      "grad_norm": 5293.27197265625,
      "learning_rate": 1.7533539731682148e-05,
      "loss": 56.3992,
      "step": 16582
    },
    {
      "epoch": 16.6,
      "grad_norm": 1961.927490234375,
      "learning_rate": 1.7528379772961816e-05,
      "loss": 53.3274,
      "step": 16583
    },
    {
      "epoch": 16.6,
      "grad_norm": 11310.6904296875,
      "learning_rate": 1.7523219814241487e-05,
      "loss": 46.1439,
      "step": 16584
    },
    {
      "epoch": 16.6,
      "grad_norm": 7259.58984375,
      "learning_rate": 1.7518059855521158e-05,
      "loss": 64.2495,
      "step": 16585
    },
    {
      "epoch": 16.6,
      "grad_norm": 3695.520751953125,
      "learning_rate": 1.7512899896800826e-05,
      "loss": 51.2736,
      "step": 16586
    },
    {
      "epoch": 16.6,
      "grad_norm": 10388.83203125,
      "learning_rate": 1.7507739938080497e-05,
      "loss": 53.5394,
      "step": 16587
    },
    {
      "epoch": 16.6,
      "grad_norm": 1985.270263671875,
      "learning_rate": 1.7502579979360165e-05,
      "loss": 57.768,
      "step": 16588
    },
    {
      "epoch": 16.61,
      "grad_norm": 7374.84033203125,
      "learning_rate": 1.7497420020639836e-05,
      "loss": 50.993,
      "step": 16589
    },
    {
      "epoch": 16.61,
      "grad_norm": 2760.13916015625,
      "learning_rate": 1.7492260061919503e-05,
      "loss": 55.2199,
      "step": 16590
    },
    {
      "epoch": 16.61,
      "grad_norm": 2071.005615234375,
      "learning_rate": 1.7487100103199175e-05,
      "loss": 68.258,
      "step": 16591
    },
    {
      "epoch": 16.61,
      "grad_norm": 6222.1796875,
      "learning_rate": 1.7481940144478846e-05,
      "loss": 66.4785,
      "step": 16592
    },
    {
      "epoch": 16.61,
      "grad_norm": 2028.967041015625,
      "learning_rate": 1.7476780185758513e-05,
      "loss": 59.3048,
      "step": 16593
    },
    {
      "epoch": 16.61,
      "grad_norm": 3946.97607421875,
      "learning_rate": 1.7471620227038185e-05,
      "loss": 18.6609,
      "step": 16594
    },
    {
      "epoch": 16.61,
      "grad_norm": 4476.955078125,
      "learning_rate": 1.7466460268317856e-05,
      "loss": 61.063,
      "step": 16595
    },
    {
      "epoch": 16.61,
      "grad_norm": 63778.0625,
      "learning_rate": 1.7461300309597527e-05,
      "loss": 52.1857,
      "step": 16596
    },
    {
      "epoch": 16.61,
      "grad_norm": 993.476806640625,
      "learning_rate": 1.745614035087719e-05,
      "loss": 59.4478,
      "step": 16597
    },
    {
      "epoch": 16.61,
      "grad_norm": 2334.8125,
      "learning_rate": 1.7450980392156862e-05,
      "loss": 53.9183,
      "step": 16598
    },
    {
      "epoch": 16.62,
      "grad_norm": 4337.41015625,
      "learning_rate": 1.7445820433436533e-05,
      "loss": 58.1864,
      "step": 16599
    },
    {
      "epoch": 16.62,
      "grad_norm": 20900.87890625,
      "learning_rate": 1.74406604747162e-05,
      "loss": 54.1173,
      "step": 16600
    },
    {
      "epoch": 16.62,
      "grad_norm": 135971.984375,
      "learning_rate": 1.7435500515995872e-05,
      "loss": 58.233,
      "step": 16601
    },
    {
      "epoch": 16.62,
      "grad_norm": 21194.140625,
      "learning_rate": 1.7430340557275543e-05,
      "loss": 51.947,
      "step": 16602
    },
    {
      "epoch": 16.62,
      "grad_norm": 22246.00390625,
      "learning_rate": 1.7425180598555215e-05,
      "loss": 52.2816,
      "step": 16603
    },
    {
      "epoch": 16.62,
      "grad_norm": 6178.7099609375,
      "learning_rate": 1.7420020639834882e-05,
      "loss": 54.6655,
      "step": 16604
    },
    {
      "epoch": 16.62,
      "grad_norm": 4031.958740234375,
      "learning_rate": 1.7414860681114553e-05,
      "loss": 56.7059,
      "step": 16605
    },
    {
      "epoch": 16.62,
      "grad_norm": 1943.6685791015625,
      "learning_rate": 1.740970072239422e-05,
      "loss": 52.5463,
      "step": 16606
    },
    {
      "epoch": 16.62,
      "grad_norm": 42453.96875,
      "learning_rate": 1.740454076367389e-05,
      "loss": 54.9721,
      "step": 16607
    },
    {
      "epoch": 16.62,
      "grad_norm": 4591.890625,
      "learning_rate": 1.739938080495356e-05,
      "loss": 44.9884,
      "step": 16608
    },
    {
      "epoch": 16.63,
      "grad_norm": 37675.53515625,
      "learning_rate": 1.739422084623323e-05,
      "loss": 53.0217,
      "step": 16609
    },
    {
      "epoch": 16.63,
      "grad_norm": 15930.6162109375,
      "learning_rate": 1.7389060887512902e-05,
      "loss": 48.9005,
      "step": 16610
    },
    {
      "epoch": 16.63,
      "grad_norm": 7818.3984375,
      "learning_rate": 1.738390092879257e-05,
      "loss": 59.4265,
      "step": 16611
    },
    {
      "epoch": 16.63,
      "grad_norm": 1848.435791015625,
      "learning_rate": 1.737874097007224e-05,
      "loss": 55.1313,
      "step": 16612
    },
    {
      "epoch": 16.63,
      "grad_norm": 31350.5078125,
      "learning_rate": 1.7373581011351912e-05,
      "loss": 50.7031,
      "step": 16613
    },
    {
      "epoch": 16.63,
      "grad_norm": 54582.0546875,
      "learning_rate": 1.736842105263158e-05,
      "loss": 50.0831,
      "step": 16614
    },
    {
      "epoch": 16.63,
      "grad_norm": 12497.0732421875,
      "learning_rate": 1.7363261093911248e-05,
      "loss": 54.2717,
      "step": 16615
    },
    {
      "epoch": 16.63,
      "grad_norm": 22259.26953125,
      "learning_rate": 1.735810113519092e-05,
      "loss": 34.956,
      "step": 16616
    },
    {
      "epoch": 16.63,
      "grad_norm": 3294.11328125,
      "learning_rate": 1.735294117647059e-05,
      "loss": 54.8137,
      "step": 16617
    },
    {
      "epoch": 16.63,
      "grad_norm": 2365.86669921875,
      "learning_rate": 1.7347781217750258e-05,
      "loss": 64.6157,
      "step": 16618
    },
    {
      "epoch": 16.64,
      "grad_norm": 4799.08251953125,
      "learning_rate": 1.734262125902993e-05,
      "loss": 59.1144,
      "step": 16619
    },
    {
      "epoch": 16.64,
      "grad_norm": 6685.546875,
      "learning_rate": 1.73374613003096e-05,
      "loss": 55.4866,
      "step": 16620
    },
    {
      "epoch": 16.64,
      "grad_norm": 24607.5625,
      "learning_rate": 1.7332301341589268e-05,
      "loss": 55.5481,
      "step": 16621
    },
    {
      "epoch": 16.64,
      "grad_norm": 17075.810546875,
      "learning_rate": 1.732714138286894e-05,
      "loss": 56.8675,
      "step": 16622
    },
    {
      "epoch": 16.64,
      "grad_norm": 22860.8359375,
      "learning_rate": 1.732198142414861e-05,
      "loss": 58.7422,
      "step": 16623
    },
    {
      "epoch": 16.64,
      "grad_norm": 63707.41015625,
      "learning_rate": 1.7316821465428278e-05,
      "loss": 27.3075,
      "step": 16624
    },
    {
      "epoch": 16.64,
      "grad_norm": 6958.97900390625,
      "learning_rate": 1.7311661506707946e-05,
      "loss": 60.3845,
      "step": 16625
    },
    {
      "epoch": 16.64,
      "grad_norm": 8473.3681640625,
      "learning_rate": 1.7306501547987617e-05,
      "loss": 60.8739,
      "step": 16626
    },
    {
      "epoch": 16.64,
      "grad_norm": 16505.95703125,
      "learning_rate": 1.7301341589267288e-05,
      "loss": 41.5811,
      "step": 16627
    },
    {
      "epoch": 16.64,
      "grad_norm": 15064.8310546875,
      "learning_rate": 1.7296181630546956e-05,
      "loss": 36.9075,
      "step": 16628
    },
    {
      "epoch": 16.65,
      "grad_norm": 4560.67626953125,
      "learning_rate": 1.7291021671826627e-05,
      "loss": 50.2826,
      "step": 16629
    },
    {
      "epoch": 16.65,
      "grad_norm": 5927.3681640625,
      "learning_rate": 1.7285861713106298e-05,
      "loss": 62.9476,
      "step": 16630
    },
    {
      "epoch": 16.65,
      "grad_norm": 6402.2919921875,
      "learning_rate": 1.7280701754385966e-05,
      "loss": 53.4532,
      "step": 16631
    },
    {
      "epoch": 16.65,
      "grad_norm": 8332.9541015625,
      "learning_rate": 1.7275541795665637e-05,
      "loss": 45.2109,
      "step": 16632
    },
    {
      "epoch": 16.65,
      "grad_norm": 3152.0859375,
      "learning_rate": 1.7270381836945304e-05,
      "loss": 55.7631,
      "step": 16633
    },
    {
      "epoch": 16.65,
      "grad_norm": 10019.03125,
      "learning_rate": 1.7265221878224976e-05,
      "loss": 55.9453,
      "step": 16634
    },
    {
      "epoch": 16.65,
      "grad_norm": 1168.8387451171875,
      "learning_rate": 1.7260061919504643e-05,
      "loss": 52.7308,
      "step": 16635
    },
    {
      "epoch": 16.65,
      "grad_norm": 2515.986572265625,
      "learning_rate": 1.7254901960784314e-05,
      "loss": 55.6442,
      "step": 16636
    },
    {
      "epoch": 16.65,
      "grad_norm": 7544.84228515625,
      "learning_rate": 1.7249742002063986e-05,
      "loss": 57.6547,
      "step": 16637
    },
    {
      "epoch": 16.65,
      "grad_norm": 86913.7734375,
      "learning_rate": 1.7244582043343653e-05,
      "loss": 53.6992,
      "step": 16638
    },
    {
      "epoch": 16.66,
      "grad_norm": 2556.369873046875,
      "learning_rate": 1.7239422084623324e-05,
      "loss": 62.0163,
      "step": 16639
    },
    {
      "epoch": 16.66,
      "grad_norm": 12052.78125,
      "learning_rate": 1.7234262125902996e-05,
      "loss": 44.209,
      "step": 16640
    },
    {
      "epoch": 16.66,
      "grad_norm": 3511.677490234375,
      "learning_rate": 1.7229102167182663e-05,
      "loss": 62.7654,
      "step": 16641
    },
    {
      "epoch": 16.66,
      "grad_norm": 6813.439453125,
      "learning_rate": 1.722394220846233e-05,
      "loss": 65.9808,
      "step": 16642
    },
    {
      "epoch": 16.66,
      "grad_norm": 3596.814697265625,
      "learning_rate": 1.7218782249742002e-05,
      "loss": 62.1775,
      "step": 16643
    },
    {
      "epoch": 16.66,
      "grad_norm": 14987.73046875,
      "learning_rate": 1.7213622291021673e-05,
      "loss": 60.4099,
      "step": 16644
    },
    {
      "epoch": 16.66,
      "grad_norm": 86510.4609375,
      "learning_rate": 1.720846233230134e-05,
      "loss": 35.5525,
      "step": 16645
    },
    {
      "epoch": 16.66,
      "grad_norm": 17670.02734375,
      "learning_rate": 1.7203302373581012e-05,
      "loss": 50.5067,
      "step": 16646
    },
    {
      "epoch": 16.66,
      "grad_norm": 8989.1787109375,
      "learning_rate": 1.7198142414860683e-05,
      "loss": 57.6682,
      "step": 16647
    },
    {
      "epoch": 16.66,
      "grad_norm": 707950.0,
      "learning_rate": 1.719298245614035e-05,
      "loss": 60.8758,
      "step": 16648
    },
    {
      "epoch": 16.67,
      "grad_norm": 216123.703125,
      "learning_rate": 1.7187822497420022e-05,
      "loss": 57.2803,
      "step": 16649
    },
    {
      "epoch": 16.67,
      "grad_norm": 3907.291015625,
      "learning_rate": 1.7182662538699693e-05,
      "loss": 62.7948,
      "step": 16650
    },
    {
      "epoch": 16.67,
      "grad_norm": 9085.5966796875,
      "learning_rate": 1.717750257997936e-05,
      "loss": 61.8041,
      "step": 16651
    },
    {
      "epoch": 16.67,
      "grad_norm": 28277.423828125,
      "learning_rate": 1.717234262125903e-05,
      "loss": 56.746,
      "step": 16652
    },
    {
      "epoch": 16.67,
      "grad_norm": 2702.993896484375,
      "learning_rate": 1.71671826625387e-05,
      "loss": 62.3542,
      "step": 16653
    },
    {
      "epoch": 16.67,
      "grad_norm": 50314.3359375,
      "learning_rate": 1.716202270381837e-05,
      "loss": 61.0111,
      "step": 16654
    },
    {
      "epoch": 16.67,
      "grad_norm": 4540.71337890625,
      "learning_rate": 1.715686274509804e-05,
      "loss": 49.5923,
      "step": 16655
    },
    {
      "epoch": 16.67,
      "grad_norm": 8171.48779296875,
      "learning_rate": 1.715170278637771e-05,
      "loss": 59.4443,
      "step": 16656
    },
    {
      "epoch": 16.67,
      "grad_norm": 36775.1640625,
      "learning_rate": 1.714654282765738e-05,
      "loss": 56.285,
      "step": 16657
    },
    {
      "epoch": 16.67,
      "grad_norm": 27582.55859375,
      "learning_rate": 1.714138286893705e-05,
      "loss": 57.3372,
      "step": 16658
    },
    {
      "epoch": 16.68,
      "grad_norm": 6259.34619140625,
      "learning_rate": 1.713622291021672e-05,
      "loss": 56.8464,
      "step": 16659
    },
    {
      "epoch": 16.68,
      "grad_norm": 9085.1875,
      "learning_rate": 1.7131062951496388e-05,
      "loss": 43.9578,
      "step": 16660
    },
    {
      "epoch": 16.68,
      "grad_norm": 10011.01171875,
      "learning_rate": 1.712590299277606e-05,
      "loss": 57.4091,
      "step": 16661
    },
    {
      "epoch": 16.68,
      "grad_norm": 4087.993896484375,
      "learning_rate": 1.7120743034055727e-05,
      "loss": 54.3385,
      "step": 16662
    },
    {
      "epoch": 16.68,
      "grad_norm": 5776.92138671875,
      "learning_rate": 1.7115583075335398e-05,
      "loss": 55.9475,
      "step": 16663
    },
    {
      "epoch": 16.68,
      "grad_norm": 47484.83984375,
      "learning_rate": 1.711042311661507e-05,
      "loss": 32.2825,
      "step": 16664
    },
    {
      "epoch": 16.68,
      "grad_norm": 14730.751953125,
      "learning_rate": 1.7105263157894737e-05,
      "loss": 42.104,
      "step": 16665
    },
    {
      "epoch": 16.68,
      "grad_norm": 3386.83935546875,
      "learning_rate": 1.7100103199174408e-05,
      "loss": 59.9565,
      "step": 16666
    },
    {
      "epoch": 16.68,
      "grad_norm": 1669.0101318359375,
      "learning_rate": 1.709494324045408e-05,
      "loss": 60.7248,
      "step": 16667
    },
    {
      "epoch": 16.68,
      "grad_norm": 7474.99609375,
      "learning_rate": 1.708978328173375e-05,
      "loss": 60.5909,
      "step": 16668
    },
    {
      "epoch": 16.69,
      "grad_norm": 1000.786376953125,
      "learning_rate": 1.7084623323013414e-05,
      "loss": 49.6455,
      "step": 16669
    },
    {
      "epoch": 16.69,
      "grad_norm": 5080.29443359375,
      "learning_rate": 1.7079463364293085e-05,
      "loss": 59.4671,
      "step": 16670
    },
    {
      "epoch": 16.69,
      "grad_norm": 37804.23046875,
      "learning_rate": 1.7074303405572757e-05,
      "loss": 54.905,
      "step": 16671
    },
    {
      "epoch": 16.69,
      "grad_norm": 11282.6513671875,
      "learning_rate": 1.7069143446852424e-05,
      "loss": 39.6175,
      "step": 16672
    },
    {
      "epoch": 16.69,
      "grad_norm": 8635.7421875,
      "learning_rate": 1.7063983488132095e-05,
      "loss": 63.7763,
      "step": 16673
    },
    {
      "epoch": 16.69,
      "grad_norm": 39624.015625,
      "learning_rate": 1.7058823529411767e-05,
      "loss": 52.9566,
      "step": 16674
    },
    {
      "epoch": 16.69,
      "grad_norm": 15617.08984375,
      "learning_rate": 1.7053663570691438e-05,
      "loss": 26.2418,
      "step": 16675
    },
    {
      "epoch": 16.69,
      "grad_norm": 8148.69580078125,
      "learning_rate": 1.7048503611971105e-05,
      "loss": 50.7772,
      "step": 16676
    },
    {
      "epoch": 16.69,
      "grad_norm": 52331.00390625,
      "learning_rate": 1.7043343653250773e-05,
      "loss": 50.8223,
      "step": 16677
    },
    {
      "epoch": 16.69,
      "grad_norm": 2401.47216796875,
      "learning_rate": 1.7038183694530444e-05,
      "loss": 61.5292,
      "step": 16678
    },
    {
      "epoch": 16.7,
      "grad_norm": 17527.03125,
      "learning_rate": 1.7033023735810112e-05,
      "loss": 43.3894,
      "step": 16679
    },
    {
      "epoch": 16.7,
      "grad_norm": 6875.9814453125,
      "learning_rate": 1.7027863777089783e-05,
      "loss": 38.2531,
      "step": 16680
    },
    {
      "epoch": 16.7,
      "grad_norm": 11415.2275390625,
      "learning_rate": 1.7022703818369454e-05,
      "loss": 42.6969,
      "step": 16681
    },
    {
      "epoch": 16.7,
      "grad_norm": 9097.373046875,
      "learning_rate": 1.7017543859649125e-05,
      "loss": 51.0533,
      "step": 16682
    },
    {
      "epoch": 16.7,
      "grad_norm": 7314.193359375,
      "learning_rate": 1.7012383900928793e-05,
      "loss": 53.3563,
      "step": 16683
    },
    {
      "epoch": 16.7,
      "grad_norm": 5235.966796875,
      "learning_rate": 1.7007223942208464e-05,
      "loss": 67.9602,
      "step": 16684
    },
    {
      "epoch": 16.7,
      "grad_norm": 17004.814453125,
      "learning_rate": 1.7002063983488135e-05,
      "loss": 56.7168,
      "step": 16685
    },
    {
      "epoch": 16.7,
      "grad_norm": 4063.1123046875,
      "learning_rate": 1.69969040247678e-05,
      "loss": 60.4855,
      "step": 16686
    },
    {
      "epoch": 16.7,
      "grad_norm": 19046.009765625,
      "learning_rate": 1.699174406604747e-05,
      "loss": 51.7045,
      "step": 16687
    },
    {
      "epoch": 16.7,
      "grad_norm": 12309.5361328125,
      "learning_rate": 1.6986584107327142e-05,
      "loss": 65.1544,
      "step": 16688
    },
    {
      "epoch": 16.71,
      "grad_norm": 11925.9033203125,
      "learning_rate": 1.698142414860681e-05,
      "loss": 42.7297,
      "step": 16689
    },
    {
      "epoch": 16.71,
      "grad_norm": 1481.04443359375,
      "learning_rate": 1.697626418988648e-05,
      "loss": 61.6209,
      "step": 16690
    },
    {
      "epoch": 16.71,
      "grad_norm": 17343.98828125,
      "learning_rate": 1.6971104231166152e-05,
      "loss": 57.848,
      "step": 16691
    },
    {
      "epoch": 16.71,
      "grad_norm": 13211.2822265625,
      "learning_rate": 1.6965944272445823e-05,
      "loss": 43.7199,
      "step": 16692
    },
    {
      "epoch": 16.71,
      "grad_norm": 9012.6435546875,
      "learning_rate": 1.696078431372549e-05,
      "loss": 56.2859,
      "step": 16693
    },
    {
      "epoch": 16.71,
      "grad_norm": 23114.6171875,
      "learning_rate": 1.6955624355005162e-05,
      "loss": 42.7606,
      "step": 16694
    },
    {
      "epoch": 16.71,
      "grad_norm": 8884.423828125,
      "learning_rate": 1.695046439628483e-05,
      "loss": 59.6559,
      "step": 16695
    },
    {
      "epoch": 16.71,
      "grad_norm": 15905.498046875,
      "learning_rate": 1.6945304437564498e-05,
      "loss": 45.1675,
      "step": 16696
    },
    {
      "epoch": 16.71,
      "grad_norm": 28924.654296875,
      "learning_rate": 1.694014447884417e-05,
      "loss": 53.7458,
      "step": 16697
    },
    {
      "epoch": 16.71,
      "grad_norm": 17932.046875,
      "learning_rate": 1.693498452012384e-05,
      "loss": 58.0759,
      "step": 16698
    },
    {
      "epoch": 16.72,
      "grad_norm": 7211.20751953125,
      "learning_rate": 1.692982456140351e-05,
      "loss": 62.4321,
      "step": 16699
    },
    {
      "epoch": 16.72,
      "grad_norm": 5950.44482421875,
      "learning_rate": 1.692466460268318e-05,
      "loss": 54.0906,
      "step": 16700
    },
    {
      "epoch": 16.72,
      "grad_norm": 1622.481201171875,
      "learning_rate": 1.691950464396285e-05,
      "loss": 49.6483,
      "step": 16701
    },
    {
      "epoch": 16.72,
      "grad_norm": 4487.9228515625,
      "learning_rate": 1.691434468524252e-05,
      "loss": 61.3894,
      "step": 16702
    },
    {
      "epoch": 16.72,
      "grad_norm": 3678.57177734375,
      "learning_rate": 1.690918472652219e-05,
      "loss": 53.4545,
      "step": 16703
    },
    {
      "epoch": 16.72,
      "grad_norm": 17518.48828125,
      "learning_rate": 1.6904024767801856e-05,
      "loss": 55.6185,
      "step": 16704
    },
    {
      "epoch": 16.72,
      "grad_norm": 13845.0810546875,
      "learning_rate": 1.6898864809081528e-05,
      "loss": 52.7309,
      "step": 16705
    },
    {
      "epoch": 16.72,
      "grad_norm": 3480.790771484375,
      "learning_rate": 1.68937048503612e-05,
      "loss": 53.1098,
      "step": 16706
    },
    {
      "epoch": 16.72,
      "grad_norm": 1693.987548828125,
      "learning_rate": 1.6888544891640866e-05,
      "loss": 57.1889,
      "step": 16707
    },
    {
      "epoch": 16.72,
      "grad_norm": 18646.421875,
      "learning_rate": 1.6883384932920538e-05,
      "loss": 47.2628,
      "step": 16708
    },
    {
      "epoch": 16.73,
      "grad_norm": 15721.7333984375,
      "learning_rate": 1.687822497420021e-05,
      "loss": 53.1044,
      "step": 16709
    },
    {
      "epoch": 16.73,
      "grad_norm": 11923.6015625,
      "learning_rate": 1.6873065015479876e-05,
      "loss": 55.2361,
      "step": 16710
    },
    {
      "epoch": 16.73,
      "grad_norm": 5435.6796875,
      "learning_rate": 1.6867905056759548e-05,
      "loss": 46.0436,
      "step": 16711
    },
    {
      "epoch": 16.73,
      "grad_norm": 76964.703125,
      "learning_rate": 1.686274509803922e-05,
      "loss": 52.0535,
      "step": 16712
    },
    {
      "epoch": 16.73,
      "grad_norm": 3416.593994140625,
      "learning_rate": 1.6857585139318886e-05,
      "loss": 60.6717,
      "step": 16713
    },
    {
      "epoch": 16.73,
      "grad_norm": 2430.326904296875,
      "learning_rate": 1.6852425180598554e-05,
      "loss": 53.9473,
      "step": 16714
    },
    {
      "epoch": 16.73,
      "grad_norm": 14775.951171875,
      "learning_rate": 1.6847265221878225e-05,
      "loss": 65.3247,
      "step": 16715
    },
    {
      "epoch": 16.73,
      "grad_norm": 2060.46728515625,
      "learning_rate": 1.6842105263157896e-05,
      "loss": 60.8099,
      "step": 16716
    },
    {
      "epoch": 16.73,
      "grad_norm": 43638.15625,
      "learning_rate": 1.6836945304437564e-05,
      "loss": 52.8712,
      "step": 16717
    },
    {
      "epoch": 16.73,
      "grad_norm": 8788.57421875,
      "learning_rate": 1.6831785345717235e-05,
      "loss": 58.2987,
      "step": 16718
    },
    {
      "epoch": 16.74,
      "grad_norm": 25942.892578125,
      "learning_rate": 1.6826625386996906e-05,
      "loss": 57.0076,
      "step": 16719
    },
    {
      "epoch": 16.74,
      "grad_norm": 54327.83984375,
      "learning_rate": 1.6821465428276574e-05,
      "loss": 55.0723,
      "step": 16720
    },
    {
      "epoch": 16.74,
      "grad_norm": 14003.333984375,
      "learning_rate": 1.6816305469556245e-05,
      "loss": 65.1918,
      "step": 16721
    },
    {
      "epoch": 16.74,
      "grad_norm": 9775.130859375,
      "learning_rate": 1.6811145510835913e-05,
      "loss": 70.3228,
      "step": 16722
    },
    {
      "epoch": 16.74,
      "grad_norm": 31999.900390625,
      "learning_rate": 1.6805985552115584e-05,
      "loss": 57.6975,
      "step": 16723
    },
    {
      "epoch": 16.74,
      "grad_norm": 3613.746826171875,
      "learning_rate": 1.6800825593395252e-05,
      "loss": 56.7206,
      "step": 16724
    },
    {
      "epoch": 16.74,
      "grad_norm": 5656.1181640625,
      "learning_rate": 1.6795665634674923e-05,
      "loss": 55.3772,
      "step": 16725
    },
    {
      "epoch": 16.74,
      "grad_norm": 2837.310546875,
      "learning_rate": 1.6790505675954594e-05,
      "loss": 50.0432,
      "step": 16726
    },
    {
      "epoch": 16.74,
      "grad_norm": 29875.4140625,
      "learning_rate": 1.6785345717234262e-05,
      "loss": 38.015,
      "step": 16727
    },
    {
      "epoch": 16.74,
      "grad_norm": 2483.651123046875,
      "learning_rate": 1.6780185758513933e-05,
      "loss": 49.8898,
      "step": 16728
    },
    {
      "epoch": 16.75,
      "grad_norm": 2017.624755859375,
      "learning_rate": 1.6775025799793604e-05,
      "loss": 57.8498,
      "step": 16729
    },
    {
      "epoch": 16.75,
      "grad_norm": 15685.7568359375,
      "learning_rate": 1.6769865841073272e-05,
      "loss": 49.6835,
      "step": 16730
    },
    {
      "epoch": 16.75,
      "grad_norm": 9921.8232421875,
      "learning_rate": 1.676470588235294e-05,
      "loss": 60.1575,
      "step": 16731
    },
    {
      "epoch": 16.75,
      "grad_norm": 7445.65771484375,
      "learning_rate": 1.675954592363261e-05,
      "loss": 56.7369,
      "step": 16732
    },
    {
      "epoch": 16.75,
      "grad_norm": 2535.70556640625,
      "learning_rate": 1.6754385964912282e-05,
      "loss": 65.2009,
      "step": 16733
    },
    {
      "epoch": 16.75,
      "grad_norm": 23287.650390625,
      "learning_rate": 1.674922600619195e-05,
      "loss": 54.2008,
      "step": 16734
    },
    {
      "epoch": 16.75,
      "grad_norm": 27683.142578125,
      "learning_rate": 1.674406604747162e-05,
      "loss": 52.2518,
      "step": 16735
    },
    {
      "epoch": 16.75,
      "grad_norm": 4300.052734375,
      "learning_rate": 1.6738906088751292e-05,
      "loss": 59.22,
      "step": 16736
    },
    {
      "epoch": 16.75,
      "grad_norm": 6483.228515625,
      "learning_rate": 1.673374613003096e-05,
      "loss": 56.2112,
      "step": 16737
    },
    {
      "epoch": 16.75,
      "grad_norm": 15183.9716796875,
      "learning_rate": 1.672858617131063e-05,
      "loss": 46.6073,
      "step": 16738
    },
    {
      "epoch": 16.76,
      "grad_norm": 14877.107421875,
      "learning_rate": 1.6723426212590302e-05,
      "loss": 58.9709,
      "step": 16739
    },
    {
      "epoch": 16.76,
      "grad_norm": 5117.546875,
      "learning_rate": 1.671826625386997e-05,
      "loss": 45.4582,
      "step": 16740
    },
    {
      "epoch": 16.76,
      "grad_norm": 6010.544921875,
      "learning_rate": 1.6713106295149637e-05,
      "loss": 51.4602,
      "step": 16741
    },
    {
      "epoch": 16.76,
      "grad_norm": 16162.2265625,
      "learning_rate": 1.670794633642931e-05,
      "loss": 41.955,
      "step": 16742
    },
    {
      "epoch": 16.76,
      "grad_norm": 15889.7080078125,
      "learning_rate": 1.670278637770898e-05,
      "loss": 59.0422,
      "step": 16743
    },
    {
      "epoch": 16.76,
      "grad_norm": 16461.80859375,
      "learning_rate": 1.6697626418988647e-05,
      "loss": 58.1252,
      "step": 16744
    },
    {
      "epoch": 16.76,
      "grad_norm": 13196.2724609375,
      "learning_rate": 1.669246646026832e-05,
      "loss": 49.5752,
      "step": 16745
    },
    {
      "epoch": 16.76,
      "grad_norm": 10647.3662109375,
      "learning_rate": 1.668730650154799e-05,
      "loss": 54.9657,
      "step": 16746
    },
    {
      "epoch": 16.76,
      "grad_norm": 19840.013671875,
      "learning_rate": 1.6682146542827657e-05,
      "loss": 45.1197,
      "step": 16747
    },
    {
      "epoch": 16.76,
      "grad_norm": 16194.2060546875,
      "learning_rate": 1.667698658410733e-05,
      "loss": 60.5382,
      "step": 16748
    },
    {
      "epoch": 16.77,
      "grad_norm": 2210.43017578125,
      "learning_rate": 1.6671826625386996e-05,
      "loss": 61.0838,
      "step": 16749
    },
    {
      "epoch": 16.77,
      "grad_norm": 16974.646484375,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 45.7347,
      "step": 16750
    },
    {
      "epoch": 16.77,
      "grad_norm": 54987.3671875,
      "learning_rate": 1.6661506707946335e-05,
      "loss": 35.1901,
      "step": 16751
    },
    {
      "epoch": 16.77,
      "grad_norm": 17057.9140625,
      "learning_rate": 1.6656346749226006e-05,
      "loss": 65.3233,
      "step": 16752
    },
    {
      "epoch": 16.77,
      "grad_norm": 17163.6875,
      "learning_rate": 1.6651186790505677e-05,
      "loss": 52.3983,
      "step": 16753
    },
    {
      "epoch": 16.77,
      "grad_norm": 120985.4609375,
      "learning_rate": 1.6646026831785345e-05,
      "loss": 36.885,
      "step": 16754
    },
    {
      "epoch": 16.77,
      "grad_norm": 8837.185546875,
      "learning_rate": 1.6640866873065016e-05,
      "loss": 45.0864,
      "step": 16755
    },
    {
      "epoch": 16.77,
      "grad_norm": 7118.51318359375,
      "learning_rate": 1.6635706914344687e-05,
      "loss": 61.5955,
      "step": 16756
    },
    {
      "epoch": 16.77,
      "grad_norm": 12637.2080078125,
      "learning_rate": 1.663054695562436e-05,
      "loss": 46.3986,
      "step": 16757
    },
    {
      "epoch": 16.77,
      "grad_norm": 6665.0537109375,
      "learning_rate": 1.6625386996904023e-05,
      "loss": 57.0896,
      "step": 16758
    },
    {
      "epoch": 16.78,
      "grad_norm": 2226.1669921875,
      "learning_rate": 1.6620227038183694e-05,
      "loss": 57.8814,
      "step": 16759
    },
    {
      "epoch": 16.78,
      "grad_norm": 9403.248046875,
      "learning_rate": 1.6615067079463365e-05,
      "loss": 59.0021,
      "step": 16760
    },
    {
      "epoch": 16.78,
      "grad_norm": 3448.771728515625,
      "learning_rate": 1.6609907120743033e-05,
      "loss": 53.3932,
      "step": 16761
    },
    {
      "epoch": 16.78,
      "grad_norm": 8690.205078125,
      "learning_rate": 1.6604747162022704e-05,
      "loss": 53.7795,
      "step": 16762
    },
    {
      "epoch": 16.78,
      "grad_norm": 1161.77490234375,
      "learning_rate": 1.6599587203302375e-05,
      "loss": 64.6887,
      "step": 16763
    },
    {
      "epoch": 16.78,
      "grad_norm": 5800.3388671875,
      "learning_rate": 1.6594427244582046e-05,
      "loss": 54.5748,
      "step": 16764
    },
    {
      "epoch": 16.78,
      "grad_norm": 2894.51611328125,
      "learning_rate": 1.6589267285861714e-05,
      "loss": 54.4396,
      "step": 16765
    },
    {
      "epoch": 16.78,
      "grad_norm": 2403.607177734375,
      "learning_rate": 1.6584107327141385e-05,
      "loss": 52.2705,
      "step": 16766
    },
    {
      "epoch": 16.78,
      "grad_norm": 8049.97509765625,
      "learning_rate": 1.6578947368421053e-05,
      "loss": 56.1794,
      "step": 16767
    },
    {
      "epoch": 16.78,
      "grad_norm": 42942.08203125,
      "learning_rate": 1.657378740970072e-05,
      "loss": 50.7039,
      "step": 16768
    },
    {
      "epoch": 16.79,
      "grad_norm": 4575.2353515625,
      "learning_rate": 1.6568627450980392e-05,
      "loss": 47.15,
      "step": 16769
    },
    {
      "epoch": 16.79,
      "grad_norm": 26740.716796875,
      "learning_rate": 1.6563467492260063e-05,
      "loss": 58.1673,
      "step": 16770
    },
    {
      "epoch": 16.79,
      "grad_norm": 516.3544921875,
      "learning_rate": 1.6558307533539734e-05,
      "loss": 57.4217,
      "step": 16771
    },
    {
      "epoch": 16.79,
      "grad_norm": 35760.70703125,
      "learning_rate": 1.6553147574819402e-05,
      "loss": 46.0692,
      "step": 16772
    },
    {
      "epoch": 16.79,
      "grad_norm": 147587.46875,
      "learning_rate": 1.6547987616099073e-05,
      "loss": 58.3753,
      "step": 16773
    },
    {
      "epoch": 16.79,
      "grad_norm": 2001.2901611328125,
      "learning_rate": 1.6542827657378744e-05,
      "loss": 61.7914,
      "step": 16774
    },
    {
      "epoch": 16.79,
      "grad_norm": 66064.5625,
      "learning_rate": 1.6537667698658412e-05,
      "loss": 51.6716,
      "step": 16775
    },
    {
      "epoch": 16.79,
      "grad_norm": 23418.18359375,
      "learning_rate": 1.653250773993808e-05,
      "loss": 54.6212,
      "step": 16776
    },
    {
      "epoch": 16.79,
      "grad_norm": 6101.4013671875,
      "learning_rate": 1.652734778121775e-05,
      "loss": 56.4995,
      "step": 16777
    },
    {
      "epoch": 16.79,
      "grad_norm": 12121.291015625,
      "learning_rate": 1.6522187822497422e-05,
      "loss": 49.4157,
      "step": 16778
    },
    {
      "epoch": 16.8,
      "grad_norm": 3808.611083984375,
      "learning_rate": 1.651702786377709e-05,
      "loss": 57.9132,
      "step": 16779
    },
    {
      "epoch": 16.8,
      "grad_norm": 1584.9202880859375,
      "learning_rate": 1.651186790505676e-05,
      "loss": 59.0473,
      "step": 16780
    },
    {
      "epoch": 16.8,
      "grad_norm": 2558.052734375,
      "learning_rate": 1.6506707946336432e-05,
      "loss": 44.8638,
      "step": 16781
    },
    {
      "epoch": 16.8,
      "grad_norm": 8726.6103515625,
      "learning_rate": 1.65015479876161e-05,
      "loss": 47.7502,
      "step": 16782
    },
    {
      "epoch": 16.8,
      "grad_norm": 19928.17578125,
      "learning_rate": 1.649638802889577e-05,
      "loss": 55.805,
      "step": 16783
    },
    {
      "epoch": 16.8,
      "grad_norm": 37532.92578125,
      "learning_rate": 1.6491228070175442e-05,
      "loss": 55.0533,
      "step": 16784
    },
    {
      "epoch": 16.8,
      "grad_norm": 20258.953125,
      "learning_rate": 1.648606811145511e-05,
      "loss": 55.9674,
      "step": 16785
    },
    {
      "epoch": 16.8,
      "grad_norm": 9510.22265625,
      "learning_rate": 1.6480908152734777e-05,
      "loss": 68.5861,
      "step": 16786
    },
    {
      "epoch": 16.8,
      "grad_norm": 2411.380126953125,
      "learning_rate": 1.647574819401445e-05,
      "loss": 42.7618,
      "step": 16787
    },
    {
      "epoch": 16.8,
      "grad_norm": 8379.4033203125,
      "learning_rate": 1.647058823529412e-05,
      "loss": 53.8801,
      "step": 16788
    },
    {
      "epoch": 16.81,
      "grad_norm": 3785.3525390625,
      "learning_rate": 1.6465428276573787e-05,
      "loss": 55.26,
      "step": 16789
    },
    {
      "epoch": 16.81,
      "grad_norm": 6645.388671875,
      "learning_rate": 1.646026831785346e-05,
      "loss": 58.9188,
      "step": 16790
    },
    {
      "epoch": 16.81,
      "grad_norm": 309012.96875,
      "learning_rate": 1.645510835913313e-05,
      "loss": 44.0863,
      "step": 16791
    },
    {
      "epoch": 16.81,
      "grad_norm": 351572.25,
      "learning_rate": 1.6449948400412797e-05,
      "loss": 39.824,
      "step": 16792
    },
    {
      "epoch": 16.81,
      "grad_norm": 6864.8427734375,
      "learning_rate": 1.644478844169247e-05,
      "loss": 62.3851,
      "step": 16793
    },
    {
      "epoch": 16.81,
      "grad_norm": 20125.900390625,
      "learning_rate": 1.6439628482972136e-05,
      "loss": 55.8753,
      "step": 16794
    },
    {
      "epoch": 16.81,
      "grad_norm": 24815.173828125,
      "learning_rate": 1.6434468524251807e-05,
      "loss": 39.8437,
      "step": 16795
    },
    {
      "epoch": 16.81,
      "grad_norm": 4984.3828125,
      "learning_rate": 1.6429308565531475e-05,
      "loss": 62.0445,
      "step": 16796
    },
    {
      "epoch": 16.81,
      "grad_norm": 11831.310546875,
      "learning_rate": 1.6424148606811146e-05,
      "loss": 50.8892,
      "step": 16797
    },
    {
      "epoch": 16.81,
      "grad_norm": 8236.1337890625,
      "learning_rate": 1.6418988648090817e-05,
      "loss": 46.2892,
      "step": 16798
    },
    {
      "epoch": 16.82,
      "grad_norm": 11110.6484375,
      "learning_rate": 1.6413828689370485e-05,
      "loss": 56.7908,
      "step": 16799
    },
    {
      "epoch": 16.82,
      "grad_norm": 26884.228515625,
      "learning_rate": 1.6408668730650156e-05,
      "loss": 51.967,
      "step": 16800
    },
    {
      "epoch": 16.82,
      "grad_norm": 3339.13916015625,
      "learning_rate": 1.6403508771929827e-05,
      "loss": 55.0124,
      "step": 16801
    },
    {
      "epoch": 16.82,
      "grad_norm": 9029.6416015625,
      "learning_rate": 1.6398348813209495e-05,
      "loss": 51.4897,
      "step": 16802
    },
    {
      "epoch": 16.82,
      "grad_norm": 42188.015625,
      "learning_rate": 1.6393188854489163e-05,
      "loss": 52.4726,
      "step": 16803
    },
    {
      "epoch": 16.82,
      "grad_norm": 2294.60693359375,
      "learning_rate": 1.6388028895768834e-05,
      "loss": 57.5159,
      "step": 16804
    },
    {
      "epoch": 16.82,
      "grad_norm": 30852.955078125,
      "learning_rate": 1.6382868937048505e-05,
      "loss": 52.295,
      "step": 16805
    },
    {
      "epoch": 16.82,
      "grad_norm": 4287.64404296875,
      "learning_rate": 1.6377708978328173e-05,
      "loss": 52.2352,
      "step": 16806
    },
    {
      "epoch": 16.82,
      "grad_norm": 6890.91796875,
      "learning_rate": 1.6372549019607844e-05,
      "loss": 52.8327,
      "step": 16807
    },
    {
      "epoch": 16.82,
      "grad_norm": 16548.310546875,
      "learning_rate": 1.6367389060887515e-05,
      "loss": 58.2889,
      "step": 16808
    },
    {
      "epoch": 16.83,
      "grad_norm": 7126.8798828125,
      "learning_rate": 1.6362229102167183e-05,
      "loss": 55.1777,
      "step": 16809
    },
    {
      "epoch": 16.83,
      "grad_norm": 7010.654296875,
      "learning_rate": 1.6357069143446854e-05,
      "loss": 49.7708,
      "step": 16810
    },
    {
      "epoch": 16.83,
      "grad_norm": 8036.0224609375,
      "learning_rate": 1.6351909184726525e-05,
      "loss": 51.8825,
      "step": 16811
    },
    {
      "epoch": 16.83,
      "grad_norm": 7301.3154296875,
      "learning_rate": 1.6346749226006193e-05,
      "loss": 57.7017,
      "step": 16812
    },
    {
      "epoch": 16.83,
      "grad_norm": 11453.09765625,
      "learning_rate": 1.634158926728586e-05,
      "loss": 52.1685,
      "step": 16813
    },
    {
      "epoch": 16.83,
      "grad_norm": 8386.1103515625,
      "learning_rate": 1.6336429308565532e-05,
      "loss": 47.0459,
      "step": 16814
    },
    {
      "epoch": 16.83,
      "grad_norm": 924.6953125,
      "learning_rate": 1.6331269349845203e-05,
      "loss": 59.3824,
      "step": 16815
    },
    {
      "epoch": 16.83,
      "grad_norm": 1756.44873046875,
      "learning_rate": 1.632610939112487e-05,
      "loss": 49.0721,
      "step": 16816
    },
    {
      "epoch": 16.83,
      "grad_norm": 4592.689453125,
      "learning_rate": 1.6320949432404542e-05,
      "loss": 56.6818,
      "step": 16817
    },
    {
      "epoch": 16.83,
      "grad_norm": 17369.529296875,
      "learning_rate": 1.6315789473684213e-05,
      "loss": 55.477,
      "step": 16818
    },
    {
      "epoch": 16.84,
      "grad_norm": 3905.34619140625,
      "learning_rate": 1.631062951496388e-05,
      "loss": 55.661,
      "step": 16819
    },
    {
      "epoch": 16.84,
      "grad_norm": 17408.86328125,
      "learning_rate": 1.6305469556243552e-05,
      "loss": 54.3692,
      "step": 16820
    },
    {
      "epoch": 16.84,
      "grad_norm": 9608.5048828125,
      "learning_rate": 1.630030959752322e-05,
      "loss": 42.1935,
      "step": 16821
    },
    {
      "epoch": 16.84,
      "grad_norm": 11090.8447265625,
      "learning_rate": 1.629514963880289e-05,
      "loss": 54.5715,
      "step": 16822
    },
    {
      "epoch": 16.84,
      "grad_norm": 2751.36328125,
      "learning_rate": 1.628998968008256e-05,
      "loss": 56.4451,
      "step": 16823
    },
    {
      "epoch": 16.84,
      "grad_norm": 7249.48828125,
      "learning_rate": 1.628482972136223e-05,
      "loss": 63.2011,
      "step": 16824
    },
    {
      "epoch": 16.84,
      "grad_norm": 7035.7158203125,
      "learning_rate": 1.62796697626419e-05,
      "loss": 55.6633,
      "step": 16825
    },
    {
      "epoch": 16.84,
      "grad_norm": 6014.24609375,
      "learning_rate": 1.627450980392157e-05,
      "loss": 57.8081,
      "step": 16826
    },
    {
      "epoch": 16.84,
      "grad_norm": 2525.4990234375,
      "learning_rate": 1.626934984520124e-05,
      "loss": 58.8293,
      "step": 16827
    },
    {
      "epoch": 16.84,
      "grad_norm": 2853.114990234375,
      "learning_rate": 1.626418988648091e-05,
      "loss": 61.9855,
      "step": 16828
    },
    {
      "epoch": 16.85,
      "grad_norm": 6294.97216796875,
      "learning_rate": 1.625902992776058e-05,
      "loss": 58.3736,
      "step": 16829
    },
    {
      "epoch": 16.85,
      "grad_norm": 2220.9775390625,
      "learning_rate": 1.6253869969040246e-05,
      "loss": 49.2902,
      "step": 16830
    },
    {
      "epoch": 16.85,
      "grad_norm": 11467.2001953125,
      "learning_rate": 1.6248710010319917e-05,
      "loss": 58.0668,
      "step": 16831
    },
    {
      "epoch": 16.85,
      "grad_norm": 1230.97705078125,
      "learning_rate": 1.624355005159959e-05,
      "loss": 53.8517,
      "step": 16832
    },
    {
      "epoch": 16.85,
      "grad_norm": 51230.14453125,
      "learning_rate": 1.6238390092879256e-05,
      "loss": 45.4594,
      "step": 16833
    },
    {
      "epoch": 16.85,
      "grad_norm": 3794.346923828125,
      "learning_rate": 1.6233230134158927e-05,
      "loss": 53.4915,
      "step": 16834
    },
    {
      "epoch": 16.85,
      "grad_norm": 5944.73828125,
      "learning_rate": 1.62280701754386e-05,
      "loss": 57.6886,
      "step": 16835
    },
    {
      "epoch": 16.85,
      "grad_norm": 108698.9140625,
      "learning_rate": 1.622291021671827e-05,
      "loss": 25.4405,
      "step": 16836
    },
    {
      "epoch": 16.85,
      "grad_norm": 11228.2607421875,
      "learning_rate": 1.6217750257997937e-05,
      "loss": 56.0242,
      "step": 16837
    },
    {
      "epoch": 16.85,
      "grad_norm": 801.01708984375,
      "learning_rate": 1.6212590299277605e-05,
      "loss": 54.3685,
      "step": 16838
    },
    {
      "epoch": 16.86,
      "grad_norm": 171347.671875,
      "learning_rate": 1.6207430340557276e-05,
      "loss": 46.0133,
      "step": 16839
    },
    {
      "epoch": 16.86,
      "grad_norm": 18722.984375,
      "learning_rate": 1.6202270381836944e-05,
      "loss": 40.2945,
      "step": 16840
    },
    {
      "epoch": 16.86,
      "grad_norm": 27444.828125,
      "learning_rate": 1.6197110423116615e-05,
      "loss": 47.3332,
      "step": 16841
    },
    {
      "epoch": 16.86,
      "grad_norm": 1929.0726318359375,
      "learning_rate": 1.6191950464396286e-05,
      "loss": 61.944,
      "step": 16842
    },
    {
      "epoch": 16.86,
      "grad_norm": 3436.16259765625,
      "learning_rate": 1.6186790505675957e-05,
      "loss": 60.0968,
      "step": 16843
    },
    {
      "epoch": 16.86,
      "grad_norm": 5206.623046875,
      "learning_rate": 1.6181630546955625e-05,
      "loss": 55.0836,
      "step": 16844
    },
    {
      "epoch": 16.86,
      "grad_norm": 8890.4658203125,
      "learning_rate": 1.6176470588235296e-05,
      "loss": 51.7315,
      "step": 16845
    },
    {
      "epoch": 16.86,
      "grad_norm": 1062.22998046875,
      "learning_rate": 1.6171310629514967e-05,
      "loss": 61.827,
      "step": 16846
    },
    {
      "epoch": 16.86,
      "grad_norm": 35041.66015625,
      "learning_rate": 1.616615067079463e-05,
      "loss": 56.4432,
      "step": 16847
    },
    {
      "epoch": 16.86,
      "grad_norm": 13599.876953125,
      "learning_rate": 1.6160990712074303e-05,
      "loss": 64.0897,
      "step": 16848
    },
    {
      "epoch": 16.87,
      "grad_norm": 4673.90966796875,
      "learning_rate": 1.6155830753353974e-05,
      "loss": 52.4246,
      "step": 16849
    },
    {
      "epoch": 16.87,
      "grad_norm": 3325.07861328125,
      "learning_rate": 1.6150670794633645e-05,
      "loss": 57.3324,
      "step": 16850
    },
    {
      "epoch": 16.87,
      "grad_norm": 31561.931640625,
      "learning_rate": 1.6145510835913313e-05,
      "loss": 47.718,
      "step": 16851
    },
    {
      "epoch": 16.87,
      "grad_norm": 1029.8868408203125,
      "learning_rate": 1.6140350877192984e-05,
      "loss": 61.3152,
      "step": 16852
    },
    {
      "epoch": 16.87,
      "grad_norm": 5454.73583984375,
      "learning_rate": 1.6135190918472655e-05,
      "loss": 47.4914,
      "step": 16853
    },
    {
      "epoch": 16.87,
      "grad_norm": 2122.771728515625,
      "learning_rate": 1.6130030959752323e-05,
      "loss": 51.2648,
      "step": 16854
    },
    {
      "epoch": 16.87,
      "grad_norm": 1952.5126953125,
      "learning_rate": 1.6124871001031994e-05,
      "loss": 59.029,
      "step": 16855
    },
    {
      "epoch": 16.87,
      "grad_norm": 15734.3759765625,
      "learning_rate": 1.611971104231166e-05,
      "loss": 52.6154,
      "step": 16856
    },
    {
      "epoch": 16.87,
      "grad_norm": 79604.609375,
      "learning_rate": 1.6114551083591333e-05,
      "loss": 59.5929,
      "step": 16857
    },
    {
      "epoch": 16.87,
      "grad_norm": 5498.2998046875,
      "learning_rate": 1.6109391124871e-05,
      "loss": 57.5079,
      "step": 16858
    },
    {
      "epoch": 16.88,
      "grad_norm": 3854.8046875,
      "learning_rate": 1.610423116615067e-05,
      "loss": 55.6509,
      "step": 16859
    },
    {
      "epoch": 16.88,
      "grad_norm": 28957.984375,
      "learning_rate": 1.6099071207430343e-05,
      "loss": 60.7224,
      "step": 16860
    },
    {
      "epoch": 16.88,
      "grad_norm": 5677.4697265625,
      "learning_rate": 1.609391124871001e-05,
      "loss": 49.9685,
      "step": 16861
    },
    {
      "epoch": 16.88,
      "grad_norm": 6744.2685546875,
      "learning_rate": 1.608875128998968e-05,
      "loss": 52.3238,
      "step": 16862
    },
    {
      "epoch": 16.88,
      "grad_norm": 40344.62109375,
      "learning_rate": 1.6083591331269353e-05,
      "loss": 49.4959,
      "step": 16863
    },
    {
      "epoch": 16.88,
      "grad_norm": 9621.966796875,
      "learning_rate": 1.607843137254902e-05,
      "loss": 65.2246,
      "step": 16864
    },
    {
      "epoch": 16.88,
      "grad_norm": 28257.96484375,
      "learning_rate": 1.6073271413828688e-05,
      "loss": 47.8234,
      "step": 16865
    },
    {
      "epoch": 16.88,
      "grad_norm": 13136.18359375,
      "learning_rate": 1.606811145510836e-05,
      "loss": 55.751,
      "step": 16866
    },
    {
      "epoch": 16.88,
      "grad_norm": 2446.200439453125,
      "learning_rate": 1.606295149638803e-05,
      "loss": 63.138,
      "step": 16867
    },
    {
      "epoch": 16.88,
      "grad_norm": 26794.623046875,
      "learning_rate": 1.6057791537667698e-05,
      "loss": 56.8614,
      "step": 16868
    },
    {
      "epoch": 16.89,
      "grad_norm": 10119.4267578125,
      "learning_rate": 1.605263157894737e-05,
      "loss": 23.4773,
      "step": 16869
    },
    {
      "epoch": 16.89,
      "grad_norm": 5575.6669921875,
      "learning_rate": 1.604747162022704e-05,
      "loss": 59.6559,
      "step": 16870
    },
    {
      "epoch": 16.89,
      "grad_norm": 107688.0625,
      "learning_rate": 1.6042311661506708e-05,
      "loss": 57.8724,
      "step": 16871
    },
    {
      "epoch": 16.89,
      "grad_norm": 2271.423828125,
      "learning_rate": 1.603715170278638e-05,
      "loss": 52.8478,
      "step": 16872
    },
    {
      "epoch": 16.89,
      "grad_norm": 8987.6142578125,
      "learning_rate": 1.603199174406605e-05,
      "loss": 58.9996,
      "step": 16873
    },
    {
      "epoch": 16.89,
      "grad_norm": 11361.162109375,
      "learning_rate": 1.6026831785345718e-05,
      "loss": 65.6301,
      "step": 16874
    },
    {
      "epoch": 16.89,
      "grad_norm": 4101.86962890625,
      "learning_rate": 1.6021671826625386e-05,
      "loss": 48.5881,
      "step": 16875
    },
    {
      "epoch": 16.89,
      "grad_norm": 12756.875,
      "learning_rate": 1.6016511867905057e-05,
      "loss": 59.7605,
      "step": 16876
    },
    {
      "epoch": 16.89,
      "grad_norm": 9064.314453125,
      "learning_rate": 1.6011351909184728e-05,
      "loss": 59.772,
      "step": 16877
    },
    {
      "epoch": 16.89,
      "grad_norm": 43198.0859375,
      "learning_rate": 1.6006191950464396e-05,
      "loss": 49.8054,
      "step": 16878
    },
    {
      "epoch": 16.9,
      "grad_norm": 10295.05859375,
      "learning_rate": 1.6001031991744067e-05,
      "loss": 60.4766,
      "step": 16879
    },
    {
      "epoch": 16.9,
      "grad_norm": 7869.51708984375,
      "learning_rate": 1.5995872033023738e-05,
      "loss": 58.8817,
      "step": 16880
    },
    {
      "epoch": 16.9,
      "grad_norm": 40486.078125,
      "learning_rate": 1.5990712074303406e-05,
      "loss": 50.3172,
      "step": 16881
    },
    {
      "epoch": 16.9,
      "grad_norm": 22265.1328125,
      "learning_rate": 1.5985552115583077e-05,
      "loss": 59.2735,
      "step": 16882
    },
    {
      "epoch": 16.9,
      "grad_norm": 12972.9267578125,
      "learning_rate": 1.5980392156862745e-05,
      "loss": 58.2398,
      "step": 16883
    },
    {
      "epoch": 16.9,
      "grad_norm": 6121.076171875,
      "learning_rate": 1.5975232198142416e-05,
      "loss": 53.5509,
      "step": 16884
    },
    {
      "epoch": 16.9,
      "grad_norm": 4274.60546875,
      "learning_rate": 1.5970072239422084e-05,
      "loss": 53.8572,
      "step": 16885
    },
    {
      "epoch": 16.9,
      "grad_norm": 8508.634765625,
      "learning_rate": 1.5964912280701755e-05,
      "loss": 49.6984,
      "step": 16886
    },
    {
      "epoch": 16.9,
      "grad_norm": 47478.453125,
      "learning_rate": 1.5959752321981426e-05,
      "loss": 44.1012,
      "step": 16887
    },
    {
      "epoch": 16.9,
      "grad_norm": 3414.51025390625,
      "learning_rate": 1.5954592363261094e-05,
      "loss": 53.0957,
      "step": 16888
    },
    {
      "epoch": 16.91,
      "grad_norm": 240198.84375,
      "learning_rate": 1.5949432404540765e-05,
      "loss": 32.7584,
      "step": 16889
    },
    {
      "epoch": 16.91,
      "grad_norm": 40952.62109375,
      "learning_rate": 1.5944272445820436e-05,
      "loss": 58.538,
      "step": 16890
    },
    {
      "epoch": 16.91,
      "grad_norm": 5442.787109375,
      "learning_rate": 1.5939112487100104e-05,
      "loss": 59.2116,
      "step": 16891
    },
    {
      "epoch": 16.91,
      "grad_norm": 2552.711669921875,
      "learning_rate": 1.593395252837977e-05,
      "loss": 61.2351,
      "step": 16892
    },
    {
      "epoch": 16.91,
      "grad_norm": 89180.2890625,
      "learning_rate": 1.5928792569659443e-05,
      "loss": 45.7792,
      "step": 16893
    },
    {
      "epoch": 16.91,
      "grad_norm": 12694.2861328125,
      "learning_rate": 1.5923632610939114e-05,
      "loss": 48.3074,
      "step": 16894
    },
    {
      "epoch": 16.91,
      "grad_norm": 41821.87890625,
      "learning_rate": 1.591847265221878e-05,
      "loss": 63.0734,
      "step": 16895
    },
    {
      "epoch": 16.91,
      "grad_norm": 10011.208984375,
      "learning_rate": 1.5913312693498453e-05,
      "loss": 58.8472,
      "step": 16896
    },
    {
      "epoch": 16.91,
      "grad_norm": 11365.69921875,
      "learning_rate": 1.5908152734778124e-05,
      "loss": 47.3249,
      "step": 16897
    },
    {
      "epoch": 16.91,
      "grad_norm": 22323.564453125,
      "learning_rate": 1.590299277605779e-05,
      "loss": 55.1843,
      "step": 16898
    },
    {
      "epoch": 16.92,
      "grad_norm": 1223.147216796875,
      "learning_rate": 1.5897832817337463e-05,
      "loss": 61.0681,
      "step": 16899
    },
    {
      "epoch": 16.92,
      "grad_norm": 12863.0498046875,
      "learning_rate": 1.5892672858617134e-05,
      "loss": 43.9932,
      "step": 16900
    },
    {
      "epoch": 16.92,
      "grad_norm": 18558.015625,
      "learning_rate": 1.58875128998968e-05,
      "loss": 66.4462,
      "step": 16901
    },
    {
      "epoch": 16.92,
      "grad_norm": 4634.533203125,
      "learning_rate": 1.588235294117647e-05,
      "loss": 52.67,
      "step": 16902
    },
    {
      "epoch": 16.92,
      "grad_norm": 21068.248046875,
      "learning_rate": 1.587719298245614e-05,
      "loss": 61.6992,
      "step": 16903
    },
    {
      "epoch": 16.92,
      "grad_norm": 1242.082275390625,
      "learning_rate": 1.587203302373581e-05,
      "loss": 58.6945,
      "step": 16904
    },
    {
      "epoch": 16.92,
      "grad_norm": 3266.7978515625,
      "learning_rate": 1.586687306501548e-05,
      "loss": 56.1636,
      "step": 16905
    },
    {
      "epoch": 16.92,
      "grad_norm": 8372.841796875,
      "learning_rate": 1.586171310629515e-05,
      "loss": 53.8184,
      "step": 16906
    },
    {
      "epoch": 16.92,
      "grad_norm": 14707.908203125,
      "learning_rate": 1.585655314757482e-05,
      "loss": 64.057,
      "step": 16907
    },
    {
      "epoch": 16.92,
      "grad_norm": 789.1405639648438,
      "learning_rate": 1.585139318885449e-05,
      "loss": 63.6829,
      "step": 16908
    },
    {
      "epoch": 16.93,
      "grad_norm": 4130.71044921875,
      "learning_rate": 1.584623323013416e-05,
      "loss": 60.0154,
      "step": 16909
    },
    {
      "epoch": 16.93,
      "grad_norm": 5627.9169921875,
      "learning_rate": 1.5841073271413828e-05,
      "loss": 61.6192,
      "step": 16910
    },
    {
      "epoch": 16.93,
      "grad_norm": 4870.24658203125,
      "learning_rate": 1.58359133126935e-05,
      "loss": 53.4368,
      "step": 16911
    },
    {
      "epoch": 16.93,
      "grad_norm": 3236.893310546875,
      "learning_rate": 1.5830753353973167e-05,
      "loss": 33.8006,
      "step": 16912
    },
    {
      "epoch": 16.93,
      "grad_norm": 2300.073486328125,
      "learning_rate": 1.5825593395252838e-05,
      "loss": 55.5191,
      "step": 16913
    },
    {
      "epoch": 16.93,
      "grad_norm": 6984.572265625,
      "learning_rate": 1.582043343653251e-05,
      "loss": 48.0934,
      "step": 16914
    },
    {
      "epoch": 16.93,
      "grad_norm": 5469.2578125,
      "learning_rate": 1.5815273477812177e-05,
      "loss": 57.0648,
      "step": 16915
    },
    {
      "epoch": 16.93,
      "grad_norm": 4867.28759765625,
      "learning_rate": 1.5810113519091848e-05,
      "loss": 54.7691,
      "step": 16916
    },
    {
      "epoch": 16.93,
      "grad_norm": 15137.5849609375,
      "learning_rate": 1.580495356037152e-05,
      "loss": 55.3904,
      "step": 16917
    },
    {
      "epoch": 16.93,
      "grad_norm": 12195.4072265625,
      "learning_rate": 1.579979360165119e-05,
      "loss": 63.2593,
      "step": 16918
    },
    {
      "epoch": 16.94,
      "grad_norm": 2328.073974609375,
      "learning_rate": 1.5794633642930855e-05,
      "loss": 49.6107,
      "step": 16919
    },
    {
      "epoch": 16.94,
      "grad_norm": 7345.33544921875,
      "learning_rate": 1.5789473684210526e-05,
      "loss": 60.267,
      "step": 16920
    },
    {
      "epoch": 16.94,
      "grad_norm": 8165.12353515625,
      "learning_rate": 1.5784313725490197e-05,
      "loss": 62.3189,
      "step": 16921
    },
    {
      "epoch": 16.94,
      "grad_norm": 3766.46533203125,
      "learning_rate": 1.5779153766769865e-05,
      "loss": 48.588,
      "step": 16922
    },
    {
      "epoch": 16.94,
      "grad_norm": 23988.015625,
      "learning_rate": 1.5773993808049536e-05,
      "loss": 34.3979,
      "step": 16923
    },
    {
      "epoch": 16.94,
      "grad_norm": 5401.66064453125,
      "learning_rate": 1.5768833849329207e-05,
      "loss": 51.0778,
      "step": 16924
    },
    {
      "epoch": 16.94,
      "grad_norm": 24535.228515625,
      "learning_rate": 1.5763673890608878e-05,
      "loss": 53.2025,
      "step": 16925
    },
    {
      "epoch": 16.94,
      "grad_norm": 1960.9432373046875,
      "learning_rate": 1.5758513931888546e-05,
      "loss": 57.028,
      "step": 16926
    },
    {
      "epoch": 16.94,
      "grad_norm": 14340.087890625,
      "learning_rate": 1.5753353973168217e-05,
      "loss": 54.2395,
      "step": 16927
    },
    {
      "epoch": 16.94,
      "grad_norm": 5777.9892578125,
      "learning_rate": 1.5748194014447885e-05,
      "loss": 62.1234,
      "step": 16928
    },
    {
      "epoch": 16.95,
      "grad_norm": 260912.875,
      "learning_rate": 1.5743034055727552e-05,
      "loss": 26.4134,
      "step": 16929
    },
    {
      "epoch": 16.95,
      "grad_norm": 36355.3828125,
      "learning_rate": 1.5737874097007224e-05,
      "loss": 44.8817,
      "step": 16930
    },
    {
      "epoch": 16.95,
      "grad_norm": 22284.43359375,
      "learning_rate": 1.5732714138286895e-05,
      "loss": 47.9094,
      "step": 16931
    },
    {
      "epoch": 16.95,
      "grad_norm": 14749.2900390625,
      "learning_rate": 1.5727554179566566e-05,
      "loss": 61.7245,
      "step": 16932
    },
    {
      "epoch": 16.95,
      "grad_norm": 1937.641845703125,
      "learning_rate": 1.5722394220846234e-05,
      "loss": 67.18,
      "step": 16933
    },
    {
      "epoch": 16.95,
      "grad_norm": 606838.875,
      "learning_rate": 1.5717234262125905e-05,
      "loss": 57.5816,
      "step": 16934
    },
    {
      "epoch": 16.95,
      "grad_norm": 12081.2734375,
      "learning_rate": 1.5712074303405576e-05,
      "loss": 57.1752,
      "step": 16935
    },
    {
      "epoch": 16.95,
      "grad_norm": 1838.439453125,
      "learning_rate": 1.5706914344685244e-05,
      "loss": 56.042,
      "step": 16936
    },
    {
      "epoch": 16.95,
      "grad_norm": 22747.1171875,
      "learning_rate": 1.570175438596491e-05,
      "loss": 37.5912,
      "step": 16937
    },
    {
      "epoch": 16.95,
      "grad_norm": 8805.544921875,
      "learning_rate": 1.5696594427244582e-05,
      "loss": 53.575,
      "step": 16938
    },
    {
      "epoch": 16.96,
      "grad_norm": 3731.784912109375,
      "learning_rate": 1.5691434468524254e-05,
      "loss": 51.5513,
      "step": 16939
    },
    {
      "epoch": 16.96,
      "grad_norm": 2592.346435546875,
      "learning_rate": 1.568627450980392e-05,
      "loss": 62.6532,
      "step": 16940
    },
    {
      "epoch": 16.96,
      "grad_norm": 4161.48095703125,
      "learning_rate": 1.5681114551083592e-05,
      "loss": 37.0815,
      "step": 16941
    },
    {
      "epoch": 16.96,
      "grad_norm": 2924.925537109375,
      "learning_rate": 1.5675954592363264e-05,
      "loss": 53.73,
      "step": 16942
    },
    {
      "epoch": 16.96,
      "grad_norm": 35709.51953125,
      "learning_rate": 1.567079463364293e-05,
      "loss": 48.7804,
      "step": 16943
    },
    {
      "epoch": 16.96,
      "grad_norm": 4578.8115234375,
      "learning_rate": 1.5665634674922602e-05,
      "loss": 61.0062,
      "step": 16944
    },
    {
      "epoch": 16.96,
      "grad_norm": 29481.52734375,
      "learning_rate": 1.5660474716202274e-05,
      "loss": 62.7285,
      "step": 16945
    },
    {
      "epoch": 16.96,
      "grad_norm": 23418.2109375,
      "learning_rate": 1.565531475748194e-05,
      "loss": 57.4985,
      "step": 16946
    },
    {
      "epoch": 16.96,
      "grad_norm": 31662.044921875,
      "learning_rate": 1.565015479876161e-05,
      "loss": 51.002,
      "step": 16947
    },
    {
      "epoch": 16.96,
      "grad_norm": 4747.82421875,
      "learning_rate": 1.564499484004128e-05,
      "loss": 67.4538,
      "step": 16948
    },
    {
      "epoch": 16.97,
      "grad_norm": 15631.88671875,
      "learning_rate": 1.563983488132095e-05,
      "loss": 56.9961,
      "step": 16949
    },
    {
      "epoch": 16.97,
      "grad_norm": 932.8637084960938,
      "learning_rate": 1.563467492260062e-05,
      "loss": 60.1754,
      "step": 16950
    },
    {
      "epoch": 16.97,
      "grad_norm": 20514.369140625,
      "learning_rate": 1.562951496388029e-05,
      "loss": 41.7423,
      "step": 16951
    },
    {
      "epoch": 16.97,
      "grad_norm": 8553.8857421875,
      "learning_rate": 1.562435500515996e-05,
      "loss": 39.9544,
      "step": 16952
    },
    {
      "epoch": 16.97,
      "grad_norm": 42848.00390625,
      "learning_rate": 1.561919504643963e-05,
      "loss": 52.8682,
      "step": 16953
    },
    {
      "epoch": 16.97,
      "grad_norm": 22809.5859375,
      "learning_rate": 1.56140350877193e-05,
      "loss": 56.6942,
      "step": 16954
    },
    {
      "epoch": 16.97,
      "grad_norm": 7176.69189453125,
      "learning_rate": 1.5608875128998968e-05,
      "loss": 42.9515,
      "step": 16955
    },
    {
      "epoch": 16.97,
      "grad_norm": 26928.833984375,
      "learning_rate": 1.560371517027864e-05,
      "loss": 37.0029,
      "step": 16956
    },
    {
      "epoch": 16.97,
      "grad_norm": 49164.5234375,
      "learning_rate": 1.5598555211558307e-05,
      "loss": 59.3373,
      "step": 16957
    },
    {
      "epoch": 16.97,
      "grad_norm": 2957.8486328125,
      "learning_rate": 1.5593395252837978e-05,
      "loss": 61.1626,
      "step": 16958
    },
    {
      "epoch": 16.98,
      "grad_norm": 1105.686279296875,
      "learning_rate": 1.558823529411765e-05,
      "loss": 61.2365,
      "step": 16959
    },
    {
      "epoch": 16.98,
      "grad_norm": 10737.9375,
      "learning_rate": 1.5583075335397317e-05,
      "loss": 58.1274,
      "step": 16960
    },
    {
      "epoch": 16.98,
      "grad_norm": 6064.78271484375,
      "learning_rate": 1.5577915376676988e-05,
      "loss": 63.6828,
      "step": 16961
    },
    {
      "epoch": 16.98,
      "grad_norm": 4407.8486328125,
      "learning_rate": 1.557275541795666e-05,
      "loss": 60.4126,
      "step": 16962
    },
    {
      "epoch": 16.98,
      "grad_norm": 4864.0869140625,
      "learning_rate": 1.5567595459236327e-05,
      "loss": 62.8011,
      "step": 16963
    },
    {
      "epoch": 16.98,
      "grad_norm": 17839.400390625,
      "learning_rate": 1.5562435500515995e-05,
      "loss": 47.0335,
      "step": 16964
    },
    {
      "epoch": 16.98,
      "grad_norm": 4145.66552734375,
      "learning_rate": 1.5557275541795666e-05,
      "loss": 52.0651,
      "step": 16965
    },
    {
      "epoch": 16.98,
      "grad_norm": 16734.544921875,
      "learning_rate": 1.5552115583075337e-05,
      "loss": 52.7244,
      "step": 16966
    },
    {
      "epoch": 16.98,
      "grad_norm": 3578.446533203125,
      "learning_rate": 1.5546955624355005e-05,
      "loss": 60.5614,
      "step": 16967
    },
    {
      "epoch": 16.98,
      "grad_norm": 2061.683837890625,
      "learning_rate": 1.5541795665634676e-05,
      "loss": 60.6165,
      "step": 16968
    },
    {
      "epoch": 16.99,
      "grad_norm": 28976.173828125,
      "learning_rate": 1.5536635706914347e-05,
      "loss": 50.6108,
      "step": 16969
    },
    {
      "epoch": 16.99,
      "grad_norm": 7277.529296875,
      "learning_rate": 1.5531475748194015e-05,
      "loss": 59.4216,
      "step": 16970
    },
    {
      "epoch": 16.99,
      "grad_norm": 34314.3046875,
      "learning_rate": 1.5526315789473686e-05,
      "loss": 51.4224,
      "step": 16971
    },
    {
      "epoch": 16.99,
      "grad_norm": 25233.673828125,
      "learning_rate": 1.5521155830753353e-05,
      "loss": 33.8356,
      "step": 16972
    },
    {
      "epoch": 16.99,
      "grad_norm": 15655.33203125,
      "learning_rate": 1.5515995872033025e-05,
      "loss": 56.2752,
      "step": 16973
    },
    {
      "epoch": 16.99,
      "grad_norm": 8549.6142578125,
      "learning_rate": 1.5510835913312692e-05,
      "loss": 54.1224,
      "step": 16974
    },
    {
      "epoch": 16.99,
      "grad_norm": 1516.5341796875,
      "learning_rate": 1.5505675954592363e-05,
      "loss": 62.5432,
      "step": 16975
    },
    {
      "epoch": 16.99,
      "grad_norm": 3307.046630859375,
      "learning_rate": 1.5500515995872035e-05,
      "loss": 62.029,
      "step": 16976
    },
    {
      "epoch": 16.99,
      "grad_norm": 20827.40234375,
      "learning_rate": 1.5495356037151702e-05,
      "loss": 59.4405,
      "step": 16977
    },
    {
      "epoch": 16.99,
      "grad_norm": 6442.9873046875,
      "learning_rate": 1.5490196078431373e-05,
      "loss": 65.5569,
      "step": 16978
    },
    {
      "epoch": 17.0,
      "grad_norm": 8872.09375,
      "learning_rate": 1.5485036119711045e-05,
      "loss": 55.9854,
      "step": 16979
    },
    {
      "epoch": 17.0,
      "grad_norm": 68525.7421875,
      "learning_rate": 1.5479876160990712e-05,
      "loss": 44.0849,
      "step": 16980
    },
    {
      "epoch": 17.0,
      "grad_norm": 3640.448486328125,
      "learning_rate": 1.547471620227038e-05,
      "loss": 50.229,
      "step": 16981
    },
    {
      "epoch": 17.0,
      "grad_norm": 5213.68701171875,
      "learning_rate": 1.546955624355005e-05,
      "loss": 54.7405,
      "step": 16982
    },
    {
      "epoch": 17.0,
      "grad_norm": 326458.46875,
      "learning_rate": 1.5464396284829722e-05,
      "loss": 63.4614,
      "step": 16983
    },
    {
      "epoch": 17.0,
      "grad_norm": 13412.537109375,
      "learning_rate": 1.545923632610939e-05,
      "loss": 55.4369,
      "step": 16984
    },
    {
      "epoch": 17.0,
      "grad_norm": 38803.359375,
      "learning_rate": 1.545407636738906e-05,
      "loss": 60.5859,
      "step": 16985
    },
    {
      "epoch": 17.0,
      "grad_norm": 4703.54052734375,
      "learning_rate": 1.5448916408668732e-05,
      "loss": 59.0287,
      "step": 16986
    },
    {
      "epoch": 17.0,
      "grad_norm": 5597.90966796875,
      "learning_rate": 1.54437564499484e-05,
      "loss": 56.3962,
      "step": 16987
    },
    {
      "epoch": 17.01,
      "grad_norm": 4062.507568359375,
      "learning_rate": 1.543859649122807e-05,
      "loss": 63.0441,
      "step": 16988
    },
    {
      "epoch": 17.01,
      "grad_norm": 1429.766845703125,
      "learning_rate": 1.5433436532507742e-05,
      "loss": 54.968,
      "step": 16989
    },
    {
      "epoch": 17.01,
      "grad_norm": 8722.3974609375,
      "learning_rate": 1.542827657378741e-05,
      "loss": 65.6152,
      "step": 16990
    },
    {
      "epoch": 17.01,
      "grad_norm": 28814.912109375,
      "learning_rate": 1.5423116615067078e-05,
      "loss": 66.352,
      "step": 16991
    },
    {
      "epoch": 17.01,
      "grad_norm": 16100.595703125,
      "learning_rate": 1.541795665634675e-05,
      "loss": 56.6811,
      "step": 16992
    },
    {
      "epoch": 17.01,
      "grad_norm": 18487.921875,
      "learning_rate": 1.541279669762642e-05,
      "loss": 50.398,
      "step": 16993
    },
    {
      "epoch": 17.01,
      "grad_norm": 1816.7037353515625,
      "learning_rate": 1.5407636738906088e-05,
      "loss": 60.3911,
      "step": 16994
    },
    {
      "epoch": 17.01,
      "grad_norm": 30904.01953125,
      "learning_rate": 1.540247678018576e-05,
      "loss": 50.6197,
      "step": 16995
    },
    {
      "epoch": 17.01,
      "grad_norm": 4149.6455078125,
      "learning_rate": 1.539731682146543e-05,
      "loss": 57.617,
      "step": 16996
    },
    {
      "epoch": 17.01,
      "grad_norm": 9806.5185546875,
      "learning_rate": 1.53921568627451e-05,
      "loss": 60.9657,
      "step": 16997
    },
    {
      "epoch": 17.02,
      "grad_norm": 15559.447265625,
      "learning_rate": 1.538699690402477e-05,
      "loss": 51.4313,
      "step": 16998
    },
    {
      "epoch": 17.02,
      "grad_norm": 1979.71923828125,
      "learning_rate": 1.5381836945304437e-05,
      "loss": 47.1829,
      "step": 16999
    },
    {
      "epoch": 17.02,
      "grad_norm": 13340.984375,
      "learning_rate": 1.5376676986584108e-05,
      "loss": 60.4322,
      "step": 17000
    },
    {
      "epoch": 17.02,
      "grad_norm": 4002.776123046875,
      "learning_rate": 1.5371517027863776e-05,
      "loss": 58.5217,
      "step": 17001
    },
    {
      "epoch": 17.02,
      "grad_norm": 4215.453125,
      "learning_rate": 1.5366357069143447e-05,
      "loss": 55.1366,
      "step": 17002
    },
    {
      "epoch": 17.02,
      "grad_norm": 7451.0078125,
      "learning_rate": 1.5361197110423118e-05,
      "loss": 45.5758,
      "step": 17003
    },
    {
      "epoch": 17.02,
      "grad_norm": 12854.4384765625,
      "learning_rate": 1.535603715170279e-05,
      "loss": 51.7739,
      "step": 17004
    },
    {
      "epoch": 17.02,
      "grad_norm": 9871.328125,
      "learning_rate": 1.5350877192982457e-05,
      "loss": 35.3227,
      "step": 17005
    },
    {
      "epoch": 17.02,
      "grad_norm": 3417.11376953125,
      "learning_rate": 1.5345717234262128e-05,
      "loss": 56.9398,
      "step": 17006
    },
    {
      "epoch": 17.02,
      "grad_norm": 5237.90869140625,
      "learning_rate": 1.53405572755418e-05,
      "loss": 59.8273,
      "step": 17007
    },
    {
      "epoch": 17.03,
      "grad_norm": 900.0722045898438,
      "learning_rate": 1.5335397316821463e-05,
      "loss": 58.4248,
      "step": 17008
    },
    {
      "epoch": 17.03,
      "grad_norm": 2300.2900390625,
      "learning_rate": 1.5330237358101134e-05,
      "loss": 46.5755,
      "step": 17009
    },
    {
      "epoch": 17.03,
      "grad_norm": 37912.38671875,
      "learning_rate": 1.5325077399380806e-05,
      "loss": 55.6127,
      "step": 17010
    },
    {
      "epoch": 17.03,
      "grad_norm": 4239.51318359375,
      "learning_rate": 1.5319917440660477e-05,
      "loss": 61.4311,
      "step": 17011
    },
    {
      "epoch": 17.03,
      "grad_norm": 3141.072021484375,
      "learning_rate": 1.5314757481940144e-05,
      "loss": 58.0224,
      "step": 17012
    },
    {
      "epoch": 17.03,
      "grad_norm": 962.3505859375,
      "learning_rate": 1.5309597523219816e-05,
      "loss": 58.1324,
      "step": 17013
    },
    {
      "epoch": 17.03,
      "grad_norm": 11797.2109375,
      "learning_rate": 1.5304437564499487e-05,
      "loss": 61.2352,
      "step": 17014
    },
    {
      "epoch": 17.03,
      "grad_norm": 13167.8017578125,
      "learning_rate": 1.5299277605779154e-05,
      "loss": 56.3867,
      "step": 17015
    },
    {
      "epoch": 17.03,
      "grad_norm": 35705.3984375,
      "learning_rate": 1.5294117647058826e-05,
      "loss": 63.8764,
      "step": 17016
    },
    {
      "epoch": 17.03,
      "grad_norm": 1625.527587890625,
      "learning_rate": 1.5288957688338493e-05,
      "loss": 65.6259,
      "step": 17017
    },
    {
      "epoch": 17.04,
      "grad_norm": 118252.359375,
      "learning_rate": 1.5283797729618164e-05,
      "loss": 51.9076,
      "step": 17018
    },
    {
      "epoch": 17.04,
      "grad_norm": 68172.3515625,
      "learning_rate": 1.5278637770897832e-05,
      "loss": 49.724,
      "step": 17019
    },
    {
      "epoch": 17.04,
      "grad_norm": 10059.7216796875,
      "learning_rate": 1.5273477812177503e-05,
      "loss": 58.6978,
      "step": 17020
    },
    {
      "epoch": 17.04,
      "grad_norm": 10124.994140625,
      "learning_rate": 1.5268317853457174e-05,
      "loss": 67.1134,
      "step": 17021
    },
    {
      "epoch": 17.04,
      "grad_norm": 9391.619140625,
      "learning_rate": 1.5263157894736842e-05,
      "loss": 49.7247,
      "step": 17022
    },
    {
      "epoch": 17.04,
      "grad_norm": 3756.799560546875,
      "learning_rate": 1.5257997936016513e-05,
      "loss": 37.8397,
      "step": 17023
    },
    {
      "epoch": 17.04,
      "grad_norm": 7065.36328125,
      "learning_rate": 1.5252837977296183e-05,
      "loss": 54.8656,
      "step": 17024
    },
    {
      "epoch": 17.04,
      "grad_norm": 12951.857421875,
      "learning_rate": 1.5247678018575854e-05,
      "loss": 62.1244,
      "step": 17025
    },
    {
      "epoch": 17.04,
      "grad_norm": 3892.45849609375,
      "learning_rate": 1.524251805985552e-05,
      "loss": 58.0707,
      "step": 17026
    },
    {
      "epoch": 17.04,
      "grad_norm": 6016.57763671875,
      "learning_rate": 1.5237358101135191e-05,
      "loss": 64.1588,
      "step": 17027
    },
    {
      "epoch": 17.05,
      "grad_norm": 58444.03515625,
      "learning_rate": 1.523219814241486e-05,
      "loss": 53.1375,
      "step": 17028
    },
    {
      "epoch": 17.05,
      "grad_norm": 2372.1025390625,
      "learning_rate": 1.5227038183694532e-05,
      "loss": 54.7156,
      "step": 17029
    },
    {
      "epoch": 17.05,
      "grad_norm": 17654.734375,
      "learning_rate": 1.5221878224974201e-05,
      "loss": 56.498,
      "step": 17030
    },
    {
      "epoch": 17.05,
      "grad_norm": 1381.8271484375,
      "learning_rate": 1.521671826625387e-05,
      "loss": 40.4619,
      "step": 17031
    },
    {
      "epoch": 17.05,
      "grad_norm": 448150.28125,
      "learning_rate": 1.5211558307533542e-05,
      "loss": 52.3394,
      "step": 17032
    },
    {
      "epoch": 17.05,
      "grad_norm": 3192.213134765625,
      "learning_rate": 1.5206398348813211e-05,
      "loss": 50.83,
      "step": 17033
    },
    {
      "epoch": 17.05,
      "grad_norm": 8809.5546875,
      "learning_rate": 1.520123839009288e-05,
      "loss": 66.2936,
      "step": 17034
    },
    {
      "epoch": 17.05,
      "grad_norm": 8548.2705078125,
      "learning_rate": 1.5196078431372548e-05,
      "loss": 53.6815,
      "step": 17035
    },
    {
      "epoch": 17.05,
      "grad_norm": 13928.8271484375,
      "learning_rate": 1.519091847265222e-05,
      "loss": 64.4085,
      "step": 17036
    },
    {
      "epoch": 17.05,
      "grad_norm": 33498.98046875,
      "learning_rate": 1.5185758513931889e-05,
      "loss": 50.1309,
      "step": 17037
    },
    {
      "epoch": 17.06,
      "grad_norm": 4075.8251953125,
      "learning_rate": 1.5180598555211558e-05,
      "loss": 51.1141,
      "step": 17038
    },
    {
      "epoch": 17.06,
      "grad_norm": 4287.83740234375,
      "learning_rate": 1.517543859649123e-05,
      "loss": 53.7921,
      "step": 17039
    },
    {
      "epoch": 17.06,
      "grad_norm": 185965.375,
      "learning_rate": 1.5170278637770899e-05,
      "loss": 36.6545,
      "step": 17040
    },
    {
      "epoch": 17.06,
      "grad_norm": 7483.896484375,
      "learning_rate": 1.5165118679050568e-05,
      "loss": 49.4823,
      "step": 17041
    },
    {
      "epoch": 17.06,
      "grad_norm": 90366.7109375,
      "learning_rate": 1.515995872033024e-05,
      "loss": 59.1981,
      "step": 17042
    },
    {
      "epoch": 17.06,
      "grad_norm": 42656.140625,
      "learning_rate": 1.5154798761609909e-05,
      "loss": 41.9949,
      "step": 17043
    },
    {
      "epoch": 17.06,
      "grad_norm": 29959.138671875,
      "learning_rate": 1.5149638802889577e-05,
      "loss": 57.7031,
      "step": 17044
    },
    {
      "epoch": 17.06,
      "grad_norm": 2133.036865234375,
      "learning_rate": 1.5144478844169246e-05,
      "loss": 62.5757,
      "step": 17045
    },
    {
      "epoch": 17.06,
      "grad_norm": 10466.451171875,
      "learning_rate": 1.5139318885448917e-05,
      "loss": 60.683,
      "step": 17046
    },
    {
      "epoch": 17.06,
      "grad_norm": 7120.8955078125,
      "learning_rate": 1.5134158926728587e-05,
      "loss": 52.7299,
      "step": 17047
    },
    {
      "epoch": 17.07,
      "grad_norm": 14908.025390625,
      "learning_rate": 1.5128998968008256e-05,
      "loss": 53.8004,
      "step": 17048
    },
    {
      "epoch": 17.07,
      "grad_norm": 5640.28466796875,
      "learning_rate": 1.5123839009287927e-05,
      "loss": 52.8391,
      "step": 17049
    },
    {
      "epoch": 17.07,
      "grad_norm": 2146.959228515625,
      "learning_rate": 1.5118679050567597e-05,
      "loss": 61.5732,
      "step": 17050
    },
    {
      "epoch": 17.07,
      "grad_norm": 6246.91357421875,
      "learning_rate": 1.5113519091847268e-05,
      "loss": 58.5537,
      "step": 17051
    },
    {
      "epoch": 17.07,
      "grad_norm": 17434.19140625,
      "learning_rate": 1.5108359133126937e-05,
      "loss": 65.5576,
      "step": 17052
    },
    {
      "epoch": 17.07,
      "grad_norm": 10074.072265625,
      "learning_rate": 1.5103199174406605e-05,
      "loss": 60.7872,
      "step": 17053
    },
    {
      "epoch": 17.07,
      "grad_norm": 3736.07568359375,
      "learning_rate": 1.5098039215686274e-05,
      "loss": 65.2223,
      "step": 17054
    },
    {
      "epoch": 17.07,
      "grad_norm": 3896.41796875,
      "learning_rate": 1.5092879256965944e-05,
      "loss": 54.6286,
      "step": 17055
    },
    {
      "epoch": 17.07,
      "grad_norm": 30167.69140625,
      "learning_rate": 1.5087719298245615e-05,
      "loss": 59.9022,
      "step": 17056
    },
    {
      "epoch": 17.07,
      "grad_norm": 40286.44921875,
      "learning_rate": 1.5082559339525284e-05,
      "loss": 58.4436,
      "step": 17057
    },
    {
      "epoch": 17.08,
      "grad_norm": 23451.431640625,
      "learning_rate": 1.5077399380804955e-05,
      "loss": 59.6868,
      "step": 17058
    },
    {
      "epoch": 17.08,
      "grad_norm": 6783.94287109375,
      "learning_rate": 1.5072239422084625e-05,
      "loss": 50.4282,
      "step": 17059
    },
    {
      "epoch": 17.08,
      "grad_norm": 3063.18408203125,
      "learning_rate": 1.5067079463364294e-05,
      "loss": 58.0067,
      "step": 17060
    },
    {
      "epoch": 17.08,
      "grad_norm": 6696.81591796875,
      "learning_rate": 1.5061919504643965e-05,
      "loss": 68.6234,
      "step": 17061
    },
    {
      "epoch": 17.08,
      "grad_norm": 3675.8740234375,
      "learning_rate": 1.5056759545923631e-05,
      "loss": 53.7466,
      "step": 17062
    },
    {
      "epoch": 17.08,
      "grad_norm": 3707.588134765625,
      "learning_rate": 1.5051599587203303e-05,
      "loss": 49.4266,
      "step": 17063
    },
    {
      "epoch": 17.08,
      "grad_norm": 12347.638671875,
      "learning_rate": 1.5046439628482972e-05,
      "loss": 58.9335,
      "step": 17064
    },
    {
      "epoch": 17.08,
      "grad_norm": 2782.03125,
      "learning_rate": 1.5041279669762643e-05,
      "loss": 60.1129,
      "step": 17065
    },
    {
      "epoch": 17.08,
      "grad_norm": 50010.6796875,
      "learning_rate": 1.5036119711042313e-05,
      "loss": 46.9534,
      "step": 17066
    },
    {
      "epoch": 17.08,
      "grad_norm": 2935974.5,
      "learning_rate": 1.5030959752321982e-05,
      "loss": 54.687,
      "step": 17067
    },
    {
      "epoch": 17.09,
      "grad_norm": 4881.75341796875,
      "learning_rate": 1.5025799793601653e-05,
      "loss": 47.6366,
      "step": 17068
    },
    {
      "epoch": 17.09,
      "grad_norm": 3786.559814453125,
      "learning_rate": 1.5020639834881323e-05,
      "loss": 68.4399,
      "step": 17069
    },
    {
      "epoch": 17.09,
      "grad_norm": 6694.955078125,
      "learning_rate": 1.5015479876160992e-05,
      "loss": 26.2849,
      "step": 17070
    },
    {
      "epoch": 17.09,
      "grad_norm": 6122.5927734375,
      "learning_rate": 1.501031991744066e-05,
      "loss": 48.6083,
      "step": 17071
    },
    {
      "epoch": 17.09,
      "grad_norm": 46338.78125,
      "learning_rate": 1.500515995872033e-05,
      "loss": 50.8908,
      "step": 17072
    },
    {
      "epoch": 17.09,
      "grad_norm": 11995.4306640625,
      "learning_rate": 1.5e-05,
      "loss": 48.2163,
      "step": 17073
    },
    {
      "epoch": 17.09,
      "grad_norm": 3733.489013671875,
      "learning_rate": 1.499484004127967e-05,
      "loss": 45.0836,
      "step": 17074
    },
    {
      "epoch": 17.09,
      "grad_norm": 38898.30078125,
      "learning_rate": 1.4989680082559341e-05,
      "loss": 57.0758,
      "step": 17075
    },
    {
      "epoch": 17.09,
      "grad_norm": 7398.14794921875,
      "learning_rate": 1.498452012383901e-05,
      "loss": 60.8414,
      "step": 17076
    },
    {
      "epoch": 17.09,
      "grad_norm": 3608.220703125,
      "learning_rate": 1.497936016511868e-05,
      "loss": 58.5889,
      "step": 17077
    },
    {
      "epoch": 17.1,
      "grad_norm": 14118.724609375,
      "learning_rate": 1.4974200206398351e-05,
      "loss": 30.5738,
      "step": 17078
    },
    {
      "epoch": 17.1,
      "grad_norm": 9951.08203125,
      "learning_rate": 1.496904024767802e-05,
      "loss": 68.2359,
      "step": 17079
    },
    {
      "epoch": 17.1,
      "grad_norm": 2444.092041015625,
      "learning_rate": 1.4963880288957688e-05,
      "loss": 57.6985,
      "step": 17080
    },
    {
      "epoch": 17.1,
      "grad_norm": 2848.956787109375,
      "learning_rate": 1.4958720330237358e-05,
      "loss": 54.6321,
      "step": 17081
    },
    {
      "epoch": 17.1,
      "grad_norm": 2104.640625,
      "learning_rate": 1.4953560371517029e-05,
      "loss": 57.7124,
      "step": 17082
    },
    {
      "epoch": 17.1,
      "grad_norm": 3381.831787109375,
      "learning_rate": 1.4948400412796698e-05,
      "loss": 54.6117,
      "step": 17083
    },
    {
      "epoch": 17.1,
      "grad_norm": 6706.5849609375,
      "learning_rate": 1.4943240454076368e-05,
      "loss": 48.6862,
      "step": 17084
    },
    {
      "epoch": 17.1,
      "grad_norm": 31288.689453125,
      "learning_rate": 1.4938080495356039e-05,
      "loss": 57.6911,
      "step": 17085
    },
    {
      "epoch": 17.1,
      "grad_norm": 3941.69677734375,
      "learning_rate": 1.4932920536635708e-05,
      "loss": 56.5667,
      "step": 17086
    },
    {
      "epoch": 17.1,
      "grad_norm": 10866.0224609375,
      "learning_rate": 1.4927760577915378e-05,
      "loss": 46.838,
      "step": 17087
    },
    {
      "epoch": 17.11,
      "grad_norm": 12198.947265625,
      "learning_rate": 1.4922600619195049e-05,
      "loss": 59.1603,
      "step": 17088
    },
    {
      "epoch": 17.11,
      "grad_norm": 13478.546875,
      "learning_rate": 1.4917440660474716e-05,
      "loss": 59.0784,
      "step": 17089
    },
    {
      "epoch": 17.11,
      "grad_norm": 73115.421875,
      "learning_rate": 1.4912280701754386e-05,
      "loss": 49.1455,
      "step": 17090
    },
    {
      "epoch": 17.11,
      "grad_norm": 5637.9619140625,
      "learning_rate": 1.4907120743034055e-05,
      "loss": 37.5253,
      "step": 17091
    },
    {
      "epoch": 17.11,
      "grad_norm": 23851.33203125,
      "learning_rate": 1.4901960784313726e-05,
      "loss": 57.3011,
      "step": 17092
    },
    {
      "epoch": 17.11,
      "grad_norm": 13266.4658203125,
      "learning_rate": 1.4896800825593396e-05,
      "loss": 59.5527,
      "step": 17093
    },
    {
      "epoch": 17.11,
      "grad_norm": 9835.1650390625,
      "learning_rate": 1.4891640866873065e-05,
      "loss": 57.0538,
      "step": 17094
    },
    {
      "epoch": 17.11,
      "grad_norm": 7614.845703125,
      "learning_rate": 1.4886480908152736e-05,
      "loss": 49.738,
      "step": 17095
    },
    {
      "epoch": 17.11,
      "grad_norm": 8761.0478515625,
      "learning_rate": 1.4881320949432406e-05,
      "loss": 64.5628,
      "step": 17096
    },
    {
      "epoch": 17.11,
      "grad_norm": 11334.2041015625,
      "learning_rate": 1.4876160990712077e-05,
      "loss": 47.6422,
      "step": 17097
    },
    {
      "epoch": 17.12,
      "grad_norm": 4624.33740234375,
      "learning_rate": 1.4871001031991743e-05,
      "loss": 59.0577,
      "step": 17098
    },
    {
      "epoch": 17.12,
      "grad_norm": 12959.986328125,
      "learning_rate": 1.4865841073271414e-05,
      "loss": 52.8401,
      "step": 17099
    },
    {
      "epoch": 17.12,
      "grad_norm": 2324.540283203125,
      "learning_rate": 1.4860681114551084e-05,
      "loss": 45.5441,
      "step": 17100
    },
    {
      "epoch": 17.12,
      "grad_norm": 8440.6396484375,
      "learning_rate": 1.4855521155830753e-05,
      "loss": 52.3604,
      "step": 17101
    },
    {
      "epoch": 17.12,
      "grad_norm": 2433.28466796875,
      "learning_rate": 1.4850361197110424e-05,
      "loss": 66.7468,
      "step": 17102
    },
    {
      "epoch": 17.12,
      "grad_norm": 973.2366943359375,
      "learning_rate": 1.4845201238390094e-05,
      "loss": 48.0556,
      "step": 17103
    },
    {
      "epoch": 17.12,
      "grad_norm": 18891.23828125,
      "learning_rate": 1.4840041279669765e-05,
      "loss": 59.315,
      "step": 17104
    },
    {
      "epoch": 17.12,
      "grad_norm": 7223.1533203125,
      "learning_rate": 1.4834881320949434e-05,
      "loss": 64.3381,
      "step": 17105
    },
    {
      "epoch": 17.12,
      "grad_norm": 4369.2890625,
      "learning_rate": 1.4829721362229104e-05,
      "loss": 58.402,
      "step": 17106
    },
    {
      "epoch": 17.12,
      "grad_norm": 5859.18505859375,
      "learning_rate": 1.4824561403508771e-05,
      "loss": 63.5058,
      "step": 17107
    },
    {
      "epoch": 17.13,
      "grad_norm": 11893.7109375,
      "learning_rate": 1.481940144478844e-05,
      "loss": 55.8292,
      "step": 17108
    },
    {
      "epoch": 17.13,
      "grad_norm": 3929.79541015625,
      "learning_rate": 1.4814241486068112e-05,
      "loss": 53.4944,
      "step": 17109
    },
    {
      "epoch": 17.13,
      "grad_norm": 29191.1875,
      "learning_rate": 1.4809081527347781e-05,
      "loss": 53.9579,
      "step": 17110
    },
    {
      "epoch": 17.13,
      "grad_norm": 49926.984375,
      "learning_rate": 1.4803921568627453e-05,
      "loss": 51.8818,
      "step": 17111
    },
    {
      "epoch": 17.13,
      "grad_norm": 5821.390625,
      "learning_rate": 1.4798761609907122e-05,
      "loss": 62.1258,
      "step": 17112
    },
    {
      "epoch": 17.13,
      "grad_norm": 2636.05224609375,
      "learning_rate": 1.4793601651186791e-05,
      "loss": 53.1727,
      "step": 17113
    },
    {
      "epoch": 17.13,
      "grad_norm": 71441.25,
      "learning_rate": 1.4788441692466463e-05,
      "loss": 63.3314,
      "step": 17114
    },
    {
      "epoch": 17.13,
      "grad_norm": 22734.51953125,
      "learning_rate": 1.4783281733746132e-05,
      "loss": 43.9679,
      "step": 17115
    },
    {
      "epoch": 17.13,
      "grad_norm": 7656.64453125,
      "learning_rate": 1.47781217750258e-05,
      "loss": 60.9544,
      "step": 17116
    },
    {
      "epoch": 17.13,
      "grad_norm": 8449.470703125,
      "learning_rate": 1.4772961816305469e-05,
      "loss": 51.8069,
      "step": 17117
    },
    {
      "epoch": 17.14,
      "grad_norm": 2237.263916015625,
      "learning_rate": 1.476780185758514e-05,
      "loss": 58.0437,
      "step": 17118
    },
    {
      "epoch": 17.14,
      "grad_norm": 1149.9488525390625,
      "learning_rate": 1.476264189886481e-05,
      "loss": 63.5621,
      "step": 17119
    },
    {
      "epoch": 17.14,
      "grad_norm": 11163.0458984375,
      "learning_rate": 1.4757481940144479e-05,
      "loss": 58.8151,
      "step": 17120
    },
    {
      "epoch": 17.14,
      "grad_norm": 42235.46484375,
      "learning_rate": 1.475232198142415e-05,
      "loss": 53.1682,
      "step": 17121
    },
    {
      "epoch": 17.14,
      "grad_norm": 3159.279541015625,
      "learning_rate": 1.474716202270382e-05,
      "loss": 63.9499,
      "step": 17122
    },
    {
      "epoch": 17.14,
      "grad_norm": 21624.59375,
      "learning_rate": 1.4742002063983489e-05,
      "loss": 47.1726,
      "step": 17123
    },
    {
      "epoch": 17.14,
      "grad_norm": 4664.82861328125,
      "learning_rate": 1.4736842105263157e-05,
      "loss": 51.4578,
      "step": 17124
    },
    {
      "epoch": 17.14,
      "grad_norm": 1881.4561767578125,
      "learning_rate": 1.4731682146542828e-05,
      "loss": 57.3147,
      "step": 17125
    },
    {
      "epoch": 17.14,
      "grad_norm": 9999.5283203125,
      "learning_rate": 1.4726522187822497e-05,
      "loss": 51.923,
      "step": 17126
    },
    {
      "epoch": 17.14,
      "grad_norm": 87199.71875,
      "learning_rate": 1.4721362229102167e-05,
      "loss": 53.5452,
      "step": 17127
    },
    {
      "epoch": 17.15,
      "grad_norm": 2832.209716796875,
      "learning_rate": 1.4716202270381838e-05,
      "loss": 68.683,
      "step": 17128
    },
    {
      "epoch": 17.15,
      "grad_norm": 8118.8740234375,
      "learning_rate": 1.4711042311661507e-05,
      "loss": 51.8789,
      "step": 17129
    },
    {
      "epoch": 17.15,
      "grad_norm": 11954.10546875,
      "learning_rate": 1.4705882352941177e-05,
      "loss": 59.7542,
      "step": 17130
    },
    {
      "epoch": 17.15,
      "grad_norm": 39323.72265625,
      "learning_rate": 1.4700722394220848e-05,
      "loss": 54.3806,
      "step": 17131
    },
    {
      "epoch": 17.15,
      "grad_norm": 13119.341796875,
      "learning_rate": 1.4695562435500517e-05,
      "loss": 46.6975,
      "step": 17132
    },
    {
      "epoch": 17.15,
      "grad_norm": 1637.2550048828125,
      "learning_rate": 1.4690402476780185e-05,
      "loss": 61.7227,
      "step": 17133
    },
    {
      "epoch": 17.15,
      "grad_norm": 1721.6904296875,
      "learning_rate": 1.4685242518059855e-05,
      "loss": 36.3333,
      "step": 17134
    },
    {
      "epoch": 17.15,
      "grad_norm": 8778.0654296875,
      "learning_rate": 1.4680082559339526e-05,
      "loss": 56.249,
      "step": 17135
    },
    {
      "epoch": 17.15,
      "grad_norm": 11711.37109375,
      "learning_rate": 1.4674922600619195e-05,
      "loss": 54.1417,
      "step": 17136
    },
    {
      "epoch": 17.15,
      "grad_norm": 5426.75048828125,
      "learning_rate": 1.4669762641898865e-05,
      "loss": 59.2282,
      "step": 17137
    },
    {
      "epoch": 17.16,
      "grad_norm": 14855.86328125,
      "learning_rate": 1.4664602683178536e-05,
      "loss": 47.7225,
      "step": 17138
    },
    {
      "epoch": 17.16,
      "grad_norm": 6642.18310546875,
      "learning_rate": 1.4659442724458205e-05,
      "loss": 53.6493,
      "step": 17139
    },
    {
      "epoch": 17.16,
      "grad_norm": 5480.9208984375,
      "learning_rate": 1.4654282765737876e-05,
      "loss": 43.5383,
      "step": 17140
    },
    {
      "epoch": 17.16,
      "grad_norm": 2444.408447265625,
      "learning_rate": 1.4649122807017546e-05,
      "loss": 59.5896,
      "step": 17141
    },
    {
      "epoch": 17.16,
      "grad_norm": 3374.3369140625,
      "learning_rate": 1.4643962848297213e-05,
      "loss": 56.841,
      "step": 17142
    },
    {
      "epoch": 17.16,
      "grad_norm": 13133.337890625,
      "learning_rate": 1.4638802889576883e-05,
      "loss": 52.3893,
      "step": 17143
    },
    {
      "epoch": 17.16,
      "grad_norm": 7211.35791015625,
      "learning_rate": 1.4633642930856552e-05,
      "loss": 53.8357,
      "step": 17144
    },
    {
      "epoch": 17.16,
      "grad_norm": 1315.70849609375,
      "learning_rate": 1.4628482972136224e-05,
      "loss": 62.4851,
      "step": 17145
    },
    {
      "epoch": 17.16,
      "grad_norm": 17909.435546875,
      "learning_rate": 1.4623323013415893e-05,
      "loss": 53.7333,
      "step": 17146
    },
    {
      "epoch": 17.16,
      "grad_norm": 5613.02685546875,
      "learning_rate": 1.4618163054695564e-05,
      "loss": 60.0041,
      "step": 17147
    },
    {
      "epoch": 17.17,
      "grad_norm": 9533.865234375,
      "learning_rate": 1.4613003095975234e-05,
      "loss": 62.4538,
      "step": 17148
    },
    {
      "epoch": 17.17,
      "grad_norm": 31837.546875,
      "learning_rate": 1.4607843137254903e-05,
      "loss": 41.5201,
      "step": 17149
    },
    {
      "epoch": 17.17,
      "grad_norm": 9390.4833984375,
      "learning_rate": 1.4602683178534574e-05,
      "loss": 56.2653,
      "step": 17150
    },
    {
      "epoch": 17.17,
      "grad_norm": 3434.53466796875,
      "learning_rate": 1.459752321981424e-05,
      "loss": 54.0905,
      "step": 17151
    },
    {
      "epoch": 17.17,
      "grad_norm": 2816.046630859375,
      "learning_rate": 1.4592363261093911e-05,
      "loss": 51.2493,
      "step": 17152
    },
    {
      "epoch": 17.17,
      "grad_norm": 28971.837890625,
      "learning_rate": 1.458720330237358e-05,
      "loss": 59.6118,
      "step": 17153
    },
    {
      "epoch": 17.17,
      "grad_norm": 2600.79541015625,
      "learning_rate": 1.4582043343653252e-05,
      "loss": 55.5511,
      "step": 17154
    },
    {
      "epoch": 17.17,
      "grad_norm": 2974.8232421875,
      "learning_rate": 1.4576883384932921e-05,
      "loss": 59.7522,
      "step": 17155
    },
    {
      "epoch": 17.17,
      "grad_norm": 16921.486328125,
      "learning_rate": 1.457172342621259e-05,
      "loss": 18.8605,
      "step": 17156
    },
    {
      "epoch": 17.17,
      "grad_norm": 16316.8837890625,
      "learning_rate": 1.4566563467492262e-05,
      "loss": 45.0123,
      "step": 17157
    },
    {
      "epoch": 17.18,
      "grad_norm": 42177.03515625,
      "learning_rate": 1.4561403508771931e-05,
      "loss": 40.7787,
      "step": 17158
    },
    {
      "epoch": 17.18,
      "grad_norm": 14687.384765625,
      "learning_rate": 1.45562435500516e-05,
      "loss": 56.283,
      "step": 17159
    },
    {
      "epoch": 17.18,
      "grad_norm": 27787.015625,
      "learning_rate": 1.4551083591331268e-05,
      "loss": 49.0396,
      "step": 17160
    },
    {
      "epoch": 17.18,
      "grad_norm": 12635.935546875,
      "learning_rate": 1.454592363261094e-05,
      "loss": 66.4979,
      "step": 17161
    },
    {
      "epoch": 17.18,
      "grad_norm": 7643.3662109375,
      "learning_rate": 1.4540763673890609e-05,
      "loss": 54.0139,
      "step": 17162
    },
    {
      "epoch": 17.18,
      "grad_norm": 8841.7216796875,
      "learning_rate": 1.4535603715170278e-05,
      "loss": 57.4971,
      "step": 17163
    },
    {
      "epoch": 17.18,
      "grad_norm": 1832.276123046875,
      "learning_rate": 1.453044375644995e-05,
      "loss": 65.3904,
      "step": 17164
    },
    {
      "epoch": 17.18,
      "grad_norm": 3599.98046875,
      "learning_rate": 1.4525283797729619e-05,
      "loss": 50.1224,
      "step": 17165
    },
    {
      "epoch": 17.18,
      "grad_norm": 13859.6064453125,
      "learning_rate": 1.4520123839009288e-05,
      "loss": 59.3869,
      "step": 17166
    },
    {
      "epoch": 17.18,
      "grad_norm": 3373.7021484375,
      "learning_rate": 1.451496388028896e-05,
      "loss": 65.7938,
      "step": 17167
    },
    {
      "epoch": 17.19,
      "grad_norm": 2517.81591796875,
      "learning_rate": 1.4509803921568629e-05,
      "loss": 62.4102,
      "step": 17168
    },
    {
      "epoch": 17.19,
      "grad_norm": 21117.763671875,
      "learning_rate": 1.4504643962848297e-05,
      "loss": 54.7422,
      "step": 17169
    },
    {
      "epoch": 17.19,
      "grad_norm": 1120.58447265625,
      "learning_rate": 1.4499484004127966e-05,
      "loss": 50.1575,
      "step": 17170
    },
    {
      "epoch": 17.19,
      "grad_norm": 2731.1416015625,
      "learning_rate": 1.4494324045407637e-05,
      "loss": 58.8407,
      "step": 17171
    },
    {
      "epoch": 17.19,
      "grad_norm": 8784.9873046875,
      "learning_rate": 1.4489164086687307e-05,
      "loss": 41.6933,
      "step": 17172
    },
    {
      "epoch": 17.19,
      "grad_norm": 111504.546875,
      "learning_rate": 1.4484004127966976e-05,
      "loss": 58.6201,
      "step": 17173
    },
    {
      "epoch": 17.19,
      "grad_norm": 80147.484375,
      "learning_rate": 1.4478844169246647e-05,
      "loss": 49.9494,
      "step": 17174
    },
    {
      "epoch": 17.19,
      "grad_norm": 16460.21875,
      "learning_rate": 1.4473684210526317e-05,
      "loss": 64.2073,
      "step": 17175
    },
    {
      "epoch": 17.19,
      "grad_norm": 7710.62353515625,
      "learning_rate": 1.4468524251805988e-05,
      "loss": 45.8101,
      "step": 17176
    },
    {
      "epoch": 17.19,
      "grad_norm": 36358.08203125,
      "learning_rate": 1.4463364293085657e-05,
      "loss": 47.0295,
      "step": 17177
    },
    {
      "epoch": 17.2,
      "grad_norm": 32862.03125,
      "learning_rate": 1.4458204334365325e-05,
      "loss": 39.3025,
      "step": 17178
    },
    {
      "epoch": 17.2,
      "grad_norm": 7951.77392578125,
      "learning_rate": 1.4453044375644994e-05,
      "loss": 54.0749,
      "step": 17179
    },
    {
      "epoch": 17.2,
      "grad_norm": 32327.642578125,
      "learning_rate": 1.4447884416924664e-05,
      "loss": 45.8959,
      "step": 17180
    },
    {
      "epoch": 17.2,
      "grad_norm": 9796.974609375,
      "learning_rate": 1.4442724458204335e-05,
      "loss": 60.2791,
      "step": 17181
    },
    {
      "epoch": 17.2,
      "grad_norm": 9531.552734375,
      "learning_rate": 1.4437564499484004e-05,
      "loss": 41.3061,
      "step": 17182
    },
    {
      "epoch": 17.2,
      "grad_norm": 6624.57861328125,
      "learning_rate": 1.4432404540763676e-05,
      "loss": 63.9267,
      "step": 17183
    },
    {
      "epoch": 17.2,
      "grad_norm": 1730.67626953125,
      "learning_rate": 1.4427244582043345e-05,
      "loss": 45.4742,
      "step": 17184
    },
    {
      "epoch": 17.2,
      "grad_norm": 8278.8623046875,
      "learning_rate": 1.4422084623323015e-05,
      "loss": 59.2274,
      "step": 17185
    },
    {
      "epoch": 17.2,
      "grad_norm": 1480.4525146484375,
      "learning_rate": 1.4416924664602686e-05,
      "loss": 53.384,
      "step": 17186
    },
    {
      "epoch": 17.2,
      "grad_norm": 11399.6201171875,
      "learning_rate": 1.4411764705882352e-05,
      "loss": 50.1361,
      "step": 17187
    },
    {
      "epoch": 17.21,
      "grad_norm": 119428.390625,
      "learning_rate": 1.4406604747162023e-05,
      "loss": 60.0422,
      "step": 17188
    },
    {
      "epoch": 17.21,
      "grad_norm": 14584.7646484375,
      "learning_rate": 1.4401444788441692e-05,
      "loss": 55.0124,
      "step": 17189
    },
    {
      "epoch": 17.21,
      "grad_norm": 20266.619140625,
      "learning_rate": 1.4396284829721363e-05,
      "loss": 54.53,
      "step": 17190
    },
    {
      "epoch": 17.21,
      "grad_norm": 9200.705078125,
      "learning_rate": 1.4391124871001033e-05,
      "loss": 55.7124,
      "step": 17191
    },
    {
      "epoch": 17.21,
      "grad_norm": 7716.06884765625,
      "learning_rate": 1.4385964912280702e-05,
      "loss": 56.3725,
      "step": 17192
    },
    {
      "epoch": 17.21,
      "grad_norm": 7995.62060546875,
      "learning_rate": 1.4380804953560373e-05,
      "loss": 55.277,
      "step": 17193
    },
    {
      "epoch": 17.21,
      "grad_norm": 11551.4033203125,
      "learning_rate": 1.4375644994840043e-05,
      "loss": 55.2608,
      "step": 17194
    },
    {
      "epoch": 17.21,
      "grad_norm": 21106.640625,
      "learning_rate": 1.4370485036119712e-05,
      "loss": 58.2093,
      "step": 17195
    },
    {
      "epoch": 17.21,
      "grad_norm": 3771.017822265625,
      "learning_rate": 1.436532507739938e-05,
      "loss": 58.2303,
      "step": 17196
    },
    {
      "epoch": 17.21,
      "grad_norm": 16770.751953125,
      "learning_rate": 1.4360165118679051e-05,
      "loss": 46.0604,
      "step": 17197
    },
    {
      "epoch": 17.22,
      "grad_norm": 12524.8583984375,
      "learning_rate": 1.435500515995872e-05,
      "loss": 59.1968,
      "step": 17198
    },
    {
      "epoch": 17.22,
      "grad_norm": 17345.306640625,
      "learning_rate": 1.434984520123839e-05,
      "loss": 52.1871,
      "step": 17199
    },
    {
      "epoch": 17.22,
      "grad_norm": 28374.177734375,
      "learning_rate": 1.4344685242518061e-05,
      "loss": 59.1429,
      "step": 17200
    },
    {
      "epoch": 17.22,
      "grad_norm": 4410.9033203125,
      "learning_rate": 1.433952528379773e-05,
      "loss": 63.8778,
      "step": 17201
    },
    {
      "epoch": 17.22,
      "grad_norm": 617.0255737304688,
      "learning_rate": 1.43343653250774e-05,
      "loss": 60.2914,
      "step": 17202
    },
    {
      "epoch": 17.22,
      "grad_norm": 2817.010986328125,
      "learning_rate": 1.4329205366357071e-05,
      "loss": 58.5704,
      "step": 17203
    },
    {
      "epoch": 17.22,
      "grad_norm": 1382.096435546875,
      "learning_rate": 1.432404540763674e-05,
      "loss": 55.5867,
      "step": 17204
    },
    {
      "epoch": 17.22,
      "grad_norm": 24816.5390625,
      "learning_rate": 1.4318885448916408e-05,
      "loss": 57.1645,
      "step": 17205
    },
    {
      "epoch": 17.22,
      "grad_norm": 5926.54248046875,
      "learning_rate": 1.4313725490196078e-05,
      "loss": 48.8758,
      "step": 17206
    },
    {
      "epoch": 17.22,
      "grad_norm": 3062.30126953125,
      "learning_rate": 1.4308565531475749e-05,
      "loss": 49.8164,
      "step": 17207
    },
    {
      "epoch": 17.23,
      "grad_norm": 1725.2655029296875,
      "learning_rate": 1.4303405572755418e-05,
      "loss": 58.7212,
      "step": 17208
    },
    {
      "epoch": 17.23,
      "grad_norm": 7371.48974609375,
      "learning_rate": 1.4298245614035088e-05,
      "loss": 54.9553,
      "step": 17209
    },
    {
      "epoch": 17.23,
      "grad_norm": 616.389892578125,
      "learning_rate": 1.4293085655314759e-05,
      "loss": 54.4568,
      "step": 17210
    },
    {
      "epoch": 17.23,
      "grad_norm": 8536.1708984375,
      "learning_rate": 1.4287925696594428e-05,
      "loss": 54.2012,
      "step": 17211
    },
    {
      "epoch": 17.23,
      "grad_norm": 41285.390625,
      "learning_rate": 1.42827657378741e-05,
      "loss": 45.9887,
      "step": 17212
    },
    {
      "epoch": 17.23,
      "grad_norm": 17908.544921875,
      "learning_rate": 1.4277605779153769e-05,
      "loss": 47.1712,
      "step": 17213
    },
    {
      "epoch": 17.23,
      "grad_norm": 9735.736328125,
      "learning_rate": 1.4272445820433437e-05,
      "loss": 62.697,
      "step": 17214
    },
    {
      "epoch": 17.23,
      "grad_norm": 3617.6533203125,
      "learning_rate": 1.4267285861713106e-05,
      "loss": 60.5391,
      "step": 17215
    },
    {
      "epoch": 17.23,
      "grad_norm": 7441.52197265625,
      "learning_rate": 1.4262125902992775e-05,
      "loss": 55.3262,
      "step": 17216
    },
    {
      "epoch": 17.23,
      "grad_norm": 129700.609375,
      "learning_rate": 1.4256965944272447e-05,
      "loss": 51.0291,
      "step": 17217
    },
    {
      "epoch": 17.24,
      "grad_norm": 2363.475830078125,
      "learning_rate": 1.4251805985552116e-05,
      "loss": 50.6295,
      "step": 17218
    },
    {
      "epoch": 17.24,
      "grad_norm": 39302.42578125,
      "learning_rate": 1.4246646026831787e-05,
      "loss": 47.8406,
      "step": 17219
    },
    {
      "epoch": 17.24,
      "grad_norm": 7904.56298828125,
      "learning_rate": 1.4241486068111457e-05,
      "loss": 45.814,
      "step": 17220
    },
    {
      "epoch": 17.24,
      "grad_norm": 2298.4560546875,
      "learning_rate": 1.4236326109391126e-05,
      "loss": 51.0068,
      "step": 17221
    },
    {
      "epoch": 17.24,
      "grad_norm": 22961.359375,
      "learning_rate": 1.4231166150670797e-05,
      "loss": 58.2052,
      "step": 17222
    },
    {
      "epoch": 17.24,
      "grad_norm": 9379.201171875,
      "learning_rate": 1.4226006191950463e-05,
      "loss": 57.5217,
      "step": 17223
    },
    {
      "epoch": 17.24,
      "grad_norm": 40102.1015625,
      "learning_rate": 1.4220846233230134e-05,
      "loss": 50.4762,
      "step": 17224
    },
    {
      "epoch": 17.24,
      "grad_norm": 3361.281494140625,
      "learning_rate": 1.4215686274509804e-05,
      "loss": 56.789,
      "step": 17225
    },
    {
      "epoch": 17.24,
      "grad_norm": 24996.626953125,
      "learning_rate": 1.4210526315789475e-05,
      "loss": 57.5348,
      "step": 17226
    },
    {
      "epoch": 17.24,
      "grad_norm": 26270.166015625,
      "learning_rate": 1.4205366357069144e-05,
      "loss": 65.3167,
      "step": 17227
    },
    {
      "epoch": 17.25,
      "grad_norm": 10924.560546875,
      "learning_rate": 1.4200206398348814e-05,
      "loss": 56.4786,
      "step": 17228
    },
    {
      "epoch": 17.25,
      "grad_norm": 11401.1142578125,
      "learning_rate": 1.4195046439628485e-05,
      "loss": 55.47,
      "step": 17229
    },
    {
      "epoch": 17.25,
      "grad_norm": 6113.54345703125,
      "learning_rate": 1.4189886480908154e-05,
      "loss": 60.7081,
      "step": 17230
    },
    {
      "epoch": 17.25,
      "grad_norm": 9361.669921875,
      "learning_rate": 1.4184726522187824e-05,
      "loss": 52.3726,
      "step": 17231
    },
    {
      "epoch": 17.25,
      "grad_norm": 2219.489501953125,
      "learning_rate": 1.4179566563467492e-05,
      "loss": 42.0537,
      "step": 17232
    },
    {
      "epoch": 17.25,
      "grad_norm": 224317.578125,
      "learning_rate": 1.4174406604747163e-05,
      "loss": 63.0947,
      "step": 17233
    },
    {
      "epoch": 17.25,
      "grad_norm": 15000.7998046875,
      "learning_rate": 1.4169246646026832e-05,
      "loss": 41.593,
      "step": 17234
    },
    {
      "epoch": 17.25,
      "grad_norm": 1973.655517578125,
      "learning_rate": 1.4164086687306502e-05,
      "loss": 63.2286,
      "step": 17235
    },
    {
      "epoch": 17.25,
      "grad_norm": 7262.97412109375,
      "learning_rate": 1.4158926728586173e-05,
      "loss": 61.2449,
      "step": 17236
    },
    {
      "epoch": 17.25,
      "grad_norm": 70504.9375,
      "learning_rate": 1.4153766769865842e-05,
      "loss": 47.8074,
      "step": 17237
    },
    {
      "epoch": 17.26,
      "grad_norm": 1183.353515625,
      "learning_rate": 1.4148606811145512e-05,
      "loss": 60.8769,
      "step": 17238
    },
    {
      "epoch": 17.26,
      "grad_norm": 2637.191162109375,
      "learning_rate": 1.4143446852425183e-05,
      "loss": 56.8441,
      "step": 17239
    },
    {
      "epoch": 17.26,
      "grad_norm": 2371.7109375,
      "learning_rate": 1.4138286893704852e-05,
      "loss": 59.5226,
      "step": 17240
    },
    {
      "epoch": 17.26,
      "grad_norm": 4649.23583984375,
      "learning_rate": 1.413312693498452e-05,
      "loss": 50.0033,
      "step": 17241
    },
    {
      "epoch": 17.26,
      "grad_norm": 560.2073974609375,
      "learning_rate": 1.412796697626419e-05,
      "loss": 49.5799,
      "step": 17242
    },
    {
      "epoch": 17.26,
      "grad_norm": 4918.41455078125,
      "learning_rate": 1.412280701754386e-05,
      "loss": 47.3001,
      "step": 17243
    },
    {
      "epoch": 17.26,
      "grad_norm": 30992.587890625,
      "learning_rate": 1.411764705882353e-05,
      "loss": 53.8182,
      "step": 17244
    },
    {
      "epoch": 17.26,
      "grad_norm": 5407.9677734375,
      "learning_rate": 1.41124871001032e-05,
      "loss": 52.5787,
      "step": 17245
    },
    {
      "epoch": 17.26,
      "grad_norm": 2249.2685546875,
      "learning_rate": 1.410732714138287e-05,
      "loss": 62.0111,
      "step": 17246
    },
    {
      "epoch": 17.26,
      "grad_norm": 13233.3525390625,
      "learning_rate": 1.410216718266254e-05,
      "loss": 49.1453,
      "step": 17247
    },
    {
      "epoch": 17.27,
      "grad_norm": 12262.5947265625,
      "learning_rate": 1.4097007223942211e-05,
      "loss": 55.0283,
      "step": 17248
    },
    {
      "epoch": 17.27,
      "grad_norm": 20141.1796875,
      "learning_rate": 1.409184726522188e-05,
      "loss": 66.552,
      "step": 17249
    },
    {
      "epoch": 17.27,
      "grad_norm": 5705.5380859375,
      "learning_rate": 1.4086687306501548e-05,
      "loss": 50.1858,
      "step": 17250
    },
    {
      "epoch": 17.27,
      "grad_norm": 4321.130859375,
      "learning_rate": 1.4081527347781218e-05,
      "loss": 56.9366,
      "step": 17251
    },
    {
      "epoch": 17.27,
      "grad_norm": 4154.41796875,
      "learning_rate": 1.4076367389060887e-05,
      "loss": 63.6228,
      "step": 17252
    },
    {
      "epoch": 17.27,
      "grad_norm": 2522.75927734375,
      "learning_rate": 1.4071207430340558e-05,
      "loss": 57.1648,
      "step": 17253
    },
    {
      "epoch": 17.27,
      "grad_norm": 8399.15625,
      "learning_rate": 1.4066047471620228e-05,
      "loss": 51.5953,
      "step": 17254
    },
    {
      "epoch": 17.27,
      "grad_norm": 14049.1884765625,
      "learning_rate": 1.4060887512899899e-05,
      "loss": 52.5949,
      "step": 17255
    },
    {
      "epoch": 17.27,
      "grad_norm": 2919.474365234375,
      "learning_rate": 1.4055727554179568e-05,
      "loss": 65.6305,
      "step": 17256
    },
    {
      "epoch": 17.27,
      "grad_norm": 5645.6884765625,
      "learning_rate": 1.4050567595459238e-05,
      "loss": 60.8009,
      "step": 17257
    },
    {
      "epoch": 17.28,
      "grad_norm": 11384.1318359375,
      "learning_rate": 1.4045407636738909e-05,
      "loss": 63.3582,
      "step": 17258
    },
    {
      "epoch": 17.28,
      "grad_norm": 172706.796875,
      "learning_rate": 1.4040247678018575e-05,
      "loss": 58.2834,
      "step": 17259
    },
    {
      "epoch": 17.28,
      "grad_norm": 1539.691162109375,
      "learning_rate": 1.4035087719298246e-05,
      "loss": 59.2677,
      "step": 17260
    },
    {
      "epoch": 17.28,
      "grad_norm": 9126.9755859375,
      "learning_rate": 1.4029927760577915e-05,
      "loss": 41.3314,
      "step": 17261
    },
    {
      "epoch": 17.28,
      "grad_norm": 5843.802734375,
      "learning_rate": 1.4024767801857586e-05,
      "loss": 58.1528,
      "step": 17262
    },
    {
      "epoch": 17.28,
      "grad_norm": 86965.453125,
      "learning_rate": 1.4019607843137256e-05,
      "loss": 39.1609,
      "step": 17263
    },
    {
      "epoch": 17.28,
      "grad_norm": 37994.03515625,
      "learning_rate": 1.4014447884416925e-05,
      "loss": 35.5576,
      "step": 17264
    },
    {
      "epoch": 17.28,
      "grad_norm": 3303.35302734375,
      "learning_rate": 1.4009287925696596e-05,
      "loss": 56.6043,
      "step": 17265
    },
    {
      "epoch": 17.28,
      "grad_norm": 3943.118408203125,
      "learning_rate": 1.4004127966976266e-05,
      "loss": 53.5989,
      "step": 17266
    },
    {
      "epoch": 17.28,
      "grad_norm": 7472.0556640625,
      "learning_rate": 1.3998968008255934e-05,
      "loss": 60.8953,
      "step": 17267
    },
    {
      "epoch": 17.29,
      "grad_norm": 36866.55078125,
      "learning_rate": 1.3993808049535603e-05,
      "loss": 29.8391,
      "step": 17268
    },
    {
      "epoch": 17.29,
      "grad_norm": 5691.00830078125,
      "learning_rate": 1.3988648090815273e-05,
      "loss": 52.4173,
      "step": 17269
    },
    {
      "epoch": 17.29,
      "grad_norm": 8077.623046875,
      "learning_rate": 1.3983488132094944e-05,
      "loss": 55.9263,
      "step": 17270
    },
    {
      "epoch": 17.29,
      "grad_norm": 66388.578125,
      "learning_rate": 1.3978328173374613e-05,
      "loss": 54.2411,
      "step": 17271
    },
    {
      "epoch": 17.29,
      "grad_norm": 31008.892578125,
      "learning_rate": 1.3973168214654284e-05,
      "loss": 55.6902,
      "step": 17272
    },
    {
      "epoch": 17.29,
      "grad_norm": 39577.07421875,
      "learning_rate": 1.3968008255933954e-05,
      "loss": 59.3034,
      "step": 17273
    },
    {
      "epoch": 17.29,
      "grad_norm": 4925.47412109375,
      "learning_rate": 1.3962848297213623e-05,
      "loss": 57.6991,
      "step": 17274
    },
    {
      "epoch": 17.29,
      "grad_norm": 12338.6875,
      "learning_rate": 1.3957688338493294e-05,
      "loss": 63.2904,
      "step": 17275
    },
    {
      "epoch": 17.29,
      "grad_norm": 40709.13671875,
      "learning_rate": 1.395252837977296e-05,
      "loss": 54.7921,
      "step": 17276
    },
    {
      "epoch": 17.29,
      "grad_norm": 7895.0849609375,
      "learning_rate": 1.3947368421052631e-05,
      "loss": 57.9326,
      "step": 17277
    },
    {
      "epoch": 17.3,
      "grad_norm": 3470.287353515625,
      "learning_rate": 1.3942208462332301e-05,
      "loss": 63.9849,
      "step": 17278
    },
    {
      "epoch": 17.3,
      "grad_norm": 26252.05859375,
      "learning_rate": 1.3937048503611972e-05,
      "loss": 57.9672,
      "step": 17279
    },
    {
      "epoch": 17.3,
      "grad_norm": 5612.501953125,
      "learning_rate": 1.3931888544891641e-05,
      "loss": 52.6578,
      "step": 17280
    },
    {
      "epoch": 17.3,
      "grad_norm": 4114.83984375,
      "learning_rate": 1.3926728586171311e-05,
      "loss": 61.2901,
      "step": 17281
    },
    {
      "epoch": 17.3,
      "grad_norm": 5510.69921875,
      "learning_rate": 1.3921568627450982e-05,
      "loss": 59.8921,
      "step": 17282
    },
    {
      "epoch": 17.3,
      "grad_norm": 25408.419921875,
      "learning_rate": 1.3916408668730651e-05,
      "loss": 43.0484,
      "step": 17283
    },
    {
      "epoch": 17.3,
      "grad_norm": 6634.2763671875,
      "learning_rate": 1.3911248710010321e-05,
      "loss": 60.5579,
      "step": 17284
    },
    {
      "epoch": 17.3,
      "grad_norm": 942.2879028320312,
      "learning_rate": 1.3906088751289989e-05,
      "loss": 57.6691,
      "step": 17285
    },
    {
      "epoch": 17.3,
      "grad_norm": 11783.064453125,
      "learning_rate": 1.390092879256966e-05,
      "loss": 39.1035,
      "step": 17286
    },
    {
      "epoch": 17.3,
      "grad_norm": 991.72119140625,
      "learning_rate": 1.389576883384933e-05,
      "loss": 52.7537,
      "step": 17287
    },
    {
      "epoch": 17.31,
      "grad_norm": 2649.163818359375,
      "learning_rate": 1.3890608875128999e-05,
      "loss": 57.3197,
      "step": 17288
    },
    {
      "epoch": 17.31,
      "grad_norm": 14163.703125,
      "learning_rate": 1.388544891640867e-05,
      "loss": 57.8193,
      "step": 17289
    },
    {
      "epoch": 17.31,
      "grad_norm": 6038.521484375,
      "learning_rate": 1.388028895768834e-05,
      "loss": 61.8038,
      "step": 17290
    },
    {
      "epoch": 17.31,
      "grad_norm": 16618.294921875,
      "learning_rate": 1.3875128998968009e-05,
      "loss": 55.3658,
      "step": 17291
    },
    {
      "epoch": 17.31,
      "grad_norm": 9589.525390625,
      "learning_rate": 1.386996904024768e-05,
      "loss": 57.293,
      "step": 17292
    },
    {
      "epoch": 17.31,
      "grad_norm": 28644.845703125,
      "learning_rate": 1.386480908152735e-05,
      "loss": 55.0565,
      "step": 17293
    },
    {
      "epoch": 17.31,
      "grad_norm": 6852.50439453125,
      "learning_rate": 1.3859649122807017e-05,
      "loss": 59.3598,
      "step": 17294
    },
    {
      "epoch": 17.31,
      "grad_norm": 5342.3115234375,
      "learning_rate": 1.3854489164086686e-05,
      "loss": 53.7728,
      "step": 17295
    },
    {
      "epoch": 17.31,
      "grad_norm": 2514.92041015625,
      "learning_rate": 1.3849329205366357e-05,
      "loss": 41.4905,
      "step": 17296
    },
    {
      "epoch": 17.31,
      "grad_norm": 1330.7491455078125,
      "learning_rate": 1.3844169246646027e-05,
      "loss": 59.6633,
      "step": 17297
    },
    {
      "epoch": 17.32,
      "grad_norm": 5690.28076171875,
      "learning_rate": 1.3839009287925696e-05,
      "loss": 54.7478,
      "step": 17298
    },
    {
      "epoch": 17.32,
      "grad_norm": 21198.986328125,
      "learning_rate": 1.3833849329205367e-05,
      "loss": 50.9325,
      "step": 17299
    },
    {
      "epoch": 17.32,
      "grad_norm": 95454.15625,
      "learning_rate": 1.3828689370485037e-05,
      "loss": 60.0164,
      "step": 17300
    },
    {
      "epoch": 17.32,
      "grad_norm": 7932.8125,
      "learning_rate": 1.3823529411764708e-05,
      "loss": 51.1402,
      "step": 17301
    },
    {
      "epoch": 17.32,
      "grad_norm": 6212.31640625,
      "learning_rate": 1.3818369453044377e-05,
      "loss": 50.339,
      "step": 17302
    },
    {
      "epoch": 17.32,
      "grad_norm": 11733.4658203125,
      "learning_rate": 1.3813209494324045e-05,
      "loss": 49.2105,
      "step": 17303
    },
    {
      "epoch": 17.32,
      "grad_norm": 3668.92236328125,
      "learning_rate": 1.3808049535603715e-05,
      "loss": 57.1717,
      "step": 17304
    },
    {
      "epoch": 17.32,
      "grad_norm": 6351.5908203125,
      "learning_rate": 1.3802889576883384e-05,
      "loss": 53.8452,
      "step": 17305
    },
    {
      "epoch": 17.32,
      "grad_norm": 3535.59814453125,
      "learning_rate": 1.3797729618163055e-05,
      "loss": 59.9538,
      "step": 17306
    },
    {
      "epoch": 17.32,
      "grad_norm": 4974.82861328125,
      "learning_rate": 1.3792569659442725e-05,
      "loss": 49.5937,
      "step": 17307
    },
    {
      "epoch": 17.33,
      "grad_norm": 3736.223876953125,
      "learning_rate": 1.3787409700722396e-05,
      "loss": 58.1437,
      "step": 17308
    },
    {
      "epoch": 17.33,
      "grad_norm": 11027.4990234375,
      "learning_rate": 1.3782249742002065e-05,
      "loss": 40.2479,
      "step": 17309
    },
    {
      "epoch": 17.33,
      "grad_norm": 3969.204833984375,
      "learning_rate": 1.3777089783281735e-05,
      "loss": 63.8966,
      "step": 17310
    },
    {
      "epoch": 17.33,
      "grad_norm": 3260.368896484375,
      "learning_rate": 1.3771929824561406e-05,
      "loss": 55.4831,
      "step": 17311
    },
    {
      "epoch": 17.33,
      "grad_norm": 9723.7841796875,
      "learning_rate": 1.3766769865841072e-05,
      "loss": 40.1146,
      "step": 17312
    },
    {
      "epoch": 17.33,
      "grad_norm": 7678.86083984375,
      "learning_rate": 1.3761609907120743e-05,
      "loss": 57.6904,
      "step": 17313
    },
    {
      "epoch": 17.33,
      "grad_norm": 2185.017822265625,
      "learning_rate": 1.3756449948400412e-05,
      "loss": 54.7525,
      "step": 17314
    },
    {
      "epoch": 17.33,
      "grad_norm": 113339.3125,
      "learning_rate": 1.3751289989680084e-05,
      "loss": 65.8829,
      "step": 17315
    },
    {
      "epoch": 17.33,
      "grad_norm": 21394.974609375,
      "learning_rate": 1.3746130030959753e-05,
      "loss": 44.4968,
      "step": 17316
    },
    {
      "epoch": 17.33,
      "grad_norm": 24540.962890625,
      "learning_rate": 1.3740970072239422e-05,
      "loss": 40.777,
      "step": 17317
    },
    {
      "epoch": 17.34,
      "grad_norm": 16590.919921875,
      "learning_rate": 1.3735810113519094e-05,
      "loss": 43.5516,
      "step": 17318
    },
    {
      "epoch": 17.34,
      "grad_norm": 86058.90625,
      "learning_rate": 1.3730650154798763e-05,
      "loss": 52.8235,
      "step": 17319
    },
    {
      "epoch": 17.34,
      "grad_norm": 668.1812744140625,
      "learning_rate": 1.3725490196078432e-05,
      "loss": 68.8377,
      "step": 17320
    },
    {
      "epoch": 17.34,
      "grad_norm": 1674.3817138671875,
      "learning_rate": 1.37203302373581e-05,
      "loss": 61.3077,
      "step": 17321
    },
    {
      "epoch": 17.34,
      "grad_norm": 956.7599487304688,
      "learning_rate": 1.3715170278637771e-05,
      "loss": 60.3225,
      "step": 17322
    },
    {
      "epoch": 17.34,
      "grad_norm": 2028.7623291015625,
      "learning_rate": 1.371001031991744e-05,
      "loss": 53.0221,
      "step": 17323
    },
    {
      "epoch": 17.34,
      "grad_norm": 14119.484375,
      "learning_rate": 1.370485036119711e-05,
      "loss": 68.4855,
      "step": 17324
    },
    {
      "epoch": 17.34,
      "grad_norm": 8289.431640625,
      "learning_rate": 1.3699690402476781e-05,
      "loss": 59.981,
      "step": 17325
    },
    {
      "epoch": 17.34,
      "grad_norm": 5776.39892578125,
      "learning_rate": 1.369453044375645e-05,
      "loss": 55.7341,
      "step": 17326
    },
    {
      "epoch": 17.34,
      "grad_norm": 6065.904296875,
      "learning_rate": 1.368937048503612e-05,
      "loss": 60.4257,
      "step": 17327
    },
    {
      "epoch": 17.35,
      "grad_norm": 25914.634765625,
      "learning_rate": 1.3684210526315791e-05,
      "loss": 56.114,
      "step": 17328
    },
    {
      "epoch": 17.35,
      "grad_norm": 19150.078125,
      "learning_rate": 1.367905056759546e-05,
      "loss": 63.8943,
      "step": 17329
    },
    {
      "epoch": 17.35,
      "grad_norm": 5262.27197265625,
      "learning_rate": 1.3673890608875128e-05,
      "loss": 55.5696,
      "step": 17330
    },
    {
      "epoch": 17.35,
      "grad_norm": 9792.705078125,
      "learning_rate": 1.3668730650154798e-05,
      "loss": 60.7927,
      "step": 17331
    },
    {
      "epoch": 17.35,
      "grad_norm": 5275.57275390625,
      "learning_rate": 1.3663570691434469e-05,
      "loss": 63.079,
      "step": 17332
    },
    {
      "epoch": 17.35,
      "grad_norm": 2083.085693359375,
      "learning_rate": 1.3658410732714138e-05,
      "loss": 63.3582,
      "step": 17333
    },
    {
      "epoch": 17.35,
      "grad_norm": 2292.095703125,
      "learning_rate": 1.3653250773993808e-05,
      "loss": 53.8926,
      "step": 17334
    },
    {
      "epoch": 17.35,
      "grad_norm": 12201.7470703125,
      "learning_rate": 1.3648090815273479e-05,
      "loss": 36.5882,
      "step": 17335
    },
    {
      "epoch": 17.35,
      "grad_norm": 30148.138671875,
      "learning_rate": 1.3642930856553148e-05,
      "loss": 57.8989,
      "step": 17336
    },
    {
      "epoch": 17.35,
      "grad_norm": 10751.4638671875,
      "learning_rate": 1.363777089783282e-05,
      "loss": 62.3757,
      "step": 17337
    },
    {
      "epoch": 17.36,
      "grad_norm": 3362.560302734375,
      "learning_rate": 1.3632610939112489e-05,
      "loss": 64.5819,
      "step": 17338
    },
    {
      "epoch": 17.36,
      "grad_norm": 11104.044921875,
      "learning_rate": 1.3627450980392157e-05,
      "loss": 61.9845,
      "step": 17339
    },
    {
      "epoch": 17.36,
      "grad_norm": 76935.671875,
      "learning_rate": 1.3622291021671826e-05,
      "loss": 42.9396,
      "step": 17340
    },
    {
      "epoch": 17.36,
      "grad_norm": 6149.1064453125,
      "learning_rate": 1.3617131062951496e-05,
      "loss": 48.2499,
      "step": 17341
    },
    {
      "epoch": 17.36,
      "grad_norm": 13715.5810546875,
      "learning_rate": 1.3611971104231167e-05,
      "loss": 58.4021,
      "step": 17342
    },
    {
      "epoch": 17.36,
      "grad_norm": 13750.2783203125,
      "learning_rate": 1.3606811145510836e-05,
      "loss": 61.4389,
      "step": 17343
    },
    {
      "epoch": 17.36,
      "grad_norm": 16780.666015625,
      "learning_rate": 1.3601651186790507e-05,
      "loss": 53.4667,
      "step": 17344
    },
    {
      "epoch": 17.36,
      "grad_norm": 2386.376220703125,
      "learning_rate": 1.3596491228070177e-05,
      "loss": 55.8084,
      "step": 17345
    },
    {
      "epoch": 17.36,
      "grad_norm": 5463.70458984375,
      "learning_rate": 1.3591331269349846e-05,
      "loss": 53.4398,
      "step": 17346
    },
    {
      "epoch": 17.36,
      "grad_norm": 5900.76513671875,
      "learning_rate": 1.3586171310629517e-05,
      "loss": 55.3547,
      "step": 17347
    },
    {
      "epoch": 17.37,
      "grad_norm": 128077.484375,
      "learning_rate": 1.3581011351909183e-05,
      "loss": 60.214,
      "step": 17348
    },
    {
      "epoch": 17.37,
      "grad_norm": 9106.1396484375,
      "learning_rate": 1.3575851393188855e-05,
      "loss": 63.296,
      "step": 17349
    },
    {
      "epoch": 17.37,
      "grad_norm": 12500.201171875,
      "learning_rate": 1.3570691434468524e-05,
      "loss": 53.9571,
      "step": 17350
    },
    {
      "epoch": 17.37,
      "grad_norm": 27441.330078125,
      "learning_rate": 1.3565531475748195e-05,
      "loss": 40.9628,
      "step": 17351
    },
    {
      "epoch": 17.37,
      "grad_norm": 525.9789428710938,
      "learning_rate": 1.3560371517027865e-05,
      "loss": 48.9198,
      "step": 17352
    },
    {
      "epoch": 17.37,
      "grad_norm": 189705.03125,
      "learning_rate": 1.3555211558307534e-05,
      "loss": 46.394,
      "step": 17353
    },
    {
      "epoch": 17.37,
      "grad_norm": 4120.3037109375,
      "learning_rate": 1.3550051599587205e-05,
      "loss": 46.3056,
      "step": 17354
    },
    {
      "epoch": 17.37,
      "grad_norm": 4603.154296875,
      "learning_rate": 1.3544891640866875e-05,
      "loss": 61.3916,
      "step": 17355
    },
    {
      "epoch": 17.37,
      "grad_norm": 11198.662109375,
      "learning_rate": 1.3539731682146544e-05,
      "loss": 55.6464,
      "step": 17356
    },
    {
      "epoch": 17.37,
      "grad_norm": 18342.90625,
      "learning_rate": 1.3534571723426212e-05,
      "loss": 52.0994,
      "step": 17357
    },
    {
      "epoch": 17.38,
      "grad_norm": 77600.9140625,
      "learning_rate": 1.3529411764705883e-05,
      "loss": 23.2318,
      "step": 17358
    },
    {
      "epoch": 17.38,
      "grad_norm": 9861.361328125,
      "learning_rate": 1.3524251805985552e-05,
      "loss": 53.2507,
      "step": 17359
    },
    {
      "epoch": 17.38,
      "grad_norm": 174042.71875,
      "learning_rate": 1.3519091847265222e-05,
      "loss": 64.7597,
      "step": 17360
    },
    {
      "epoch": 17.38,
      "grad_norm": 1402.494873046875,
      "learning_rate": 1.3513931888544893e-05,
      "loss": 58.6763,
      "step": 17361
    },
    {
      "epoch": 17.38,
      "grad_norm": 32370.294921875,
      "learning_rate": 1.3508771929824562e-05,
      "loss": 59.4969,
      "step": 17362
    },
    {
      "epoch": 17.38,
      "grad_norm": 12340.05859375,
      "learning_rate": 1.3503611971104232e-05,
      "loss": 60.2658,
      "step": 17363
    },
    {
      "epoch": 17.38,
      "grad_norm": 1179.669921875,
      "learning_rate": 1.3498452012383903e-05,
      "loss": 54.3576,
      "step": 17364
    },
    {
      "epoch": 17.38,
      "grad_norm": 7802.1142578125,
      "learning_rate": 1.3493292053663572e-05,
      "loss": 58.1971,
      "step": 17365
    },
    {
      "epoch": 17.38,
      "grad_norm": 4001.89990234375,
      "learning_rate": 1.348813209494324e-05,
      "loss": 58.0541,
      "step": 17366
    },
    {
      "epoch": 17.38,
      "grad_norm": 1095.560302734375,
      "learning_rate": 1.348297213622291e-05,
      "loss": 55.1545,
      "step": 17367
    },
    {
      "epoch": 17.39,
      "grad_norm": 29066.75390625,
      "learning_rate": 1.347781217750258e-05,
      "loss": 53.0872,
      "step": 17368
    },
    {
      "epoch": 17.39,
      "grad_norm": 33071.765625,
      "learning_rate": 1.347265221878225e-05,
      "loss": 53.7307,
      "step": 17369
    },
    {
      "epoch": 17.39,
      "grad_norm": 20138.833984375,
      "learning_rate": 1.346749226006192e-05,
      "loss": 58.8861,
      "step": 17370
    },
    {
      "epoch": 17.39,
      "grad_norm": 3858.115234375,
      "learning_rate": 1.346233230134159e-05,
      "loss": 64.9027,
      "step": 17371
    },
    {
      "epoch": 17.39,
      "grad_norm": 7785.1396484375,
      "learning_rate": 1.345717234262126e-05,
      "loss": 53.1343,
      "step": 17372
    },
    {
      "epoch": 17.39,
      "grad_norm": 8148.49365234375,
      "learning_rate": 1.3452012383900931e-05,
      "loss": 55.2529,
      "step": 17373
    },
    {
      "epoch": 17.39,
      "grad_norm": 2958.55517578125,
      "learning_rate": 1.34468524251806e-05,
      "loss": 60.4348,
      "step": 17374
    },
    {
      "epoch": 17.39,
      "grad_norm": 33007.41015625,
      "learning_rate": 1.3441692466460268e-05,
      "loss": 36.8178,
      "step": 17375
    },
    {
      "epoch": 17.39,
      "grad_norm": 9189.53125,
      "learning_rate": 1.3436532507739938e-05,
      "loss": 60.0245,
      "step": 17376
    },
    {
      "epoch": 17.39,
      "grad_norm": 2955.578369140625,
      "learning_rate": 1.3431372549019607e-05,
      "loss": 59.118,
      "step": 17377
    },
    {
      "epoch": 17.4,
      "grad_norm": 3131.99169921875,
      "learning_rate": 1.3426212590299278e-05,
      "loss": 51.3186,
      "step": 17378
    },
    {
      "epoch": 17.4,
      "grad_norm": 11807.189453125,
      "learning_rate": 1.3421052631578948e-05,
      "loss": 57.7207,
      "step": 17379
    },
    {
      "epoch": 17.4,
      "grad_norm": 6050.9501953125,
      "learning_rate": 1.3415892672858619e-05,
      "loss": 59.9609,
      "step": 17380
    },
    {
      "epoch": 17.4,
      "grad_norm": 908.773193359375,
      "learning_rate": 1.3410732714138288e-05,
      "loss": 55.7666,
      "step": 17381
    },
    {
      "epoch": 17.4,
      "grad_norm": 6754.79052734375,
      "learning_rate": 1.3405572755417958e-05,
      "loss": 54.5241,
      "step": 17382
    },
    {
      "epoch": 17.4,
      "grad_norm": 20934.5078125,
      "learning_rate": 1.3400412796697629e-05,
      "loss": 47.1832,
      "step": 17383
    },
    {
      "epoch": 17.4,
      "grad_norm": 17086.390625,
      "learning_rate": 1.3395252837977295e-05,
      "loss": 26.2045,
      "step": 17384
    },
    {
      "epoch": 17.4,
      "grad_norm": 10477.2275390625,
      "learning_rate": 1.3390092879256966e-05,
      "loss": 56.2562,
      "step": 17385
    },
    {
      "epoch": 17.4,
      "grad_norm": 29856.986328125,
      "learning_rate": 1.3384932920536636e-05,
      "loss": 50.6702,
      "step": 17386
    },
    {
      "epoch": 17.4,
      "grad_norm": 30134.4765625,
      "learning_rate": 1.3379772961816307e-05,
      "loss": 55.4425,
      "step": 17387
    },
    {
      "epoch": 17.41,
      "grad_norm": 18134.05859375,
      "learning_rate": 1.3374613003095976e-05,
      "loss": 60.4779,
      "step": 17388
    },
    {
      "epoch": 17.41,
      "grad_norm": 23110.853515625,
      "learning_rate": 1.3369453044375646e-05,
      "loss": 46.6028,
      "step": 17389
    },
    {
      "epoch": 17.41,
      "grad_norm": 77340.5703125,
      "learning_rate": 1.3364293085655317e-05,
      "loss": 64.9607,
      "step": 17390
    },
    {
      "epoch": 17.41,
      "grad_norm": 3941.55712890625,
      "learning_rate": 1.3359133126934986e-05,
      "loss": 64.5182,
      "step": 17391
    },
    {
      "epoch": 17.41,
      "grad_norm": 14007.20703125,
      "learning_rate": 1.3353973168214656e-05,
      "loss": 43.9832,
      "step": 17392
    },
    {
      "epoch": 17.41,
      "grad_norm": 3771.6337890625,
      "learning_rate": 1.3348813209494323e-05,
      "loss": 64.6942,
      "step": 17393
    },
    {
      "epoch": 17.41,
      "grad_norm": 9841.7412109375,
      "learning_rate": 1.3343653250773994e-05,
      "loss": 54.5456,
      "step": 17394
    },
    {
      "epoch": 17.41,
      "grad_norm": 3581.24560546875,
      "learning_rate": 1.3338493292053664e-05,
      "loss": 58.3777,
      "step": 17395
    },
    {
      "epoch": 17.41,
      "grad_norm": 10704.33984375,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 44.4403,
      "step": 17396
    },
    {
      "epoch": 17.41,
      "grad_norm": 2550.738037109375,
      "learning_rate": 1.3328173374613004e-05,
      "loss": 53.8417,
      "step": 17397
    },
    {
      "epoch": 17.42,
      "grad_norm": 1334.152099609375,
      "learning_rate": 1.3323013415892674e-05,
      "loss": 54.4949,
      "step": 17398
    },
    {
      "epoch": 17.42,
      "grad_norm": 27857.796875,
      "learning_rate": 1.3317853457172343e-05,
      "loss": 49.5375,
      "step": 17399
    },
    {
      "epoch": 17.42,
      "grad_norm": 4548.193359375,
      "learning_rate": 1.3312693498452014e-05,
      "loss": 49.0977,
      "step": 17400
    },
    {
      "epoch": 17.42,
      "grad_norm": 10500.953125,
      "learning_rate": 1.3307533539731684e-05,
      "loss": 51.2422,
      "step": 17401
    },
    {
      "epoch": 17.42,
      "grad_norm": 8573.0791015625,
      "learning_rate": 1.3302373581011352e-05,
      "loss": 39.1422,
      "step": 17402
    },
    {
      "epoch": 17.42,
      "grad_norm": 5112.02197265625,
      "learning_rate": 1.3297213622291021e-05,
      "loss": 52.1625,
      "step": 17403
    },
    {
      "epoch": 17.42,
      "grad_norm": 5751.12158203125,
      "learning_rate": 1.3292053663570692e-05,
      "loss": 52.0046,
      "step": 17404
    },
    {
      "epoch": 17.42,
      "grad_norm": 10202.427734375,
      "learning_rate": 1.3286893704850362e-05,
      "loss": 55.6276,
      "step": 17405
    },
    {
      "epoch": 17.42,
      "grad_norm": 11249.9658203125,
      "learning_rate": 1.3281733746130031e-05,
      "loss": 62.2041,
      "step": 17406
    },
    {
      "epoch": 17.42,
      "grad_norm": 8899.7626953125,
      "learning_rate": 1.3276573787409702e-05,
      "loss": 38.4155,
      "step": 17407
    },
    {
      "epoch": 17.43,
      "grad_norm": 22457.7265625,
      "learning_rate": 1.3271413828689372e-05,
      "loss": 65.046,
      "step": 17408
    },
    {
      "epoch": 17.43,
      "grad_norm": 4152.70654296875,
      "learning_rate": 1.3266253869969043e-05,
      "loss": 52.7493,
      "step": 17409
    },
    {
      "epoch": 17.43,
      "grad_norm": 20431.359375,
      "learning_rate": 1.3261093911248712e-05,
      "loss": 46.1638,
      "step": 17410
    },
    {
      "epoch": 17.43,
      "grad_norm": 28626.365234375,
      "learning_rate": 1.325593395252838e-05,
      "loss": 49.1637,
      "step": 17411
    },
    {
      "epoch": 17.43,
      "grad_norm": 11084.595703125,
      "learning_rate": 1.325077399380805e-05,
      "loss": 63.0323,
      "step": 17412
    },
    {
      "epoch": 17.43,
      "grad_norm": 15116.2890625,
      "learning_rate": 1.3245614035087719e-05,
      "loss": 47.1078,
      "step": 17413
    },
    {
      "epoch": 17.43,
      "grad_norm": 1546.1900634765625,
      "learning_rate": 1.324045407636739e-05,
      "loss": 64.5343,
      "step": 17414
    },
    {
      "epoch": 17.43,
      "grad_norm": 4847.14697265625,
      "learning_rate": 1.323529411764706e-05,
      "loss": 63.8875,
      "step": 17415
    },
    {
      "epoch": 17.43,
      "grad_norm": 30761.0859375,
      "learning_rate": 1.323013415892673e-05,
      "loss": 47.9633,
      "step": 17416
    },
    {
      "epoch": 17.43,
      "grad_norm": 26521.33984375,
      "learning_rate": 1.32249742002064e-05,
      "loss": 58.4742,
      "step": 17417
    },
    {
      "epoch": 17.44,
      "grad_norm": 9471.201171875,
      "learning_rate": 1.321981424148607e-05,
      "loss": 27.5223,
      "step": 17418
    },
    {
      "epoch": 17.44,
      "grad_norm": 20476.11328125,
      "learning_rate": 1.3214654282765737e-05,
      "loss": 59.8343,
      "step": 17419
    },
    {
      "epoch": 17.44,
      "grad_norm": 12485.9853515625,
      "learning_rate": 1.3209494324045407e-05,
      "loss": 54.8224,
      "step": 17420
    },
    {
      "epoch": 17.44,
      "grad_norm": 30649.4921875,
      "learning_rate": 1.3204334365325078e-05,
      "loss": 59.0516,
      "step": 17421
    },
    {
      "epoch": 17.44,
      "grad_norm": 136556.234375,
      "learning_rate": 1.3199174406604747e-05,
      "loss": 36.896,
      "step": 17422
    },
    {
      "epoch": 17.44,
      "grad_norm": 34834.9140625,
      "learning_rate": 1.3194014447884418e-05,
      "loss": 60.2651,
      "step": 17423
    },
    {
      "epoch": 17.44,
      "grad_norm": 1583.7469482421875,
      "learning_rate": 1.3188854489164088e-05,
      "loss": 63.2059,
      "step": 17424
    },
    {
      "epoch": 17.44,
      "grad_norm": 1404.440673828125,
      "learning_rate": 1.3183694530443757e-05,
      "loss": 66.3958,
      "step": 17425
    },
    {
      "epoch": 17.44,
      "grad_norm": 533.7601318359375,
      "learning_rate": 1.3178534571723428e-05,
      "loss": 64.6848,
      "step": 17426
    },
    {
      "epoch": 17.44,
      "grad_norm": 1868.6656494140625,
      "learning_rate": 1.3173374613003098e-05,
      "loss": 63.4615,
      "step": 17427
    },
    {
      "epoch": 17.45,
      "grad_norm": 5672.48193359375,
      "learning_rate": 1.3168214654282765e-05,
      "loss": 61.8273,
      "step": 17428
    },
    {
      "epoch": 17.45,
      "grad_norm": 4061.6650390625,
      "learning_rate": 1.3163054695562435e-05,
      "loss": 62.6186,
      "step": 17429
    },
    {
      "epoch": 17.45,
      "grad_norm": 6637.52197265625,
      "learning_rate": 1.3157894736842106e-05,
      "loss": 58.7461,
      "step": 17430
    },
    {
      "epoch": 17.45,
      "grad_norm": 5952.58837890625,
      "learning_rate": 1.3152734778121775e-05,
      "loss": 43.0149,
      "step": 17431
    },
    {
      "epoch": 17.45,
      "grad_norm": 1506.604736328125,
      "learning_rate": 1.3147574819401445e-05,
      "loss": 64.8185,
      "step": 17432
    },
    {
      "epoch": 17.45,
      "grad_norm": 9822.22265625,
      "learning_rate": 1.3142414860681116e-05,
      "loss": 55.4652,
      "step": 17433
    },
    {
      "epoch": 17.45,
      "grad_norm": 2723.0478515625,
      "learning_rate": 1.3137254901960785e-05,
      "loss": 62.4907,
      "step": 17434
    },
    {
      "epoch": 17.45,
      "grad_norm": 6974.41015625,
      "learning_rate": 1.3132094943240455e-05,
      "loss": 56.5699,
      "step": 17435
    },
    {
      "epoch": 17.45,
      "grad_norm": 3244.858154296875,
      "learning_rate": 1.3126934984520126e-05,
      "loss": 54.1203,
      "step": 17436
    },
    {
      "epoch": 17.45,
      "grad_norm": 4273.953125,
      "learning_rate": 1.3121775025799794e-05,
      "loss": 57.1272,
      "step": 17437
    },
    {
      "epoch": 17.46,
      "grad_norm": 14509.197265625,
      "learning_rate": 1.3116615067079463e-05,
      "loss": 63.5915,
      "step": 17438
    },
    {
      "epoch": 17.46,
      "grad_norm": 799.5439453125,
      "learning_rate": 1.3111455108359133e-05,
      "loss": 58.4602,
      "step": 17439
    },
    {
      "epoch": 17.46,
      "grad_norm": 4637.08349609375,
      "learning_rate": 1.3106295149638804e-05,
      "loss": 61.054,
      "step": 17440
    },
    {
      "epoch": 17.46,
      "grad_norm": 5769.6572265625,
      "learning_rate": 1.3101135190918473e-05,
      "loss": 43.2473,
      "step": 17441
    },
    {
      "epoch": 17.46,
      "grad_norm": 32621.357421875,
      "learning_rate": 1.3095975232198143e-05,
      "loss": 58.2381,
      "step": 17442
    },
    {
      "epoch": 17.46,
      "grad_norm": 740.5299072265625,
      "learning_rate": 1.3090815273477814e-05,
      "loss": 61.7316,
      "step": 17443
    },
    {
      "epoch": 17.46,
      "grad_norm": 1330.372314453125,
      "learning_rate": 1.3085655314757483e-05,
      "loss": 62.0459,
      "step": 17444
    },
    {
      "epoch": 17.46,
      "grad_norm": 138469.6875,
      "learning_rate": 1.3080495356037154e-05,
      "loss": 35.6613,
      "step": 17445
    },
    {
      "epoch": 17.46,
      "grad_norm": 4115.7109375,
      "learning_rate": 1.307533539731682e-05,
      "loss": 64.9187,
      "step": 17446
    },
    {
      "epoch": 17.46,
      "grad_norm": 26754.49609375,
      "learning_rate": 1.3070175438596491e-05,
      "loss": 60.9492,
      "step": 17447
    },
    {
      "epoch": 17.47,
      "grad_norm": 25435.66015625,
      "learning_rate": 1.3065015479876161e-05,
      "loss": 62.9028,
      "step": 17448
    },
    {
      "epoch": 17.47,
      "grad_norm": 82883.4609375,
      "learning_rate": 1.305985552115583e-05,
      "loss": 54.4793,
      "step": 17449
    },
    {
      "epoch": 17.47,
      "grad_norm": 3797.982177734375,
      "learning_rate": 1.3054695562435501e-05,
      "loss": 56.382,
      "step": 17450
    },
    {
      "epoch": 17.47,
      "grad_norm": 36618.25390625,
      "learning_rate": 1.3049535603715171e-05,
      "loss": 52.7436,
      "step": 17451
    },
    {
      "epoch": 17.47,
      "grad_norm": 9314.8525390625,
      "learning_rate": 1.3044375644994842e-05,
      "loss": 52.9706,
      "step": 17452
    },
    {
      "epoch": 17.47,
      "grad_norm": 4624.74169921875,
      "learning_rate": 1.3039215686274511e-05,
      "loss": 44.8413,
      "step": 17453
    },
    {
      "epoch": 17.47,
      "grad_norm": 4927.20166015625,
      "learning_rate": 1.3034055727554181e-05,
      "loss": 56.0417,
      "step": 17454
    },
    {
      "epoch": 17.47,
      "grad_norm": 926.06591796875,
      "learning_rate": 1.3028895768833849e-05,
      "loss": 55.2505,
      "step": 17455
    },
    {
      "epoch": 17.47,
      "grad_norm": 33017.20703125,
      "learning_rate": 1.3023735810113518e-05,
      "loss": 30.2703,
      "step": 17456
    },
    {
      "epoch": 17.47,
      "grad_norm": 12016.13671875,
      "learning_rate": 1.301857585139319e-05,
      "loss": 46.76,
      "step": 17457
    },
    {
      "epoch": 17.48,
      "grad_norm": 97918.375,
      "learning_rate": 1.3013415892672859e-05,
      "loss": 55.2194,
      "step": 17458
    },
    {
      "epoch": 17.48,
      "grad_norm": 3601.435302734375,
      "learning_rate": 1.300825593395253e-05,
      "loss": 61.3201,
      "step": 17459
    },
    {
      "epoch": 17.48,
      "grad_norm": 3735.873291015625,
      "learning_rate": 1.30030959752322e-05,
      "loss": 62.1347,
      "step": 17460
    },
    {
      "epoch": 17.48,
      "grad_norm": 5765.41943359375,
      "learning_rate": 1.2997936016511869e-05,
      "loss": 55.8583,
      "step": 17461
    },
    {
      "epoch": 17.48,
      "grad_norm": 7713.1416015625,
      "learning_rate": 1.299277605779154e-05,
      "loss": 67.2716,
      "step": 17462
    },
    {
      "epoch": 17.48,
      "grad_norm": 6955.56396484375,
      "learning_rate": 1.298761609907121e-05,
      "loss": 50.2663,
      "step": 17463
    },
    {
      "epoch": 17.48,
      "grad_norm": 3911.990234375,
      "learning_rate": 1.2982456140350877e-05,
      "loss": 39.247,
      "step": 17464
    },
    {
      "epoch": 17.48,
      "grad_norm": 3833.05908203125,
      "learning_rate": 1.2977296181630546e-05,
      "loss": 60.8018,
      "step": 17465
    },
    {
      "epoch": 17.48,
      "grad_norm": 10304.48046875,
      "learning_rate": 1.2972136222910216e-05,
      "loss": 58.217,
      "step": 17466
    },
    {
      "epoch": 17.48,
      "grad_norm": 3411.452392578125,
      "learning_rate": 1.2966976264189887e-05,
      "loss": 64.3787,
      "step": 17467
    },
    {
      "epoch": 17.49,
      "grad_norm": 7063.5732421875,
      "learning_rate": 1.2961816305469556e-05,
      "loss": 52.3185,
      "step": 17468
    },
    {
      "epoch": 17.49,
      "grad_norm": 11054.0458984375,
      "learning_rate": 1.2956656346749228e-05,
      "loss": 52.5032,
      "step": 17469
    },
    {
      "epoch": 17.49,
      "grad_norm": 6718.4013671875,
      "learning_rate": 1.2951496388028897e-05,
      "loss": 51.0389,
      "step": 17470
    },
    {
      "epoch": 17.49,
      "grad_norm": 6482.56689453125,
      "learning_rate": 1.2946336429308566e-05,
      "loss": 13.6948,
      "step": 17471
    },
    {
      "epoch": 17.49,
      "grad_norm": 16225.8984375,
      "learning_rate": 1.2941176470588238e-05,
      "loss": 48.374,
      "step": 17472
    },
    {
      "epoch": 17.49,
      "grad_norm": 8527.7861328125,
      "learning_rate": 1.2936016511867904e-05,
      "loss": 44.9356,
      "step": 17473
    },
    {
      "epoch": 17.49,
      "grad_norm": 37877.01953125,
      "learning_rate": 1.2930856553147575e-05,
      "loss": 53.4641,
      "step": 17474
    },
    {
      "epoch": 17.49,
      "grad_norm": 7986.53564453125,
      "learning_rate": 1.2925696594427244e-05,
      "loss": 56.3959,
      "step": 17475
    },
    {
      "epoch": 17.49,
      "grad_norm": 1137.0592041015625,
      "learning_rate": 1.2920536635706915e-05,
      "loss": 67.0395,
      "step": 17476
    },
    {
      "epoch": 17.49,
      "grad_norm": 27345.92578125,
      "learning_rate": 1.2915376676986585e-05,
      "loss": 57.6646,
      "step": 17477
    },
    {
      "epoch": 17.5,
      "grad_norm": 12384.8154296875,
      "learning_rate": 1.2910216718266254e-05,
      "loss": 45.251,
      "step": 17478
    },
    {
      "epoch": 17.5,
      "grad_norm": 16296.859375,
      "learning_rate": 1.2905056759545925e-05,
      "loss": 55.6508,
      "step": 17479
    },
    {
      "epoch": 17.5,
      "grad_norm": 2774.726318359375,
      "learning_rate": 1.2899896800825595e-05,
      "loss": 53.4105,
      "step": 17480
    },
    {
      "epoch": 17.5,
      "grad_norm": 9716.7568359375,
      "learning_rate": 1.2894736842105264e-05,
      "loss": 59.0301,
      "step": 17481
    },
    {
      "epoch": 17.5,
      "grad_norm": 36718.64453125,
      "learning_rate": 1.2889576883384932e-05,
      "loss": 58.7629,
      "step": 17482
    },
    {
      "epoch": 17.5,
      "grad_norm": 12936.4658203125,
      "learning_rate": 1.2884416924664603e-05,
      "loss": 68.1397,
      "step": 17483
    },
    {
      "epoch": 17.5,
      "grad_norm": 19010.115234375,
      "learning_rate": 1.2879256965944272e-05,
      "loss": 50.6035,
      "step": 17484
    },
    {
      "epoch": 17.5,
      "grad_norm": 1407.9593505859375,
      "learning_rate": 1.2874097007223942e-05,
      "loss": 63.8743,
      "step": 17485
    },
    {
      "epoch": 17.5,
      "grad_norm": 2892.09130859375,
      "learning_rate": 1.2868937048503613e-05,
      "loss": 61.5588,
      "step": 17486
    },
    {
      "epoch": 17.5,
      "grad_norm": 959.6503295898438,
      "learning_rate": 1.2863777089783282e-05,
      "loss": 63.6935,
      "step": 17487
    },
    {
      "epoch": 17.51,
      "grad_norm": 2010.197021484375,
      "learning_rate": 1.2858617131062952e-05,
      "loss": 55.5135,
      "step": 17488
    },
    {
      "epoch": 17.51,
      "grad_norm": 4878.3876953125,
      "learning_rate": 1.2853457172342623e-05,
      "loss": 62.6766,
      "step": 17489
    },
    {
      "epoch": 17.51,
      "grad_norm": 84147.6484375,
      "learning_rate": 1.2848297213622292e-05,
      "loss": 26.7504,
      "step": 17490
    },
    {
      "epoch": 17.51,
      "grad_norm": 61562.7890625,
      "learning_rate": 1.284313725490196e-05,
      "loss": 66.7848,
      "step": 17491
    },
    {
      "epoch": 17.51,
      "grad_norm": 10847.4326171875,
      "learning_rate": 1.283797729618163e-05,
      "loss": 50.8904,
      "step": 17492
    },
    {
      "epoch": 17.51,
      "grad_norm": 25793.455078125,
      "learning_rate": 1.28328173374613e-05,
      "loss": 56.0086,
      "step": 17493
    },
    {
      "epoch": 17.51,
      "grad_norm": 10961.974609375,
      "learning_rate": 1.282765737874097e-05,
      "loss": 49.8085,
      "step": 17494
    },
    {
      "epoch": 17.51,
      "grad_norm": 14599.685546875,
      "learning_rate": 1.282249742002064e-05,
      "loss": 51.5579,
      "step": 17495
    },
    {
      "epoch": 17.51,
      "grad_norm": 1150.8895263671875,
      "learning_rate": 1.281733746130031e-05,
      "loss": 60.8157,
      "step": 17496
    },
    {
      "epoch": 17.51,
      "grad_norm": 9098.314453125,
      "learning_rate": 1.281217750257998e-05,
      "loss": 49.7348,
      "step": 17497
    },
    {
      "epoch": 17.52,
      "grad_norm": 32793.390625,
      "learning_rate": 1.2807017543859651e-05,
      "loss": 64.5595,
      "step": 17498
    },
    {
      "epoch": 17.52,
      "grad_norm": 40859.45703125,
      "learning_rate": 1.280185758513932e-05,
      "loss": 58.4199,
      "step": 17499
    },
    {
      "epoch": 17.52,
      "grad_norm": 11604.8427734375,
      "learning_rate": 1.2796697626418989e-05,
      "loss": 48.7158,
      "step": 17500
    },
    {
      "epoch": 17.52,
      "grad_norm": 1862.5875244140625,
      "learning_rate": 1.2791537667698658e-05,
      "loss": 52.3239,
      "step": 17501
    },
    {
      "epoch": 17.52,
      "grad_norm": 10497.8916015625,
      "learning_rate": 1.2786377708978327e-05,
      "loss": 47.1192,
      "step": 17502
    },
    {
      "epoch": 17.52,
      "grad_norm": 6990.14501953125,
      "learning_rate": 1.2781217750257999e-05,
      "loss": 57.8292,
      "step": 17503
    },
    {
      "epoch": 17.52,
      "grad_norm": 11685.4736328125,
      "learning_rate": 1.2776057791537668e-05,
      "loss": 49.0411,
      "step": 17504
    },
    {
      "epoch": 17.52,
      "grad_norm": 11261.5888671875,
      "learning_rate": 1.2770897832817339e-05,
      "loss": 54.4908,
      "step": 17505
    },
    {
      "epoch": 17.52,
      "grad_norm": 24025.703125,
      "learning_rate": 1.2765737874097009e-05,
      "loss": 30.1709,
      "step": 17506
    },
    {
      "epoch": 17.52,
      "grad_norm": 4140.44189453125,
      "learning_rate": 1.2760577915376678e-05,
      "loss": 63.415,
      "step": 17507
    },
    {
      "epoch": 17.53,
      "grad_norm": 24533.830078125,
      "learning_rate": 1.2755417956656349e-05,
      "loss": 47.9252,
      "step": 17508
    },
    {
      "epoch": 17.53,
      "grad_norm": 28223.923828125,
      "learning_rate": 1.2750257997936015e-05,
      "loss": 63.0944,
      "step": 17509
    },
    {
      "epoch": 17.53,
      "grad_norm": 3829.34716796875,
      "learning_rate": 1.2745098039215686e-05,
      "loss": 62.1817,
      "step": 17510
    },
    {
      "epoch": 17.53,
      "grad_norm": 10656.4638671875,
      "learning_rate": 1.2739938080495356e-05,
      "loss": 50.1031,
      "step": 17511
    },
    {
      "epoch": 17.53,
      "grad_norm": 2392.6103515625,
      "learning_rate": 1.2734778121775027e-05,
      "loss": 57.8633,
      "step": 17512
    },
    {
      "epoch": 17.53,
      "grad_norm": 5077.11083984375,
      "learning_rate": 1.2729618163054696e-05,
      "loss": 50.8224,
      "step": 17513
    },
    {
      "epoch": 17.53,
      "grad_norm": 58549.21875,
      "learning_rate": 1.2724458204334366e-05,
      "loss": 44.4476,
      "step": 17514
    },
    {
      "epoch": 17.53,
      "grad_norm": 6796.0029296875,
      "learning_rate": 1.2719298245614037e-05,
      "loss": 53.1977,
      "step": 17515
    },
    {
      "epoch": 17.53,
      "grad_norm": 157792.203125,
      "learning_rate": 1.2714138286893706e-05,
      "loss": 66.2754,
      "step": 17516
    },
    {
      "epoch": 17.53,
      "grad_norm": 16559.412109375,
      "learning_rate": 1.2708978328173376e-05,
      "loss": 54.8468,
      "step": 17517
    },
    {
      "epoch": 17.54,
      "grad_norm": 6888.625,
      "learning_rate": 1.2703818369453043e-05,
      "loss": 45.0205,
      "step": 17518
    },
    {
      "epoch": 17.54,
      "grad_norm": 15950.984375,
      "learning_rate": 1.2698658410732715e-05,
      "loss": 58.9352,
      "step": 17519
    },
    {
      "epoch": 17.54,
      "grad_norm": 5690.32080078125,
      "learning_rate": 1.2693498452012384e-05,
      "loss": 56.0302,
      "step": 17520
    },
    {
      "epoch": 17.54,
      "grad_norm": 11709.4755859375,
      "learning_rate": 1.2688338493292053e-05,
      "loss": 56.3265,
      "step": 17521
    },
    {
      "epoch": 17.54,
      "grad_norm": 120626.5625,
      "learning_rate": 1.2683178534571725e-05,
      "loss": 48.0969,
      "step": 17522
    },
    {
      "epoch": 17.54,
      "grad_norm": 7177.111328125,
      "learning_rate": 1.2678018575851394e-05,
      "loss": 55.1847,
      "step": 17523
    },
    {
      "epoch": 17.54,
      "grad_norm": 11544.474609375,
      "learning_rate": 1.2672858617131063e-05,
      "loss": 65.9167,
      "step": 17524
    },
    {
      "epoch": 17.54,
      "grad_norm": 46759.28515625,
      "learning_rate": 1.2667698658410735e-05,
      "loss": 47.657,
      "step": 17525
    },
    {
      "epoch": 17.54,
      "grad_norm": 2386.877685546875,
      "learning_rate": 1.2662538699690404e-05,
      "loss": 48.9758,
      "step": 17526
    },
    {
      "epoch": 17.54,
      "grad_norm": 19450.54296875,
      "learning_rate": 1.2657378740970072e-05,
      "loss": 51.7521,
      "step": 17527
    },
    {
      "epoch": 17.55,
      "grad_norm": 17329.103515625,
      "learning_rate": 1.2652218782249741e-05,
      "loss": 53.777,
      "step": 17528
    },
    {
      "epoch": 17.55,
      "grad_norm": 11086.0673828125,
      "learning_rate": 1.2647058823529412e-05,
      "loss": 55.2659,
      "step": 17529
    },
    {
      "epoch": 17.55,
      "grad_norm": 2862.18701171875,
      "learning_rate": 1.2641898864809082e-05,
      "loss": 48.4023,
      "step": 17530
    },
    {
      "epoch": 17.55,
      "grad_norm": 2695.332275390625,
      "learning_rate": 1.2636738906088751e-05,
      "loss": 59.821,
      "step": 17531
    },
    {
      "epoch": 17.55,
      "grad_norm": 15583.31640625,
      "learning_rate": 1.2631578947368422e-05,
      "loss": 54.4324,
      "step": 17532
    },
    {
      "epoch": 17.55,
      "grad_norm": 2889.8837890625,
      "learning_rate": 1.2626418988648092e-05,
      "loss": 51.6613,
      "step": 17533
    },
    {
      "epoch": 17.55,
      "grad_norm": 17902.154296875,
      "learning_rate": 1.2621259029927763e-05,
      "loss": 63.4719,
      "step": 17534
    },
    {
      "epoch": 17.55,
      "grad_norm": 68790.578125,
      "learning_rate": 1.2616099071207432e-05,
      "loss": 38.2255,
      "step": 17535
    },
    {
      "epoch": 17.55,
      "grad_norm": 8254.6259765625,
      "learning_rate": 1.26109391124871e-05,
      "loss": 62.4214,
      "step": 17536
    },
    {
      "epoch": 17.55,
      "grad_norm": 19219.169921875,
      "learning_rate": 1.260577915376677e-05,
      "loss": 60.5415,
      "step": 17537
    },
    {
      "epoch": 17.56,
      "grad_norm": 13288.8544921875,
      "learning_rate": 1.2600619195046439e-05,
      "loss": 52.7901,
      "step": 17538
    },
    {
      "epoch": 17.56,
      "grad_norm": 4040.025634765625,
      "learning_rate": 1.259545923632611e-05,
      "loss": 54.897,
      "step": 17539
    },
    {
      "epoch": 17.56,
      "grad_norm": 8231.755859375,
      "learning_rate": 1.259029927760578e-05,
      "loss": 53.6418,
      "step": 17540
    },
    {
      "epoch": 17.56,
      "grad_norm": 2322.92919921875,
      "learning_rate": 1.258513931888545e-05,
      "loss": 54.4469,
      "step": 17541
    },
    {
      "epoch": 17.56,
      "grad_norm": 26716.111328125,
      "learning_rate": 1.257997936016512e-05,
      "loss": 53.5211,
      "step": 17542
    },
    {
      "epoch": 17.56,
      "grad_norm": 5500.64306640625,
      "learning_rate": 1.257481940144479e-05,
      "loss": 64.7751,
      "step": 17543
    },
    {
      "epoch": 17.56,
      "grad_norm": 5667.00244140625,
      "learning_rate": 1.256965944272446e-05,
      "loss": 52.0799,
      "step": 17544
    },
    {
      "epoch": 17.56,
      "grad_norm": 52713.18359375,
      "learning_rate": 1.2564499484004127e-05,
      "loss": 52.6709,
      "step": 17545
    },
    {
      "epoch": 17.56,
      "grad_norm": 14297.3564453125,
      "learning_rate": 1.2559339525283798e-05,
      "loss": 51.769,
      "step": 17546
    },
    {
      "epoch": 17.56,
      "grad_norm": 24703.77734375,
      "learning_rate": 1.2554179566563467e-05,
      "loss": 48.6302,
      "step": 17547
    },
    {
      "epoch": 17.57,
      "grad_norm": 32080.81640625,
      "learning_rate": 1.2549019607843138e-05,
      "loss": 60.5939,
      "step": 17548
    },
    {
      "epoch": 17.57,
      "grad_norm": 4387.1455078125,
      "learning_rate": 1.2543859649122808e-05,
      "loss": 62.8779,
      "step": 17549
    },
    {
      "epoch": 17.57,
      "grad_norm": 1834.9638671875,
      "learning_rate": 1.2538699690402477e-05,
      "loss": 62.5166,
      "step": 17550
    },
    {
      "epoch": 17.57,
      "grad_norm": 9467.5732421875,
      "learning_rate": 1.2533539731682148e-05,
      "loss": 39.242,
      "step": 17551
    },
    {
      "epoch": 17.57,
      "grad_norm": 18347.7109375,
      "learning_rate": 1.2528379772961818e-05,
      "loss": 52.3498,
      "step": 17552
    },
    {
      "epoch": 17.57,
      "grad_norm": 9014.255859375,
      "learning_rate": 1.2523219814241487e-05,
      "loss": 59.1375,
      "step": 17553
    },
    {
      "epoch": 17.57,
      "grad_norm": 1668.93310546875,
      "learning_rate": 1.2518059855521155e-05,
      "loss": 62.4734,
      "step": 17554
    },
    {
      "epoch": 17.57,
      "grad_norm": 2259.155517578125,
      "learning_rate": 1.2512899896800826e-05,
      "loss": 65.6084,
      "step": 17555
    },
    {
      "epoch": 17.57,
      "grad_norm": 11739.384765625,
      "learning_rate": 1.2507739938080496e-05,
      "loss": 56.3647,
      "step": 17556
    },
    {
      "epoch": 17.57,
      "grad_norm": 3076.460205078125,
      "learning_rate": 1.2502579979360165e-05,
      "loss": 57.788,
      "step": 17557
    },
    {
      "epoch": 17.58,
      "grad_norm": 35178.76953125,
      "learning_rate": 1.2497420020639836e-05,
      "loss": 59.0707,
      "step": 17558
    },
    {
      "epoch": 17.58,
      "grad_norm": 5319.93359375,
      "learning_rate": 1.2492260061919506e-05,
      "loss": 61.6098,
      "step": 17559
    },
    {
      "epoch": 17.58,
      "grad_norm": 25966.015625,
      "learning_rate": 1.2487100103199175e-05,
      "loss": 50.3765,
      "step": 17560
    },
    {
      "epoch": 17.58,
      "grad_norm": 3665.046875,
      "learning_rate": 1.2481940144478844e-05,
      "loss": 52.8993,
      "step": 17561
    },
    {
      "epoch": 17.58,
      "grad_norm": 7886.8447265625,
      "learning_rate": 1.2476780185758514e-05,
      "loss": 42.9049,
      "step": 17562
    },
    {
      "epoch": 17.58,
      "grad_norm": 5995.12255859375,
      "learning_rate": 1.2471620227038185e-05,
      "loss": 53.0288,
      "step": 17563
    },
    {
      "epoch": 17.58,
      "grad_norm": 12237.1416015625,
      "learning_rate": 1.2466460268317854e-05,
      "loss": 63.5529,
      "step": 17564
    },
    {
      "epoch": 17.58,
      "grad_norm": 1769.26318359375,
      "learning_rate": 1.2461300309597524e-05,
      "loss": 56.927,
      "step": 17565
    },
    {
      "epoch": 17.58,
      "grad_norm": 8531.68359375,
      "learning_rate": 1.2456140350877193e-05,
      "loss": 56.9627,
      "step": 17566
    },
    {
      "epoch": 17.58,
      "grad_norm": 7305.515625,
      "learning_rate": 1.2450980392156863e-05,
      "loss": 54.4172,
      "step": 17567
    },
    {
      "epoch": 17.59,
      "grad_norm": 8503.87890625,
      "learning_rate": 1.2445820433436534e-05,
      "loss": 45.5376,
      "step": 17568
    },
    {
      "epoch": 17.59,
      "grad_norm": 2878.08642578125,
      "learning_rate": 1.2440660474716202e-05,
      "loss": 59.9381,
      "step": 17569
    },
    {
      "epoch": 17.59,
      "grad_norm": 9729.064453125,
      "learning_rate": 1.2435500515995873e-05,
      "loss": 52.4839,
      "step": 17570
    },
    {
      "epoch": 17.59,
      "grad_norm": 3711.65478515625,
      "learning_rate": 1.2430340557275542e-05,
      "loss": 46.6064,
      "step": 17571
    },
    {
      "epoch": 17.59,
      "grad_norm": 2128.864501953125,
      "learning_rate": 1.2425180598555212e-05,
      "loss": 60.4253,
      "step": 17572
    },
    {
      "epoch": 17.59,
      "grad_norm": 1779.295166015625,
      "learning_rate": 1.2420020639834883e-05,
      "loss": 53.4827,
      "step": 17573
    },
    {
      "epoch": 17.59,
      "grad_norm": 24454.47265625,
      "learning_rate": 1.241486068111455e-05,
      "loss": 56.5314,
      "step": 17574
    },
    {
      "epoch": 17.59,
      "grad_norm": 6518.3076171875,
      "learning_rate": 1.2409700722394222e-05,
      "loss": 61.6679,
      "step": 17575
    },
    {
      "epoch": 17.59,
      "grad_norm": 34251.515625,
      "learning_rate": 1.2404540763673891e-05,
      "loss": 53.5222,
      "step": 17576
    },
    {
      "epoch": 17.59,
      "grad_norm": 4110.56640625,
      "learning_rate": 1.2399380804953562e-05,
      "loss": 56.0226,
      "step": 17577
    },
    {
      "epoch": 17.6,
      "grad_norm": 40063.0546875,
      "learning_rate": 1.239422084623323e-05,
      "loss": 61.5849,
      "step": 17578
    },
    {
      "epoch": 17.6,
      "grad_norm": 98074.5546875,
      "learning_rate": 1.23890608875129e-05,
      "loss": 36.2055,
      "step": 17579
    },
    {
      "epoch": 17.6,
      "grad_norm": 1707.3526611328125,
      "learning_rate": 1.238390092879257e-05,
      "loss": 56.669,
      "step": 17580
    },
    {
      "epoch": 17.6,
      "grad_norm": 515326.96875,
      "learning_rate": 1.237874097007224e-05,
      "loss": 51.1279,
      "step": 17581
    },
    {
      "epoch": 17.6,
      "grad_norm": 2589.34521484375,
      "learning_rate": 1.2373581011351911e-05,
      "loss": 61.521,
      "step": 17582
    },
    {
      "epoch": 17.6,
      "grad_norm": 1936.680419921875,
      "learning_rate": 1.2368421052631579e-05,
      "loss": 54.8933,
      "step": 17583
    },
    {
      "epoch": 17.6,
      "grad_norm": 35148.3359375,
      "learning_rate": 1.236326109391125e-05,
      "loss": 64.1138,
      "step": 17584
    },
    {
      "epoch": 17.6,
      "grad_norm": 15949.2060546875,
      "learning_rate": 1.235810113519092e-05,
      "loss": 62.3322,
      "step": 17585
    },
    {
      "epoch": 17.6,
      "grad_norm": 17475.4609375,
      "learning_rate": 1.2352941176470589e-05,
      "loss": 60.1041,
      "step": 17586
    },
    {
      "epoch": 17.6,
      "grad_norm": 1189.1845703125,
      "learning_rate": 1.2347781217750258e-05,
      "loss": 54.013,
      "step": 17587
    },
    {
      "epoch": 17.61,
      "grad_norm": 5487.193359375,
      "learning_rate": 1.2342621259029928e-05,
      "loss": 48.534,
      "step": 17588
    },
    {
      "epoch": 17.61,
      "grad_norm": 16977.28515625,
      "learning_rate": 1.2337461300309599e-05,
      "loss": 59.4842,
      "step": 17589
    },
    {
      "epoch": 17.61,
      "grad_norm": 15820.8994140625,
      "learning_rate": 1.2332301341589268e-05,
      "loss": 44.5813,
      "step": 17590
    },
    {
      "epoch": 17.61,
      "grad_norm": 7554.93017578125,
      "learning_rate": 1.2327141382868938e-05,
      "loss": 66.3568,
      "step": 17591
    },
    {
      "epoch": 17.61,
      "grad_norm": 53982.140625,
      "learning_rate": 1.2321981424148607e-05,
      "loss": 62.6422,
      "step": 17592
    },
    {
      "epoch": 17.61,
      "grad_norm": 3762.294189453125,
      "learning_rate": 1.2316821465428277e-05,
      "loss": 56.5038,
      "step": 17593
    },
    {
      "epoch": 17.61,
      "grad_norm": 13475.150390625,
      "learning_rate": 1.2311661506707948e-05,
      "loss": 63.1473,
      "step": 17594
    },
    {
      "epoch": 17.61,
      "grad_norm": 19207.4140625,
      "learning_rate": 1.2306501547987617e-05,
      "loss": 61.7294,
      "step": 17595
    },
    {
      "epoch": 17.61,
      "grad_norm": 21237.533203125,
      "learning_rate": 1.2301341589267287e-05,
      "loss": 56.769,
      "step": 17596
    },
    {
      "epoch": 17.61,
      "grad_norm": 2278.2060546875,
      "learning_rate": 1.2296181630546956e-05,
      "loss": 46.283,
      "step": 17597
    },
    {
      "epoch": 17.62,
      "grad_norm": 24969.134765625,
      "learning_rate": 1.2291021671826625e-05,
      "loss": 59.6928,
      "step": 17598
    },
    {
      "epoch": 17.62,
      "grad_norm": 9070.75390625,
      "learning_rate": 1.2285861713106297e-05,
      "loss": 56.888,
      "step": 17599
    },
    {
      "epoch": 17.62,
      "grad_norm": 52299.7265625,
      "learning_rate": 1.2280701754385964e-05,
      "loss": 53.487,
      "step": 17600
    },
    {
      "epoch": 17.62,
      "grad_norm": 13496.1025390625,
      "learning_rate": 1.2275541795665635e-05,
      "loss": 61.7964,
      "step": 17601
    },
    {
      "epoch": 17.62,
      "grad_norm": 2895.2275390625,
      "learning_rate": 1.2270381836945305e-05,
      "loss": 55.848,
      "step": 17602
    },
    {
      "epoch": 17.62,
      "grad_norm": 47170.14453125,
      "learning_rate": 1.2265221878224974e-05,
      "loss": 37.7381,
      "step": 17603
    },
    {
      "epoch": 17.62,
      "grad_norm": 5641.81298828125,
      "learning_rate": 1.2260061919504645e-05,
      "loss": 54.6119,
      "step": 17604
    },
    {
      "epoch": 17.62,
      "grad_norm": 8948.7939453125,
      "learning_rate": 1.2254901960784313e-05,
      "loss": 58.0839,
      "step": 17605
    },
    {
      "epoch": 17.62,
      "grad_norm": 3278.55712890625,
      "learning_rate": 1.2249742002063984e-05,
      "loss": 50.9626,
      "step": 17606
    },
    {
      "epoch": 17.62,
      "grad_norm": 44089.19921875,
      "learning_rate": 1.2244582043343654e-05,
      "loss": 58.2708,
      "step": 17607
    },
    {
      "epoch": 17.63,
      "grad_norm": 6250.427734375,
      "learning_rate": 1.2239422084623323e-05,
      "loss": 53.266,
      "step": 17608
    },
    {
      "epoch": 17.63,
      "grad_norm": 7920.1455078125,
      "learning_rate": 1.2234262125902993e-05,
      "loss": 58.0193,
      "step": 17609
    },
    {
      "epoch": 17.63,
      "grad_norm": 3098.290771484375,
      "learning_rate": 1.2229102167182662e-05,
      "loss": 64.8167,
      "step": 17610
    },
    {
      "epoch": 17.63,
      "grad_norm": 1874.12255859375,
      "learning_rate": 1.2223942208462333e-05,
      "loss": 60.4143,
      "step": 17611
    },
    {
      "epoch": 17.63,
      "grad_norm": 25631.921875,
      "learning_rate": 1.2218782249742003e-05,
      "loss": 60.7747,
      "step": 17612
    },
    {
      "epoch": 17.63,
      "grad_norm": 12497.693359375,
      "learning_rate": 1.2213622291021674e-05,
      "loss": 57.4169,
      "step": 17613
    },
    {
      "epoch": 17.63,
      "grad_norm": 6525.33837890625,
      "learning_rate": 1.2208462332301342e-05,
      "loss": 46.5474,
      "step": 17614
    },
    {
      "epoch": 17.63,
      "grad_norm": 7455.74560546875,
      "learning_rate": 1.2203302373581011e-05,
      "loss": 50.5739,
      "step": 17615
    },
    {
      "epoch": 17.63,
      "grad_norm": 6768.71630859375,
      "learning_rate": 1.2198142414860682e-05,
      "loss": 42.2679,
      "step": 17616
    },
    {
      "epoch": 17.63,
      "grad_norm": 78985.9453125,
      "learning_rate": 1.2192982456140352e-05,
      "loss": 31.599,
      "step": 17617
    },
    {
      "epoch": 17.64,
      "grad_norm": 1709.2042236328125,
      "learning_rate": 1.2187822497420021e-05,
      "loss": 63.3247,
      "step": 17618
    },
    {
      "epoch": 17.64,
      "grad_norm": 22844.662109375,
      "learning_rate": 1.218266253869969e-05,
      "loss": 53.7599,
      "step": 17619
    },
    {
      "epoch": 17.64,
      "grad_norm": 6860.5,
      "learning_rate": 1.2177502579979362e-05,
      "loss": 61.6612,
      "step": 17620
    },
    {
      "epoch": 17.64,
      "grad_norm": 65772.15625,
      "learning_rate": 1.2172342621259031e-05,
      "loss": 45.1317,
      "step": 17621
    },
    {
      "epoch": 17.64,
      "grad_norm": 2617.41064453125,
      "learning_rate": 1.21671826625387e-05,
      "loss": 64.6981,
      "step": 17622
    },
    {
      "epoch": 17.64,
      "grad_norm": 13067.4921875,
      "learning_rate": 1.216202270381837e-05,
      "loss": 56.4052,
      "step": 17623
    },
    {
      "epoch": 17.64,
      "grad_norm": 11122.615234375,
      "learning_rate": 1.215686274509804e-05,
      "loss": 36.8695,
      "step": 17624
    },
    {
      "epoch": 17.64,
      "grad_norm": 2350.934326171875,
      "learning_rate": 1.215170278637771e-05,
      "loss": 66.6764,
      "step": 17625
    },
    {
      "epoch": 17.64,
      "grad_norm": 24988.91796875,
      "learning_rate": 1.214654282765738e-05,
      "loss": 65.1654,
      "step": 17626
    },
    {
      "epoch": 17.64,
      "grad_norm": 4476.29052734375,
      "learning_rate": 1.214138286893705e-05,
      "loss": 55.0891,
      "step": 17627
    },
    {
      "epoch": 17.65,
      "grad_norm": 11633.1904296875,
      "learning_rate": 1.2136222910216719e-05,
      "loss": 51.5067,
      "step": 17628
    },
    {
      "epoch": 17.65,
      "grad_norm": 8472.2509765625,
      "learning_rate": 1.2131062951496388e-05,
      "loss": 31.3834,
      "step": 17629
    },
    {
      "epoch": 17.65,
      "grad_norm": 12025.4404296875,
      "learning_rate": 1.212590299277606e-05,
      "loss": 43.3304,
      "step": 17630
    },
    {
      "epoch": 17.65,
      "grad_norm": 997.2538452148438,
      "learning_rate": 1.2120743034055729e-05,
      "loss": 64.4862,
      "step": 17631
    },
    {
      "epoch": 17.65,
      "grad_norm": 25182.1796875,
      "learning_rate": 1.2115583075335398e-05,
      "loss": 57.5056,
      "step": 17632
    },
    {
      "epoch": 17.65,
      "grad_norm": 3219.957763671875,
      "learning_rate": 1.2110423116615068e-05,
      "loss": 62.0703,
      "step": 17633
    },
    {
      "epoch": 17.65,
      "grad_norm": 15794.2666015625,
      "learning_rate": 1.2105263157894737e-05,
      "loss": 58.7465,
      "step": 17634
    },
    {
      "epoch": 17.65,
      "grad_norm": 11817.494140625,
      "learning_rate": 1.2100103199174408e-05,
      "loss": 36.4498,
      "step": 17635
    },
    {
      "epoch": 17.65,
      "grad_norm": 2341.385498046875,
      "learning_rate": 1.2094943240454076e-05,
      "loss": 60.509,
      "step": 17636
    },
    {
      "epoch": 17.65,
      "grad_norm": 9531.611328125,
      "learning_rate": 1.2089783281733747e-05,
      "loss": 24.0773,
      "step": 17637
    },
    {
      "epoch": 17.66,
      "grad_norm": 10328.94921875,
      "learning_rate": 1.2084623323013416e-05,
      "loss": 61.1555,
      "step": 17638
    },
    {
      "epoch": 17.66,
      "grad_norm": 25487.939453125,
      "learning_rate": 1.2079463364293086e-05,
      "loss": 67.3146,
      "step": 17639
    },
    {
      "epoch": 17.66,
      "grad_norm": 5232.99267578125,
      "learning_rate": 1.2074303405572757e-05,
      "loss": 55.1044,
      "step": 17640
    },
    {
      "epoch": 17.66,
      "grad_norm": 28750.67578125,
      "learning_rate": 1.2069143446852425e-05,
      "loss": 52.8879,
      "step": 17641
    },
    {
      "epoch": 17.66,
      "grad_norm": 12032.1806640625,
      "learning_rate": 1.2063983488132096e-05,
      "loss": 58.1688,
      "step": 17642
    },
    {
      "epoch": 17.66,
      "grad_norm": 753.4974975585938,
      "learning_rate": 1.2058823529411765e-05,
      "loss": 58.6585,
      "step": 17643
    },
    {
      "epoch": 17.66,
      "grad_norm": 56673.28515625,
      "learning_rate": 1.2053663570691435e-05,
      "loss": 55.131,
      "step": 17644
    },
    {
      "epoch": 17.66,
      "grad_norm": 35321.84375,
      "learning_rate": 1.2048503611971104e-05,
      "loss": 53.8619,
      "step": 17645
    },
    {
      "epoch": 17.66,
      "grad_norm": 1009.9290771484375,
      "learning_rate": 1.2043343653250774e-05,
      "loss": 58.283,
      "step": 17646
    },
    {
      "epoch": 17.66,
      "grad_norm": 8599.01171875,
      "learning_rate": 1.2038183694530445e-05,
      "loss": 65.6836,
      "step": 17647
    },
    {
      "epoch": 17.67,
      "grad_norm": 8011.40771484375,
      "learning_rate": 1.2033023735810114e-05,
      "loss": 60.5346,
      "step": 17648
    },
    {
      "epoch": 17.67,
      "grad_norm": 13193.04296875,
      "learning_rate": 1.2027863777089785e-05,
      "loss": 34.988,
      "step": 17649
    },
    {
      "epoch": 17.67,
      "grad_norm": 9047.142578125,
      "learning_rate": 1.2022703818369453e-05,
      "loss": 52.9693,
      "step": 17650
    },
    {
      "epoch": 17.67,
      "grad_norm": 7547.81884765625,
      "learning_rate": 1.2017543859649123e-05,
      "loss": 56.025,
      "step": 17651
    },
    {
      "epoch": 17.67,
      "grad_norm": 4844.66943359375,
      "learning_rate": 1.2012383900928794e-05,
      "loss": 64.2169,
      "step": 17652
    },
    {
      "epoch": 17.67,
      "grad_norm": 88670.34375,
      "learning_rate": 1.2007223942208463e-05,
      "loss": 57.1987,
      "step": 17653
    },
    {
      "epoch": 17.67,
      "grad_norm": 12479.185546875,
      "learning_rate": 1.2002063983488133e-05,
      "loss": 62.6008,
      "step": 17654
    },
    {
      "epoch": 17.67,
      "grad_norm": 3198.885498046875,
      "learning_rate": 1.1996904024767802e-05,
      "loss": 61.6603,
      "step": 17655
    },
    {
      "epoch": 17.67,
      "grad_norm": 12304.5400390625,
      "learning_rate": 1.1991744066047471e-05,
      "loss": 59.4403,
      "step": 17656
    },
    {
      "epoch": 17.67,
      "grad_norm": 8473.908203125,
      "learning_rate": 1.1986584107327143e-05,
      "loss": 60.2376,
      "step": 17657
    },
    {
      "epoch": 17.68,
      "grad_norm": 1025.327392578125,
      "learning_rate": 1.1981424148606812e-05,
      "loss": 60.0993,
      "step": 17658
    },
    {
      "epoch": 17.68,
      "grad_norm": 4270.0107421875,
      "learning_rate": 1.1976264189886481e-05,
      "loss": 52.6396,
      "step": 17659
    },
    {
      "epoch": 17.68,
      "grad_norm": 8165.34228515625,
      "learning_rate": 1.197110423116615e-05,
      "loss": 61.8401,
      "step": 17660
    },
    {
      "epoch": 17.68,
      "grad_norm": 11015.4091796875,
      "learning_rate": 1.1965944272445822e-05,
      "loss": 32.9222,
      "step": 17661
    },
    {
      "epoch": 17.68,
      "grad_norm": 20714.046875,
      "learning_rate": 1.1960784313725491e-05,
      "loss": 50.303,
      "step": 17662
    },
    {
      "epoch": 17.68,
      "grad_norm": 104120.8828125,
      "learning_rate": 1.1955624355005159e-05,
      "loss": 61.4931,
      "step": 17663
    },
    {
      "epoch": 17.68,
      "grad_norm": 12315.4072265625,
      "learning_rate": 1.195046439628483e-05,
      "loss": 62.5529,
      "step": 17664
    },
    {
      "epoch": 17.68,
      "grad_norm": 7818.3779296875,
      "learning_rate": 1.19453044375645e-05,
      "loss": 54.9294,
      "step": 17665
    },
    {
      "epoch": 17.68,
      "grad_norm": 1184.41357421875,
      "learning_rate": 1.194014447884417e-05,
      "loss": 60.945,
      "step": 17666
    },
    {
      "epoch": 17.68,
      "grad_norm": 4910.6328125,
      "learning_rate": 1.193498452012384e-05,
      "loss": 62.0325,
      "step": 17667
    },
    {
      "epoch": 17.69,
      "grad_norm": 4265.4306640625,
      "learning_rate": 1.192982456140351e-05,
      "loss": 58.3738,
      "step": 17668
    },
    {
      "epoch": 17.69,
      "grad_norm": 3253.999267578125,
      "learning_rate": 1.192466460268318e-05,
      "loss": 48.6524,
      "step": 17669
    },
    {
      "epoch": 17.69,
      "grad_norm": 4527.5458984375,
      "learning_rate": 1.1919504643962849e-05,
      "loss": 64.1722,
      "step": 17670
    },
    {
      "epoch": 17.69,
      "grad_norm": 1075.7637939453125,
      "learning_rate": 1.191434468524252e-05,
      "loss": 51.6514,
      "step": 17671
    },
    {
      "epoch": 17.69,
      "grad_norm": 2469.005615234375,
      "learning_rate": 1.1909184726522187e-05,
      "loss": 61.8372,
      "step": 17672
    },
    {
      "epoch": 17.69,
      "grad_norm": 6834.24462890625,
      "learning_rate": 1.1904024767801859e-05,
      "loss": 48.9518,
      "step": 17673
    },
    {
      "epoch": 17.69,
      "grad_norm": 9087.2333984375,
      "learning_rate": 1.1898864809081528e-05,
      "loss": 39.4571,
      "step": 17674
    },
    {
      "epoch": 17.69,
      "grad_norm": 7760.056640625,
      "learning_rate": 1.1893704850361197e-05,
      "loss": 49.5697,
      "step": 17675
    },
    {
      "epoch": 17.69,
      "grad_norm": 25004.154296875,
      "learning_rate": 1.1888544891640867e-05,
      "loss": 55.711,
      "step": 17676
    },
    {
      "epoch": 17.69,
      "grad_norm": 46965.69140625,
      "learning_rate": 1.1883384932920536e-05,
      "loss": 22.9111,
      "step": 17677
    },
    {
      "epoch": 17.7,
      "grad_norm": 1663.220947265625,
      "learning_rate": 1.1878224974200207e-05,
      "loss": 61.0269,
      "step": 17678
    },
    {
      "epoch": 17.7,
      "grad_norm": 57925.66796875,
      "learning_rate": 1.1873065015479877e-05,
      "loss": 36.2926,
      "step": 17679
    },
    {
      "epoch": 17.7,
      "grad_norm": 5012.10693359375,
      "learning_rate": 1.1867905056759546e-05,
      "loss": 58.5504,
      "step": 17680
    },
    {
      "epoch": 17.7,
      "grad_norm": 73114.6484375,
      "learning_rate": 1.1862745098039216e-05,
      "loss": 56.4151,
      "step": 17681
    },
    {
      "epoch": 17.7,
      "grad_norm": 20803.240234375,
      "learning_rate": 1.1857585139318885e-05,
      "loss": 61.6683,
      "step": 17682
    },
    {
      "epoch": 17.7,
      "grad_norm": 2525.0751953125,
      "learning_rate": 1.1852425180598556e-05,
      "loss": 57.0279,
      "step": 17683
    },
    {
      "epoch": 17.7,
      "grad_norm": 4850.46533203125,
      "learning_rate": 1.1847265221878226e-05,
      "loss": 53.2875,
      "step": 17684
    },
    {
      "epoch": 17.7,
      "grad_norm": 27722.5703125,
      "learning_rate": 1.1842105263157895e-05,
      "loss": 50.5879,
      "step": 17685
    },
    {
      "epoch": 17.7,
      "grad_norm": 4362.3115234375,
      "learning_rate": 1.1836945304437565e-05,
      "loss": 56.5088,
      "step": 17686
    },
    {
      "epoch": 17.7,
      "grad_norm": 3601.663330078125,
      "learning_rate": 1.1831785345717234e-05,
      "loss": 62.0351,
      "step": 17687
    },
    {
      "epoch": 17.71,
      "grad_norm": 52059.796875,
      "learning_rate": 1.1826625386996905e-05,
      "loss": 27.1661,
      "step": 17688
    },
    {
      "epoch": 17.71,
      "grad_norm": 9082.64453125,
      "learning_rate": 1.1821465428276575e-05,
      "loss": 61.5337,
      "step": 17689
    },
    {
      "epoch": 17.71,
      "grad_norm": 75893.453125,
      "learning_rate": 1.1816305469556244e-05,
      "loss": 54.3179,
      "step": 17690
    },
    {
      "epoch": 17.71,
      "grad_norm": 3465.7099609375,
      "learning_rate": 1.1811145510835914e-05,
      "loss": 60.0041,
      "step": 17691
    },
    {
      "epoch": 17.71,
      "grad_norm": 6571.75439453125,
      "learning_rate": 1.1805985552115583e-05,
      "loss": 46.0785,
      "step": 17692
    },
    {
      "epoch": 17.71,
      "grad_norm": 7238.73779296875,
      "learning_rate": 1.1800825593395254e-05,
      "loss": 60.1099,
      "step": 17693
    },
    {
      "epoch": 17.71,
      "grad_norm": 36340.51953125,
      "learning_rate": 1.1795665634674922e-05,
      "loss": 55.8886,
      "step": 17694
    },
    {
      "epoch": 17.71,
      "grad_norm": 3416.00830078125,
      "learning_rate": 1.1790505675954593e-05,
      "loss": 51.0985,
      "step": 17695
    },
    {
      "epoch": 17.71,
      "grad_norm": 9814.724609375,
      "learning_rate": 1.1785345717234262e-05,
      "loss": 51.1693,
      "step": 17696
    },
    {
      "epoch": 17.71,
      "grad_norm": 21141.595703125,
      "learning_rate": 1.1780185758513934e-05,
      "loss": 64.0764,
      "step": 17697
    },
    {
      "epoch": 17.72,
      "grad_norm": 10395.0166015625,
      "learning_rate": 1.1775025799793603e-05,
      "loss": 56.6149,
      "step": 17698
    },
    {
      "epoch": 17.72,
      "grad_norm": 14190.3564453125,
      "learning_rate": 1.176986584107327e-05,
      "loss": 58.5494,
      "step": 17699
    },
    {
      "epoch": 17.72,
      "grad_norm": 27056.8046875,
      "learning_rate": 1.1764705882352942e-05,
      "loss": 58.8475,
      "step": 17700
    },
    {
      "epoch": 17.72,
      "grad_norm": 15106.150390625,
      "learning_rate": 1.1759545923632611e-05,
      "loss": 58.5283,
      "step": 17701
    },
    {
      "epoch": 17.72,
      "grad_norm": 41754.359375,
      "learning_rate": 1.1754385964912282e-05,
      "loss": 54.4694,
      "step": 17702
    },
    {
      "epoch": 17.72,
      "grad_norm": 4207.849609375,
      "learning_rate": 1.174922600619195e-05,
      "loss": 62.7136,
      "step": 17703
    },
    {
      "epoch": 17.72,
      "grad_norm": 4666.70849609375,
      "learning_rate": 1.1744066047471621e-05,
      "loss": 66.1099,
      "step": 17704
    },
    {
      "epoch": 17.72,
      "grad_norm": 5586.06591796875,
      "learning_rate": 1.173890608875129e-05,
      "loss": 55.8326,
      "step": 17705
    },
    {
      "epoch": 17.72,
      "grad_norm": 16870.025390625,
      "learning_rate": 1.173374613003096e-05,
      "loss": 55.0262,
      "step": 17706
    },
    {
      "epoch": 17.72,
      "grad_norm": 4939.13037109375,
      "learning_rate": 1.1728586171310631e-05,
      "loss": 60.7749,
      "step": 17707
    },
    {
      "epoch": 17.73,
      "grad_norm": 3071.2470703125,
      "learning_rate": 1.1723426212590299e-05,
      "loss": 57.1578,
      "step": 17708
    },
    {
      "epoch": 17.73,
      "grad_norm": 11426.0390625,
      "learning_rate": 1.171826625386997e-05,
      "loss": 61.1906,
      "step": 17709
    },
    {
      "epoch": 17.73,
      "grad_norm": 2473.862548828125,
      "learning_rate": 1.171310629514964e-05,
      "loss": 59.8347,
      "step": 17710
    },
    {
      "epoch": 17.73,
      "grad_norm": 9890.3330078125,
      "learning_rate": 1.1707946336429309e-05,
      "loss": 49.814,
      "step": 17711
    },
    {
      "epoch": 17.73,
      "grad_norm": 7136.798828125,
      "learning_rate": 1.1702786377708978e-05,
      "loss": 42.2199,
      "step": 17712
    },
    {
      "epoch": 17.73,
      "grad_norm": 5471.93505859375,
      "learning_rate": 1.1697626418988648e-05,
      "loss": 46.5577,
      "step": 17713
    },
    {
      "epoch": 17.73,
      "grad_norm": 30108.2734375,
      "learning_rate": 1.1692466460268319e-05,
      "loss": 68.0172,
      "step": 17714
    },
    {
      "epoch": 17.73,
      "grad_norm": 12388.810546875,
      "learning_rate": 1.1687306501547988e-05,
      "loss": 66.5277,
      "step": 17715
    },
    {
      "epoch": 17.73,
      "grad_norm": 1574.5697021484375,
      "learning_rate": 1.1682146542827658e-05,
      "loss": 66.4948,
      "step": 17716
    },
    {
      "epoch": 17.73,
      "grad_norm": 24885.74609375,
      "learning_rate": 1.1676986584107327e-05,
      "loss": 60.1335,
      "step": 17717
    },
    {
      "epoch": 17.74,
      "grad_norm": 1199.305419921875,
      "learning_rate": 1.1671826625386997e-05,
      "loss": 56.6126,
      "step": 17718
    },
    {
      "epoch": 17.74,
      "grad_norm": 3829.415771484375,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 51.6452,
      "step": 17719
    },
    {
      "epoch": 17.74,
      "grad_norm": 3926.248046875,
      "learning_rate": 1.1661506707946337e-05,
      "loss": 56.9457,
      "step": 17720
    },
    {
      "epoch": 17.74,
      "grad_norm": 7242.2626953125,
      "learning_rate": 1.1656346749226007e-05,
      "loss": 67.0307,
      "step": 17721
    },
    {
      "epoch": 17.74,
      "grad_norm": 49297.70703125,
      "learning_rate": 1.1651186790505676e-05,
      "loss": 46.9029,
      "step": 17722
    },
    {
      "epoch": 17.74,
      "grad_norm": 10378.8115234375,
      "learning_rate": 1.1646026831785346e-05,
      "loss": 35.3937,
      "step": 17723
    },
    {
      "epoch": 17.74,
      "grad_norm": 57900.78515625,
      "learning_rate": 1.1640866873065017e-05,
      "loss": 56.8982,
      "step": 17724
    },
    {
      "epoch": 17.74,
      "grad_norm": 5570.3251953125,
      "learning_rate": 1.1635706914344686e-05,
      "loss": 52.8049,
      "step": 17725
    },
    {
      "epoch": 17.74,
      "grad_norm": 3183.26416015625,
      "learning_rate": 1.1630546955624356e-05,
      "loss": 57.4867,
      "step": 17726
    },
    {
      "epoch": 17.74,
      "grad_norm": 10316.2841796875,
      "learning_rate": 1.1625386996904025e-05,
      "loss": 58.2228,
      "step": 17727
    },
    {
      "epoch": 17.75,
      "grad_norm": 1546.685791015625,
      "learning_rate": 1.1620227038183695e-05,
      "loss": 65.1435,
      "step": 17728
    },
    {
      "epoch": 17.75,
      "grad_norm": 22388.001953125,
      "learning_rate": 1.1615067079463366e-05,
      "loss": 42.0038,
      "step": 17729
    },
    {
      "epoch": 17.75,
      "grad_norm": 2263.365234375,
      "learning_rate": 1.1609907120743033e-05,
      "loss": 50.4567,
      "step": 17730
    },
    {
      "epoch": 17.75,
      "grad_norm": 2494.469482421875,
      "learning_rate": 1.1604747162022705e-05,
      "loss": 51.5966,
      "step": 17731
    },
    {
      "epoch": 17.75,
      "grad_norm": 3113.376708984375,
      "learning_rate": 1.1599587203302374e-05,
      "loss": 65.4955,
      "step": 17732
    },
    {
      "epoch": 17.75,
      "grad_norm": 8838.1494140625,
      "learning_rate": 1.1594427244582045e-05,
      "loss": 51.731,
      "step": 17733
    },
    {
      "epoch": 17.75,
      "grad_norm": 9077.830078125,
      "learning_rate": 1.1589267285861715e-05,
      "loss": 61.2761,
      "step": 17734
    },
    {
      "epoch": 17.75,
      "grad_norm": 9924.30859375,
      "learning_rate": 1.1584107327141382e-05,
      "loss": 35.5674,
      "step": 17735
    },
    {
      "epoch": 17.75,
      "grad_norm": 253610.296875,
      "learning_rate": 1.1578947368421053e-05,
      "loss": 62.7678,
      "step": 17736
    },
    {
      "epoch": 17.75,
      "grad_norm": 1653.8331298828125,
      "learning_rate": 1.1573787409700723e-05,
      "loss": 50.7548,
      "step": 17737
    },
    {
      "epoch": 17.76,
      "grad_norm": 10489.701171875,
      "learning_rate": 1.1568627450980394e-05,
      "loss": 60.1596,
      "step": 17738
    },
    {
      "epoch": 17.76,
      "grad_norm": 90532.7734375,
      "learning_rate": 1.1563467492260062e-05,
      "loss": 58.0899,
      "step": 17739
    },
    {
      "epoch": 17.76,
      "grad_norm": 69029.3984375,
      "learning_rate": 1.1558307533539733e-05,
      "loss": 63.1544,
      "step": 17740
    },
    {
      "epoch": 17.76,
      "grad_norm": 45359.42578125,
      "learning_rate": 1.1553147574819402e-05,
      "loss": 24.7921,
      "step": 17741
    },
    {
      "epoch": 17.76,
      "grad_norm": 4141.5341796875,
      "learning_rate": 1.1547987616099072e-05,
      "loss": 64.3498,
      "step": 17742
    },
    {
      "epoch": 17.76,
      "grad_norm": 3539.988525390625,
      "learning_rate": 1.1542827657378741e-05,
      "loss": 49.4362,
      "step": 17743
    },
    {
      "epoch": 17.76,
      "grad_norm": 821.5667114257812,
      "learning_rate": 1.153766769865841e-05,
      "loss": 63.8009,
      "step": 17744
    },
    {
      "epoch": 17.76,
      "grad_norm": 2678.13818359375,
      "learning_rate": 1.1532507739938082e-05,
      "loss": 55.8753,
      "step": 17745
    },
    {
      "epoch": 17.76,
      "grad_norm": 17061.400390625,
      "learning_rate": 1.1527347781217751e-05,
      "loss": 42.9788,
      "step": 17746
    },
    {
      "epoch": 17.76,
      "grad_norm": 1840.426025390625,
      "learning_rate": 1.152218782249742e-05,
      "loss": 63.589,
      "step": 17747
    },
    {
      "epoch": 17.77,
      "grad_norm": 2336.244140625,
      "learning_rate": 1.151702786377709e-05,
      "loss": 53.9771,
      "step": 17748
    },
    {
      "epoch": 17.77,
      "grad_norm": 1852.509521484375,
      "learning_rate": 1.151186790505676e-05,
      "loss": 62.7721,
      "step": 17749
    },
    {
      "epoch": 17.77,
      "grad_norm": 3831.840087890625,
      "learning_rate": 1.150670794633643e-05,
      "loss": 61.8551,
      "step": 17750
    },
    {
      "epoch": 17.77,
      "grad_norm": 44410.1328125,
      "learning_rate": 1.15015479876161e-05,
      "loss": 62.0323,
      "step": 17751
    },
    {
      "epoch": 17.77,
      "grad_norm": 5177.59619140625,
      "learning_rate": 1.149638802889577e-05,
      "loss": 52.2851,
      "step": 17752
    },
    {
      "epoch": 17.77,
      "grad_norm": 8595.4814453125,
      "learning_rate": 1.1491228070175439e-05,
      "loss": 47.2413,
      "step": 17753
    },
    {
      "epoch": 17.77,
      "grad_norm": 48966.8984375,
      "learning_rate": 1.1486068111455108e-05,
      "loss": 62.5,
      "step": 17754
    },
    {
      "epoch": 17.77,
      "grad_norm": 4153.2109375,
      "learning_rate": 1.148090815273478e-05,
      "loss": 58.1207,
      "step": 17755
    },
    {
      "epoch": 17.77,
      "grad_norm": 42841.0078125,
      "learning_rate": 1.1475748194014449e-05,
      "loss": 38.3075,
      "step": 17756
    },
    {
      "epoch": 17.77,
      "grad_norm": 4778.5966796875,
      "learning_rate": 1.1470588235294118e-05,
      "loss": 64.3947,
      "step": 17757
    },
    {
      "epoch": 17.78,
      "grad_norm": 8085.673828125,
      "learning_rate": 1.1465428276573788e-05,
      "loss": 55.6354,
      "step": 17758
    },
    {
      "epoch": 17.78,
      "grad_norm": 6129.32080078125,
      "learning_rate": 1.1460268317853457e-05,
      "loss": 63.4874,
      "step": 17759
    },
    {
      "epoch": 17.78,
      "grad_norm": 777.6839599609375,
      "learning_rate": 1.1455108359133128e-05,
      "loss": 58.3774,
      "step": 17760
    },
    {
      "epoch": 17.78,
      "grad_norm": 59113.0390625,
      "learning_rate": 1.1449948400412796e-05,
      "loss": 47.9086,
      "step": 17761
    },
    {
      "epoch": 17.78,
      "grad_norm": 39158.36328125,
      "learning_rate": 1.1444788441692467e-05,
      "loss": 63.9892,
      "step": 17762
    },
    {
      "epoch": 17.78,
      "grad_norm": 9254.2880859375,
      "learning_rate": 1.1439628482972137e-05,
      "loss": 57.1739,
      "step": 17763
    },
    {
      "epoch": 17.78,
      "grad_norm": 29027.806640625,
      "learning_rate": 1.1434468524251806e-05,
      "loss": 54.12,
      "step": 17764
    },
    {
      "epoch": 17.78,
      "grad_norm": 93631.28125,
      "learning_rate": 1.1429308565531477e-05,
      "loss": 62.9526,
      "step": 17765
    },
    {
      "epoch": 17.78,
      "grad_norm": 3722.089599609375,
      "learning_rate": 1.1424148606811145e-05,
      "loss": 52.3625,
      "step": 17766
    },
    {
      "epoch": 17.78,
      "grad_norm": 9026.1083984375,
      "learning_rate": 1.1418988648090816e-05,
      "loss": 54.5591,
      "step": 17767
    },
    {
      "epoch": 17.79,
      "grad_norm": 4145.16552734375,
      "learning_rate": 1.1413828689370486e-05,
      "loss": 49.3742,
      "step": 17768
    },
    {
      "epoch": 17.79,
      "grad_norm": 11958.1552734375,
      "learning_rate": 1.1408668730650155e-05,
      "loss": 54.6949,
      "step": 17769
    },
    {
      "epoch": 17.79,
      "grad_norm": 97252.890625,
      "learning_rate": 1.1403508771929824e-05,
      "loss": 40.9776,
      "step": 17770
    },
    {
      "epoch": 17.79,
      "grad_norm": 1187.5721435546875,
      "learning_rate": 1.1398348813209494e-05,
      "loss": 64.3417,
      "step": 17771
    },
    {
      "epoch": 17.79,
      "grad_norm": 56190.42578125,
      "learning_rate": 1.1393188854489165e-05,
      "loss": 55.7857,
      "step": 17772
    },
    {
      "epoch": 17.79,
      "grad_norm": 58447.7265625,
      "learning_rate": 1.1388028895768834e-05,
      "loss": 37.1662,
      "step": 17773
    },
    {
      "epoch": 17.79,
      "grad_norm": 59863.953125,
      "learning_rate": 1.1382868937048506e-05,
      "loss": 54.0671,
      "step": 17774
    },
    {
      "epoch": 17.79,
      "grad_norm": 23304.18359375,
      "learning_rate": 1.1377708978328173e-05,
      "loss": 51.0709,
      "step": 17775
    },
    {
      "epoch": 17.79,
      "grad_norm": 10023.072265625,
      "learning_rate": 1.1372549019607843e-05,
      "loss": 64.176,
      "step": 17776
    },
    {
      "epoch": 17.79,
      "grad_norm": 36811.41796875,
      "learning_rate": 1.1367389060887514e-05,
      "loss": 50.9157,
      "step": 17777
    },
    {
      "epoch": 17.8,
      "grad_norm": 7900.923828125,
      "learning_rate": 1.1362229102167183e-05,
      "loss": 58.7312,
      "step": 17778
    },
    {
      "epoch": 17.8,
      "grad_norm": 3214.747802734375,
      "learning_rate": 1.1357069143446853e-05,
      "loss": 57.424,
      "step": 17779
    },
    {
      "epoch": 17.8,
      "grad_norm": 6746.96435546875,
      "learning_rate": 1.1351909184726522e-05,
      "loss": 50.975,
      "step": 17780
    },
    {
      "epoch": 17.8,
      "grad_norm": 5711.64599609375,
      "learning_rate": 1.1346749226006193e-05,
      "loss": 54.4412,
      "step": 17781
    },
    {
      "epoch": 17.8,
      "grad_norm": 4876.3916015625,
      "learning_rate": 1.1341589267285863e-05,
      "loss": 50.3968,
      "step": 17782
    },
    {
      "epoch": 17.8,
      "grad_norm": 12394.2177734375,
      "learning_rate": 1.1336429308565532e-05,
      "loss": 50.9886,
      "step": 17783
    },
    {
      "epoch": 17.8,
      "grad_norm": 4578.76806640625,
      "learning_rate": 1.1331269349845202e-05,
      "loss": 59.1164,
      "step": 17784
    },
    {
      "epoch": 17.8,
      "grad_norm": 4344.3779296875,
      "learning_rate": 1.1326109391124871e-05,
      "loss": 62.4352,
      "step": 17785
    },
    {
      "epoch": 17.8,
      "grad_norm": 10916.9990234375,
      "learning_rate": 1.1320949432404542e-05,
      "loss": 59.1378,
      "step": 17786
    },
    {
      "epoch": 17.8,
      "grad_norm": 13887.87109375,
      "learning_rate": 1.1315789473684212e-05,
      "loss": 25.7177,
      "step": 17787
    },
    {
      "epoch": 17.81,
      "grad_norm": 15284.447265625,
      "learning_rate": 1.1310629514963881e-05,
      "loss": 57.174,
      "step": 17788
    },
    {
      "epoch": 17.81,
      "grad_norm": 9574.1650390625,
      "learning_rate": 1.130546955624355e-05,
      "loss": 65.3528,
      "step": 17789
    },
    {
      "epoch": 17.81,
      "grad_norm": 9683.8125,
      "learning_rate": 1.130030959752322e-05,
      "loss": 26.3767,
      "step": 17790
    },
    {
      "epoch": 17.81,
      "grad_norm": 5968.2705078125,
      "learning_rate": 1.1295149638802891e-05,
      "loss": 46.7733,
      "step": 17791
    },
    {
      "epoch": 17.81,
      "grad_norm": 2220.555419921875,
      "learning_rate": 1.128998968008256e-05,
      "loss": 62.5253,
      "step": 17792
    },
    {
      "epoch": 17.81,
      "grad_norm": 58853.98046875,
      "learning_rate": 1.128482972136223e-05,
      "loss": 48.3999,
      "step": 17793
    },
    {
      "epoch": 17.81,
      "grad_norm": 4894.3828125,
      "learning_rate": 1.12796697626419e-05,
      "loss": 59.6801,
      "step": 17794
    },
    {
      "epoch": 17.81,
      "grad_norm": 5811.9248046875,
      "learning_rate": 1.1274509803921569e-05,
      "loss": 56.4657,
      "step": 17795
    },
    {
      "epoch": 17.81,
      "grad_norm": 19535.033203125,
      "learning_rate": 1.126934984520124e-05,
      "loss": 17.1301,
      "step": 17796
    },
    {
      "epoch": 17.81,
      "grad_norm": 7626.5693359375,
      "learning_rate": 1.1264189886480908e-05,
      "loss": 61.2055,
      "step": 17797
    },
    {
      "epoch": 17.82,
      "grad_norm": 13924.2021484375,
      "learning_rate": 1.1259029927760579e-05,
      "loss": 61.4064,
      "step": 17798
    },
    {
      "epoch": 17.82,
      "grad_norm": 2022.2294921875,
      "learning_rate": 1.1253869969040248e-05,
      "loss": 60.2887,
      "step": 17799
    },
    {
      "epoch": 17.82,
      "grad_norm": 3117.309814453125,
      "learning_rate": 1.1248710010319918e-05,
      "loss": 62.2519,
      "step": 17800
    },
    {
      "epoch": 17.82,
      "grad_norm": 12189.6435546875,
      "learning_rate": 1.1243550051599589e-05,
      "loss": 45.4079,
      "step": 17801
    },
    {
      "epoch": 17.82,
      "grad_norm": 2181.371337890625,
      "learning_rate": 1.1238390092879257e-05,
      "loss": 60.6024,
      "step": 17802
    },
    {
      "epoch": 17.82,
      "grad_norm": 232231.3125,
      "learning_rate": 1.1233230134158928e-05,
      "loss": 57.0515,
      "step": 17803
    },
    {
      "epoch": 17.82,
      "grad_norm": 4154.966796875,
      "learning_rate": 1.1228070175438597e-05,
      "loss": 62.3538,
      "step": 17804
    },
    {
      "epoch": 17.82,
      "grad_norm": 3466.380126953125,
      "learning_rate": 1.1222910216718267e-05,
      "loss": 63.6901,
      "step": 17805
    },
    {
      "epoch": 17.82,
      "grad_norm": 17425.17578125,
      "learning_rate": 1.1217750257997936e-05,
      "loss": 49.4113,
      "step": 17806
    },
    {
      "epoch": 17.82,
      "grad_norm": 22258.826171875,
      "learning_rate": 1.1212590299277605e-05,
      "loss": 56.5059,
      "step": 17807
    },
    {
      "epoch": 17.83,
      "grad_norm": 54533.71484375,
      "learning_rate": 1.1207430340557277e-05,
      "loss": 54.4742,
      "step": 17808
    },
    {
      "epoch": 17.83,
      "grad_norm": 20606.083984375,
      "learning_rate": 1.1202270381836946e-05,
      "loss": 56.8698,
      "step": 17809
    },
    {
      "epoch": 17.83,
      "grad_norm": 28570.29296875,
      "learning_rate": 1.1197110423116617e-05,
      "loss": 52.5375,
      "step": 17810
    },
    {
      "epoch": 17.83,
      "grad_norm": 10475.373046875,
      "learning_rate": 1.1191950464396285e-05,
      "loss": 55.1632,
      "step": 17811
    },
    {
      "epoch": 17.83,
      "grad_norm": 4361.9765625,
      "learning_rate": 1.1186790505675954e-05,
      "loss": 59.7785,
      "step": 17812
    },
    {
      "epoch": 17.83,
      "grad_norm": 15900.263671875,
      "learning_rate": 1.1181630546955625e-05,
      "loss": 59.5495,
      "step": 17813
    },
    {
      "epoch": 17.83,
      "grad_norm": 7249.1787109375,
      "learning_rate": 1.1176470588235295e-05,
      "loss": 45.2825,
      "step": 17814
    },
    {
      "epoch": 17.83,
      "grad_norm": 48566.1640625,
      "learning_rate": 1.1171310629514964e-05,
      "loss": 35.8049,
      "step": 17815
    },
    {
      "epoch": 17.83,
      "grad_norm": 28141.3046875,
      "learning_rate": 1.1166150670794634e-05,
      "loss": 55.7736,
      "step": 17816
    },
    {
      "epoch": 17.83,
      "grad_norm": 27938.013671875,
      "learning_rate": 1.1160990712074305e-05,
      "loss": 51.6362,
      "step": 17817
    },
    {
      "epoch": 17.84,
      "grad_norm": 4400.23974609375,
      "learning_rate": 1.1155830753353974e-05,
      "loss": 60.6257,
      "step": 17818
    },
    {
      "epoch": 17.84,
      "grad_norm": 120046.328125,
      "learning_rate": 1.1150670794633642e-05,
      "loss": 59.0133,
      "step": 17819
    },
    {
      "epoch": 17.84,
      "grad_norm": 23483.78125,
      "learning_rate": 1.1145510835913313e-05,
      "loss": 37.3352,
      "step": 17820
    },
    {
      "epoch": 17.84,
      "grad_norm": 3422.19775390625,
      "learning_rate": 1.1140350877192983e-05,
      "loss": 56.6037,
      "step": 17821
    },
    {
      "epoch": 17.84,
      "grad_norm": 23205.658203125,
      "learning_rate": 1.1135190918472654e-05,
      "loss": 42.2559,
      "step": 17822
    },
    {
      "epoch": 17.84,
      "grad_norm": 4482.52001953125,
      "learning_rate": 1.1130030959752323e-05,
      "loss": 68.3635,
      "step": 17823
    },
    {
      "epoch": 17.84,
      "grad_norm": 3834.467041015625,
      "learning_rate": 1.1124871001031993e-05,
      "loss": 58.4505,
      "step": 17824
    },
    {
      "epoch": 17.84,
      "grad_norm": 17952.87890625,
      "learning_rate": 1.1119711042311662e-05,
      "loss": 45.0814,
      "step": 17825
    },
    {
      "epoch": 17.84,
      "grad_norm": 12032.251953125,
      "learning_rate": 1.1114551083591331e-05,
      "loss": 43.4219,
      "step": 17826
    },
    {
      "epoch": 17.84,
      "grad_norm": 1796.6959228515625,
      "learning_rate": 1.1109391124871003e-05,
      "loss": 66.089,
      "step": 17827
    },
    {
      "epoch": 17.85,
      "grad_norm": 8003.90966796875,
      "learning_rate": 1.110423116615067e-05,
      "loss": 53.9911,
      "step": 17828
    },
    {
      "epoch": 17.85,
      "grad_norm": 1496.4619140625,
      "learning_rate": 1.1099071207430341e-05,
      "loss": 64.1438,
      "step": 17829
    },
    {
      "epoch": 17.85,
      "grad_norm": 5697.93701171875,
      "learning_rate": 1.1093911248710011e-05,
      "loss": 60.4888,
      "step": 17830
    },
    {
      "epoch": 17.85,
      "grad_norm": 27618.859375,
      "learning_rate": 1.108875128998968e-05,
      "loss": 58.7992,
      "step": 17831
    },
    {
      "epoch": 17.85,
      "grad_norm": 6215.24853515625,
      "learning_rate": 1.1083591331269351e-05,
      "loss": 58.307,
      "step": 17832
    },
    {
      "epoch": 17.85,
      "grad_norm": 4009.87646484375,
      "learning_rate": 1.107843137254902e-05,
      "loss": 55.0279,
      "step": 17833
    },
    {
      "epoch": 17.85,
      "grad_norm": 12570.7138671875,
      "learning_rate": 1.107327141382869e-05,
      "loss": 51.9157,
      "step": 17834
    },
    {
      "epoch": 17.85,
      "grad_norm": 5173.58544921875,
      "learning_rate": 1.106811145510836e-05,
      "loss": 39.3175,
      "step": 17835
    },
    {
      "epoch": 17.85,
      "grad_norm": 4803.544921875,
      "learning_rate": 1.106295149638803e-05,
      "loss": 56.441,
      "step": 17836
    },
    {
      "epoch": 17.85,
      "grad_norm": 34649.203125,
      "learning_rate": 1.1057791537667699e-05,
      "loss": 46.6364,
      "step": 17837
    },
    {
      "epoch": 17.86,
      "grad_norm": 7780.44287109375,
      "learning_rate": 1.1052631578947368e-05,
      "loss": 51.7958,
      "step": 17838
    },
    {
      "epoch": 17.86,
      "grad_norm": 16122.4033203125,
      "learning_rate": 1.104747162022704e-05,
      "loss": 57.7231,
      "step": 17839
    },
    {
      "epoch": 17.86,
      "grad_norm": 47472.046875,
      "learning_rate": 1.1042311661506709e-05,
      "loss": 52.4635,
      "step": 17840
    },
    {
      "epoch": 17.86,
      "grad_norm": 6630.0478515625,
      "learning_rate": 1.1037151702786378e-05,
      "loss": 52.7992,
      "step": 17841
    },
    {
      "epoch": 17.86,
      "grad_norm": 10679.03515625,
      "learning_rate": 1.1031991744066048e-05,
      "loss": 51.9622,
      "step": 17842
    },
    {
      "epoch": 17.86,
      "grad_norm": 25208.021484375,
      "learning_rate": 1.1026831785345717e-05,
      "loss": 46.49,
      "step": 17843
    },
    {
      "epoch": 17.86,
      "grad_norm": 927.93994140625,
      "learning_rate": 1.1021671826625388e-05,
      "loss": 56.926,
      "step": 17844
    },
    {
      "epoch": 17.86,
      "grad_norm": 18651.06640625,
      "learning_rate": 1.1016511867905058e-05,
      "loss": 37.7855,
      "step": 17845
    },
    {
      "epoch": 17.86,
      "grad_norm": 5554.52587890625,
      "learning_rate": 1.1011351909184727e-05,
      "loss": 58.4123,
      "step": 17846
    },
    {
      "epoch": 17.86,
      "grad_norm": 964.9662475585938,
      "learning_rate": 1.1006191950464396e-05,
      "loss": 58.8997,
      "step": 17847
    },
    {
      "epoch": 17.87,
      "grad_norm": 10505.35546875,
      "learning_rate": 1.1001031991744066e-05,
      "loss": 47.4176,
      "step": 17848
    },
    {
      "epoch": 17.87,
      "grad_norm": 6757.74072265625,
      "learning_rate": 1.0995872033023737e-05,
      "loss": 60.4334,
      "step": 17849
    },
    {
      "epoch": 17.87,
      "grad_norm": 125758.6015625,
      "learning_rate": 1.0990712074303406e-05,
      "loss": 48.4505,
      "step": 17850
    },
    {
      "epoch": 17.87,
      "grad_norm": 3668.4443359375,
      "learning_rate": 1.0985552115583076e-05,
      "loss": 58.029,
      "step": 17851
    },
    {
      "epoch": 17.87,
      "grad_norm": 259101.5,
      "learning_rate": 1.0980392156862745e-05,
      "loss": 55.7093,
      "step": 17852
    },
    {
      "epoch": 17.87,
      "grad_norm": 2994.64599609375,
      "learning_rate": 1.0975232198142415e-05,
      "loss": 54.4403,
      "step": 17853
    },
    {
      "epoch": 17.87,
      "grad_norm": 16529.876953125,
      "learning_rate": 1.0970072239422086e-05,
      "loss": 56.948,
      "step": 17854
    },
    {
      "epoch": 17.87,
      "grad_norm": 6364.43701171875,
      "learning_rate": 1.0964912280701754e-05,
      "loss": 49.5236,
      "step": 17855
    },
    {
      "epoch": 17.87,
      "grad_norm": 5514.32080078125,
      "learning_rate": 1.0959752321981425e-05,
      "loss": 50.4788,
      "step": 17856
    },
    {
      "epoch": 17.87,
      "grad_norm": 5319.02783203125,
      "learning_rate": 1.0954592363261094e-05,
      "loss": 55.8743,
      "step": 17857
    },
    {
      "epoch": 17.88,
      "grad_norm": 1926.720703125,
      "learning_rate": 1.0949432404540765e-05,
      "loss": 60.5207,
      "step": 17858
    },
    {
      "epoch": 17.88,
      "grad_norm": 2286.135498046875,
      "learning_rate": 1.0944272445820435e-05,
      "loss": 64.1494,
      "step": 17859
    },
    {
      "epoch": 17.88,
      "grad_norm": 4179.30810546875,
      "learning_rate": 1.0939112487100102e-05,
      "loss": 51.1105,
      "step": 17860
    },
    {
      "epoch": 17.88,
      "grad_norm": 15833.306640625,
      "learning_rate": 1.0933952528379774e-05,
      "loss": 68.053,
      "step": 17861
    },
    {
      "epoch": 17.88,
      "grad_norm": 5722.44970703125,
      "learning_rate": 1.0928792569659443e-05,
      "loss": 61.6983,
      "step": 17862
    },
    {
      "epoch": 17.88,
      "grad_norm": 23489.66015625,
      "learning_rate": 1.0923632610939114e-05,
      "loss": 58.8374,
      "step": 17863
    },
    {
      "epoch": 17.88,
      "grad_norm": 8187.10595703125,
      "learning_rate": 1.0918472652218782e-05,
      "loss": 61.8709,
      "step": 17864
    },
    {
      "epoch": 17.88,
      "grad_norm": 8554.8974609375,
      "learning_rate": 1.0913312693498453e-05,
      "loss": 57.7336,
      "step": 17865
    },
    {
      "epoch": 17.88,
      "grad_norm": 11619.158203125,
      "learning_rate": 1.0908152734778122e-05,
      "loss": 59.435,
      "step": 17866
    },
    {
      "epoch": 17.88,
      "grad_norm": 16371.625,
      "learning_rate": 1.0902992776057792e-05,
      "loss": 49.4523,
      "step": 17867
    },
    {
      "epoch": 17.89,
      "grad_norm": 1735.2554931640625,
      "learning_rate": 1.0897832817337463e-05,
      "loss": 70.8964,
      "step": 17868
    },
    {
      "epoch": 17.89,
      "grad_norm": 708.6337280273438,
      "learning_rate": 1.089267285861713e-05,
      "loss": 57.024,
      "step": 17869
    },
    {
      "epoch": 17.89,
      "grad_norm": 2117.524658203125,
      "learning_rate": 1.0887512899896802e-05,
      "loss": 58.1646,
      "step": 17870
    },
    {
      "epoch": 17.89,
      "grad_norm": 2263.976806640625,
      "learning_rate": 1.0882352941176471e-05,
      "loss": 57.1581,
      "step": 17871
    },
    {
      "epoch": 17.89,
      "grad_norm": 20955.083984375,
      "learning_rate": 1.087719298245614e-05,
      "loss": 50.6876,
      "step": 17872
    },
    {
      "epoch": 17.89,
      "grad_norm": 2123.831298828125,
      "learning_rate": 1.087203302373581e-05,
      "loss": 56.2693,
      "step": 17873
    },
    {
      "epoch": 17.89,
      "grad_norm": 1864.22119140625,
      "learning_rate": 1.086687306501548e-05,
      "loss": 57.4916,
      "step": 17874
    },
    {
      "epoch": 17.89,
      "grad_norm": 10032.046875,
      "learning_rate": 1.086171310629515e-05,
      "loss": 59.9858,
      "step": 17875
    },
    {
      "epoch": 17.89,
      "grad_norm": 15720.4892578125,
      "learning_rate": 1.085655314757482e-05,
      "loss": 49.1603,
      "step": 17876
    },
    {
      "epoch": 17.89,
      "grad_norm": 1744.038818359375,
      "learning_rate": 1.085139318885449e-05,
      "loss": 46.2947,
      "step": 17877
    },
    {
      "epoch": 17.9,
      "grad_norm": 2775.599853515625,
      "learning_rate": 1.0846233230134159e-05,
      "loss": 67.9492,
      "step": 17878
    },
    {
      "epoch": 17.9,
      "grad_norm": 5973.29443359375,
      "learning_rate": 1.0841073271413829e-05,
      "loss": 62.9479,
      "step": 17879
    },
    {
      "epoch": 17.9,
      "grad_norm": 7608.234375,
      "learning_rate": 1.08359133126935e-05,
      "loss": 52.9042,
      "step": 17880
    },
    {
      "epoch": 17.9,
      "grad_norm": 17738.9609375,
      "learning_rate": 1.0830753353973169e-05,
      "loss": 59.6351,
      "step": 17881
    },
    {
      "epoch": 17.9,
      "grad_norm": 2680.96826171875,
      "learning_rate": 1.0825593395252839e-05,
      "loss": 54.4997,
      "step": 17882
    },
    {
      "epoch": 17.9,
      "grad_norm": 1670.45751953125,
      "learning_rate": 1.0820433436532508e-05,
      "loss": 61.4975,
      "step": 17883
    },
    {
      "epoch": 17.9,
      "grad_norm": 3989.07861328125,
      "learning_rate": 1.0815273477812177e-05,
      "loss": 52.8164,
      "step": 17884
    },
    {
      "epoch": 17.9,
      "grad_norm": 28663.814453125,
      "learning_rate": 1.0810113519091849e-05,
      "loss": 19.9491,
      "step": 17885
    },
    {
      "epoch": 17.9,
      "grad_norm": 326260.84375,
      "learning_rate": 1.0804953560371518e-05,
      "loss": 25.0256,
      "step": 17886
    },
    {
      "epoch": 17.9,
      "grad_norm": 3027.629150390625,
      "learning_rate": 1.0799793601651187e-05,
      "loss": 60.3793,
      "step": 17887
    },
    {
      "epoch": 17.91,
      "grad_norm": 252558.8125,
      "learning_rate": 1.0794633642930857e-05,
      "loss": 48.3789,
      "step": 17888
    },
    {
      "epoch": 17.91,
      "grad_norm": 216751.875,
      "learning_rate": 1.0789473684210526e-05,
      "loss": 59.9468,
      "step": 17889
    },
    {
      "epoch": 17.91,
      "grad_norm": 16867.501953125,
      "learning_rate": 1.0784313725490197e-05,
      "loss": 62.0982,
      "step": 17890
    },
    {
      "epoch": 17.91,
      "grad_norm": 48675.59375,
      "learning_rate": 1.0779153766769865e-05,
      "loss": 33.0737,
      "step": 17891
    },
    {
      "epoch": 17.91,
      "grad_norm": 15969.4052734375,
      "learning_rate": 1.0773993808049536e-05,
      "loss": 51.0183,
      "step": 17892
    },
    {
      "epoch": 17.91,
      "grad_norm": 4915.07763671875,
      "learning_rate": 1.0768833849329206e-05,
      "loss": 58.3867,
      "step": 17893
    },
    {
      "epoch": 17.91,
      "grad_norm": 52873.1328125,
      "learning_rate": 1.0763673890608877e-05,
      "loss": 58.6693,
      "step": 17894
    },
    {
      "epoch": 17.91,
      "grad_norm": 21376.4453125,
      "learning_rate": 1.0758513931888545e-05,
      "loss": 58.6437,
      "step": 17895
    },
    {
      "epoch": 17.91,
      "grad_norm": 36571.92578125,
      "learning_rate": 1.0753353973168214e-05,
      "loss": 57.6648,
      "step": 17896
    },
    {
      "epoch": 17.91,
      "grad_norm": 11782.6923828125,
      "learning_rate": 1.0748194014447885e-05,
      "loss": 61.3246,
      "step": 17897
    },
    {
      "epoch": 17.92,
      "grad_norm": 6462.57275390625,
      "learning_rate": 1.0743034055727555e-05,
      "loss": 43.1373,
      "step": 17898
    },
    {
      "epoch": 17.92,
      "grad_norm": 8514.3720703125,
      "learning_rate": 1.0737874097007226e-05,
      "loss": 49.5632,
      "step": 17899
    },
    {
      "epoch": 17.92,
      "grad_norm": 20447.203125,
      "learning_rate": 1.0732714138286893e-05,
      "loss": 60.8907,
      "step": 17900
    },
    {
      "epoch": 17.92,
      "grad_norm": 3731.65234375,
      "learning_rate": 1.0727554179566565e-05,
      "loss": 48.457,
      "step": 17901
    },
    {
      "epoch": 17.92,
      "grad_norm": 71046.046875,
      "learning_rate": 1.0722394220846234e-05,
      "loss": 59.5469,
      "step": 17902
    },
    {
      "epoch": 17.92,
      "grad_norm": 11693.541015625,
      "learning_rate": 1.0717234262125903e-05,
      "loss": 56.0765,
      "step": 17903
    },
    {
      "epoch": 17.92,
      "grad_norm": 9443.4130859375,
      "learning_rate": 1.0712074303405573e-05,
      "loss": 64.6047,
      "step": 17904
    },
    {
      "epoch": 17.92,
      "grad_norm": 29795.416015625,
      "learning_rate": 1.0706914344685242e-05,
      "loss": 59.9586,
      "step": 17905
    },
    {
      "epoch": 17.92,
      "grad_norm": 3131.75146484375,
      "learning_rate": 1.0701754385964913e-05,
      "loss": 55.9246,
      "step": 17906
    },
    {
      "epoch": 17.92,
      "grad_norm": 4716.3681640625,
      "learning_rate": 1.0696594427244583e-05,
      "loss": 63.7992,
      "step": 17907
    },
    {
      "epoch": 17.93,
      "grad_norm": 19264.103515625,
      "learning_rate": 1.0691434468524252e-05,
      "loss": 64.6433,
      "step": 17908
    },
    {
      "epoch": 17.93,
      "grad_norm": 5550.72021484375,
      "learning_rate": 1.0686274509803922e-05,
      "loss": 63.3571,
      "step": 17909
    },
    {
      "epoch": 17.93,
      "grad_norm": 1894.58056640625,
      "learning_rate": 1.0681114551083591e-05,
      "loss": 50.6777,
      "step": 17910
    },
    {
      "epoch": 17.93,
      "grad_norm": 172372.671875,
      "learning_rate": 1.0675954592363262e-05,
      "loss": 64.864,
      "step": 17911
    },
    {
      "epoch": 17.93,
      "grad_norm": 3419.06591796875,
      "learning_rate": 1.0670794633642932e-05,
      "loss": 63.5416,
      "step": 17912
    },
    {
      "epoch": 17.93,
      "grad_norm": 3345.006103515625,
      "learning_rate": 1.0665634674922601e-05,
      "loss": 58.4147,
      "step": 17913
    },
    {
      "epoch": 17.93,
      "grad_norm": 6215.49462890625,
      "learning_rate": 1.066047471620227e-05,
      "loss": 68.189,
      "step": 17914
    },
    {
      "epoch": 17.93,
      "grad_norm": 22535.123046875,
      "learning_rate": 1.065531475748194e-05,
      "loss": 59.1268,
      "step": 17915
    },
    {
      "epoch": 17.93,
      "grad_norm": 3414.428466796875,
      "learning_rate": 1.0650154798761611e-05,
      "loss": 62.9548,
      "step": 17916
    },
    {
      "epoch": 17.93,
      "grad_norm": 10095.9921875,
      "learning_rate": 1.064499484004128e-05,
      "loss": 55.5694,
      "step": 17917
    },
    {
      "epoch": 17.94,
      "grad_norm": 5337.5048828125,
      "learning_rate": 1.063983488132095e-05,
      "loss": 57.6023,
      "step": 17918
    },
    {
      "epoch": 17.94,
      "grad_norm": 10925.3408203125,
      "learning_rate": 1.063467492260062e-05,
      "loss": 47.8453,
      "step": 17919
    },
    {
      "epoch": 17.94,
      "grad_norm": 2751.3984375,
      "learning_rate": 1.0629514963880289e-05,
      "loss": 63.9349,
      "step": 17920
    },
    {
      "epoch": 17.94,
      "grad_norm": 3264.4619140625,
      "learning_rate": 1.062435500515996e-05,
      "loss": 64.595,
      "step": 17921
    },
    {
      "epoch": 17.94,
      "grad_norm": 5348.26220703125,
      "learning_rate": 1.0619195046439628e-05,
      "loss": 52.5979,
      "step": 17922
    },
    {
      "epoch": 17.94,
      "grad_norm": 6566.35009765625,
      "learning_rate": 1.0614035087719299e-05,
      "loss": 57.1628,
      "step": 17923
    },
    {
      "epoch": 17.94,
      "grad_norm": 26419.94140625,
      "learning_rate": 1.0608875128998968e-05,
      "loss": 58.3469,
      "step": 17924
    },
    {
      "epoch": 17.94,
      "grad_norm": 2255.94140625,
      "learning_rate": 1.0603715170278638e-05,
      "loss": 46.7988,
      "step": 17925
    },
    {
      "epoch": 17.94,
      "grad_norm": 4798.93505859375,
      "learning_rate": 1.0598555211558309e-05,
      "loss": 59.8674,
      "step": 17926
    },
    {
      "epoch": 17.94,
      "grad_norm": 7471.6767578125,
      "learning_rate": 1.0593395252837977e-05,
      "loss": 52.4907,
      "step": 17927
    },
    {
      "epoch": 17.95,
      "grad_norm": 4007.8759765625,
      "learning_rate": 1.0588235294117648e-05,
      "loss": 63.7257,
      "step": 17928
    },
    {
      "epoch": 17.95,
      "grad_norm": 213593.609375,
      "learning_rate": 1.0583075335397317e-05,
      "loss": 56.2667,
      "step": 17929
    },
    {
      "epoch": 17.95,
      "grad_norm": 30620.8671875,
      "learning_rate": 1.0577915376676988e-05,
      "loss": 63.8704,
      "step": 17930
    },
    {
      "epoch": 17.95,
      "grad_norm": 9148.84375,
      "learning_rate": 1.0572755417956656e-05,
      "loss": 60.689,
      "step": 17931
    },
    {
      "epoch": 17.95,
      "grad_norm": 16546.810546875,
      "learning_rate": 1.0567595459236326e-05,
      "loss": 36.2834,
      "step": 17932
    },
    {
      "epoch": 17.95,
      "grad_norm": 12069.005859375,
      "learning_rate": 1.0562435500515997e-05,
      "loss": 61.2602,
      "step": 17933
    },
    {
      "epoch": 17.95,
      "grad_norm": 5716.5859375,
      "learning_rate": 1.0557275541795666e-05,
      "loss": 67.3872,
      "step": 17934
    },
    {
      "epoch": 17.95,
      "grad_norm": 2186.71630859375,
      "learning_rate": 1.0552115583075337e-05,
      "loss": 54.7973,
      "step": 17935
    },
    {
      "epoch": 17.95,
      "grad_norm": 7320.0888671875,
      "learning_rate": 1.0546955624355005e-05,
      "loss": 50.5051,
      "step": 17936
    },
    {
      "epoch": 17.95,
      "grad_norm": 12666.79296875,
      "learning_rate": 1.0541795665634676e-05,
      "loss": 51.9317,
      "step": 17937
    },
    {
      "epoch": 17.96,
      "grad_norm": 1344.9764404296875,
      "learning_rate": 1.0536635706914346e-05,
      "loss": 64.2831,
      "step": 17938
    },
    {
      "epoch": 17.96,
      "grad_norm": 1284.7752685546875,
      "learning_rate": 1.0531475748194015e-05,
      "loss": 61.5746,
      "step": 17939
    },
    {
      "epoch": 17.96,
      "grad_norm": 11110.0,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 58.2005,
      "step": 17940
    },
    {
      "epoch": 17.96,
      "grad_norm": 4438.68994140625,
      "learning_rate": 1.0521155830753354e-05,
      "loss": 60.9058,
      "step": 17941
    },
    {
      "epoch": 17.96,
      "grad_norm": 1813.1751708984375,
      "learning_rate": 1.0515995872033025e-05,
      "loss": 60.057,
      "step": 17942
    },
    {
      "epoch": 17.96,
      "grad_norm": 15873.966796875,
      "learning_rate": 1.0510835913312694e-05,
      "loss": 52.7886,
      "step": 17943
    },
    {
      "epoch": 17.96,
      "grad_norm": 7136.69580078125,
      "learning_rate": 1.0505675954592364e-05,
      "loss": 55.606,
      "step": 17944
    },
    {
      "epoch": 17.96,
      "grad_norm": 11982.3515625,
      "learning_rate": 1.0500515995872033e-05,
      "loss": 54.5,
      "step": 17945
    },
    {
      "epoch": 17.96,
      "grad_norm": 16503.974609375,
      "learning_rate": 1.0495356037151703e-05,
      "loss": 55.1552,
      "step": 17946
    },
    {
      "epoch": 17.96,
      "grad_norm": 146956.078125,
      "learning_rate": 1.0490196078431374e-05,
      "loss": 45.8459,
      "step": 17947
    },
    {
      "epoch": 17.97,
      "grad_norm": 11065.361328125,
      "learning_rate": 1.0485036119711043e-05,
      "loss": 60.5512,
      "step": 17948
    },
    {
      "epoch": 17.97,
      "grad_norm": 4234.27978515625,
      "learning_rate": 1.0479876160990713e-05,
      "loss": 43.8902,
      "step": 17949
    },
    {
      "epoch": 17.97,
      "grad_norm": 20897.212890625,
      "learning_rate": 1.0474716202270382e-05,
      "loss": 54.1452,
      "step": 17950
    },
    {
      "epoch": 17.97,
      "grad_norm": 12472.1630859375,
      "learning_rate": 1.0469556243550052e-05,
      "loss": 50.9467,
      "step": 17951
    },
    {
      "epoch": 17.97,
      "grad_norm": 10463.3232421875,
      "learning_rate": 1.0464396284829723e-05,
      "loss": 62.3617,
      "step": 17952
    },
    {
      "epoch": 17.97,
      "grad_norm": 5313.81591796875,
      "learning_rate": 1.0459236326109392e-05,
      "loss": 51.1087,
      "step": 17953
    },
    {
      "epoch": 17.97,
      "grad_norm": 28524.681640625,
      "learning_rate": 1.0454076367389062e-05,
      "loss": 47.8722,
      "step": 17954
    },
    {
      "epoch": 17.97,
      "grad_norm": 4269.18310546875,
      "learning_rate": 1.0448916408668731e-05,
      "loss": 57.4096,
      "step": 17955
    },
    {
      "epoch": 17.97,
      "grad_norm": 5104.55615234375,
      "learning_rate": 1.04437564499484e-05,
      "loss": 64.8668,
      "step": 17956
    },
    {
      "epoch": 17.97,
      "grad_norm": 3944.365966796875,
      "learning_rate": 1.0438596491228072e-05,
      "loss": 54.1531,
      "step": 17957
    },
    {
      "epoch": 17.98,
      "grad_norm": 5961.2724609375,
      "learning_rate": 1.043343653250774e-05,
      "loss": 67.1495,
      "step": 17958
    },
    {
      "epoch": 17.98,
      "grad_norm": 6879.85009765625,
      "learning_rate": 1.042827657378741e-05,
      "loss": 66.0698,
      "step": 17959
    },
    {
      "epoch": 17.98,
      "grad_norm": 3413.1708984375,
      "learning_rate": 1.042311661506708e-05,
      "loss": 53.9734,
      "step": 17960
    },
    {
      "epoch": 17.98,
      "grad_norm": 1890.8853759765625,
      "learning_rate": 1.041795665634675e-05,
      "loss": 61.5943,
      "step": 17961
    },
    {
      "epoch": 17.98,
      "grad_norm": 6592.94287109375,
      "learning_rate": 1.0412796697626419e-05,
      "loss": 61.6061,
      "step": 17962
    },
    {
      "epoch": 17.98,
      "grad_norm": 24082.62890625,
      "learning_rate": 1.0407636738906088e-05,
      "loss": 46.7601,
      "step": 17963
    },
    {
      "epoch": 17.98,
      "grad_norm": 1721.602294921875,
      "learning_rate": 1.040247678018576e-05,
      "loss": 58.5624,
      "step": 17964
    },
    {
      "epoch": 17.98,
      "grad_norm": 206826.21875,
      "learning_rate": 1.0397316821465429e-05,
      "loss": 49.6837,
      "step": 17965
    },
    {
      "epoch": 17.98,
      "grad_norm": 47506.05078125,
      "learning_rate": 1.0392156862745098e-05,
      "loss": 54.7169,
      "step": 17966
    },
    {
      "epoch": 17.98,
      "grad_norm": 9335.66796875,
      "learning_rate": 1.0386996904024768e-05,
      "loss": 51.9494,
      "step": 17967
    },
    {
      "epoch": 17.99,
      "grad_norm": 3471.901123046875,
      "learning_rate": 1.0381836945304437e-05,
      "loss": 53.8232,
      "step": 17968
    },
    {
      "epoch": 17.99,
      "grad_norm": 2638.788818359375,
      "learning_rate": 1.0376676986584108e-05,
      "loss": 61.2274,
      "step": 17969
    },
    {
      "epoch": 17.99,
      "grad_norm": 70842.6953125,
      "learning_rate": 1.0371517027863778e-05,
      "loss": 57.1035,
      "step": 17970
    },
    {
      "epoch": 17.99,
      "grad_norm": 6477.390625,
      "learning_rate": 1.0366357069143447e-05,
      "loss": 66.0948,
      "step": 17971
    },
    {
      "epoch": 17.99,
      "grad_norm": 6714.427734375,
      "learning_rate": 1.0361197110423117e-05,
      "loss": 56.6564,
      "step": 17972
    },
    {
      "epoch": 17.99,
      "grad_norm": 12601.8671875,
      "learning_rate": 1.0356037151702786e-05,
      "loss": 54.2602,
      "step": 17973
    },
    {
      "epoch": 17.99,
      "grad_norm": 5479.69189453125,
      "learning_rate": 1.0350877192982457e-05,
      "loss": 60.9072,
      "step": 17974
    },
    {
      "epoch": 17.99,
      "grad_norm": 12725.7060546875,
      "learning_rate": 1.0345717234262127e-05,
      "loss": 62.173,
      "step": 17975
    },
    {
      "epoch": 17.99,
      "grad_norm": 172058.75,
      "learning_rate": 1.0340557275541796e-05,
      "loss": 60.4652,
      "step": 17976
    },
    {
      "epoch": 17.99,
      "grad_norm": 893.9109497070312,
      "learning_rate": 1.0335397316821465e-05,
      "loss": 57.4418,
      "step": 17977
    },
    {
      "epoch": 18.0,
      "grad_norm": 6317.24560546875,
      "learning_rate": 1.0330237358101137e-05,
      "loss": 63.2093,
      "step": 17978
    },
    {
      "epoch": 18.0,
      "grad_norm": 2116.518310546875,
      "learning_rate": 1.0325077399380806e-05,
      "loss": 62.3284,
      "step": 17979
    },
    {
      "epoch": 18.0,
      "grad_norm": 9439.2509765625,
      "learning_rate": 1.0319917440660474e-05,
      "loss": 57.4009,
      "step": 17980
    },
    {
      "epoch": 18.0,
      "grad_norm": 2792.453857421875,
      "learning_rate": 1.0314757481940145e-05,
      "loss": 64.1065,
      "step": 17981
    },
    {
      "epoch": 18.0,
      "grad_norm": 330551.5625,
      "learning_rate": 1.0309597523219814e-05,
      "loss": 58.0378,
      "step": 17982
    },
    {
      "epoch": 18.0,
      "grad_norm": 3945.648681640625,
      "learning_rate": 1.0304437564499485e-05,
      "loss": 56.869,
      "step": 17983
    },
    {
      "epoch": 18.0,
      "grad_norm": 11231.5419921875,
      "learning_rate": 1.0299277605779155e-05,
      "loss": 46.5045,
      "step": 17984
    },
    {
      "epoch": 18.0,
      "grad_norm": 15179.2880859375,
      "learning_rate": 1.0294117647058824e-05,
      "loss": 51.1155,
      "step": 17985
    },
    {
      "epoch": 18.0,
      "grad_norm": 1948.493896484375,
      "learning_rate": 1.0288957688338494e-05,
      "loss": 53.6221,
      "step": 17986
    },
    {
      "epoch": 18.01,
      "grad_norm": 5069.89599609375,
      "learning_rate": 1.0283797729618163e-05,
      "loss": 53.5846,
      "step": 17987
    },
    {
      "epoch": 18.01,
      "grad_norm": 2741.2119140625,
      "learning_rate": 1.0278637770897834e-05,
      "loss": 57.6387,
      "step": 17988
    },
    {
      "epoch": 18.01,
      "grad_norm": 30300.4765625,
      "learning_rate": 1.0273477812177502e-05,
      "loss": 55.7641,
      "step": 17989
    },
    {
      "epoch": 18.01,
      "grad_norm": 5251.74462890625,
      "learning_rate": 1.0268317853457173e-05,
      "loss": 53.1476,
      "step": 17990
    },
    {
      "epoch": 18.01,
      "grad_norm": 4238.28271484375,
      "learning_rate": 1.0263157894736843e-05,
      "loss": 57.7399,
      "step": 17991
    },
    {
      "epoch": 18.01,
      "grad_norm": 2878.24072265625,
      "learning_rate": 1.0257997936016512e-05,
      "loss": 59.8216,
      "step": 17992
    },
    {
      "epoch": 18.01,
      "grad_norm": 2728.193603515625,
      "learning_rate": 1.0252837977296183e-05,
      "loss": 60.9694,
      "step": 17993
    },
    {
      "epoch": 18.01,
      "grad_norm": 7944.45654296875,
      "learning_rate": 1.0247678018575851e-05,
      "loss": 43.2661,
      "step": 17994
    },
    {
      "epoch": 18.01,
      "grad_norm": 10112.1474609375,
      "learning_rate": 1.0242518059855522e-05,
      "loss": 57.0758,
      "step": 17995
    },
    {
      "epoch": 18.01,
      "grad_norm": 3021.565673828125,
      "learning_rate": 1.0237358101135192e-05,
      "loss": 51.1486,
      "step": 17996
    },
    {
      "epoch": 18.02,
      "grad_norm": 48456.91796875,
      "learning_rate": 1.0232198142414861e-05,
      "loss": 47.8353,
      "step": 17997
    },
    {
      "epoch": 18.02,
      "grad_norm": 22092.22265625,
      "learning_rate": 1.022703818369453e-05,
      "loss": 60.2637,
      "step": 17998
    },
    {
      "epoch": 18.02,
      "grad_norm": 18669.01953125,
      "learning_rate": 1.02218782249742e-05,
      "loss": 61.3604,
      "step": 17999
    },
    {
      "epoch": 18.02,
      "grad_norm": 10172.98046875,
      "learning_rate": 1.0216718266253871e-05,
      "loss": 28.7455,
      "step": 18000
    },
    {
      "epoch": 18.02,
      "grad_norm": 10818.2646484375,
      "learning_rate": 1.021155830753354e-05,
      "loss": 61.5675,
      "step": 18001
    },
    {
      "epoch": 18.02,
      "grad_norm": 5912.81884765625,
      "learning_rate": 1.020639834881321e-05,
      "loss": 60.2836,
      "step": 18002
    },
    {
      "epoch": 18.02,
      "grad_norm": 5681.3447265625,
      "learning_rate": 1.020123839009288e-05,
      "loss": 48.5846,
      "step": 18003
    },
    {
      "epoch": 18.02,
      "grad_norm": 13575.501953125,
      "learning_rate": 1.0196078431372549e-05,
      "loss": 33.4023,
      "step": 18004
    },
    {
      "epoch": 18.02,
      "grad_norm": 7512.75390625,
      "learning_rate": 1.019091847265222e-05,
      "loss": 54.5586,
      "step": 18005
    },
    {
      "epoch": 18.02,
      "grad_norm": 1660.1435546875,
      "learning_rate": 1.018575851393189e-05,
      "loss": 64.5955,
      "step": 18006
    },
    {
      "epoch": 18.03,
      "grad_norm": 692.86181640625,
      "learning_rate": 1.0180598555211559e-05,
      "loss": 55.0992,
      "step": 18007
    },
    {
      "epoch": 18.03,
      "grad_norm": 83943.234375,
      "learning_rate": 1.0175438596491228e-05,
      "loss": 61.4418,
      "step": 18008
    },
    {
      "epoch": 18.03,
      "grad_norm": 11177.0400390625,
      "learning_rate": 1.0170278637770898e-05,
      "loss": 62.389,
      "step": 18009
    },
    {
      "epoch": 18.03,
      "grad_norm": 3202.29736328125,
      "learning_rate": 1.0165118679050569e-05,
      "loss": 50.9375,
      "step": 18010
    },
    {
      "epoch": 18.03,
      "grad_norm": 12585.2783203125,
      "learning_rate": 1.0159958720330238e-05,
      "loss": 61.9335,
      "step": 18011
    },
    {
      "epoch": 18.03,
      "grad_norm": 113586.546875,
      "learning_rate": 1.0154798761609908e-05,
      "loss": 33.2752,
      "step": 18012
    },
    {
      "epoch": 18.03,
      "grad_norm": 1013.334228515625,
      "learning_rate": 1.0149638802889577e-05,
      "loss": 61.4497,
      "step": 18013
    },
    {
      "epoch": 18.03,
      "grad_norm": 593.7078247070312,
      "learning_rate": 1.0144478844169248e-05,
      "loss": 63.9981,
      "step": 18014
    },
    {
      "epoch": 18.03,
      "grad_norm": 4242.8173828125,
      "learning_rate": 1.0139318885448918e-05,
      "loss": 50.4407,
      "step": 18015
    },
    {
      "epoch": 18.03,
      "grad_norm": 4567.82666015625,
      "learning_rate": 1.0134158926728585e-05,
      "loss": 62.3248,
      "step": 18016
    },
    {
      "epoch": 18.04,
      "grad_norm": 18705.826171875,
      "learning_rate": 1.0128998968008256e-05,
      "loss": 60.8134,
      "step": 18017
    },
    {
      "epoch": 18.04,
      "grad_norm": 42162.24609375,
      "learning_rate": 1.0123839009287926e-05,
      "loss": 34.0289,
      "step": 18018
    },
    {
      "epoch": 18.04,
      "grad_norm": 2542.615966796875,
      "learning_rate": 1.0118679050567597e-05,
      "loss": 60.1459,
      "step": 18019
    },
    {
      "epoch": 18.04,
      "grad_norm": 7087.46435546875,
      "learning_rate": 1.0113519091847266e-05,
      "loss": 45.8671,
      "step": 18020
    },
    {
      "epoch": 18.04,
      "grad_norm": 2248.324462890625,
      "learning_rate": 1.0108359133126936e-05,
      "loss": 51.9861,
      "step": 18021
    },
    {
      "epoch": 18.04,
      "grad_norm": 4581.8056640625,
      "learning_rate": 1.0103199174406605e-05,
      "loss": 58.8145,
      "step": 18022
    },
    {
      "epoch": 18.04,
      "grad_norm": 22490.974609375,
      "learning_rate": 1.0098039215686275e-05,
      "loss": 55.8003,
      "step": 18023
    },
    {
      "epoch": 18.04,
      "grad_norm": 34742.48046875,
      "learning_rate": 1.0092879256965946e-05,
      "loss": 49.2869,
      "step": 18024
    },
    {
      "epoch": 18.04,
      "grad_norm": 16955.23046875,
      "learning_rate": 1.0087719298245614e-05,
      "loss": 63.1695,
      "step": 18025
    },
    {
      "epoch": 18.04,
      "grad_norm": 4341.45751953125,
      "learning_rate": 1.0082559339525285e-05,
      "loss": 55.6004,
      "step": 18026
    },
    {
      "epoch": 18.05,
      "grad_norm": 4123.09375,
      "learning_rate": 1.0077399380804954e-05,
      "loss": 64.5284,
      "step": 18027
    },
    {
      "epoch": 18.05,
      "grad_norm": 29384.916015625,
      "learning_rate": 1.0072239422084624e-05,
      "loss": 52.5117,
      "step": 18028
    },
    {
      "epoch": 18.05,
      "grad_norm": 13309.4814453125,
      "learning_rate": 1.0067079463364295e-05,
      "loss": 55.8217,
      "step": 18029
    },
    {
      "epoch": 18.05,
      "grad_norm": 15351.173828125,
      "learning_rate": 1.0061919504643963e-05,
      "loss": 57.3272,
      "step": 18030
    },
    {
      "epoch": 18.05,
      "grad_norm": 2910.2080078125,
      "learning_rate": 1.0056759545923634e-05,
      "loss": 60.423,
      "step": 18031
    },
    {
      "epoch": 18.05,
      "grad_norm": 2367.440185546875,
      "learning_rate": 1.0051599587203303e-05,
      "loss": 63.6464,
      "step": 18032
    },
    {
      "epoch": 18.05,
      "grad_norm": 1697.85107421875,
      "learning_rate": 1.0046439628482973e-05,
      "loss": 59.5138,
      "step": 18033
    },
    {
      "epoch": 18.05,
      "grad_norm": 21746.376953125,
      "learning_rate": 1.0041279669762642e-05,
      "loss": 54.6411,
      "step": 18034
    },
    {
      "epoch": 18.05,
      "grad_norm": 2345.334228515625,
      "learning_rate": 1.0036119711042311e-05,
      "loss": 54.0866,
      "step": 18035
    },
    {
      "epoch": 18.05,
      "grad_norm": 14103.49609375,
      "learning_rate": 1.0030959752321983e-05,
      "loss": 55.8508,
      "step": 18036
    },
    {
      "epoch": 18.06,
      "grad_norm": 14739.87109375,
      "learning_rate": 1.0025799793601652e-05,
      "loss": 58.184,
      "step": 18037
    },
    {
      "epoch": 18.06,
      "grad_norm": 21320.068359375,
      "learning_rate": 1.0020639834881321e-05,
      "loss": 49.3394,
      "step": 18038
    },
    {
      "epoch": 18.06,
      "grad_norm": 39706.84375,
      "learning_rate": 1.001547987616099e-05,
      "loss": 54.8945,
      "step": 18039
    },
    {
      "epoch": 18.06,
      "grad_norm": 85058.515625,
      "learning_rate": 1.001031991744066e-05,
      "loss": 58.361,
      "step": 18040
    },
    {
      "epoch": 18.06,
      "grad_norm": 13702.2119140625,
      "learning_rate": 1.0005159958720331e-05,
      "loss": 46.8129,
      "step": 18041
    },
    {
      "epoch": 18.06,
      "grad_norm": 4687.6875,
      "learning_rate": 1e-05,
      "loss": 58.9997,
      "step": 18042
    },
    {
      "epoch": 18.06,
      "grad_norm": 5704.5087890625,
      "learning_rate": 9.99484004127967e-06,
      "loss": 65.0044,
      "step": 18043
    },
    {
      "epoch": 18.06,
      "grad_norm": 5140.140625,
      "learning_rate": 9.98968008255934e-06,
      "loss": 48.5741,
      "step": 18044
    },
    {
      "epoch": 18.06,
      "grad_norm": 7163.1259765625,
      "learning_rate": 9.984520123839009e-06,
      "loss": 43.3039,
      "step": 18045
    },
    {
      "epoch": 18.06,
      "grad_norm": 4147.89453125,
      "learning_rate": 9.97936016511868e-06,
      "loss": 49.4729,
      "step": 18046
    },
    {
      "epoch": 18.07,
      "grad_norm": 47259.11328125,
      "learning_rate": 9.974200206398348e-06,
      "loss": 41.5214,
      "step": 18047
    },
    {
      "epoch": 18.07,
      "grad_norm": 87679.109375,
      "learning_rate": 9.969040247678019e-06,
      "loss": 49.4444,
      "step": 18048
    },
    {
      "epoch": 18.07,
      "grad_norm": 23925.23046875,
      "learning_rate": 9.963880288957689e-06,
      "loss": 45.7658,
      "step": 18049
    },
    {
      "epoch": 18.07,
      "grad_norm": 1794.278076171875,
      "learning_rate": 9.958720330237358e-06,
      "loss": 64.6493,
      "step": 18050
    },
    {
      "epoch": 18.07,
      "grad_norm": 2435.23095703125,
      "learning_rate": 9.953560371517029e-06,
      "loss": 57.6509,
      "step": 18051
    },
    {
      "epoch": 18.07,
      "grad_norm": 13254.8642578125,
      "learning_rate": 9.948400412796697e-06,
      "loss": 46.3477,
      "step": 18052
    },
    {
      "epoch": 18.07,
      "grad_norm": 22065.66015625,
      "learning_rate": 9.943240454076368e-06,
      "loss": 27.9508,
      "step": 18053
    },
    {
      "epoch": 18.07,
      "grad_norm": 14593.1044921875,
      "learning_rate": 9.938080495356037e-06,
      "loss": 56.1451,
      "step": 18054
    },
    {
      "epoch": 18.07,
      "grad_norm": 6176.8662109375,
      "learning_rate": 9.932920536635709e-06,
      "loss": 57.2064,
      "step": 18055
    },
    {
      "epoch": 18.07,
      "grad_norm": 2734.734375,
      "learning_rate": 9.927760577915376e-06,
      "loss": 62.3241,
      "step": 18056
    },
    {
      "epoch": 18.08,
      "grad_norm": 5723.5009765625,
      "learning_rate": 9.922600619195046e-06,
      "loss": 54.1872,
      "step": 18057
    },
    {
      "epoch": 18.08,
      "grad_norm": 1071.041259765625,
      "learning_rate": 9.917440660474717e-06,
      "loss": 61.6304,
      "step": 18058
    },
    {
      "epoch": 18.08,
      "grad_norm": 25829.470703125,
      "learning_rate": 9.912280701754386e-06,
      "loss": 68.4595,
      "step": 18059
    },
    {
      "epoch": 18.08,
      "grad_norm": 2091.449462890625,
      "learning_rate": 9.907120743034057e-06,
      "loss": 56.5462,
      "step": 18060
    },
    {
      "epoch": 18.08,
      "grad_norm": 2146.194091796875,
      "learning_rate": 9.901960784313725e-06,
      "loss": 63.7423,
      "step": 18061
    },
    {
      "epoch": 18.08,
      "grad_norm": 19783.66796875,
      "learning_rate": 9.896800825593396e-06,
      "loss": 42.2061,
      "step": 18062
    },
    {
      "epoch": 18.08,
      "grad_norm": 5550.544921875,
      "learning_rate": 9.891640866873066e-06,
      "loss": 61.8075,
      "step": 18063
    },
    {
      "epoch": 18.08,
      "grad_norm": 16072.5048828125,
      "learning_rate": 9.886480908152735e-06,
      "loss": 53.7452,
      "step": 18064
    },
    {
      "epoch": 18.08,
      "grad_norm": 6795.09619140625,
      "learning_rate": 9.881320949432405e-06,
      "loss": 65.4471,
      "step": 18065
    },
    {
      "epoch": 18.08,
      "grad_norm": 6516.5302734375,
      "learning_rate": 9.876160990712074e-06,
      "loss": 53.6898,
      "step": 18066
    },
    {
      "epoch": 18.09,
      "grad_norm": 6408.14990234375,
      "learning_rate": 9.871001031991745e-06,
      "loss": 62.1361,
      "step": 18067
    },
    {
      "epoch": 18.09,
      "grad_norm": 1031.7757568359375,
      "learning_rate": 9.865841073271415e-06,
      "loss": 47.4979,
      "step": 18068
    },
    {
      "epoch": 18.09,
      "grad_norm": 9885.705078125,
      "learning_rate": 9.860681114551084e-06,
      "loss": 59.7558,
      "step": 18069
    },
    {
      "epoch": 18.09,
      "grad_norm": 10547.2470703125,
      "learning_rate": 9.855521155830754e-06,
      "loss": 62.8344,
      "step": 18070
    },
    {
      "epoch": 18.09,
      "grad_norm": 21012.248046875,
      "learning_rate": 9.850361197110423e-06,
      "loss": 33.6388,
      "step": 18071
    },
    {
      "epoch": 18.09,
      "grad_norm": 5338.3154296875,
      "learning_rate": 9.845201238390094e-06,
      "loss": 56.2793,
      "step": 18072
    },
    {
      "epoch": 18.09,
      "grad_norm": 13007.4013671875,
      "learning_rate": 9.840041279669764e-06,
      "loss": 53.6663,
      "step": 18073
    },
    {
      "epoch": 18.09,
      "grad_norm": 1959.1766357421875,
      "learning_rate": 9.834881320949433e-06,
      "loss": 62.6248,
      "step": 18074
    },
    {
      "epoch": 18.09,
      "grad_norm": 11272.4189453125,
      "learning_rate": 9.829721362229102e-06,
      "loss": 57.5627,
      "step": 18075
    },
    {
      "epoch": 18.09,
      "grad_norm": 10779.2958984375,
      "learning_rate": 9.824561403508772e-06,
      "loss": 51.6967,
      "step": 18076
    },
    {
      "epoch": 18.1,
      "grad_norm": 9920.16796875,
      "learning_rate": 9.819401444788443e-06,
      "loss": 61.5665,
      "step": 18077
    },
    {
      "epoch": 18.1,
      "grad_norm": 12456.0234375,
      "learning_rate": 9.814241486068112e-06,
      "loss": 40.2649,
      "step": 18078
    },
    {
      "epoch": 18.1,
      "grad_norm": 50262.078125,
      "learning_rate": 9.809081527347782e-06,
      "loss": 63.0424,
      "step": 18079
    },
    {
      "epoch": 18.1,
      "grad_norm": 9070.5478515625,
      "learning_rate": 9.803921568627451e-06,
      "loss": 51.3905,
      "step": 18080
    },
    {
      "epoch": 18.1,
      "grad_norm": 6756.7294921875,
      "learning_rate": 9.79876160990712e-06,
      "loss": 62.7405,
      "step": 18081
    },
    {
      "epoch": 18.1,
      "grad_norm": 14107.8447265625,
      "learning_rate": 9.793601651186792e-06,
      "loss": 45.279,
      "step": 18082
    },
    {
      "epoch": 18.1,
      "grad_norm": 16169.6943359375,
      "learning_rate": 9.78844169246646e-06,
      "loss": 66.2069,
      "step": 18083
    },
    {
      "epoch": 18.1,
      "grad_norm": 3240.017822265625,
      "learning_rate": 9.78328173374613e-06,
      "loss": 60.8434,
      "step": 18084
    },
    {
      "epoch": 18.1,
      "grad_norm": 2277.155517578125,
      "learning_rate": 9.7781217750258e-06,
      "loss": 54.611,
      "step": 18085
    },
    {
      "epoch": 18.1,
      "grad_norm": 4931.98583984375,
      "learning_rate": 9.77296181630547e-06,
      "loss": 28.4279,
      "step": 18086
    },
    {
      "epoch": 18.11,
      "grad_norm": 16280.67578125,
      "learning_rate": 9.76780185758514e-06,
      "loss": 60.6588,
      "step": 18087
    },
    {
      "epoch": 18.11,
      "grad_norm": 6867.1015625,
      "learning_rate": 9.762641898864808e-06,
      "loss": 61.8093,
      "step": 18088
    },
    {
      "epoch": 18.11,
      "grad_norm": 11758.6201171875,
      "learning_rate": 9.75748194014448e-06,
      "loss": 63.3905,
      "step": 18089
    },
    {
      "epoch": 18.11,
      "grad_norm": 5101.42431640625,
      "learning_rate": 9.752321981424149e-06,
      "loss": 57.8664,
      "step": 18090
    },
    {
      "epoch": 18.11,
      "grad_norm": 2984.409423828125,
      "learning_rate": 9.74716202270382e-06,
      "loss": 44.7628,
      "step": 18091
    },
    {
      "epoch": 18.11,
      "grad_norm": 2035.808837890625,
      "learning_rate": 9.742002063983488e-06,
      "loss": 53.7714,
      "step": 18092
    },
    {
      "epoch": 18.11,
      "grad_norm": 20224.0625,
      "learning_rate": 9.736842105263157e-06,
      "loss": 60.9911,
      "step": 18093
    },
    {
      "epoch": 18.11,
      "grad_norm": 10481.904296875,
      "learning_rate": 9.731682146542828e-06,
      "loss": 66.3788,
      "step": 18094
    },
    {
      "epoch": 18.11,
      "grad_norm": 25232.74609375,
      "learning_rate": 9.726522187822498e-06,
      "loss": 48.1274,
      "step": 18095
    },
    {
      "epoch": 18.11,
      "grad_norm": 4298.63427734375,
      "learning_rate": 9.721362229102169e-06,
      "loss": 61.4222,
      "step": 18096
    },
    {
      "epoch": 18.12,
      "grad_norm": 14870.470703125,
      "learning_rate": 9.716202270381837e-06,
      "loss": 52.1672,
      "step": 18097
    },
    {
      "epoch": 18.12,
      "grad_norm": 1182.9549560546875,
      "learning_rate": 9.711042311661508e-06,
      "loss": 66.8372,
      "step": 18098
    },
    {
      "epoch": 18.12,
      "grad_norm": 18165.361328125,
      "learning_rate": 9.705882352941177e-06,
      "loss": 59.4653,
      "step": 18099
    },
    {
      "epoch": 18.12,
      "grad_norm": 1200.775146484375,
      "learning_rate": 9.700722394220847e-06,
      "loss": 63.0364,
      "step": 18100
    },
    {
      "epoch": 18.12,
      "grad_norm": 4888.05859375,
      "learning_rate": 9.695562435500516e-06,
      "loss": 53.7843,
      "step": 18101
    },
    {
      "epoch": 18.12,
      "grad_norm": 1678.2998046875,
      "learning_rate": 9.690402476780186e-06,
      "loss": 59.987,
      "step": 18102
    },
    {
      "epoch": 18.12,
      "grad_norm": 7550.9677734375,
      "learning_rate": 9.685242518059857e-06,
      "loss": 60.0835,
      "step": 18103
    },
    {
      "epoch": 18.12,
      "grad_norm": 2673.396240234375,
      "learning_rate": 9.680082559339526e-06,
      "loss": 57.6573,
      "step": 18104
    },
    {
      "epoch": 18.12,
      "grad_norm": 55189.9765625,
      "learning_rate": 9.674922600619196e-06,
      "loss": 52.677,
      "step": 18105
    },
    {
      "epoch": 18.12,
      "grad_norm": 770.0545043945312,
      "learning_rate": 9.669762641898865e-06,
      "loss": 58.792,
      "step": 18106
    },
    {
      "epoch": 18.13,
      "grad_norm": 12450.025390625,
      "learning_rate": 9.664602683178534e-06,
      "loss": 58.2549,
      "step": 18107
    },
    {
      "epoch": 18.13,
      "grad_norm": 20602.4609375,
      "learning_rate": 9.659442724458206e-06,
      "loss": 52.7729,
      "step": 18108
    },
    {
      "epoch": 18.13,
      "grad_norm": 1696.194580078125,
      "learning_rate": 9.654282765737875e-06,
      "loss": 58.4018,
      "step": 18109
    },
    {
      "epoch": 18.13,
      "grad_norm": 12045.9482421875,
      "learning_rate": 9.649122807017545e-06,
      "loss": 58.1998,
      "step": 18110
    },
    {
      "epoch": 18.13,
      "grad_norm": 2884.396728515625,
      "learning_rate": 9.643962848297214e-06,
      "loss": 32.0553,
      "step": 18111
    },
    {
      "epoch": 18.13,
      "grad_norm": 7128.20654296875,
      "learning_rate": 9.638802889576883e-06,
      "loss": 65.0373,
      "step": 18112
    },
    {
      "epoch": 18.13,
      "grad_norm": 1750.03369140625,
      "learning_rate": 9.633642930856555e-06,
      "loss": 58.966,
      "step": 18113
    },
    {
      "epoch": 18.13,
      "grad_norm": 4404.32763671875,
      "learning_rate": 9.628482972136222e-06,
      "loss": 50.0024,
      "step": 18114
    },
    {
      "epoch": 18.13,
      "grad_norm": 1497.5382080078125,
      "learning_rate": 9.623323013415893e-06,
      "loss": 63.7266,
      "step": 18115
    },
    {
      "epoch": 18.13,
      "grad_norm": 10836.0791015625,
      "learning_rate": 9.618163054695563e-06,
      "loss": 63.6519,
      "step": 18116
    },
    {
      "epoch": 18.14,
      "grad_norm": 37678.5,
      "learning_rate": 9.613003095975232e-06,
      "loss": 56.0064,
      "step": 18117
    },
    {
      "epoch": 18.14,
      "grad_norm": 12963.873046875,
      "learning_rate": 9.607843137254903e-06,
      "loss": 53.948,
      "step": 18118
    },
    {
      "epoch": 18.14,
      "grad_norm": 80136.015625,
      "learning_rate": 9.602683178534571e-06,
      "loss": 37.6896,
      "step": 18119
    },
    {
      "epoch": 18.14,
      "grad_norm": 1688.5594482421875,
      "learning_rate": 9.597523219814242e-06,
      "loss": 59.4272,
      "step": 18120
    },
    {
      "epoch": 18.14,
      "grad_norm": 6646.666015625,
      "learning_rate": 9.592363261093912e-06,
      "loss": 42.4361,
      "step": 18121
    },
    {
      "epoch": 18.14,
      "grad_norm": 5234.43310546875,
      "learning_rate": 9.587203302373581e-06,
      "loss": 52.2572,
      "step": 18122
    },
    {
      "epoch": 18.14,
      "grad_norm": 2332.673583984375,
      "learning_rate": 9.58204334365325e-06,
      "loss": 62.5511,
      "step": 18123
    },
    {
      "epoch": 18.14,
      "grad_norm": 3025.67919921875,
      "learning_rate": 9.57688338493292e-06,
      "loss": 63.6426,
      "step": 18124
    },
    {
      "epoch": 18.14,
      "grad_norm": 56492.59765625,
      "learning_rate": 9.571723426212591e-06,
      "loss": 57.2584,
      "step": 18125
    },
    {
      "epoch": 18.14,
      "grad_norm": 11295.46484375,
      "learning_rate": 9.56656346749226e-06,
      "loss": 63.1632,
      "step": 18126
    },
    {
      "epoch": 18.15,
      "grad_norm": 13847.09765625,
      "learning_rate": 9.561403508771932e-06,
      "loss": 66.0414,
      "step": 18127
    },
    {
      "epoch": 18.15,
      "grad_norm": 4614.06396484375,
      "learning_rate": 9.5562435500516e-06,
      "loss": 58.0893,
      "step": 18128
    },
    {
      "epoch": 18.15,
      "grad_norm": 3009.4130859375,
      "learning_rate": 9.551083591331269e-06,
      "loss": 61.687,
      "step": 18129
    },
    {
      "epoch": 18.15,
      "grad_norm": 9447.9990234375,
      "learning_rate": 9.54592363261094e-06,
      "loss": 59.6294,
      "step": 18130
    },
    {
      "epoch": 18.15,
      "grad_norm": 17179.275390625,
      "learning_rate": 9.54076367389061e-06,
      "loss": 57.2472,
      "step": 18131
    },
    {
      "epoch": 18.15,
      "grad_norm": 6216.91552734375,
      "learning_rate": 9.535603715170279e-06,
      "loss": 61.7214,
      "step": 18132
    },
    {
      "epoch": 18.15,
      "grad_norm": 3904.799072265625,
      "learning_rate": 9.530443756449948e-06,
      "loss": 64.1173,
      "step": 18133
    },
    {
      "epoch": 18.15,
      "grad_norm": 3747.164306640625,
      "learning_rate": 9.52528379772962e-06,
      "loss": 65.707,
      "step": 18134
    },
    {
      "epoch": 18.15,
      "grad_norm": 2143.354248046875,
      "learning_rate": 9.520123839009289e-06,
      "loss": 59.198,
      "step": 18135
    },
    {
      "epoch": 18.15,
      "grad_norm": 6487.72216796875,
      "learning_rate": 9.514963880288958e-06,
      "loss": 59.5328,
      "step": 18136
    },
    {
      "epoch": 18.16,
      "grad_norm": 8581.744140625,
      "learning_rate": 9.509803921568628e-06,
      "loss": 54.7869,
      "step": 18137
    },
    {
      "epoch": 18.16,
      "grad_norm": 55571.8984375,
      "learning_rate": 9.504643962848297e-06,
      "loss": 49.2339,
      "step": 18138
    },
    {
      "epoch": 18.16,
      "grad_norm": 5335.421875,
      "learning_rate": 9.499484004127968e-06,
      "loss": 65.4056,
      "step": 18139
    },
    {
      "epoch": 18.16,
      "grad_norm": 62465.59375,
      "learning_rate": 9.494324045407638e-06,
      "loss": 61.4602,
      "step": 18140
    },
    {
      "epoch": 18.16,
      "grad_norm": 10968.5400390625,
      "learning_rate": 9.489164086687305e-06,
      "loss": 42.4446,
      "step": 18141
    },
    {
      "epoch": 18.16,
      "grad_norm": 1453.308837890625,
      "learning_rate": 9.484004127966977e-06,
      "loss": 54.4032,
      "step": 18142
    },
    {
      "epoch": 18.16,
      "grad_norm": 4493.85791015625,
      "learning_rate": 9.478844169246646e-06,
      "loss": 53.1492,
      "step": 18143
    },
    {
      "epoch": 18.16,
      "grad_norm": 1352.8499755859375,
      "learning_rate": 9.473684210526317e-06,
      "loss": 57.489,
      "step": 18144
    },
    {
      "epoch": 18.16,
      "grad_norm": 42858.76171875,
      "learning_rate": 9.468524251805987e-06,
      "loss": 60.3868,
      "step": 18145
    },
    {
      "epoch": 18.16,
      "grad_norm": 76153.6328125,
      "learning_rate": 9.463364293085656e-06,
      "loss": 60.4449,
      "step": 18146
    },
    {
      "epoch": 18.17,
      "grad_norm": 3757.573974609375,
      "learning_rate": 9.458204334365325e-06,
      "loss": 57.8437,
      "step": 18147
    },
    {
      "epoch": 18.17,
      "grad_norm": 7591.9892578125,
      "learning_rate": 9.453044375644995e-06,
      "loss": 58.8019,
      "step": 18148
    },
    {
      "epoch": 18.17,
      "grad_norm": 3441.80615234375,
      "learning_rate": 9.447884416924666e-06,
      "loss": 49.7403,
      "step": 18149
    },
    {
      "epoch": 18.17,
      "grad_norm": 3885.399658203125,
      "learning_rate": 9.442724458204334e-06,
      "loss": 55.3714,
      "step": 18150
    },
    {
      "epoch": 18.17,
      "grad_norm": 160340.609375,
      "learning_rate": 9.437564499484005e-06,
      "loss": 44.5547,
      "step": 18151
    },
    {
      "epoch": 18.17,
      "grad_norm": 1680.173828125,
      "learning_rate": 9.432404540763674e-06,
      "loss": 50.7699,
      "step": 18152
    },
    {
      "epoch": 18.17,
      "grad_norm": 21962.19140625,
      "learning_rate": 9.427244582043344e-06,
      "loss": 55.7852,
      "step": 18153
    },
    {
      "epoch": 18.17,
      "grad_norm": 5698.8017578125,
      "learning_rate": 9.422084623323015e-06,
      "loss": 51.8298,
      "step": 18154
    },
    {
      "epoch": 18.17,
      "grad_norm": 7556.97314453125,
      "learning_rate": 9.416924664602683e-06,
      "loss": 64.2919,
      "step": 18155
    },
    {
      "epoch": 18.17,
      "grad_norm": 27407.431640625,
      "learning_rate": 9.411764705882354e-06,
      "loss": 49.6398,
      "step": 18156
    },
    {
      "epoch": 18.18,
      "grad_norm": 8484.4423828125,
      "learning_rate": 9.406604747162023e-06,
      "loss": 45.1735,
      "step": 18157
    },
    {
      "epoch": 18.18,
      "grad_norm": 1081.755126953125,
      "learning_rate": 9.401444788441693e-06,
      "loss": 62.3222,
      "step": 18158
    },
    {
      "epoch": 18.18,
      "grad_norm": 16873.94140625,
      "learning_rate": 9.396284829721362e-06,
      "loss": 49.1715,
      "step": 18159
    },
    {
      "epoch": 18.18,
      "grad_norm": 36285.2421875,
      "learning_rate": 9.391124871001032e-06,
      "loss": 62.9467,
      "step": 18160
    },
    {
      "epoch": 18.18,
      "grad_norm": 2073.461181640625,
      "learning_rate": 9.385964912280703e-06,
      "loss": 63.723,
      "step": 18161
    },
    {
      "epoch": 18.18,
      "grad_norm": 606277.875,
      "learning_rate": 9.380804953560372e-06,
      "loss": 60.1956,
      "step": 18162
    },
    {
      "epoch": 18.18,
      "grad_norm": 3577.78466796875,
      "learning_rate": 9.375644994840042e-06,
      "loss": 51.4784,
      "step": 18163
    },
    {
      "epoch": 18.18,
      "grad_norm": 10698.88671875,
      "learning_rate": 9.370485036119711e-06,
      "loss": 60.6827,
      "step": 18164
    },
    {
      "epoch": 18.18,
      "grad_norm": 4275.91162109375,
      "learning_rate": 9.36532507739938e-06,
      "loss": 44.797,
      "step": 18165
    },
    {
      "epoch": 18.18,
      "grad_norm": 3467.92626953125,
      "learning_rate": 9.360165118679052e-06,
      "loss": 52.5252,
      "step": 18166
    },
    {
      "epoch": 18.19,
      "grad_norm": 62667.171875,
      "learning_rate": 9.355005159958721e-06,
      "loss": 59.9175,
      "step": 18167
    },
    {
      "epoch": 18.19,
      "grad_norm": 10128.927734375,
      "learning_rate": 9.34984520123839e-06,
      "loss": 54.3354,
      "step": 18168
    },
    {
      "epoch": 18.19,
      "grad_norm": 22458.2734375,
      "learning_rate": 9.34468524251806e-06,
      "loss": 67.2318,
      "step": 18169
    },
    {
      "epoch": 18.19,
      "grad_norm": 668.9127197265625,
      "learning_rate": 9.33952528379773e-06,
      "loss": 64.2354,
      "step": 18170
    },
    {
      "epoch": 18.19,
      "grad_norm": 18303.546875,
      "learning_rate": 9.3343653250774e-06,
      "loss": 55.7415,
      "step": 18171
    },
    {
      "epoch": 18.19,
      "grad_norm": 27524.705078125,
      "learning_rate": 9.32920536635707e-06,
      "loss": 61.9655,
      "step": 18172
    },
    {
      "epoch": 18.19,
      "grad_norm": 1876.93798828125,
      "learning_rate": 9.32404540763674e-06,
      "loss": 60.7217,
      "step": 18173
    },
    {
      "epoch": 18.19,
      "grad_norm": 14840.9345703125,
      "learning_rate": 9.318885448916409e-06,
      "loss": 44.7957,
      "step": 18174
    },
    {
      "epoch": 18.19,
      "grad_norm": 2069.25,
      "learning_rate": 9.31372549019608e-06,
      "loss": 56.0558,
      "step": 18175
    },
    {
      "epoch": 18.19,
      "grad_norm": 12839.384765625,
      "learning_rate": 9.30856553147575e-06,
      "loss": 59.4252,
      "step": 18176
    },
    {
      "epoch": 18.2,
      "grad_norm": 1699.3109130859375,
      "learning_rate": 9.303405572755417e-06,
      "loss": 56.3693,
      "step": 18177
    },
    {
      "epoch": 18.2,
      "grad_norm": 5777.6220703125,
      "learning_rate": 9.298245614035088e-06,
      "loss": 52.2945,
      "step": 18178
    },
    {
      "epoch": 18.2,
      "grad_norm": 14144.3740234375,
      "learning_rate": 9.293085655314758e-06,
      "loss": 54.2355,
      "step": 18179
    },
    {
      "epoch": 18.2,
      "grad_norm": 10300.0439453125,
      "learning_rate": 9.287925696594429e-06,
      "loss": 45.1879,
      "step": 18180
    },
    {
      "epoch": 18.2,
      "grad_norm": 19115.234375,
      "learning_rate": 9.282765737874098e-06,
      "loss": 52.0657,
      "step": 18181
    },
    {
      "epoch": 18.2,
      "grad_norm": 26660.54296875,
      "learning_rate": 9.277605779153768e-06,
      "loss": 62.7755,
      "step": 18182
    },
    {
      "epoch": 18.2,
      "grad_norm": 3751.613525390625,
      "learning_rate": 9.272445820433437e-06,
      "loss": 53.9528,
      "step": 18183
    },
    {
      "epoch": 18.2,
      "grad_norm": 5527.29248046875,
      "learning_rate": 9.267285861713106e-06,
      "loss": 58.0121,
      "step": 18184
    },
    {
      "epoch": 18.2,
      "grad_norm": 2562.05810546875,
      "learning_rate": 9.262125902992778e-06,
      "loss": 64.9543,
      "step": 18185
    },
    {
      "epoch": 18.2,
      "grad_norm": 5146.8408203125,
      "learning_rate": 9.256965944272445e-06,
      "loss": 50.0044,
      "step": 18186
    },
    {
      "epoch": 18.21,
      "grad_norm": 5153.32177734375,
      "learning_rate": 9.251805985552116e-06,
      "loss": 60.0662,
      "step": 18187
    },
    {
      "epoch": 18.21,
      "grad_norm": 9408.681640625,
      "learning_rate": 9.246646026831786e-06,
      "loss": 50.1514,
      "step": 18188
    },
    {
      "epoch": 18.21,
      "grad_norm": 1931.6939697265625,
      "learning_rate": 9.241486068111455e-06,
      "loss": 54.7191,
      "step": 18189
    },
    {
      "epoch": 18.21,
      "grad_norm": 60780.578125,
      "learning_rate": 9.236326109391125e-06,
      "loss": 55.3894,
      "step": 18190
    },
    {
      "epoch": 18.21,
      "grad_norm": 13833.5693359375,
      "learning_rate": 9.231166150670794e-06,
      "loss": 49.8641,
      "step": 18191
    },
    {
      "epoch": 18.21,
      "grad_norm": 16749.791015625,
      "learning_rate": 9.226006191950465e-06,
      "loss": 61.959,
      "step": 18192
    },
    {
      "epoch": 18.21,
      "grad_norm": 5148.56640625,
      "learning_rate": 9.220846233230135e-06,
      "loss": 61.757,
      "step": 18193
    },
    {
      "epoch": 18.21,
      "grad_norm": 36832.828125,
      "learning_rate": 9.215686274509804e-06,
      "loss": 55.1077,
      "step": 18194
    },
    {
      "epoch": 18.21,
      "grad_norm": 56973.0625,
      "learning_rate": 9.210526315789474e-06,
      "loss": 55.4595,
      "step": 18195
    },
    {
      "epoch": 18.21,
      "grad_norm": 40524.8359375,
      "learning_rate": 9.205366357069143e-06,
      "loss": 60.9757,
      "step": 18196
    },
    {
      "epoch": 18.22,
      "grad_norm": 4120.84912109375,
      "learning_rate": 9.200206398348814e-06,
      "loss": 64.6599,
      "step": 18197
    },
    {
      "epoch": 18.22,
      "grad_norm": 76643.8125,
      "learning_rate": 9.195046439628484e-06,
      "loss": 24.0045,
      "step": 18198
    },
    {
      "epoch": 18.22,
      "grad_norm": 44088.484375,
      "learning_rate": 9.189886480908153e-06,
      "loss": 53.1042,
      "step": 18199
    },
    {
      "epoch": 18.22,
      "grad_norm": 23102.490234375,
      "learning_rate": 9.184726522187823e-06,
      "loss": 53.7977,
      "step": 18200
    },
    {
      "epoch": 18.22,
      "grad_norm": 9498.599609375,
      "learning_rate": 9.179566563467492e-06,
      "loss": 49.2516,
      "step": 18201
    },
    {
      "epoch": 18.22,
      "grad_norm": 2525.031005859375,
      "learning_rate": 9.174406604747163e-06,
      "loss": 63.6895,
      "step": 18202
    },
    {
      "epoch": 18.22,
      "grad_norm": 4530.294921875,
      "learning_rate": 9.169246646026833e-06,
      "loss": 55.1878,
      "step": 18203
    },
    {
      "epoch": 18.22,
      "grad_norm": 3015.587890625,
      "learning_rate": 9.164086687306502e-06,
      "loss": 53.0844,
      "step": 18204
    },
    {
      "epoch": 18.22,
      "grad_norm": 4970.4169921875,
      "learning_rate": 9.158926728586171e-06,
      "loss": 55.0595,
      "step": 18205
    },
    {
      "epoch": 18.22,
      "grad_norm": 4715.92919921875,
      "learning_rate": 9.153766769865841e-06,
      "loss": 66.1508,
      "step": 18206
    },
    {
      "epoch": 18.23,
      "grad_norm": 2491.846435546875,
      "learning_rate": 9.148606811145512e-06,
      "loss": 58.5522,
      "step": 18207
    },
    {
      "epoch": 18.23,
      "grad_norm": 8456.03515625,
      "learning_rate": 9.14344685242518e-06,
      "loss": 51.4612,
      "step": 18208
    },
    {
      "epoch": 18.23,
      "grad_norm": 9829.7548828125,
      "learning_rate": 9.138286893704851e-06,
      "loss": 62.4161,
      "step": 18209
    },
    {
      "epoch": 18.23,
      "grad_norm": 4946.828125,
      "learning_rate": 9.13312693498452e-06,
      "loss": 60.2968,
      "step": 18210
    },
    {
      "epoch": 18.23,
      "grad_norm": 872.5829467773438,
      "learning_rate": 9.127966976264191e-06,
      "loss": 53.4167,
      "step": 18211
    },
    {
      "epoch": 18.23,
      "grad_norm": 2798.384033203125,
      "learning_rate": 9.122807017543861e-06,
      "loss": 62.9415,
      "step": 18212
    },
    {
      "epoch": 18.23,
      "grad_norm": 9239.0771484375,
      "learning_rate": 9.117647058823529e-06,
      "loss": 49.2698,
      "step": 18213
    },
    {
      "epoch": 18.23,
      "grad_norm": 70211.796875,
      "learning_rate": 9.1124871001032e-06,
      "loss": 62.0797,
      "step": 18214
    },
    {
      "epoch": 18.23,
      "grad_norm": 2437.524658203125,
      "learning_rate": 9.10732714138287e-06,
      "loss": 67.2391,
      "step": 18215
    },
    {
      "epoch": 18.23,
      "grad_norm": 5698.1416015625,
      "learning_rate": 9.10216718266254e-06,
      "loss": 50.4797,
      "step": 18216
    },
    {
      "epoch": 18.24,
      "grad_norm": 26340.357421875,
      "learning_rate": 9.097007223942208e-06,
      "loss": 44.4899,
      "step": 18217
    },
    {
      "epoch": 18.24,
      "grad_norm": 10644.3466796875,
      "learning_rate": 9.09184726522188e-06,
      "loss": 64.166,
      "step": 18218
    },
    {
      "epoch": 18.24,
      "grad_norm": 2301.68994140625,
      "learning_rate": 9.086687306501549e-06,
      "loss": 65.4409,
      "step": 18219
    },
    {
      "epoch": 18.24,
      "grad_norm": 13079.427734375,
      "learning_rate": 9.081527347781218e-06,
      "loss": 55.7962,
      "step": 18220
    },
    {
      "epoch": 18.24,
      "grad_norm": 4887.73046875,
      "learning_rate": 9.07636738906089e-06,
      "loss": 66.9512,
      "step": 18221
    },
    {
      "epoch": 18.24,
      "grad_norm": 2925.357177734375,
      "learning_rate": 9.071207430340557e-06,
      "loss": 53.5046,
      "step": 18222
    },
    {
      "epoch": 18.24,
      "grad_norm": 84207.5,
      "learning_rate": 9.066047471620228e-06,
      "loss": 58.6087,
      "step": 18223
    },
    {
      "epoch": 18.24,
      "grad_norm": 14755.19921875,
      "learning_rate": 9.060887512899897e-06,
      "loss": 60.998,
      "step": 18224
    },
    {
      "epoch": 18.24,
      "grad_norm": 1189.2916259765625,
      "learning_rate": 9.055727554179567e-06,
      "loss": 61.736,
      "step": 18225
    },
    {
      "epoch": 18.24,
      "grad_norm": 118379.9296875,
      "learning_rate": 9.050567595459236e-06,
      "loss": 56.1404,
      "step": 18226
    },
    {
      "epoch": 18.25,
      "grad_norm": 1208.231689453125,
      "learning_rate": 9.045407636738906e-06,
      "loss": 65.4534,
      "step": 18227
    },
    {
      "epoch": 18.25,
      "grad_norm": 2784.064208984375,
      "learning_rate": 9.040247678018577e-06,
      "loss": 50.4417,
      "step": 18228
    },
    {
      "epoch": 18.25,
      "grad_norm": 1775.5228271484375,
      "learning_rate": 9.035087719298246e-06,
      "loss": 61.6557,
      "step": 18229
    },
    {
      "epoch": 18.25,
      "grad_norm": 7016.0859375,
      "learning_rate": 9.029927760577916e-06,
      "loss": 61.0267,
      "step": 18230
    },
    {
      "epoch": 18.25,
      "grad_norm": 6568.7255859375,
      "learning_rate": 9.024767801857585e-06,
      "loss": 39.3195,
      "step": 18231
    },
    {
      "epoch": 18.25,
      "grad_norm": 5528.7646484375,
      "learning_rate": 9.019607843137255e-06,
      "loss": 50.5926,
      "step": 18232
    },
    {
      "epoch": 18.25,
      "grad_norm": 1161.926025390625,
      "learning_rate": 9.014447884416926e-06,
      "loss": 61.6238,
      "step": 18233
    },
    {
      "epoch": 18.25,
      "grad_norm": 6418.6806640625,
      "learning_rate": 9.009287925696595e-06,
      "loss": 54.0098,
      "step": 18234
    },
    {
      "epoch": 18.25,
      "grad_norm": 13797.1943359375,
      "learning_rate": 9.004127966976265e-06,
      "loss": 58.9964,
      "step": 18235
    },
    {
      "epoch": 18.25,
      "grad_norm": 13510.4482421875,
      "learning_rate": 8.998968008255934e-06,
      "loss": 43.9186,
      "step": 18236
    },
    {
      "epoch": 18.26,
      "grad_norm": 3157.09228515625,
      "learning_rate": 8.993808049535604e-06,
      "loss": 64.3493,
      "step": 18237
    },
    {
      "epoch": 18.26,
      "grad_norm": 37661.4921875,
      "learning_rate": 8.988648090815275e-06,
      "loss": 59.3802,
      "step": 18238
    },
    {
      "epoch": 18.26,
      "grad_norm": 15546.314453125,
      "learning_rate": 8.983488132094944e-06,
      "loss": 60.0319,
      "step": 18239
    },
    {
      "epoch": 18.26,
      "grad_norm": 25757.865234375,
      "learning_rate": 8.978328173374614e-06,
      "loss": 58.6472,
      "step": 18240
    },
    {
      "epoch": 18.26,
      "grad_norm": 9074.3857421875,
      "learning_rate": 8.973168214654283e-06,
      "loss": 53.3488,
      "step": 18241
    },
    {
      "epoch": 18.26,
      "grad_norm": 5706.73046875,
      "learning_rate": 8.968008255933952e-06,
      "loss": 58.5068,
      "step": 18242
    },
    {
      "epoch": 18.26,
      "grad_norm": 33039.5703125,
      "learning_rate": 8.962848297213624e-06,
      "loss": 58.3225,
      "step": 18243
    },
    {
      "epoch": 18.26,
      "grad_norm": 1298.004638671875,
      "learning_rate": 8.957688338493291e-06,
      "loss": 66.0606,
      "step": 18244
    },
    {
      "epoch": 18.26,
      "grad_norm": 1968.9075927734375,
      "learning_rate": 8.952528379772962e-06,
      "loss": 64.824,
      "step": 18245
    },
    {
      "epoch": 18.26,
      "grad_norm": 4535.14208984375,
      "learning_rate": 8.947368421052632e-06,
      "loss": 57.688,
      "step": 18246
    },
    {
      "epoch": 18.27,
      "grad_norm": 2273.860107421875,
      "learning_rate": 8.942208462332301e-06,
      "loss": 60.5324,
      "step": 18247
    },
    {
      "epoch": 18.27,
      "grad_norm": 7126.6650390625,
      "learning_rate": 8.937048503611972e-06,
      "loss": 56.9011,
      "step": 18248
    },
    {
      "epoch": 18.27,
      "grad_norm": 28023.06640625,
      "learning_rate": 8.93188854489164e-06,
      "loss": 61.2902,
      "step": 18249
    },
    {
      "epoch": 18.27,
      "grad_norm": 2561.89306640625,
      "learning_rate": 8.926728586171311e-06,
      "loss": 60.9227,
      "step": 18250
    },
    {
      "epoch": 18.27,
      "grad_norm": 12466.482421875,
      "learning_rate": 8.92156862745098e-06,
      "loss": 62.767,
      "step": 18251
    },
    {
      "epoch": 18.27,
      "grad_norm": 14332.7109375,
      "learning_rate": 8.916408668730652e-06,
      "loss": 68.7276,
      "step": 18252
    },
    {
      "epoch": 18.27,
      "grad_norm": 8383.5615234375,
      "learning_rate": 8.91124871001032e-06,
      "loss": 50.7325,
      "step": 18253
    },
    {
      "epoch": 18.27,
      "grad_norm": 12608.080078125,
      "learning_rate": 8.906088751289989e-06,
      "loss": 49.9781,
      "step": 18254
    },
    {
      "epoch": 18.27,
      "grad_norm": 23179.451171875,
      "learning_rate": 8.90092879256966e-06,
      "loss": 56.5246,
      "step": 18255
    },
    {
      "epoch": 18.27,
      "grad_norm": 12558.625,
      "learning_rate": 8.89576883384933e-06,
      "loss": 48.9536,
      "step": 18256
    },
    {
      "epoch": 18.28,
      "grad_norm": 3816.508056640625,
      "learning_rate": 8.890608875128999e-06,
      "loss": 55.57,
      "step": 18257
    },
    {
      "epoch": 18.28,
      "grad_norm": 5044.92138671875,
      "learning_rate": 8.885448916408668e-06,
      "loss": 58.9078,
      "step": 18258
    },
    {
      "epoch": 18.28,
      "grad_norm": 5439.525390625,
      "learning_rate": 8.88028895768834e-06,
      "loss": 46.1696,
      "step": 18259
    },
    {
      "epoch": 18.28,
      "grad_norm": 64050.29296875,
      "learning_rate": 8.875128998968009e-06,
      "loss": 35.035,
      "step": 18260
    },
    {
      "epoch": 18.28,
      "grad_norm": 7933.197265625,
      "learning_rate": 8.869969040247678e-06,
      "loss": 54.6974,
      "step": 18261
    },
    {
      "epoch": 18.28,
      "grad_norm": 1913.4681396484375,
      "learning_rate": 8.864809081527348e-06,
      "loss": 57.9875,
      "step": 18262
    },
    {
      "epoch": 18.28,
      "grad_norm": 140003.40625,
      "learning_rate": 8.859649122807017e-06,
      "loss": 47.5315,
      "step": 18263
    },
    {
      "epoch": 18.28,
      "grad_norm": 1993.611572265625,
      "learning_rate": 8.854489164086688e-06,
      "loss": 39.6208,
      "step": 18264
    },
    {
      "epoch": 18.28,
      "grad_norm": 7806.9052734375,
      "learning_rate": 8.849329205366358e-06,
      "loss": 57.9102,
      "step": 18265
    },
    {
      "epoch": 18.28,
      "grad_norm": 2330.413330078125,
      "learning_rate": 8.844169246646027e-06,
      "loss": 59.8815,
      "step": 18266
    },
    {
      "epoch": 18.29,
      "grad_norm": 9313.5751953125,
      "learning_rate": 8.839009287925697e-06,
      "loss": 38.8503,
      "step": 18267
    },
    {
      "epoch": 18.29,
      "grad_norm": 11466.8046875,
      "learning_rate": 8.833849329205366e-06,
      "loss": 61.8539,
      "step": 18268
    },
    {
      "epoch": 18.29,
      "grad_norm": 5152.955078125,
      "learning_rate": 8.828689370485037e-06,
      "loss": 39.9799,
      "step": 18269
    },
    {
      "epoch": 18.29,
      "grad_norm": 2035.9461669921875,
      "learning_rate": 8.823529411764707e-06,
      "loss": 57.2935,
      "step": 18270
    },
    {
      "epoch": 18.29,
      "grad_norm": 14858.779296875,
      "learning_rate": 8.818369453044376e-06,
      "loss": 66.0447,
      "step": 18271
    },
    {
      "epoch": 18.29,
      "grad_norm": 28896.4921875,
      "learning_rate": 8.813209494324046e-06,
      "loss": 53.3777,
      "step": 18272
    },
    {
      "epoch": 18.29,
      "grad_norm": 9111.880859375,
      "learning_rate": 8.808049535603715e-06,
      "loss": 41.7946,
      "step": 18273
    },
    {
      "epoch": 18.29,
      "grad_norm": 4597.64111328125,
      "learning_rate": 8.802889576883386e-06,
      "loss": 47.3993,
      "step": 18274
    },
    {
      "epoch": 18.29,
      "grad_norm": 50464.640625,
      "learning_rate": 8.797729618163054e-06,
      "loss": 54.7469,
      "step": 18275
    },
    {
      "epoch": 18.29,
      "grad_norm": 52787.34375,
      "learning_rate": 8.792569659442725e-06,
      "loss": 61.0663,
      "step": 18276
    },
    {
      "epoch": 18.3,
      "grad_norm": 63026.6875,
      "learning_rate": 8.787409700722395e-06,
      "loss": 48.1124,
      "step": 18277
    },
    {
      "epoch": 18.3,
      "grad_norm": 24554.791015625,
      "learning_rate": 8.782249742002064e-06,
      "loss": 60.28,
      "step": 18278
    },
    {
      "epoch": 18.3,
      "grad_norm": 17854.033203125,
      "learning_rate": 8.777089783281735e-06,
      "loss": 48.8878,
      "step": 18279
    },
    {
      "epoch": 18.3,
      "grad_norm": 13794.0908203125,
      "learning_rate": 8.771929824561403e-06,
      "loss": 59.7761,
      "step": 18280
    },
    {
      "epoch": 18.3,
      "grad_norm": 31465.2578125,
      "learning_rate": 8.766769865841074e-06,
      "loss": 23.4894,
      "step": 18281
    },
    {
      "epoch": 18.3,
      "grad_norm": 7122.62939453125,
      "learning_rate": 8.761609907120743e-06,
      "loss": 43.2122,
      "step": 18282
    },
    {
      "epoch": 18.3,
      "grad_norm": 2745.68212890625,
      "learning_rate": 8.756449948400413e-06,
      "loss": 66.6172,
      "step": 18283
    },
    {
      "epoch": 18.3,
      "grad_norm": 9249.0263671875,
      "learning_rate": 8.751289989680082e-06,
      "loss": 60.2885,
      "step": 18284
    },
    {
      "epoch": 18.3,
      "grad_norm": 1830.2945556640625,
      "learning_rate": 8.746130030959752e-06,
      "loss": 57.9632,
      "step": 18285
    },
    {
      "epoch": 18.3,
      "grad_norm": 8476.009765625,
      "learning_rate": 8.740970072239423e-06,
      "loss": 66.5644,
      "step": 18286
    },
    {
      "epoch": 18.31,
      "grad_norm": 5732.43994140625,
      "learning_rate": 8.735810113519092e-06,
      "loss": 62.5527,
      "step": 18287
    },
    {
      "epoch": 18.31,
      "grad_norm": 6140.5966796875,
      "learning_rate": 8.730650154798763e-06,
      "loss": 59.3047,
      "step": 18288
    },
    {
      "epoch": 18.31,
      "grad_norm": 22655.9921875,
      "learning_rate": 8.725490196078431e-06,
      "loss": 54.1596,
      "step": 18289
    },
    {
      "epoch": 18.31,
      "grad_norm": 5626.185546875,
      "learning_rate": 8.7203302373581e-06,
      "loss": 63.3966,
      "step": 18290
    },
    {
      "epoch": 18.31,
      "grad_norm": 12901.0546875,
      "learning_rate": 8.715170278637772e-06,
      "loss": 35.9708,
      "step": 18291
    },
    {
      "epoch": 18.31,
      "grad_norm": 40542.97265625,
      "learning_rate": 8.710010319917441e-06,
      "loss": 53.5846,
      "step": 18292
    },
    {
      "epoch": 18.31,
      "grad_norm": 3350.895751953125,
      "learning_rate": 8.70485036119711e-06,
      "loss": 64.4546,
      "step": 18293
    },
    {
      "epoch": 18.31,
      "grad_norm": 16637.0078125,
      "learning_rate": 8.69969040247678e-06,
      "loss": 48.4069,
      "step": 18294
    },
    {
      "epoch": 18.31,
      "grad_norm": 8796.505859375,
      "learning_rate": 8.694530443756451e-06,
      "loss": 52.1392,
      "step": 18295
    },
    {
      "epoch": 18.31,
      "grad_norm": 6763.92919921875,
      "learning_rate": 8.68937048503612e-06,
      "loss": 68.0326,
      "step": 18296
    },
    {
      "epoch": 18.32,
      "grad_norm": 53066.6328125,
      "learning_rate": 8.68421052631579e-06,
      "loss": 43.2697,
      "step": 18297
    },
    {
      "epoch": 18.32,
      "grad_norm": 42655.8125,
      "learning_rate": 8.67905056759546e-06,
      "loss": 38.7353,
      "step": 18298
    },
    {
      "epoch": 18.32,
      "grad_norm": 5763.98388671875,
      "learning_rate": 8.673890608875129e-06,
      "loss": 58.2627,
      "step": 18299
    },
    {
      "epoch": 18.32,
      "grad_norm": 7526.6708984375,
      "learning_rate": 8.6687306501548e-06,
      "loss": 66.0669,
      "step": 18300
    },
    {
      "epoch": 18.32,
      "grad_norm": 1715.17626953125,
      "learning_rate": 8.66357069143447e-06,
      "loss": 60.8241,
      "step": 18301
    },
    {
      "epoch": 18.32,
      "grad_norm": 1065.25244140625,
      "learning_rate": 8.658410732714139e-06,
      "loss": 62.4883,
      "step": 18302
    },
    {
      "epoch": 18.32,
      "grad_norm": 9924.0673828125,
      "learning_rate": 8.653250773993808e-06,
      "loss": 59.3084,
      "step": 18303
    },
    {
      "epoch": 18.32,
      "grad_norm": 15858.6728515625,
      "learning_rate": 8.648090815273478e-06,
      "loss": 54.4877,
      "step": 18304
    },
    {
      "epoch": 18.32,
      "grad_norm": 9213.8974609375,
      "learning_rate": 8.642930856553149e-06,
      "loss": 66.462,
      "step": 18305
    },
    {
      "epoch": 18.32,
      "grad_norm": 8167.9873046875,
      "learning_rate": 8.637770897832818e-06,
      "loss": 50.8317,
      "step": 18306
    },
    {
      "epoch": 18.33,
      "grad_norm": 26044.474609375,
      "learning_rate": 8.632610939112488e-06,
      "loss": 57.1433,
      "step": 18307
    },
    {
      "epoch": 18.33,
      "grad_norm": 3699.233642578125,
      "learning_rate": 8.627450980392157e-06,
      "loss": 63.6458,
      "step": 18308
    },
    {
      "epoch": 18.33,
      "grad_norm": 22330.607421875,
      "learning_rate": 8.622291021671827e-06,
      "loss": 63.2691,
      "step": 18309
    },
    {
      "epoch": 18.33,
      "grad_norm": 24480.30078125,
      "learning_rate": 8.617131062951498e-06,
      "loss": 61.0145,
      "step": 18310
    },
    {
      "epoch": 18.33,
      "grad_norm": 5523.2939453125,
      "learning_rate": 8.611971104231166e-06,
      "loss": 60.2119,
      "step": 18311
    },
    {
      "epoch": 18.33,
      "grad_norm": 6277.23583984375,
      "learning_rate": 8.606811145510837e-06,
      "loss": 51.7935,
      "step": 18312
    },
    {
      "epoch": 18.33,
      "grad_norm": 43181.734375,
      "learning_rate": 8.601651186790506e-06,
      "loss": 61.896,
      "step": 18313
    },
    {
      "epoch": 18.33,
      "grad_norm": 8943.388671875,
      "learning_rate": 8.596491228070176e-06,
      "loss": 59.0539,
      "step": 18314
    },
    {
      "epoch": 18.33,
      "grad_norm": 3029.443359375,
      "learning_rate": 8.591331269349847e-06,
      "loss": 45.9416,
      "step": 18315
    },
    {
      "epoch": 18.33,
      "grad_norm": 5835.75830078125,
      "learning_rate": 8.586171310629514e-06,
      "loss": 64.5288,
      "step": 18316
    },
    {
      "epoch": 18.34,
      "grad_norm": 4400.3388671875,
      "learning_rate": 8.581011351909186e-06,
      "loss": 62.4442,
      "step": 18317
    },
    {
      "epoch": 18.34,
      "grad_norm": 4670.048828125,
      "learning_rate": 8.575851393188855e-06,
      "loss": 43.8811,
      "step": 18318
    },
    {
      "epoch": 18.34,
      "grad_norm": 5832.478515625,
      "learning_rate": 8.570691434468524e-06,
      "loss": 65.2296,
      "step": 18319
    },
    {
      "epoch": 18.34,
      "grad_norm": 2456.905517578125,
      "learning_rate": 8.565531475748194e-06,
      "loss": 51.2234,
      "step": 18320
    },
    {
      "epoch": 18.34,
      "grad_norm": 11589.955078125,
      "learning_rate": 8.560371517027863e-06,
      "loss": 52.4143,
      "step": 18321
    },
    {
      "epoch": 18.34,
      "grad_norm": 2855.369873046875,
      "learning_rate": 8.555211558307534e-06,
      "loss": 50.9594,
      "step": 18322
    },
    {
      "epoch": 18.34,
      "grad_norm": 40894.171875,
      "learning_rate": 8.550051599587204e-06,
      "loss": 57.4475,
      "step": 18323
    },
    {
      "epoch": 18.34,
      "grad_norm": 8872.015625,
      "learning_rate": 8.544891640866875e-06,
      "loss": 62.057,
      "step": 18324
    },
    {
      "epoch": 18.34,
      "grad_norm": 50788.33984375,
      "learning_rate": 8.539731682146543e-06,
      "loss": 59.2165,
      "step": 18325
    },
    {
      "epoch": 18.34,
      "grad_norm": 18664.111328125,
      "learning_rate": 8.534571723426212e-06,
      "loss": 55.0209,
      "step": 18326
    },
    {
      "epoch": 18.35,
      "grad_norm": 9502.4501953125,
      "learning_rate": 8.529411764705883e-06,
      "loss": 52.8419,
      "step": 18327
    },
    {
      "epoch": 18.35,
      "grad_norm": 32209.419921875,
      "learning_rate": 8.524251805985553e-06,
      "loss": 23.2261,
      "step": 18328
    },
    {
      "epoch": 18.35,
      "grad_norm": 41397.984375,
      "learning_rate": 8.519091847265222e-06,
      "loss": 49.1442,
      "step": 18329
    },
    {
      "epoch": 18.35,
      "grad_norm": 92461.9453125,
      "learning_rate": 8.513931888544892e-06,
      "loss": 60.7875,
      "step": 18330
    },
    {
      "epoch": 18.35,
      "grad_norm": 19647.314453125,
      "learning_rate": 8.508771929824563e-06,
      "loss": 60.5787,
      "step": 18331
    },
    {
      "epoch": 18.35,
      "grad_norm": 3552.76025390625,
      "learning_rate": 8.503611971104232e-06,
      "loss": 50.4089,
      "step": 18332
    },
    {
      "epoch": 18.35,
      "grad_norm": 20926.70703125,
      "learning_rate": 8.4984520123839e-06,
      "loss": 60.914,
      "step": 18333
    },
    {
      "epoch": 18.35,
      "grad_norm": 3811.2939453125,
      "learning_rate": 8.493292053663571e-06,
      "loss": 54.2823,
      "step": 18334
    },
    {
      "epoch": 18.35,
      "grad_norm": 20829.822265625,
      "learning_rate": 8.48813209494324e-06,
      "loss": 67.0115,
      "step": 18335
    },
    {
      "epoch": 18.35,
      "grad_norm": 6673.966796875,
      "learning_rate": 8.482972136222912e-06,
      "loss": 51.4011,
      "step": 18336
    },
    {
      "epoch": 18.36,
      "grad_norm": 4108.27099609375,
      "learning_rate": 8.477812177502581e-06,
      "loss": 62.2871,
      "step": 18337
    },
    {
      "epoch": 18.36,
      "grad_norm": 38532.8359375,
      "learning_rate": 8.472652218782249e-06,
      "loss": 68.1296,
      "step": 18338
    },
    {
      "epoch": 18.36,
      "grad_norm": 2263.2568359375,
      "learning_rate": 8.46749226006192e-06,
      "loss": 62.2738,
      "step": 18339
    },
    {
      "epoch": 18.36,
      "grad_norm": 37574.92578125,
      "learning_rate": 8.46233230134159e-06,
      "loss": 59.804,
      "step": 18340
    },
    {
      "epoch": 18.36,
      "grad_norm": 12535.9599609375,
      "learning_rate": 8.45717234262126e-06,
      "loss": 57.9585,
      "step": 18341
    },
    {
      "epoch": 18.36,
      "grad_norm": 2782.20703125,
      "learning_rate": 8.452012383900928e-06,
      "loss": 55.4895,
      "step": 18342
    },
    {
      "epoch": 18.36,
      "grad_norm": 2224.991455078125,
      "learning_rate": 8.4468524251806e-06,
      "loss": 59.9675,
      "step": 18343
    },
    {
      "epoch": 18.36,
      "grad_norm": 16638.388671875,
      "learning_rate": 8.441692466460269e-06,
      "loss": 56.2747,
      "step": 18344
    },
    {
      "epoch": 18.36,
      "grad_norm": 173020.359375,
      "learning_rate": 8.436532507739938e-06,
      "loss": 58.1257,
      "step": 18345
    },
    {
      "epoch": 18.36,
      "grad_norm": 9478.3154296875,
      "learning_rate": 8.43137254901961e-06,
      "loss": 57.1486,
      "step": 18346
    },
    {
      "epoch": 18.37,
      "grad_norm": 2772.502685546875,
      "learning_rate": 8.426212590299277e-06,
      "loss": 58.113,
      "step": 18347
    },
    {
      "epoch": 18.37,
      "grad_norm": 7520.8037109375,
      "learning_rate": 8.421052631578948e-06,
      "loss": 64.1891,
      "step": 18348
    },
    {
      "epoch": 18.37,
      "grad_norm": 27422.158203125,
      "learning_rate": 8.415892672858618e-06,
      "loss": 60.15,
      "step": 18349
    },
    {
      "epoch": 18.37,
      "grad_norm": 3207.123291015625,
      "learning_rate": 8.410732714138287e-06,
      "loss": 62.1843,
      "step": 18350
    },
    {
      "epoch": 18.37,
      "grad_norm": 17967.908203125,
      "learning_rate": 8.405572755417957e-06,
      "loss": 59.7453,
      "step": 18351
    },
    {
      "epoch": 18.37,
      "grad_norm": 7042.955078125,
      "learning_rate": 8.400412796697626e-06,
      "loss": 35.5954,
      "step": 18352
    },
    {
      "epoch": 18.37,
      "grad_norm": 66199.90625,
      "learning_rate": 8.395252837977297e-06,
      "loss": 50.9067,
      "step": 18353
    },
    {
      "epoch": 18.37,
      "grad_norm": 11693.9130859375,
      "learning_rate": 8.390092879256967e-06,
      "loss": 56.4516,
      "step": 18354
    },
    {
      "epoch": 18.37,
      "grad_norm": 5197.89599609375,
      "learning_rate": 8.384932920536636e-06,
      "loss": 49.025,
      "step": 18355
    },
    {
      "epoch": 18.37,
      "grad_norm": 5656.56689453125,
      "learning_rate": 8.379772961816305e-06,
      "loss": 49.0949,
      "step": 18356
    },
    {
      "epoch": 18.38,
      "grad_norm": 35954.52734375,
      "learning_rate": 8.374613003095975e-06,
      "loss": 25.0417,
      "step": 18357
    },
    {
      "epoch": 18.38,
      "grad_norm": 203325.6875,
      "learning_rate": 8.369453044375646e-06,
      "loss": 34.2699,
      "step": 18358
    },
    {
      "epoch": 18.38,
      "grad_norm": 3894.95654296875,
      "learning_rate": 8.364293085655315e-06,
      "loss": 57.7189,
      "step": 18359
    },
    {
      "epoch": 18.38,
      "grad_norm": 1712.3231201171875,
      "learning_rate": 8.359133126934985e-06,
      "loss": 63.6163,
      "step": 18360
    },
    {
      "epoch": 18.38,
      "grad_norm": 5577.58642578125,
      "learning_rate": 8.353973168214654e-06,
      "loss": 52.3515,
      "step": 18361
    },
    {
      "epoch": 18.38,
      "grad_norm": 18437.9140625,
      "learning_rate": 8.348813209494324e-06,
      "loss": 46.2932,
      "step": 18362
    },
    {
      "epoch": 18.38,
      "grad_norm": 14516.634765625,
      "learning_rate": 8.343653250773995e-06,
      "loss": 62.1239,
      "step": 18363
    },
    {
      "epoch": 18.38,
      "grad_norm": 11580.41796875,
      "learning_rate": 8.338493292053664e-06,
      "loss": 58.7343,
      "step": 18364
    },
    {
      "epoch": 18.38,
      "grad_norm": 4354.82470703125,
      "learning_rate": 8.333333333333334e-06,
      "loss": 64.9748,
      "step": 18365
    },
    {
      "epoch": 18.38,
      "grad_norm": 9821.505859375,
      "learning_rate": 8.328173374613003e-06,
      "loss": 47.725,
      "step": 18366
    },
    {
      "epoch": 18.39,
      "grad_norm": 3303.70166015625,
      "learning_rate": 8.323013415892673e-06,
      "loss": 65.3612,
      "step": 18367
    },
    {
      "epoch": 18.39,
      "grad_norm": 9815.5302734375,
      "learning_rate": 8.317853457172344e-06,
      "loss": 54.9155,
      "step": 18368
    },
    {
      "epoch": 18.39,
      "grad_norm": 16567.482421875,
      "learning_rate": 8.312693498452011e-06,
      "loss": 34.5704,
      "step": 18369
    },
    {
      "epoch": 18.39,
      "grad_norm": 3850.13671875,
      "learning_rate": 8.307533539731683e-06,
      "loss": 55.3046,
      "step": 18370
    },
    {
      "epoch": 18.39,
      "grad_norm": 1659.7247314453125,
      "learning_rate": 8.302373581011352e-06,
      "loss": 51.9736,
      "step": 18371
    },
    {
      "epoch": 18.39,
      "grad_norm": 15001.8330078125,
      "learning_rate": 8.297213622291023e-06,
      "loss": 58.4964,
      "step": 18372
    },
    {
      "epoch": 18.39,
      "grad_norm": 5988.18701171875,
      "learning_rate": 8.292053663570693e-06,
      "loss": 57.6275,
      "step": 18373
    },
    {
      "epoch": 18.39,
      "grad_norm": 44697.26953125,
      "learning_rate": 8.28689370485036e-06,
      "loss": 47.024,
      "step": 18374
    },
    {
      "epoch": 18.39,
      "grad_norm": 11978.0771484375,
      "learning_rate": 8.281733746130031e-06,
      "loss": 63.6308,
      "step": 18375
    },
    {
      "epoch": 18.39,
      "grad_norm": 109383.1328125,
      "learning_rate": 8.276573787409701e-06,
      "loss": 32.2664,
      "step": 18376
    },
    {
      "epoch": 18.4,
      "grad_norm": 67107.4765625,
      "learning_rate": 8.271413828689372e-06,
      "loss": 62.2034,
      "step": 18377
    },
    {
      "epoch": 18.4,
      "grad_norm": 5169.78369140625,
      "learning_rate": 8.26625386996904e-06,
      "loss": 57.1755,
      "step": 18378
    },
    {
      "epoch": 18.4,
      "grad_norm": 2252.015380859375,
      "learning_rate": 8.261093911248711e-06,
      "loss": 56.8288,
      "step": 18379
    },
    {
      "epoch": 18.4,
      "grad_norm": 7347.3369140625,
      "learning_rate": 8.25593395252838e-06,
      "loss": 64.8811,
      "step": 18380
    },
    {
      "epoch": 18.4,
      "grad_norm": 9144.6513671875,
      "learning_rate": 8.25077399380805e-06,
      "loss": 59.6437,
      "step": 18381
    },
    {
      "epoch": 18.4,
      "grad_norm": 10952.251953125,
      "learning_rate": 8.245614035087721e-06,
      "loss": 61.9973,
      "step": 18382
    },
    {
      "epoch": 18.4,
      "grad_norm": 17571.0625,
      "learning_rate": 8.240454076367389e-06,
      "loss": 52.3566,
      "step": 18383
    },
    {
      "epoch": 18.4,
      "grad_norm": 7452.19921875,
      "learning_rate": 8.23529411764706e-06,
      "loss": 55.9554,
      "step": 18384
    },
    {
      "epoch": 18.4,
      "grad_norm": 4683.3076171875,
      "learning_rate": 8.23013415892673e-06,
      "loss": 60.3177,
      "step": 18385
    },
    {
      "epoch": 18.4,
      "grad_norm": 4055.31884765625,
      "learning_rate": 8.224974200206399e-06,
      "loss": 60.2025,
      "step": 18386
    },
    {
      "epoch": 18.41,
      "grad_norm": 7120.95068359375,
      "learning_rate": 8.219814241486068e-06,
      "loss": 52.8365,
      "step": 18387
    },
    {
      "epoch": 18.41,
      "grad_norm": 11130.904296875,
      "learning_rate": 8.214654282765738e-06,
      "loss": 58.6079,
      "step": 18388
    },
    {
      "epoch": 18.41,
      "grad_norm": 43748.6953125,
      "learning_rate": 8.209494324045409e-06,
      "loss": 65.5066,
      "step": 18389
    },
    {
      "epoch": 18.41,
      "grad_norm": 15232.2255859375,
      "learning_rate": 8.204334365325078e-06,
      "loss": 58.1432,
      "step": 18390
    },
    {
      "epoch": 18.41,
      "grad_norm": 1075.3834228515625,
      "learning_rate": 8.199174406604748e-06,
      "loss": 60.872,
      "step": 18391
    },
    {
      "epoch": 18.41,
      "grad_norm": 1014.2669067382812,
      "learning_rate": 8.194014447884417e-06,
      "loss": 64.4246,
      "step": 18392
    },
    {
      "epoch": 18.41,
      "grad_norm": 19076.728515625,
      "learning_rate": 8.188854489164086e-06,
      "loss": 57.839,
      "step": 18393
    },
    {
      "epoch": 18.41,
      "grad_norm": 22211.828125,
      "learning_rate": 8.183694530443758e-06,
      "loss": 53.5647,
      "step": 18394
    },
    {
      "epoch": 18.41,
      "grad_norm": 3003.75439453125,
      "learning_rate": 8.178534571723427e-06,
      "loss": 46.7943,
      "step": 18395
    },
    {
      "epoch": 18.41,
      "grad_norm": 9467.7138671875,
      "learning_rate": 8.173374613003096e-06,
      "loss": 24.0187,
      "step": 18396
    },
    {
      "epoch": 18.42,
      "grad_norm": 1807.388427734375,
      "learning_rate": 8.168214654282766e-06,
      "loss": 56.7424,
      "step": 18397
    },
    {
      "epoch": 18.42,
      "grad_norm": 8937.4873046875,
      "learning_rate": 8.163054695562435e-06,
      "loss": 40.3443,
      "step": 18398
    },
    {
      "epoch": 18.42,
      "grad_norm": 2582.651123046875,
      "learning_rate": 8.157894736842106e-06,
      "loss": 60.0486,
      "step": 18399
    },
    {
      "epoch": 18.42,
      "grad_norm": 6473.73779296875,
      "learning_rate": 8.152734778121776e-06,
      "loss": 54.0719,
      "step": 18400
    },
    {
      "epoch": 18.42,
      "grad_norm": 2951.986083984375,
      "learning_rate": 8.147574819401445e-06,
      "loss": 63.6118,
      "step": 18401
    },
    {
      "epoch": 18.42,
      "grad_norm": 1518.6407470703125,
      "learning_rate": 8.142414860681115e-06,
      "loss": 50.3919,
      "step": 18402
    },
    {
      "epoch": 18.42,
      "grad_norm": 72242.203125,
      "learning_rate": 8.137254901960784e-06,
      "loss": 50.5252,
      "step": 18403
    },
    {
      "epoch": 18.42,
      "grad_norm": 25691.35546875,
      "learning_rate": 8.132094943240455e-06,
      "loss": 39.6919,
      "step": 18404
    },
    {
      "epoch": 18.42,
      "grad_norm": 17743.671875,
      "learning_rate": 8.126934984520123e-06,
      "loss": 43.0307,
      "step": 18405
    },
    {
      "epoch": 18.42,
      "grad_norm": 9265.919921875,
      "learning_rate": 8.121775025799794e-06,
      "loss": 55.2552,
      "step": 18406
    },
    {
      "epoch": 18.43,
      "grad_norm": 4143.2294921875,
      "learning_rate": 8.116615067079464e-06,
      "loss": 46.6139,
      "step": 18407
    },
    {
      "epoch": 18.43,
      "grad_norm": 1430.6649169921875,
      "learning_rate": 8.111455108359135e-06,
      "loss": 56.9957,
      "step": 18408
    },
    {
      "epoch": 18.43,
      "grad_norm": 2274.1640625,
      "learning_rate": 8.106295149638802e-06,
      "loss": 67.9975,
      "step": 18409
    },
    {
      "epoch": 18.43,
      "grad_norm": 15104.8349609375,
      "learning_rate": 8.101135190918472e-06,
      "loss": 56.0077,
      "step": 18410
    },
    {
      "epoch": 18.43,
      "grad_norm": 57623.203125,
      "learning_rate": 8.095975232198143e-06,
      "loss": 55.8341,
      "step": 18411
    },
    {
      "epoch": 18.43,
      "grad_norm": 4556.2412109375,
      "learning_rate": 8.090815273477812e-06,
      "loss": 51.403,
      "step": 18412
    },
    {
      "epoch": 18.43,
      "grad_norm": 5798.5517578125,
      "learning_rate": 8.085655314757484e-06,
      "loss": 53.7696,
      "step": 18413
    },
    {
      "epoch": 18.43,
      "grad_norm": 16286.08984375,
      "learning_rate": 8.080495356037151e-06,
      "loss": 53.4625,
      "step": 18414
    },
    {
      "epoch": 18.43,
      "grad_norm": 29285.91796875,
      "learning_rate": 8.075335397316822e-06,
      "loss": 39.0555,
      "step": 18415
    },
    {
      "epoch": 18.43,
      "grad_norm": 5131.783203125,
      "learning_rate": 8.070175438596492e-06,
      "loss": 59.4509,
      "step": 18416
    },
    {
      "epoch": 18.44,
      "grad_norm": 6386.03173828125,
      "learning_rate": 8.065015479876161e-06,
      "loss": 51.4618,
      "step": 18417
    },
    {
      "epoch": 18.44,
      "grad_norm": 1886.630615234375,
      "learning_rate": 8.05985552115583e-06,
      "loss": 56.9446,
      "step": 18418
    },
    {
      "epoch": 18.44,
      "grad_norm": 3183.210205078125,
      "learning_rate": 8.0546955624355e-06,
      "loss": 56.2849,
      "step": 18419
    },
    {
      "epoch": 18.44,
      "grad_norm": 6180.1279296875,
      "learning_rate": 8.049535603715171e-06,
      "loss": 55.1572,
      "step": 18420
    },
    {
      "epoch": 18.44,
      "grad_norm": 2607.0517578125,
      "learning_rate": 8.04437564499484e-06,
      "loss": 49.2553,
      "step": 18421
    },
    {
      "epoch": 18.44,
      "grad_norm": 1999.741943359375,
      "learning_rate": 8.03921568627451e-06,
      "loss": 66.6663,
      "step": 18422
    },
    {
      "epoch": 18.44,
      "grad_norm": 3871.19970703125,
      "learning_rate": 8.03405572755418e-06,
      "loss": 60.2086,
      "step": 18423
    },
    {
      "epoch": 18.44,
      "grad_norm": 8882.1201171875,
      "learning_rate": 8.028895768833849e-06,
      "loss": 62.2452,
      "step": 18424
    },
    {
      "epoch": 18.44,
      "grad_norm": 8666.373046875,
      "learning_rate": 8.02373581011352e-06,
      "loss": 64.2417,
      "step": 18425
    },
    {
      "epoch": 18.44,
      "grad_norm": 2384.291259765625,
      "learning_rate": 8.01857585139319e-06,
      "loss": 52.618,
      "step": 18426
    },
    {
      "epoch": 18.45,
      "grad_norm": 7187.43359375,
      "learning_rate": 8.013415892672859e-06,
      "loss": 60.5849,
      "step": 18427
    },
    {
      "epoch": 18.45,
      "grad_norm": 13895.4296875,
      "learning_rate": 8.008255933952529e-06,
      "loss": 54.7741,
      "step": 18428
    },
    {
      "epoch": 18.45,
      "grad_norm": 5464.9287109375,
      "learning_rate": 8.003095975232198e-06,
      "loss": 58.3027,
      "step": 18429
    },
    {
      "epoch": 18.45,
      "grad_norm": 119285.7265625,
      "learning_rate": 7.997936016511869e-06,
      "loss": 55.9952,
      "step": 18430
    },
    {
      "epoch": 18.45,
      "grad_norm": 1341.7552490234375,
      "learning_rate": 7.992776057791539e-06,
      "loss": 62.6748,
      "step": 18431
    },
    {
      "epoch": 18.45,
      "grad_norm": 4624.76171875,
      "learning_rate": 7.987616099071208e-06,
      "loss": 60.5814,
      "step": 18432
    },
    {
      "epoch": 18.45,
      "grad_norm": 35541.16796875,
      "learning_rate": 7.982456140350877e-06,
      "loss": 66.4217,
      "step": 18433
    },
    {
      "epoch": 18.45,
      "grad_norm": 7875.859375,
      "learning_rate": 7.977296181630547e-06,
      "loss": 52.8504,
      "step": 18434
    },
    {
      "epoch": 18.45,
      "grad_norm": 6887.3349609375,
      "learning_rate": 7.972136222910218e-06,
      "loss": 54.55,
      "step": 18435
    },
    {
      "epoch": 18.45,
      "grad_norm": 5739.97802734375,
      "learning_rate": 7.966976264189886e-06,
      "loss": 59.6788,
      "step": 18436
    },
    {
      "epoch": 18.46,
      "grad_norm": 4556.87109375,
      "learning_rate": 7.961816305469557e-06,
      "loss": 55.0244,
      "step": 18437
    },
    {
      "epoch": 18.46,
      "grad_norm": 8250.716796875,
      "learning_rate": 7.956656346749226e-06,
      "loss": 57.3381,
      "step": 18438
    },
    {
      "epoch": 18.46,
      "grad_norm": 22745.380859375,
      "learning_rate": 7.951496388028896e-06,
      "loss": 62.0979,
      "step": 18439
    },
    {
      "epoch": 18.46,
      "grad_norm": 30350.6875,
      "learning_rate": 7.946336429308567e-06,
      "loss": 60.7476,
      "step": 18440
    },
    {
      "epoch": 18.46,
      "grad_norm": 12662.3662109375,
      "learning_rate": 7.941176470588235e-06,
      "loss": 49.5285,
      "step": 18441
    },
    {
      "epoch": 18.46,
      "grad_norm": 8199.1279296875,
      "learning_rate": 7.936016511867906e-06,
      "loss": 61.2139,
      "step": 18442
    },
    {
      "epoch": 18.46,
      "grad_norm": 16733.63671875,
      "learning_rate": 7.930856553147575e-06,
      "loss": 53.2387,
      "step": 18443
    },
    {
      "epoch": 18.46,
      "grad_norm": 15627.140625,
      "learning_rate": 7.925696594427245e-06,
      "loss": 48.5906,
      "step": 18444
    },
    {
      "epoch": 18.46,
      "grad_norm": 2679.369384765625,
      "learning_rate": 7.920536635706914e-06,
      "loss": 58.871,
      "step": 18445
    },
    {
      "epoch": 18.46,
      "grad_norm": 7681.55517578125,
      "learning_rate": 7.915376676986583e-06,
      "loss": 66.1128,
      "step": 18446
    },
    {
      "epoch": 18.47,
      "grad_norm": 43004.00390625,
      "learning_rate": 7.910216718266255e-06,
      "loss": 59.6325,
      "step": 18447
    },
    {
      "epoch": 18.47,
      "grad_norm": 18467.87890625,
      "learning_rate": 7.905056759545924e-06,
      "loss": 50.8066,
      "step": 18448
    },
    {
      "epoch": 18.47,
      "grad_norm": 7271.94677734375,
      "learning_rate": 7.899896800825595e-06,
      "loss": 43.8132,
      "step": 18449
    },
    {
      "epoch": 18.47,
      "grad_norm": 22949.47265625,
      "learning_rate": 7.894736842105263e-06,
      "loss": 54.6633,
      "step": 18450
    },
    {
      "epoch": 18.47,
      "grad_norm": 1686.34326171875,
      "learning_rate": 7.889576883384932e-06,
      "loss": 59.8483,
      "step": 18451
    },
    {
      "epoch": 18.47,
      "grad_norm": 19694.20703125,
      "learning_rate": 7.884416924664603e-06,
      "loss": 56.5957,
      "step": 18452
    },
    {
      "epoch": 18.47,
      "grad_norm": 18423.458984375,
      "learning_rate": 7.879256965944273e-06,
      "loss": 52.6106,
      "step": 18453
    },
    {
      "epoch": 18.47,
      "grad_norm": 4784.79443359375,
      "learning_rate": 7.874097007223942e-06,
      "loss": 43.1055,
      "step": 18454
    },
    {
      "epoch": 18.47,
      "grad_norm": 47641.30859375,
      "learning_rate": 7.868937048503612e-06,
      "loss": 59.9321,
      "step": 18455
    },
    {
      "epoch": 18.47,
      "grad_norm": 20031.630859375,
      "learning_rate": 7.863777089783283e-06,
      "loss": 55.9506,
      "step": 18456
    },
    {
      "epoch": 18.48,
      "grad_norm": 5756.6357421875,
      "learning_rate": 7.858617131062952e-06,
      "loss": 50.0,
      "step": 18457
    },
    {
      "epoch": 18.48,
      "grad_norm": 1548.3240966796875,
      "learning_rate": 7.853457172342622e-06,
      "loss": 63.4074,
      "step": 18458
    },
    {
      "epoch": 18.48,
      "grad_norm": 3754.66943359375,
      "learning_rate": 7.848297213622291e-06,
      "loss": 69.5243,
      "step": 18459
    },
    {
      "epoch": 18.48,
      "grad_norm": 23531.73046875,
      "learning_rate": 7.84313725490196e-06,
      "loss": 54.2177,
      "step": 18460
    },
    {
      "epoch": 18.48,
      "grad_norm": 18424.390625,
      "learning_rate": 7.837977296181632e-06,
      "loss": 60.5529,
      "step": 18461
    },
    {
      "epoch": 18.48,
      "grad_norm": 5539.96435546875,
      "learning_rate": 7.832817337461301e-06,
      "loss": 52.0471,
      "step": 18462
    },
    {
      "epoch": 18.48,
      "grad_norm": 13117.1572265625,
      "learning_rate": 7.82765737874097e-06,
      "loss": 59.7871,
      "step": 18463
    },
    {
      "epoch": 18.48,
      "grad_norm": 22622.029296875,
      "learning_rate": 7.82249742002064e-06,
      "loss": 44.2362,
      "step": 18464
    },
    {
      "epoch": 18.48,
      "grad_norm": 4570.77587890625,
      "learning_rate": 7.81733746130031e-06,
      "loss": 60.872,
      "step": 18465
    },
    {
      "epoch": 18.48,
      "grad_norm": 18011.763671875,
      "learning_rate": 7.81217750257998e-06,
      "loss": 57.1804,
      "step": 18466
    },
    {
      "epoch": 18.49,
      "grad_norm": 485.9735412597656,
      "learning_rate": 7.80701754385965e-06,
      "loss": 51.8316,
      "step": 18467
    },
    {
      "epoch": 18.49,
      "grad_norm": 15943.3115234375,
      "learning_rate": 7.80185758513932e-06,
      "loss": 59.0109,
      "step": 18468
    },
    {
      "epoch": 18.49,
      "grad_norm": 26689.318359375,
      "learning_rate": 7.796697626418989e-06,
      "loss": 37.8642,
      "step": 18469
    },
    {
      "epoch": 18.49,
      "grad_norm": 7580.658203125,
      "learning_rate": 7.791537667698658e-06,
      "loss": 66.0218,
      "step": 18470
    },
    {
      "epoch": 18.49,
      "grad_norm": 49710.46875,
      "learning_rate": 7.78637770897833e-06,
      "loss": 31.0917,
      "step": 18471
    },
    {
      "epoch": 18.49,
      "grad_norm": 5759.6162109375,
      "learning_rate": 7.781217750257997e-06,
      "loss": 64.5906,
      "step": 18472
    },
    {
      "epoch": 18.49,
      "grad_norm": 34195.55859375,
      "learning_rate": 7.776057791537668e-06,
      "loss": 66.8552,
      "step": 18473
    },
    {
      "epoch": 18.49,
      "grad_norm": 5664.49853515625,
      "learning_rate": 7.770897832817338e-06,
      "loss": 41.545,
      "step": 18474
    },
    {
      "epoch": 18.49,
      "grad_norm": 4847.41748046875,
      "learning_rate": 7.765737874097007e-06,
      "loss": 60.7747,
      "step": 18475
    },
    {
      "epoch": 18.49,
      "grad_norm": 62083.5390625,
      "learning_rate": 7.760577915376677e-06,
      "loss": 58.3,
      "step": 18476
    },
    {
      "epoch": 18.5,
      "grad_norm": 23726.947265625,
      "learning_rate": 7.755417956656346e-06,
      "loss": 62.6501,
      "step": 18477
    },
    {
      "epoch": 18.5,
      "grad_norm": 13019.2001953125,
      "learning_rate": 7.750257997936017e-06,
      "loss": 31.8179,
      "step": 18478
    },
    {
      "epoch": 18.5,
      "grad_norm": 18064.279296875,
      "learning_rate": 7.745098039215687e-06,
      "loss": 48.7196,
      "step": 18479
    },
    {
      "epoch": 18.5,
      "grad_norm": 23269.55859375,
      "learning_rate": 7.739938080495356e-06,
      "loss": 59.1957,
      "step": 18480
    },
    {
      "epoch": 18.5,
      "grad_norm": 64187.08984375,
      "learning_rate": 7.734778121775026e-06,
      "loss": 51.5192,
      "step": 18481
    },
    {
      "epoch": 18.5,
      "grad_norm": 33337.48046875,
      "learning_rate": 7.729618163054695e-06,
      "loss": 50.3709,
      "step": 18482
    },
    {
      "epoch": 18.5,
      "grad_norm": 10831.470703125,
      "learning_rate": 7.724458204334366e-06,
      "loss": 63.74,
      "step": 18483
    },
    {
      "epoch": 18.5,
      "grad_norm": 3936.040283203125,
      "learning_rate": 7.719298245614036e-06,
      "loss": 53.346,
      "step": 18484
    },
    {
      "epoch": 18.5,
      "grad_norm": 7403.4619140625,
      "learning_rate": 7.714138286893705e-06,
      "loss": 61.2595,
      "step": 18485
    },
    {
      "epoch": 18.5,
      "grad_norm": 14237.9609375,
      "learning_rate": 7.708978328173374e-06,
      "loss": 43.4558,
      "step": 18486
    },
    {
      "epoch": 18.51,
      "grad_norm": 10369.6904296875,
      "learning_rate": 7.703818369453044e-06,
      "loss": 63.2057,
      "step": 18487
    },
    {
      "epoch": 18.51,
      "grad_norm": 14979.48046875,
      "learning_rate": 7.698658410732715e-06,
      "loss": 63.85,
      "step": 18488
    },
    {
      "epoch": 18.51,
      "grad_norm": 4350.89990234375,
      "learning_rate": 7.693498452012384e-06,
      "loss": 64.0973,
      "step": 18489
    },
    {
      "epoch": 18.51,
      "grad_norm": 41553.8125,
      "learning_rate": 7.688338493292054e-06,
      "loss": 56.901,
      "step": 18490
    },
    {
      "epoch": 18.51,
      "grad_norm": 9733.5234375,
      "learning_rate": 7.683178534571723e-06,
      "loss": 55.7953,
      "step": 18491
    },
    {
      "epoch": 18.51,
      "grad_norm": 1927.4842529296875,
      "learning_rate": 7.678018575851394e-06,
      "loss": 59.6191,
      "step": 18492
    },
    {
      "epoch": 18.51,
      "grad_norm": 6770.57568359375,
      "learning_rate": 7.672858617131064e-06,
      "loss": 42.3525,
      "step": 18493
    },
    {
      "epoch": 18.51,
      "grad_norm": 5942.2421875,
      "learning_rate": 7.667698658410732e-06,
      "loss": 36.672,
      "step": 18494
    },
    {
      "epoch": 18.51,
      "grad_norm": 6490.80419921875,
      "learning_rate": 7.662538699690403e-06,
      "loss": 62.4892,
      "step": 18495
    },
    {
      "epoch": 18.51,
      "grad_norm": 59355.3125,
      "learning_rate": 7.657378740970072e-06,
      "loss": 51.0325,
      "step": 18496
    },
    {
      "epoch": 18.52,
      "grad_norm": 5887.2021484375,
      "learning_rate": 7.652218782249743e-06,
      "loss": 56.0334,
      "step": 18497
    },
    {
      "epoch": 18.52,
      "grad_norm": 1707.2984619140625,
      "learning_rate": 7.647058823529413e-06,
      "loss": 59.8107,
      "step": 18498
    },
    {
      "epoch": 18.52,
      "grad_norm": 3618.14404296875,
      "learning_rate": 7.641898864809082e-06,
      "loss": 64.11,
      "step": 18499
    },
    {
      "epoch": 18.52,
      "grad_norm": 4162.11279296875,
      "learning_rate": 7.636738906088752e-06,
      "loss": 60.5651,
      "step": 18500
    },
    {
      "epoch": 18.52,
      "grad_norm": 3595.333740234375,
      "learning_rate": 7.631578947368421e-06,
      "loss": 55.2145,
      "step": 18501
    },
    {
      "epoch": 18.52,
      "grad_norm": 13109.5185546875,
      "learning_rate": 7.626418988648091e-06,
      "loss": 63.9055,
      "step": 18502
    },
    {
      "epoch": 18.52,
      "grad_norm": 6762.28369140625,
      "learning_rate": 7.62125902992776e-06,
      "loss": 56.9557,
      "step": 18503
    },
    {
      "epoch": 18.52,
      "grad_norm": 95251.171875,
      "learning_rate": 7.61609907120743e-06,
      "loss": 62.0121,
      "step": 18504
    },
    {
      "epoch": 18.52,
      "grad_norm": 22065.74609375,
      "learning_rate": 7.6109391124871005e-06,
      "loss": 46.0021,
      "step": 18505
    },
    {
      "epoch": 18.52,
      "grad_norm": 5975.77587890625,
      "learning_rate": 7.605779153766771e-06,
      "loss": 61.4089,
      "step": 18506
    },
    {
      "epoch": 18.53,
      "grad_norm": 150808.875,
      "learning_rate": 7.60061919504644e-06,
      "loss": 43.3612,
      "step": 18507
    },
    {
      "epoch": 18.53,
      "grad_norm": 5918.6787109375,
      "learning_rate": 7.59545923632611e-06,
      "loss": 57.2951,
      "step": 18508
    },
    {
      "epoch": 18.53,
      "grad_norm": 680795.5625,
      "learning_rate": 7.590299277605779e-06,
      "loss": 47.3628,
      "step": 18509
    },
    {
      "epoch": 18.53,
      "grad_norm": 63016.9609375,
      "learning_rate": 7.585139318885449e-06,
      "loss": 57.6358,
      "step": 18510
    },
    {
      "epoch": 18.53,
      "grad_norm": 2256.605712890625,
      "learning_rate": 7.57997936016512e-06,
      "loss": 61.26,
      "step": 18511
    },
    {
      "epoch": 18.53,
      "grad_norm": 17911.771484375,
      "learning_rate": 7.574819401444788e-06,
      "loss": 64.2004,
      "step": 18512
    },
    {
      "epoch": 18.53,
      "grad_norm": 3661.203125,
      "learning_rate": 7.5696594427244586e-06,
      "loss": 53.7962,
      "step": 18513
    },
    {
      "epoch": 18.53,
      "grad_norm": 3465.46826171875,
      "learning_rate": 7.564499484004128e-06,
      "loss": 44.0913,
      "step": 18514
    },
    {
      "epoch": 18.53,
      "grad_norm": 9946.3701171875,
      "learning_rate": 7.559339525283798e-06,
      "loss": 56.0341,
      "step": 18515
    },
    {
      "epoch": 18.53,
      "grad_norm": 4544.61962890625,
      "learning_rate": 7.5541795665634686e-06,
      "loss": 59.2721,
      "step": 18516
    },
    {
      "epoch": 18.54,
      "grad_norm": 5125.05322265625,
      "learning_rate": 7.549019607843137e-06,
      "loss": 66.7613,
      "step": 18517
    },
    {
      "epoch": 18.54,
      "grad_norm": 6949.12353515625,
      "learning_rate": 7.5438596491228074e-06,
      "loss": 50.2121,
      "step": 18518
    },
    {
      "epoch": 18.54,
      "grad_norm": 27455.267578125,
      "learning_rate": 7.538699690402478e-06,
      "loss": 48.1126,
      "step": 18519
    },
    {
      "epoch": 18.54,
      "grad_norm": 3728.604248046875,
      "learning_rate": 7.533539731682147e-06,
      "loss": 53.4718,
      "step": 18520
    },
    {
      "epoch": 18.54,
      "grad_norm": 2072.63525390625,
      "learning_rate": 7.528379772961816e-06,
      "loss": 62.2937,
      "step": 18521
    },
    {
      "epoch": 18.54,
      "grad_norm": 7171.298828125,
      "learning_rate": 7.523219814241486e-06,
      "loss": 60.9147,
      "step": 18522
    },
    {
      "epoch": 18.54,
      "grad_norm": 2875.765869140625,
      "learning_rate": 7.518059855521156e-06,
      "loss": 62.4188,
      "step": 18523
    },
    {
      "epoch": 18.54,
      "grad_norm": 1729.024658203125,
      "learning_rate": 7.512899896800827e-06,
      "loss": 57.3218,
      "step": 18524
    },
    {
      "epoch": 18.54,
      "grad_norm": 20070.61328125,
      "learning_rate": 7.507739938080496e-06,
      "loss": 58.3175,
      "step": 18525
    },
    {
      "epoch": 18.54,
      "grad_norm": 29187.8125,
      "learning_rate": 7.502579979360165e-06,
      "loss": 61.1437,
      "step": 18526
    },
    {
      "epoch": 18.55,
      "grad_norm": 84823.875,
      "learning_rate": 7.497420020639835e-06,
      "loss": 54.135,
      "step": 18527
    },
    {
      "epoch": 18.55,
      "grad_norm": 8791.5400390625,
      "learning_rate": 7.492260061919505e-06,
      "loss": 53.5825,
      "step": 18528
    },
    {
      "epoch": 18.55,
      "grad_norm": 159531.65625,
      "learning_rate": 7.4871001031991755e-06,
      "loss": 33.3528,
      "step": 18529
    },
    {
      "epoch": 18.55,
      "grad_norm": 6787.31298828125,
      "learning_rate": 7.481940144478844e-06,
      "loss": 57.9088,
      "step": 18530
    },
    {
      "epoch": 18.55,
      "grad_norm": 24073.953125,
      "learning_rate": 7.476780185758514e-06,
      "loss": 51.8245,
      "step": 18531
    },
    {
      "epoch": 18.55,
      "grad_norm": 95578.8125,
      "learning_rate": 7.471620227038184e-06,
      "loss": 48.5663,
      "step": 18532
    },
    {
      "epoch": 18.55,
      "grad_norm": 6238.8916015625,
      "learning_rate": 7.466460268317854e-06,
      "loss": 60.3779,
      "step": 18533
    },
    {
      "epoch": 18.55,
      "grad_norm": 4825.55224609375,
      "learning_rate": 7.461300309597524e-06,
      "loss": 43.1149,
      "step": 18534
    },
    {
      "epoch": 18.55,
      "grad_norm": 31307.412109375,
      "learning_rate": 7.456140350877193e-06,
      "loss": 59.0474,
      "step": 18535
    },
    {
      "epoch": 18.55,
      "grad_norm": 1887.4678955078125,
      "learning_rate": 7.450980392156863e-06,
      "loss": 67.4344,
      "step": 18536
    },
    {
      "epoch": 18.56,
      "grad_norm": 14064.12890625,
      "learning_rate": 7.445820433436533e-06,
      "loss": 31.2542,
      "step": 18537
    },
    {
      "epoch": 18.56,
      "grad_norm": 13886.2109375,
      "learning_rate": 7.440660474716203e-06,
      "loss": 45.753,
      "step": 18538
    },
    {
      "epoch": 18.56,
      "grad_norm": 44721.30078125,
      "learning_rate": 7.4355005159958715e-06,
      "loss": 43.2201,
      "step": 18539
    },
    {
      "epoch": 18.56,
      "grad_norm": 5056.8515625,
      "learning_rate": 7.430340557275542e-06,
      "loss": 57.4998,
      "step": 18540
    },
    {
      "epoch": 18.56,
      "grad_norm": 11536.8935546875,
      "learning_rate": 7.425180598555212e-06,
      "loss": 62.3845,
      "step": 18541
    },
    {
      "epoch": 18.56,
      "grad_norm": 25900.9765625,
      "learning_rate": 7.420020639834882e-06,
      "loss": 58.5508,
      "step": 18542
    },
    {
      "epoch": 18.56,
      "grad_norm": 368891.84375,
      "learning_rate": 7.414860681114552e-06,
      "loss": 61.1381,
      "step": 18543
    },
    {
      "epoch": 18.56,
      "grad_norm": 6472.107421875,
      "learning_rate": 7.40970072239422e-06,
      "loss": 54.8437,
      "step": 18544
    },
    {
      "epoch": 18.56,
      "grad_norm": 43308.765625,
      "learning_rate": 7.404540763673891e-06,
      "loss": 24.9473,
      "step": 18545
    },
    {
      "epoch": 18.56,
      "grad_norm": 1202.43310546875,
      "learning_rate": 7.399380804953561e-06,
      "loss": 52.6007,
      "step": 18546
    },
    {
      "epoch": 18.57,
      "grad_norm": 1515.853515625,
      "learning_rate": 7.394220846233231e-06,
      "loss": 58.4927,
      "step": 18547
    },
    {
      "epoch": 18.57,
      "grad_norm": 2316.14453125,
      "learning_rate": 7.3890608875129e-06,
      "loss": 66.834,
      "step": 18548
    },
    {
      "epoch": 18.57,
      "grad_norm": 11643.076171875,
      "learning_rate": 7.38390092879257e-06,
      "loss": 54.7041,
      "step": 18549
    },
    {
      "epoch": 18.57,
      "grad_norm": 3689.52587890625,
      "learning_rate": 7.3787409700722396e-06,
      "loss": 59.2631,
      "step": 18550
    },
    {
      "epoch": 18.57,
      "grad_norm": 2218.1943359375,
      "learning_rate": 7.37358101135191e-06,
      "loss": 62.1219,
      "step": 18551
    },
    {
      "epoch": 18.57,
      "grad_norm": 6863.3408203125,
      "learning_rate": 7.3684210526315784e-06,
      "loss": 62.4583,
      "step": 18552
    },
    {
      "epoch": 18.57,
      "grad_norm": 21079.02734375,
      "learning_rate": 7.363261093911249e-06,
      "loss": 55.2664,
      "step": 18553
    },
    {
      "epoch": 18.57,
      "grad_norm": 18038.01953125,
      "learning_rate": 7.358101135190919e-06,
      "loss": 59.7743,
      "step": 18554
    },
    {
      "epoch": 18.57,
      "grad_norm": 18263.07421875,
      "learning_rate": 7.3529411764705884e-06,
      "loss": 54.0547,
      "step": 18555
    },
    {
      "epoch": 18.57,
      "grad_norm": 14337.677734375,
      "learning_rate": 7.347781217750259e-06,
      "loss": 52.2154,
      "step": 18556
    },
    {
      "epoch": 18.58,
      "grad_norm": 3462.787353515625,
      "learning_rate": 7.342621259029927e-06,
      "loss": 56.5377,
      "step": 18557
    },
    {
      "epoch": 18.58,
      "grad_norm": 27475.681640625,
      "learning_rate": 7.337461300309598e-06,
      "loss": 55.2184,
      "step": 18558
    },
    {
      "epoch": 18.58,
      "grad_norm": 4232.4951171875,
      "learning_rate": 7.332301341589268e-06,
      "loss": 55.3074,
      "step": 18559
    },
    {
      "epoch": 18.58,
      "grad_norm": 1076.5791015625,
      "learning_rate": 7.327141382868938e-06,
      "loss": 61.3924,
      "step": 18560
    },
    {
      "epoch": 18.58,
      "grad_norm": 8506.5849609375,
      "learning_rate": 7.321981424148607e-06,
      "loss": 64.7521,
      "step": 18561
    },
    {
      "epoch": 18.58,
      "grad_norm": 5034.8623046875,
      "learning_rate": 7.316821465428276e-06,
      "loss": 61.1934,
      "step": 18562
    },
    {
      "epoch": 18.58,
      "grad_norm": 1114.9495849609375,
      "learning_rate": 7.3116615067079465e-06,
      "loss": 61.8454,
      "step": 18563
    },
    {
      "epoch": 18.58,
      "grad_norm": 3026.61328125,
      "learning_rate": 7.306501547987617e-06,
      "loss": 51.2238,
      "step": 18564
    },
    {
      "epoch": 18.58,
      "grad_norm": 17009.0234375,
      "learning_rate": 7.301341589267287e-06,
      "loss": 58.74,
      "step": 18565
    },
    {
      "epoch": 18.58,
      "grad_norm": 2678.150390625,
      "learning_rate": 7.296181630546956e-06,
      "loss": 48.4234,
      "step": 18566
    },
    {
      "epoch": 18.59,
      "grad_norm": 2780.211181640625,
      "learning_rate": 7.291021671826626e-06,
      "loss": 59.4869,
      "step": 18567
    },
    {
      "epoch": 18.59,
      "grad_norm": 4475.076171875,
      "learning_rate": 7.285861713106295e-06,
      "loss": 59.8062,
      "step": 18568
    },
    {
      "epoch": 18.59,
      "grad_norm": 10715.1279296875,
      "learning_rate": 7.280701754385966e-06,
      "loss": 63.3037,
      "step": 18569
    },
    {
      "epoch": 18.59,
      "grad_norm": 7083.12890625,
      "learning_rate": 7.275541795665634e-06,
      "loss": 60.6888,
      "step": 18570
    },
    {
      "epoch": 18.59,
      "grad_norm": 78528.8984375,
      "learning_rate": 7.2703818369453045e-06,
      "loss": 55.3747,
      "step": 18571
    },
    {
      "epoch": 18.59,
      "grad_norm": 50207.98828125,
      "learning_rate": 7.265221878224975e-06,
      "loss": 61.3096,
      "step": 18572
    },
    {
      "epoch": 18.59,
      "grad_norm": 1854.275634765625,
      "learning_rate": 7.260061919504644e-06,
      "loss": 58.9928,
      "step": 18573
    },
    {
      "epoch": 18.59,
      "grad_norm": 17208.58984375,
      "learning_rate": 7.2549019607843145e-06,
      "loss": 59.0891,
      "step": 18574
    },
    {
      "epoch": 18.59,
      "grad_norm": 3248.7490234375,
      "learning_rate": 7.249742002063983e-06,
      "loss": 57.5639,
      "step": 18575
    },
    {
      "epoch": 18.59,
      "grad_norm": 24388.232421875,
      "learning_rate": 7.244582043343653e-06,
      "loss": 59.9584,
      "step": 18576
    },
    {
      "epoch": 18.6,
      "grad_norm": 8560.7060546875,
      "learning_rate": 7.239422084623324e-06,
      "loss": 62.978,
      "step": 18577
    },
    {
      "epoch": 18.6,
      "grad_norm": 4743.3330078125,
      "learning_rate": 7.234262125902994e-06,
      "loss": 59.3392,
      "step": 18578
    },
    {
      "epoch": 18.6,
      "grad_norm": 1194.75,
      "learning_rate": 7.2291021671826625e-06,
      "loss": 55.5228,
      "step": 18579
    },
    {
      "epoch": 18.6,
      "grad_norm": 8869.9736328125,
      "learning_rate": 7.223942208462332e-06,
      "loss": 61.6942,
      "step": 18580
    },
    {
      "epoch": 18.6,
      "grad_norm": 5323.87744140625,
      "learning_rate": 7.218782249742002e-06,
      "loss": 59.0953,
      "step": 18581
    },
    {
      "epoch": 18.6,
      "grad_norm": 2487.818115234375,
      "learning_rate": 7.2136222910216725e-06,
      "loss": 62.684,
      "step": 18582
    },
    {
      "epoch": 18.6,
      "grad_norm": 3044.304443359375,
      "learning_rate": 7.208462332301343e-06,
      "loss": 43.386,
      "step": 18583
    },
    {
      "epoch": 18.6,
      "grad_norm": 23595.36328125,
      "learning_rate": 7.203302373581011e-06,
      "loss": 58.9148,
      "step": 18584
    },
    {
      "epoch": 18.6,
      "grad_norm": 5786.96044921875,
      "learning_rate": 7.198142414860682e-06,
      "loss": 56.5229,
      "step": 18585
    },
    {
      "epoch": 18.6,
      "grad_norm": 4048.69580078125,
      "learning_rate": 7.192982456140351e-06,
      "loss": 56.0361,
      "step": 18586
    },
    {
      "epoch": 18.61,
      "grad_norm": 62894.67578125,
      "learning_rate": 7.187822497420021e-06,
      "loss": 41.1549,
      "step": 18587
    },
    {
      "epoch": 18.61,
      "grad_norm": 8112.69580078125,
      "learning_rate": 7.18266253869969e-06,
      "loss": 56.311,
      "step": 18588
    },
    {
      "epoch": 18.61,
      "grad_norm": 1739.616455078125,
      "learning_rate": 7.17750257997936e-06,
      "loss": 61.5417,
      "step": 18589
    },
    {
      "epoch": 18.61,
      "grad_norm": 4687.44873046875,
      "learning_rate": 7.1723426212590306e-06,
      "loss": 55.4745,
      "step": 18590
    },
    {
      "epoch": 18.61,
      "grad_norm": 39860.5859375,
      "learning_rate": 7.1671826625387e-06,
      "loss": 58.3813,
      "step": 18591
    },
    {
      "epoch": 18.61,
      "grad_norm": 2498.5869140625,
      "learning_rate": 7.16202270381837e-06,
      "loss": 57.3809,
      "step": 18592
    },
    {
      "epoch": 18.61,
      "grad_norm": 7345.87890625,
      "learning_rate": 7.156862745098039e-06,
      "loss": 55.4611,
      "step": 18593
    },
    {
      "epoch": 18.61,
      "grad_norm": 4933.69384765625,
      "learning_rate": 7.151702786377709e-06,
      "loss": 54.6236,
      "step": 18594
    },
    {
      "epoch": 18.61,
      "grad_norm": 2155.8701171875,
      "learning_rate": 7.1465428276573794e-06,
      "loss": 42.0879,
      "step": 18595
    },
    {
      "epoch": 18.61,
      "grad_norm": 73349.3828125,
      "learning_rate": 7.14138286893705e-06,
      "loss": 61.7902,
      "step": 18596
    },
    {
      "epoch": 18.62,
      "grad_norm": 14200.8505859375,
      "learning_rate": 7.136222910216718e-06,
      "loss": 50.3654,
      "step": 18597
    },
    {
      "epoch": 18.62,
      "grad_norm": 6617.451171875,
      "learning_rate": 7.131062951496388e-06,
      "loss": 63.5375,
      "step": 18598
    },
    {
      "epoch": 18.62,
      "grad_norm": 3905.843994140625,
      "learning_rate": 7.125902992776058e-06,
      "loss": 53.1876,
      "step": 18599
    },
    {
      "epoch": 18.62,
      "grad_norm": 8218.787109375,
      "learning_rate": 7.120743034055728e-06,
      "loss": 57.2638,
      "step": 18600
    },
    {
      "epoch": 18.62,
      "grad_norm": 6378.263671875,
      "learning_rate": 7.115583075335399e-06,
      "loss": 59.1275,
      "step": 18601
    },
    {
      "epoch": 18.62,
      "grad_norm": 21059.642578125,
      "learning_rate": 7.110423116615067e-06,
      "loss": 55.9395,
      "step": 18602
    },
    {
      "epoch": 18.62,
      "grad_norm": 3436.748779296875,
      "learning_rate": 7.1052631578947375e-06,
      "loss": 44.0284,
      "step": 18603
    },
    {
      "epoch": 18.62,
      "grad_norm": 8849.7314453125,
      "learning_rate": 7.100103199174407e-06,
      "loss": 66.0697,
      "step": 18604
    },
    {
      "epoch": 18.62,
      "grad_norm": 2322.474365234375,
      "learning_rate": 7.094943240454077e-06,
      "loss": 48.0895,
      "step": 18605
    },
    {
      "epoch": 18.62,
      "grad_norm": 2593.061767578125,
      "learning_rate": 7.089783281733746e-06,
      "loss": 47.6843,
      "step": 18606
    },
    {
      "epoch": 18.63,
      "grad_norm": 12888.3974609375,
      "learning_rate": 7.084623323013416e-06,
      "loss": 56.8138,
      "step": 18607
    },
    {
      "epoch": 18.63,
      "grad_norm": 8377.7314453125,
      "learning_rate": 7.079463364293086e-06,
      "loss": 37.7243,
      "step": 18608
    },
    {
      "epoch": 18.63,
      "grad_norm": 3020.3818359375,
      "learning_rate": 7.074303405572756e-06,
      "loss": 59.2176,
      "step": 18609
    },
    {
      "epoch": 18.63,
      "grad_norm": 9526.04296875,
      "learning_rate": 7.069143446852426e-06,
      "loss": 50.8599,
      "step": 18610
    },
    {
      "epoch": 18.63,
      "grad_norm": 12236.001953125,
      "learning_rate": 7.063983488132095e-06,
      "loss": 54.9967,
      "step": 18611
    },
    {
      "epoch": 18.63,
      "grad_norm": 7150.25634765625,
      "learning_rate": 7.058823529411765e-06,
      "loss": 57.3745,
      "step": 18612
    },
    {
      "epoch": 18.63,
      "grad_norm": 2953.1669921875,
      "learning_rate": 7.053663570691435e-06,
      "loss": 53.278,
      "step": 18613
    },
    {
      "epoch": 18.63,
      "grad_norm": 23322.8203125,
      "learning_rate": 7.0485036119711055e-06,
      "loss": 59.0957,
      "step": 18614
    },
    {
      "epoch": 18.63,
      "grad_norm": 28223.236328125,
      "learning_rate": 7.043343653250774e-06,
      "loss": 63.4538,
      "step": 18615
    },
    {
      "epoch": 18.63,
      "grad_norm": 2497.435546875,
      "learning_rate": 7.0381836945304435e-06,
      "loss": 56.6727,
      "step": 18616
    },
    {
      "epoch": 18.64,
      "grad_norm": 22779.953125,
      "learning_rate": 7.033023735810114e-06,
      "loss": 57.338,
      "step": 18617
    },
    {
      "epoch": 18.64,
      "grad_norm": 19861.244140625,
      "learning_rate": 7.027863777089784e-06,
      "loss": 61.0024,
      "step": 18618
    },
    {
      "epoch": 18.64,
      "grad_norm": 5943.15966796875,
      "learning_rate": 7.022703818369454e-06,
      "loss": 40.6949,
      "step": 18619
    },
    {
      "epoch": 18.64,
      "grad_norm": 22269.40625,
      "learning_rate": 7.017543859649123e-06,
      "loss": 54.6887,
      "step": 18620
    },
    {
      "epoch": 18.64,
      "grad_norm": 3147.756103515625,
      "learning_rate": 7.012383900928793e-06,
      "loss": 52.726,
      "step": 18621
    },
    {
      "epoch": 18.64,
      "grad_norm": 7638.50048828125,
      "learning_rate": 7.007223942208463e-06,
      "loss": 51.3983,
      "step": 18622
    },
    {
      "epoch": 18.64,
      "grad_norm": 695.299560546875,
      "learning_rate": 7.002063983488133e-06,
      "loss": 57.4327,
      "step": 18623
    },
    {
      "epoch": 18.64,
      "grad_norm": 2790.6650390625,
      "learning_rate": 6.9969040247678016e-06,
      "loss": 53.9648,
      "step": 18624
    },
    {
      "epoch": 18.64,
      "grad_norm": 10293.6494140625,
      "learning_rate": 6.991744066047472e-06,
      "loss": 59.6099,
      "step": 18625
    },
    {
      "epoch": 18.64,
      "grad_norm": 7513.60888671875,
      "learning_rate": 6.986584107327142e-06,
      "loss": 60.96,
      "step": 18626
    },
    {
      "epoch": 18.65,
      "grad_norm": 5453.98095703125,
      "learning_rate": 6.9814241486068116e-06,
      "loss": 61.7103,
      "step": 18627
    },
    {
      "epoch": 18.65,
      "grad_norm": 1068.9486083984375,
      "learning_rate": 6.97626418988648e-06,
      "loss": 65.9147,
      "step": 18628
    },
    {
      "epoch": 18.65,
      "grad_norm": 1801.4730224609375,
      "learning_rate": 6.9711042311661504e-06,
      "loss": 61.7271,
      "step": 18629
    },
    {
      "epoch": 18.65,
      "grad_norm": 7108.7666015625,
      "learning_rate": 6.965944272445821e-06,
      "loss": 54.6289,
      "step": 18630
    },
    {
      "epoch": 18.65,
      "grad_norm": 24086.490234375,
      "learning_rate": 6.960784313725491e-06,
      "loss": 48.8942,
      "step": 18631
    },
    {
      "epoch": 18.65,
      "grad_norm": 4674.5849609375,
      "learning_rate": 6.9556243550051604e-06,
      "loss": 50.7171,
      "step": 18632
    },
    {
      "epoch": 18.65,
      "grad_norm": 13358.28125,
      "learning_rate": 6.95046439628483e-06,
      "loss": 57.0876,
      "step": 18633
    },
    {
      "epoch": 18.65,
      "grad_norm": 4378.578125,
      "learning_rate": 6.945304437564499e-06,
      "loss": 52.0633,
      "step": 18634
    },
    {
      "epoch": 18.65,
      "grad_norm": 6183.6064453125,
      "learning_rate": 6.94014447884417e-06,
      "loss": 53.5194,
      "step": 18635
    },
    {
      "epoch": 18.65,
      "grad_norm": 8495.7802734375,
      "learning_rate": 6.93498452012384e-06,
      "loss": 64.779,
      "step": 18636
    },
    {
      "epoch": 18.66,
      "grad_norm": 42894.49609375,
      "learning_rate": 6.9298245614035085e-06,
      "loss": 57.9981,
      "step": 18637
    },
    {
      "epoch": 18.66,
      "grad_norm": 31759.83203125,
      "learning_rate": 6.924664602683179e-06,
      "loss": 49.2213,
      "step": 18638
    },
    {
      "epoch": 18.66,
      "grad_norm": 5703.11279296875,
      "learning_rate": 6.919504643962848e-06,
      "loss": 47.6452,
      "step": 18639
    },
    {
      "epoch": 18.66,
      "grad_norm": 12736.677734375,
      "learning_rate": 6.9143446852425185e-06,
      "loss": 63.3658,
      "step": 18640
    },
    {
      "epoch": 18.66,
      "grad_norm": 34682.90625,
      "learning_rate": 6.909184726522189e-06,
      "loss": 63.3993,
      "step": 18641
    },
    {
      "epoch": 18.66,
      "grad_norm": 32999.359375,
      "learning_rate": 6.904024767801857e-06,
      "loss": 60.7487,
      "step": 18642
    },
    {
      "epoch": 18.66,
      "grad_norm": 4365.58203125,
      "learning_rate": 6.898864809081528e-06,
      "loss": 60.1894,
      "step": 18643
    },
    {
      "epoch": 18.66,
      "grad_norm": 14561.2177734375,
      "learning_rate": 6.893704850361198e-06,
      "loss": 66.1164,
      "step": 18644
    },
    {
      "epoch": 18.66,
      "grad_norm": 2659.806884765625,
      "learning_rate": 6.888544891640867e-06,
      "loss": 54.6283,
      "step": 18645
    },
    {
      "epoch": 18.66,
      "grad_norm": 1147.3272705078125,
      "learning_rate": 6.883384932920536e-06,
      "loss": 63.343,
      "step": 18646
    },
    {
      "epoch": 18.67,
      "grad_norm": 17279.966796875,
      "learning_rate": 6.878224974200206e-06,
      "loss": 64.3835,
      "step": 18647
    },
    {
      "epoch": 18.67,
      "grad_norm": 1808.1661376953125,
      "learning_rate": 6.8730650154798765e-06,
      "loss": 65.2927,
      "step": 18648
    },
    {
      "epoch": 18.67,
      "grad_norm": 7476.0126953125,
      "learning_rate": 6.867905056759547e-06,
      "loss": 56.3091,
      "step": 18649
    },
    {
      "epoch": 18.67,
      "grad_norm": 1282.9453125,
      "learning_rate": 6.862745098039216e-06,
      "loss": 56.0558,
      "step": 18650
    },
    {
      "epoch": 18.67,
      "grad_norm": 4791.19384765625,
      "learning_rate": 6.857585139318886e-06,
      "loss": 59.5729,
      "step": 18651
    },
    {
      "epoch": 18.67,
      "grad_norm": 5551.93994140625,
      "learning_rate": 6.852425180598555e-06,
      "loss": 51.8527,
      "step": 18652
    },
    {
      "epoch": 18.67,
      "grad_norm": 4509.34716796875,
      "learning_rate": 6.847265221878225e-06,
      "loss": 65.4809,
      "step": 18653
    },
    {
      "epoch": 18.67,
      "grad_norm": 19289.921875,
      "learning_rate": 6.842105263157896e-06,
      "loss": 58.8212,
      "step": 18654
    },
    {
      "epoch": 18.67,
      "grad_norm": 23172.513671875,
      "learning_rate": 6.836945304437564e-06,
      "loss": 60.1157,
      "step": 18655
    },
    {
      "epoch": 18.67,
      "grad_norm": 6509.02685546875,
      "learning_rate": 6.8317853457172345e-06,
      "loss": 55.4465,
      "step": 18656
    },
    {
      "epoch": 18.68,
      "grad_norm": 6407.5498046875,
      "learning_rate": 6.826625386996904e-06,
      "loss": 43.7387,
      "step": 18657
    },
    {
      "epoch": 18.68,
      "grad_norm": 70068.234375,
      "learning_rate": 6.821465428276574e-06,
      "loss": 46.6893,
      "step": 18658
    },
    {
      "epoch": 18.68,
      "grad_norm": 1792.61181640625,
      "learning_rate": 6.8163054695562445e-06,
      "loss": 64.4256,
      "step": 18659
    },
    {
      "epoch": 18.68,
      "grad_norm": 6301.8134765625,
      "learning_rate": 6.811145510835913e-06,
      "loss": 64.5138,
      "step": 18660
    },
    {
      "epoch": 18.68,
      "grad_norm": 8209.39453125,
      "learning_rate": 6.805985552115583e-06,
      "loss": 51.3564,
      "step": 18661
    },
    {
      "epoch": 18.68,
      "grad_norm": 3236.866943359375,
      "learning_rate": 6.800825593395254e-06,
      "loss": 63.9469,
      "step": 18662
    },
    {
      "epoch": 18.68,
      "grad_norm": 20548.90625,
      "learning_rate": 6.795665634674923e-06,
      "loss": 42.3644,
      "step": 18663
    },
    {
      "epoch": 18.68,
      "grad_norm": 2255.14208984375,
      "learning_rate": 6.790505675954592e-06,
      "loss": 49.9973,
      "step": 18664
    },
    {
      "epoch": 18.68,
      "grad_norm": 23243.529296875,
      "learning_rate": 6.785345717234262e-06,
      "loss": 52.4328,
      "step": 18665
    },
    {
      "epoch": 18.68,
      "grad_norm": 35162.234375,
      "learning_rate": 6.780185758513932e-06,
      "loss": 62.5221,
      "step": 18666
    },
    {
      "epoch": 18.69,
      "grad_norm": 4165.8740234375,
      "learning_rate": 6.7750257997936026e-06,
      "loss": 43.7794,
      "step": 18667
    },
    {
      "epoch": 18.69,
      "grad_norm": 23277.138671875,
      "learning_rate": 6.769865841073272e-06,
      "loss": 57.847,
      "step": 18668
    },
    {
      "epoch": 18.69,
      "grad_norm": 9676.568359375,
      "learning_rate": 6.7647058823529414e-06,
      "loss": 62.316,
      "step": 18669
    },
    {
      "epoch": 18.69,
      "grad_norm": 2086.899169921875,
      "learning_rate": 6.759545923632611e-06,
      "loss": 57.7414,
      "step": 18670
    },
    {
      "epoch": 18.69,
      "grad_norm": 5278.71484375,
      "learning_rate": 6.754385964912281e-06,
      "loss": 57.1806,
      "step": 18671
    },
    {
      "epoch": 18.69,
      "grad_norm": 16285.0576171875,
      "learning_rate": 6.7492260061919514e-06,
      "loss": 53.7051,
      "step": 18672
    },
    {
      "epoch": 18.69,
      "grad_norm": 1413.26904296875,
      "learning_rate": 6.74406604747162e-06,
      "loss": 61.0291,
      "step": 18673
    },
    {
      "epoch": 18.69,
      "grad_norm": 22467.552734375,
      "learning_rate": 6.73890608875129e-06,
      "loss": 54.8779,
      "step": 18674
    },
    {
      "epoch": 18.69,
      "grad_norm": 11409.5166015625,
      "learning_rate": 6.73374613003096e-06,
      "loss": 49.1865,
      "step": 18675
    },
    {
      "epoch": 18.69,
      "grad_norm": 7960.775390625,
      "learning_rate": 6.72858617131063e-06,
      "loss": 63.0932,
      "step": 18676
    },
    {
      "epoch": 18.7,
      "grad_norm": 2377.778076171875,
      "learning_rate": 6.7234262125903e-06,
      "loss": 57.2048,
      "step": 18677
    },
    {
      "epoch": 18.7,
      "grad_norm": 1464.655029296875,
      "learning_rate": 6.718266253869969e-06,
      "loss": 55.7239,
      "step": 18678
    },
    {
      "epoch": 18.7,
      "grad_norm": 20524.021484375,
      "learning_rate": 6.713106295149639e-06,
      "loss": 43.834,
      "step": 18679
    },
    {
      "epoch": 18.7,
      "grad_norm": 2197.540283203125,
      "learning_rate": 6.7079463364293095e-06,
      "loss": 61.313,
      "step": 18680
    },
    {
      "epoch": 18.7,
      "grad_norm": 3713.951904296875,
      "learning_rate": 6.702786377708979e-06,
      "loss": 56.8019,
      "step": 18681
    },
    {
      "epoch": 18.7,
      "grad_norm": 3225.23681640625,
      "learning_rate": 6.6976264189886475e-06,
      "loss": 57.4035,
      "step": 18682
    },
    {
      "epoch": 18.7,
      "grad_norm": 31327.19140625,
      "learning_rate": 6.692466460268318e-06,
      "loss": 36.7772,
      "step": 18683
    },
    {
      "epoch": 18.7,
      "grad_norm": 7401.625,
      "learning_rate": 6.687306501547988e-06,
      "loss": 58.0758,
      "step": 18684
    },
    {
      "epoch": 18.7,
      "grad_norm": 4122.69775390625,
      "learning_rate": 6.682146542827658e-06,
      "loss": 61.0984,
      "step": 18685
    },
    {
      "epoch": 18.7,
      "grad_norm": 28555.001953125,
      "learning_rate": 6.676986584107328e-06,
      "loss": 58.6257,
      "step": 18686
    },
    {
      "epoch": 18.71,
      "grad_norm": 3022.092041015625,
      "learning_rate": 6.671826625386997e-06,
      "loss": 51.0084,
      "step": 18687
    },
    {
      "epoch": 18.71,
      "grad_norm": 668.3029174804688,
      "learning_rate": 6.666666666666667e-06,
      "loss": 60.2675,
      "step": 18688
    },
    {
      "epoch": 18.71,
      "grad_norm": 7576.1982421875,
      "learning_rate": 6.661506707946337e-06,
      "loss": 54.5298,
      "step": 18689
    },
    {
      "epoch": 18.71,
      "grad_norm": 5376.6845703125,
      "learning_rate": 6.656346749226007e-06,
      "loss": 38.4997,
      "step": 18690
    },
    {
      "epoch": 18.71,
      "grad_norm": 12627.728515625,
      "learning_rate": 6.651186790505676e-06,
      "loss": 50.1549,
      "step": 18691
    },
    {
      "epoch": 18.71,
      "grad_norm": 5369.79296875,
      "learning_rate": 6.646026831785346e-06,
      "loss": 45.9366,
      "step": 18692
    },
    {
      "epoch": 18.71,
      "grad_norm": 19085.048828125,
      "learning_rate": 6.6408668730650155e-06,
      "loss": 61.0792,
      "step": 18693
    },
    {
      "epoch": 18.71,
      "grad_norm": 48766.66796875,
      "learning_rate": 6.635706914344686e-06,
      "loss": 30.6367,
      "step": 18694
    },
    {
      "epoch": 18.71,
      "grad_norm": 7558.591796875,
      "learning_rate": 6.630546955624356e-06,
      "loss": 49.9841,
      "step": 18695
    },
    {
      "epoch": 18.71,
      "grad_norm": 6824.42529296875,
      "learning_rate": 6.625386996904025e-06,
      "loss": 46.6653,
      "step": 18696
    },
    {
      "epoch": 18.72,
      "grad_norm": 218660.59375,
      "learning_rate": 6.620227038183695e-06,
      "loss": 48.1882,
      "step": 18697
    },
    {
      "epoch": 18.72,
      "grad_norm": 13140.7685546875,
      "learning_rate": 6.615067079463365e-06,
      "loss": 64.3603,
      "step": 18698
    },
    {
      "epoch": 18.72,
      "grad_norm": 2216.35107421875,
      "learning_rate": 6.609907120743035e-06,
      "loss": 60.7643,
      "step": 18699
    },
    {
      "epoch": 18.72,
      "grad_norm": 17823.033203125,
      "learning_rate": 6.604747162022703e-06,
      "loss": 53.6644,
      "step": 18700
    },
    {
      "epoch": 18.72,
      "grad_norm": 25073.97265625,
      "learning_rate": 6.5995872033023736e-06,
      "loss": 58.9107,
      "step": 18701
    },
    {
      "epoch": 18.72,
      "grad_norm": 17238.53515625,
      "learning_rate": 6.594427244582044e-06,
      "loss": 53.903,
      "step": 18702
    },
    {
      "epoch": 18.72,
      "grad_norm": 3301.88427734375,
      "learning_rate": 6.589267285861714e-06,
      "loss": 56.3119,
      "step": 18703
    },
    {
      "epoch": 18.72,
      "grad_norm": 9829.537109375,
      "learning_rate": 6.584107327141383e-06,
      "loss": 46.5449,
      "step": 18704
    },
    {
      "epoch": 18.72,
      "grad_norm": 9554.357421875,
      "learning_rate": 6.578947368421053e-06,
      "loss": 53.4688,
      "step": 18705
    },
    {
      "epoch": 18.72,
      "grad_norm": 8719.6298828125,
      "learning_rate": 6.5737874097007224e-06,
      "loss": 56.3696,
      "step": 18706
    },
    {
      "epoch": 18.73,
      "grad_norm": 12244.4248046875,
      "learning_rate": 6.568627450980393e-06,
      "loss": 59.9318,
      "step": 18707
    },
    {
      "epoch": 18.73,
      "grad_norm": 33257.79296875,
      "learning_rate": 6.563467492260063e-06,
      "loss": 58.8299,
      "step": 18708
    },
    {
      "epoch": 18.73,
      "grad_norm": 2859.384033203125,
      "learning_rate": 6.558307533539732e-06,
      "loss": 52.4543,
      "step": 18709
    },
    {
      "epoch": 18.73,
      "grad_norm": 3429.97705078125,
      "learning_rate": 6.553147574819402e-06,
      "loss": 56.7252,
      "step": 18710
    },
    {
      "epoch": 18.73,
      "grad_norm": 6267.7705078125,
      "learning_rate": 6.547987616099071e-06,
      "loss": 49.1454,
      "step": 18711
    },
    {
      "epoch": 18.73,
      "grad_norm": 11276.8720703125,
      "learning_rate": 6.542827657378742e-06,
      "loss": 59.8217,
      "step": 18712
    },
    {
      "epoch": 18.73,
      "grad_norm": 2319.260009765625,
      "learning_rate": 6.53766769865841e-06,
      "loss": 53.7167,
      "step": 18713
    },
    {
      "epoch": 18.73,
      "grad_norm": 1116.030517578125,
      "learning_rate": 6.5325077399380805e-06,
      "loss": 48.0357,
      "step": 18714
    },
    {
      "epoch": 18.73,
      "grad_norm": 14417.814453125,
      "learning_rate": 6.527347781217751e-06,
      "loss": 56.0045,
      "step": 18715
    },
    {
      "epoch": 18.73,
      "grad_norm": 21482.595703125,
      "learning_rate": 6.522187822497421e-06,
      "loss": 53.2414,
      "step": 18716
    },
    {
      "epoch": 18.74,
      "grad_norm": 5975.4794921875,
      "learning_rate": 6.5170278637770905e-06,
      "loss": 47.925,
      "step": 18717
    },
    {
      "epoch": 18.74,
      "grad_norm": 8254.8076171875,
      "learning_rate": 6.511867905056759e-06,
      "loss": 57.6636,
      "step": 18718
    },
    {
      "epoch": 18.74,
      "grad_norm": 4079.998779296875,
      "learning_rate": 6.506707946336429e-06,
      "loss": 58.1429,
      "step": 18719
    },
    {
      "epoch": 18.74,
      "grad_norm": 29222.3671875,
      "learning_rate": 6.5015479876161e-06,
      "loss": 47.4311,
      "step": 18720
    },
    {
      "epoch": 18.74,
      "grad_norm": 18565.96875,
      "learning_rate": 6.49638802889577e-06,
      "loss": 45.5987,
      "step": 18721
    },
    {
      "epoch": 18.74,
      "grad_norm": 10407.779296875,
      "learning_rate": 6.4912280701754385e-06,
      "loss": 58.6625,
      "step": 18722
    },
    {
      "epoch": 18.74,
      "grad_norm": 2651.408935546875,
      "learning_rate": 6.486068111455108e-06,
      "loss": 58.3384,
      "step": 18723
    },
    {
      "epoch": 18.74,
      "grad_norm": 191347.828125,
      "learning_rate": 6.480908152734778e-06,
      "loss": 53.8287,
      "step": 18724
    },
    {
      "epoch": 18.74,
      "grad_norm": 9452.0810546875,
      "learning_rate": 6.4757481940144485e-06,
      "loss": 60.5306,
      "step": 18725
    },
    {
      "epoch": 18.74,
      "grad_norm": 12697.783203125,
      "learning_rate": 6.470588235294119e-06,
      "loss": 45.8169,
      "step": 18726
    },
    {
      "epoch": 18.75,
      "grad_norm": 9040.5791015625,
      "learning_rate": 6.465428276573787e-06,
      "loss": 61.4013,
      "step": 18727
    },
    {
      "epoch": 18.75,
      "grad_norm": 5256.9423828125,
      "learning_rate": 6.460268317853458e-06,
      "loss": 68.6701,
      "step": 18728
    },
    {
      "epoch": 18.75,
      "grad_norm": 112212.8984375,
      "learning_rate": 6.455108359133127e-06,
      "loss": 14.7932,
      "step": 18729
    },
    {
      "epoch": 18.75,
      "grad_norm": 6424.619140625,
      "learning_rate": 6.449948400412797e-06,
      "loss": 47.37,
      "step": 18730
    },
    {
      "epoch": 18.75,
      "grad_norm": 5773.1171875,
      "learning_rate": 6.444788441692466e-06,
      "loss": 63.3136,
      "step": 18731
    },
    {
      "epoch": 18.75,
      "grad_norm": 4114.962890625,
      "learning_rate": 6.439628482972136e-06,
      "loss": 61.0203,
      "step": 18732
    },
    {
      "epoch": 18.75,
      "grad_norm": 100079.5546875,
      "learning_rate": 6.4344685242518065e-06,
      "loss": 43.0389,
      "step": 18733
    },
    {
      "epoch": 18.75,
      "grad_norm": 37654.328125,
      "learning_rate": 6.429308565531476e-06,
      "loss": 61.2653,
      "step": 18734
    },
    {
      "epoch": 18.75,
      "grad_norm": 22491.638671875,
      "learning_rate": 6.424148606811146e-06,
      "loss": 55.946,
      "step": 18735
    },
    {
      "epoch": 18.75,
      "grad_norm": 36937.67578125,
      "learning_rate": 6.418988648090815e-06,
      "loss": 61.8741,
      "step": 18736
    },
    {
      "epoch": 18.76,
      "grad_norm": 3110.710693359375,
      "learning_rate": 6.413828689370485e-06,
      "loss": 60.2196,
      "step": 18737
    },
    {
      "epoch": 18.76,
      "grad_norm": 3175.095703125,
      "learning_rate": 6.408668730650155e-06,
      "loss": 54.0289,
      "step": 18738
    },
    {
      "epoch": 18.76,
      "grad_norm": 1933.5634765625,
      "learning_rate": 6.403508771929826e-06,
      "loss": 57.6232,
      "step": 18739
    },
    {
      "epoch": 18.76,
      "grad_norm": 8206.6396484375,
      "learning_rate": 6.398348813209494e-06,
      "loss": 46.8714,
      "step": 18740
    },
    {
      "epoch": 18.76,
      "grad_norm": 49395.14453125,
      "learning_rate": 6.393188854489164e-06,
      "loss": 60.8093,
      "step": 18741
    },
    {
      "epoch": 18.76,
      "grad_norm": 44447.125,
      "learning_rate": 6.388028895768834e-06,
      "loss": 62.1763,
      "step": 18742
    },
    {
      "epoch": 18.76,
      "grad_norm": 4345.931640625,
      "learning_rate": 6.382868937048504e-06,
      "loss": 65.4847,
      "step": 18743
    },
    {
      "epoch": 18.76,
      "grad_norm": 34212.00390625,
      "learning_rate": 6.3777089783281746e-06,
      "loss": 59.1612,
      "step": 18744
    },
    {
      "epoch": 18.76,
      "grad_norm": 148994.53125,
      "learning_rate": 6.372549019607843e-06,
      "loss": 49.043,
      "step": 18745
    },
    {
      "epoch": 18.76,
      "grad_norm": 743.6915893554688,
      "learning_rate": 6.3673890608875134e-06,
      "loss": 57.4662,
      "step": 18746
    },
    {
      "epoch": 18.77,
      "grad_norm": 798.7772827148438,
      "learning_rate": 6.362229102167183e-06,
      "loss": 55.6507,
      "step": 18747
    },
    {
      "epoch": 18.77,
      "grad_norm": 96330.609375,
      "learning_rate": 6.357069143446853e-06,
      "loss": 54.6252,
      "step": 18748
    },
    {
      "epoch": 18.77,
      "grad_norm": 3024.747314453125,
      "learning_rate": 6.351909184726522e-06,
      "loss": 53.8753,
      "step": 18749
    },
    {
      "epoch": 18.77,
      "grad_norm": 2621.4423828125,
      "learning_rate": 6.346749226006192e-06,
      "loss": 41.3863,
      "step": 18750
    },
    {
      "epoch": 18.77,
      "grad_norm": 3457.96875,
      "learning_rate": 6.341589267285862e-06,
      "loss": 62.1328,
      "step": 18751
    },
    {
      "epoch": 18.77,
      "grad_norm": 2135.222412109375,
      "learning_rate": 6.336429308565532e-06,
      "loss": 59.6079,
      "step": 18752
    },
    {
      "epoch": 18.77,
      "grad_norm": 4565.9228515625,
      "learning_rate": 6.331269349845202e-06,
      "loss": 56.9463,
      "step": 18753
    },
    {
      "epoch": 18.77,
      "grad_norm": 2886.0419921875,
      "learning_rate": 6.326109391124871e-06,
      "loss": 55.955,
      "step": 18754
    },
    {
      "epoch": 18.77,
      "grad_norm": 42636.3046875,
      "learning_rate": 6.320949432404541e-06,
      "loss": 64.8574,
      "step": 18755
    },
    {
      "epoch": 18.77,
      "grad_norm": 5355.64453125,
      "learning_rate": 6.315789473684211e-06,
      "loss": 60.2092,
      "step": 18756
    },
    {
      "epoch": 18.78,
      "grad_norm": 24292.576171875,
      "learning_rate": 6.3106295149638815e-06,
      "loss": 57.4911,
      "step": 18757
    },
    {
      "epoch": 18.78,
      "grad_norm": 5605.923828125,
      "learning_rate": 6.30546955624355e-06,
      "loss": 53.859,
      "step": 18758
    },
    {
      "epoch": 18.78,
      "grad_norm": 1945.066162109375,
      "learning_rate": 6.3003095975232195e-06,
      "loss": 52.5114,
      "step": 18759
    },
    {
      "epoch": 18.78,
      "grad_norm": 2611.244140625,
      "learning_rate": 6.29514963880289e-06,
      "loss": 62.5991,
      "step": 18760
    },
    {
      "epoch": 18.78,
      "grad_norm": 2189.74462890625,
      "learning_rate": 6.28998968008256e-06,
      "loss": 64.5141,
      "step": 18761
    },
    {
      "epoch": 18.78,
      "grad_norm": 22477.078125,
      "learning_rate": 6.28482972136223e-06,
      "loss": 59.7109,
      "step": 18762
    },
    {
      "epoch": 18.78,
      "grad_norm": 16246.9140625,
      "learning_rate": 6.279669762641899e-06,
      "loss": 48.9176,
      "step": 18763
    },
    {
      "epoch": 18.78,
      "grad_norm": 3712.01220703125,
      "learning_rate": 6.274509803921569e-06,
      "loss": 62.4592,
      "step": 18764
    },
    {
      "epoch": 18.78,
      "grad_norm": 93327.875,
      "learning_rate": 6.269349845201239e-06,
      "loss": 52.4047,
      "step": 18765
    },
    {
      "epoch": 18.78,
      "grad_norm": 69897.3984375,
      "learning_rate": 6.264189886480909e-06,
      "loss": 63.4594,
      "step": 18766
    },
    {
      "epoch": 18.79,
      "grad_norm": 118654.984375,
      "learning_rate": 6.2590299277605775e-06,
      "loss": 55.7756,
      "step": 18767
    },
    {
      "epoch": 18.79,
      "grad_norm": 7148.24365234375,
      "learning_rate": 6.253869969040248e-06,
      "loss": 51.4183,
      "step": 18768
    },
    {
      "epoch": 18.79,
      "grad_norm": 17758.34765625,
      "learning_rate": 6.248710010319918e-06,
      "loss": 56.2533,
      "step": 18769
    },
    {
      "epoch": 18.79,
      "grad_norm": 551.2589111328125,
      "learning_rate": 6.2435500515995875e-06,
      "loss": 60.9787,
      "step": 18770
    },
    {
      "epoch": 18.79,
      "grad_norm": 7621.45751953125,
      "learning_rate": 6.238390092879257e-06,
      "loss": 54.0977,
      "step": 18771
    },
    {
      "epoch": 18.79,
      "grad_norm": 4605.77734375,
      "learning_rate": 6.233230134158927e-06,
      "loss": 44.8799,
      "step": 18772
    },
    {
      "epoch": 18.79,
      "grad_norm": 4045.69482421875,
      "learning_rate": 6.228070175438597e-06,
      "loss": 52.4629,
      "step": 18773
    },
    {
      "epoch": 18.79,
      "grad_norm": 3074.5400390625,
      "learning_rate": 6.222910216718267e-06,
      "loss": 54.8792,
      "step": 18774
    },
    {
      "epoch": 18.79,
      "grad_norm": 15707.939453125,
      "learning_rate": 6.217750257997936e-06,
      "loss": 48.8571,
      "step": 18775
    },
    {
      "epoch": 18.79,
      "grad_norm": 20204.865234375,
      "learning_rate": 6.212590299277606e-06,
      "loss": 40.2481,
      "step": 18776
    },
    {
      "epoch": 18.8,
      "grad_norm": 7038.03076171875,
      "learning_rate": 6.207430340557275e-06,
      "loss": 45.4213,
      "step": 18777
    },
    {
      "epoch": 18.8,
      "grad_norm": 16637.943359375,
      "learning_rate": 6.2022703818369455e-06,
      "loss": 57.5325,
      "step": 18778
    },
    {
      "epoch": 18.8,
      "grad_norm": 6664.07421875,
      "learning_rate": 6.197110423116615e-06,
      "loss": 61.2135,
      "step": 18779
    },
    {
      "epoch": 18.8,
      "grad_norm": 15458.7373046875,
      "learning_rate": 6.191950464396285e-06,
      "loss": 58.1241,
      "step": 18780
    },
    {
      "epoch": 18.8,
      "grad_norm": 61788.02734375,
      "learning_rate": 6.1867905056759556e-06,
      "loss": 66.5181,
      "step": 18781
    },
    {
      "epoch": 18.8,
      "grad_norm": 6034.7890625,
      "learning_rate": 6.181630546955625e-06,
      "loss": 54.4749,
      "step": 18782
    },
    {
      "epoch": 18.8,
      "grad_norm": 3070.44482421875,
      "learning_rate": 6.1764705882352944e-06,
      "loss": 58.8782,
      "step": 18783
    },
    {
      "epoch": 18.8,
      "grad_norm": 5887.0693359375,
      "learning_rate": 6.171310629514964e-06,
      "loss": 57.3425,
      "step": 18784
    },
    {
      "epoch": 18.8,
      "grad_norm": 28772.78125,
      "learning_rate": 6.166150670794634e-06,
      "loss": 38.8743,
      "step": 18785
    },
    {
      "epoch": 18.8,
      "grad_norm": 21706.44140625,
      "learning_rate": 6.160990712074304e-06,
      "loss": 65.1063,
      "step": 18786
    },
    {
      "epoch": 18.81,
      "grad_norm": 12225.0546875,
      "learning_rate": 6.155830753353974e-06,
      "loss": 52.512,
      "step": 18787
    },
    {
      "epoch": 18.81,
      "grad_norm": 7031.73681640625,
      "learning_rate": 6.150670794633643e-06,
      "loss": 47.5915,
      "step": 18788
    },
    {
      "epoch": 18.81,
      "grad_norm": 24569.296875,
      "learning_rate": 6.145510835913313e-06,
      "loss": 30.629,
      "step": 18789
    },
    {
      "epoch": 18.81,
      "grad_norm": 1340.2862548828125,
      "learning_rate": 6.140350877192982e-06,
      "loss": 52.5289,
      "step": 18790
    },
    {
      "epoch": 18.81,
      "grad_norm": 3910.156982421875,
      "learning_rate": 6.1351909184726525e-06,
      "loss": 61.638,
      "step": 18791
    },
    {
      "epoch": 18.81,
      "grad_norm": 53327.31640625,
      "learning_rate": 6.130030959752323e-06,
      "loss": 56.657,
      "step": 18792
    },
    {
      "epoch": 18.81,
      "grad_norm": 20701.658203125,
      "learning_rate": 6.124871001031992e-06,
      "loss": 53.05,
      "step": 18793
    },
    {
      "epoch": 18.81,
      "grad_norm": 21187.84765625,
      "learning_rate": 6.119711042311662e-06,
      "loss": 40.0915,
      "step": 18794
    },
    {
      "epoch": 18.81,
      "grad_norm": 17109.556640625,
      "learning_rate": 6.114551083591331e-06,
      "loss": 55.729,
      "step": 18795
    },
    {
      "epoch": 18.81,
      "grad_norm": 8224.759765625,
      "learning_rate": 6.109391124871001e-06,
      "loss": 60.8964,
      "step": 18796
    },
    {
      "epoch": 18.82,
      "grad_norm": 889.4946899414062,
      "learning_rate": 6.104231166150671e-06,
      "loss": 61.6069,
      "step": 18797
    },
    {
      "epoch": 18.82,
      "grad_norm": 23987.6015625,
      "learning_rate": 6.099071207430341e-06,
      "loss": 66.3878,
      "step": 18798
    },
    {
      "epoch": 18.82,
      "grad_norm": 10538.373046875,
      "learning_rate": 6.0939112487100105e-06,
      "loss": 58.3224,
      "step": 18799
    },
    {
      "epoch": 18.82,
      "grad_norm": 1197.044677734375,
      "learning_rate": 6.088751289989681e-06,
      "loss": 61.3672,
      "step": 18800
    },
    {
      "epoch": 18.82,
      "grad_norm": 79094.1328125,
      "learning_rate": 6.08359133126935e-06,
      "loss": 58.1929,
      "step": 18801
    },
    {
      "epoch": 18.82,
      "grad_norm": 74509.40625,
      "learning_rate": 6.07843137254902e-06,
      "loss": 54.4136,
      "step": 18802
    },
    {
      "epoch": 18.82,
      "grad_norm": 2198.71826171875,
      "learning_rate": 6.07327141382869e-06,
      "loss": 59.0694,
      "step": 18803
    },
    {
      "epoch": 18.82,
      "grad_norm": 7735.7958984375,
      "learning_rate": 6.068111455108359e-06,
      "loss": 52.6685,
      "step": 18804
    },
    {
      "epoch": 18.82,
      "grad_norm": 2890.65673828125,
      "learning_rate": 6.06295149638803e-06,
      "loss": 60.3164,
      "step": 18805
    },
    {
      "epoch": 18.82,
      "grad_norm": 18555.462890625,
      "learning_rate": 6.057791537667699e-06,
      "loss": 56.2613,
      "step": 18806
    },
    {
      "epoch": 18.83,
      "grad_norm": 1238.430908203125,
      "learning_rate": 6.0526315789473685e-06,
      "loss": 55.8379,
      "step": 18807
    },
    {
      "epoch": 18.83,
      "grad_norm": 1800.2469482421875,
      "learning_rate": 6.047471620227038e-06,
      "loss": 60.0625,
      "step": 18808
    },
    {
      "epoch": 18.83,
      "grad_norm": 7305.78857421875,
      "learning_rate": 6.042311661506708e-06,
      "loss": 59.714,
      "step": 18809
    },
    {
      "epoch": 18.83,
      "grad_norm": 15798.53515625,
      "learning_rate": 6.0371517027863785e-06,
      "loss": 41.9975,
      "step": 18810
    },
    {
      "epoch": 18.83,
      "grad_norm": 3587.75244140625,
      "learning_rate": 6.031991744066048e-06,
      "loss": 60.3363,
      "step": 18811
    },
    {
      "epoch": 18.83,
      "grad_norm": 5371.46533203125,
      "learning_rate": 6.026831785345717e-06,
      "loss": 60.2911,
      "step": 18812
    },
    {
      "epoch": 18.83,
      "grad_norm": 1592.3603515625,
      "learning_rate": 6.021671826625387e-06,
      "loss": 52.964,
      "step": 18813
    },
    {
      "epoch": 18.83,
      "grad_norm": 2748.7724609375,
      "learning_rate": 6.016511867905057e-06,
      "loss": 55.29,
      "step": 18814
    },
    {
      "epoch": 18.83,
      "grad_norm": 12414.029296875,
      "learning_rate": 6.0113519091847265e-06,
      "loss": 56.5004,
      "step": 18815
    },
    {
      "epoch": 18.83,
      "grad_norm": 2762.345947265625,
      "learning_rate": 6.006191950464397e-06,
      "loss": 52.8887,
      "step": 18816
    },
    {
      "epoch": 18.84,
      "grad_norm": 5400.67626953125,
      "learning_rate": 6.001031991744066e-06,
      "loss": 55.327,
      "step": 18817
    },
    {
      "epoch": 18.84,
      "grad_norm": 40583.09375,
      "learning_rate": 5.995872033023736e-06,
      "loss": 26.9062,
      "step": 18818
    },
    {
      "epoch": 18.84,
      "grad_norm": 24533.517578125,
      "learning_rate": 5.990712074303406e-06,
      "loss": 38.7469,
      "step": 18819
    },
    {
      "epoch": 18.84,
      "grad_norm": 39782.9765625,
      "learning_rate": 5.985552115583075e-06,
      "loss": 37.128,
      "step": 18820
    },
    {
      "epoch": 18.84,
      "grad_norm": 3188.10009765625,
      "learning_rate": 5.980392156862746e-06,
      "loss": 61.2382,
      "step": 18821
    },
    {
      "epoch": 18.84,
      "grad_norm": 3646.708251953125,
      "learning_rate": 5.975232198142415e-06,
      "loss": 68.5252,
      "step": 18822
    },
    {
      "epoch": 18.84,
      "grad_norm": 9501.1298828125,
      "learning_rate": 5.970072239422085e-06,
      "loss": 40.4481,
      "step": 18823
    },
    {
      "epoch": 18.84,
      "grad_norm": 10838.4052734375,
      "learning_rate": 5.964912280701755e-06,
      "loss": 56.2578,
      "step": 18824
    },
    {
      "epoch": 18.84,
      "grad_norm": 1408.254150390625,
      "learning_rate": 5.959752321981424e-06,
      "loss": 66.7029,
      "step": 18825
    },
    {
      "epoch": 18.84,
      "grad_norm": 6225.8994140625,
      "learning_rate": 5.954592363261094e-06,
      "loss": 63.2375,
      "step": 18826
    },
    {
      "epoch": 18.85,
      "grad_norm": 7522.708984375,
      "learning_rate": 5.949432404540764e-06,
      "loss": 55.2584,
      "step": 18827
    },
    {
      "epoch": 18.85,
      "grad_norm": 5163.83837890625,
      "learning_rate": 5.9442724458204335e-06,
      "loss": 59.0875,
      "step": 18828
    },
    {
      "epoch": 18.85,
      "grad_norm": 2087.12060546875,
      "learning_rate": 5.939112487100104e-06,
      "loss": 65.5634,
      "step": 18829
    },
    {
      "epoch": 18.85,
      "grad_norm": 3936.0888671875,
      "learning_rate": 5.933952528379773e-06,
      "loss": 60.5379,
      "step": 18830
    },
    {
      "epoch": 18.85,
      "grad_norm": 18717.806640625,
      "learning_rate": 5.928792569659443e-06,
      "loss": 60.7501,
      "step": 18831
    },
    {
      "epoch": 18.85,
      "grad_norm": 3945.40673828125,
      "learning_rate": 5.923632610939113e-06,
      "loss": 64.1734,
      "step": 18832
    },
    {
      "epoch": 18.85,
      "grad_norm": 2894.117431640625,
      "learning_rate": 5.918472652218782e-06,
      "loss": 58.8449,
      "step": 18833
    },
    {
      "epoch": 18.85,
      "grad_norm": 2785.7158203125,
      "learning_rate": 5.913312693498453e-06,
      "loss": 62.5384,
      "step": 18834
    },
    {
      "epoch": 18.85,
      "grad_norm": 41537.16796875,
      "learning_rate": 5.908152734778122e-06,
      "loss": 64.2747,
      "step": 18835
    },
    {
      "epoch": 18.85,
      "grad_norm": 889.2376098632812,
      "learning_rate": 5.9029927760577915e-06,
      "loss": 63.0702,
      "step": 18836
    },
    {
      "epoch": 18.86,
      "grad_norm": 7736.17431640625,
      "learning_rate": 5.897832817337461e-06,
      "loss": 55.5185,
      "step": 18837
    },
    {
      "epoch": 18.86,
      "grad_norm": 2624.59521484375,
      "learning_rate": 5.892672858617131e-06,
      "loss": 53.8232,
      "step": 18838
    },
    {
      "epoch": 18.86,
      "grad_norm": 57511.15625,
      "learning_rate": 5.8875128998968015e-06,
      "loss": 57.0679,
      "step": 18839
    },
    {
      "epoch": 18.86,
      "grad_norm": 14360.4580078125,
      "learning_rate": 5.882352941176471e-06,
      "loss": 65.6568,
      "step": 18840
    },
    {
      "epoch": 18.86,
      "grad_norm": 3632.672119140625,
      "learning_rate": 5.877192982456141e-06,
      "loss": 61.2929,
      "step": 18841
    },
    {
      "epoch": 18.86,
      "grad_norm": 3411.8037109375,
      "learning_rate": 5.872033023735811e-06,
      "loss": 61.9995,
      "step": 18842
    },
    {
      "epoch": 18.86,
      "grad_norm": 49923.44921875,
      "learning_rate": 5.86687306501548e-06,
      "loss": 58.9663,
      "step": 18843
    },
    {
      "epoch": 18.86,
      "grad_norm": 13407.6796875,
      "learning_rate": 5.8617131062951495e-06,
      "loss": 64.2317,
      "step": 18844
    },
    {
      "epoch": 18.86,
      "grad_norm": 22162.29296875,
      "learning_rate": 5.85655314757482e-06,
      "loss": 60.8159,
      "step": 18845
    },
    {
      "epoch": 18.86,
      "grad_norm": 16383.580078125,
      "learning_rate": 5.851393188854489e-06,
      "loss": 59.6836,
      "step": 18846
    },
    {
      "epoch": 18.87,
      "grad_norm": 6042.3779296875,
      "learning_rate": 5.8462332301341595e-06,
      "loss": 58.9245,
      "step": 18847
    },
    {
      "epoch": 18.87,
      "grad_norm": 4157.28662109375,
      "learning_rate": 5.841073271413829e-06,
      "loss": 65.799,
      "step": 18848
    },
    {
      "epoch": 18.87,
      "grad_norm": 8448.689453125,
      "learning_rate": 5.835913312693498e-06,
      "loss": 59.2292,
      "step": 18849
    },
    {
      "epoch": 18.87,
      "grad_norm": 4129.36474609375,
      "learning_rate": 5.830753353973169e-06,
      "loss": 53.3747,
      "step": 18850
    },
    {
      "epoch": 18.87,
      "grad_norm": 1227.2760009765625,
      "learning_rate": 5.825593395252838e-06,
      "loss": 62.3658,
      "step": 18851
    },
    {
      "epoch": 18.87,
      "grad_norm": 5490.89501953125,
      "learning_rate": 5.820433436532508e-06,
      "loss": 52.0323,
      "step": 18852
    },
    {
      "epoch": 18.87,
      "grad_norm": 8934.5,
      "learning_rate": 5.815273477812178e-06,
      "loss": 50.4528,
      "step": 18853
    },
    {
      "epoch": 18.87,
      "grad_norm": 10712.0791015625,
      "learning_rate": 5.810113519091847e-06,
      "loss": 55.6988,
      "step": 18854
    },
    {
      "epoch": 18.87,
      "grad_norm": 24112.400390625,
      "learning_rate": 5.804953560371517e-06,
      "loss": 51.4987,
      "step": 18855
    },
    {
      "epoch": 18.87,
      "grad_norm": 5306.71484375,
      "learning_rate": 5.799793601651187e-06,
      "loss": 60.1755,
      "step": 18856
    },
    {
      "epoch": 18.88,
      "grad_norm": 7919.4658203125,
      "learning_rate": 5.794633642930857e-06,
      "loss": 59.6272,
      "step": 18857
    },
    {
      "epoch": 18.88,
      "grad_norm": 2380.975341796875,
      "learning_rate": 5.789473684210527e-06,
      "loss": 58.8076,
      "step": 18858
    },
    {
      "epoch": 18.88,
      "grad_norm": 5947.0322265625,
      "learning_rate": 5.784313725490197e-06,
      "loss": 61.4949,
      "step": 18859
    },
    {
      "epoch": 18.88,
      "grad_norm": 5803.60546875,
      "learning_rate": 5.779153766769866e-06,
      "loss": 59.4516,
      "step": 18860
    },
    {
      "epoch": 18.88,
      "grad_norm": 11202.837890625,
      "learning_rate": 5.773993808049536e-06,
      "loss": 47.7024,
      "step": 18861
    },
    {
      "epoch": 18.88,
      "grad_norm": 22989.783203125,
      "learning_rate": 5.768833849329205e-06,
      "loss": 54.7959,
      "step": 18862
    },
    {
      "epoch": 18.88,
      "grad_norm": 4316.86865234375,
      "learning_rate": 5.7636738906088756e-06,
      "loss": 60.3602,
      "step": 18863
    },
    {
      "epoch": 18.88,
      "grad_norm": 6304.63671875,
      "learning_rate": 5.758513931888545e-06,
      "loss": 43.9563,
      "step": 18864
    },
    {
      "epoch": 18.88,
      "grad_norm": 8651.8662109375,
      "learning_rate": 5.753353973168215e-06,
      "loss": 67.1434,
      "step": 18865
    },
    {
      "epoch": 18.88,
      "grad_norm": 7124.833984375,
      "learning_rate": 5.748194014447885e-06,
      "loss": 41.4671,
      "step": 18866
    },
    {
      "epoch": 18.89,
      "grad_norm": 42585.9921875,
      "learning_rate": 5.743034055727554e-06,
      "loss": 59.1628,
      "step": 18867
    },
    {
      "epoch": 18.89,
      "grad_norm": 22521.4609375,
      "learning_rate": 5.7378740970072245e-06,
      "loss": 43.7836,
      "step": 18868
    },
    {
      "epoch": 18.89,
      "grad_norm": 3193.69775390625,
      "learning_rate": 5.732714138286894e-06,
      "loss": 55.0072,
      "step": 18869
    },
    {
      "epoch": 18.89,
      "grad_norm": 3448.2412109375,
      "learning_rate": 5.727554179566564e-06,
      "loss": 51.3191,
      "step": 18870
    },
    {
      "epoch": 18.89,
      "grad_norm": 32735.447265625,
      "learning_rate": 5.722394220846234e-06,
      "loss": 43.067,
      "step": 18871
    },
    {
      "epoch": 18.89,
      "grad_norm": 31522.23828125,
      "learning_rate": 5.717234262125903e-06,
      "loss": 43.3359,
      "step": 18872
    },
    {
      "epoch": 18.89,
      "grad_norm": 11742.478515625,
      "learning_rate": 5.7120743034055725e-06,
      "loss": 63.7542,
      "step": 18873
    },
    {
      "epoch": 18.89,
      "grad_norm": 15282.7080078125,
      "learning_rate": 5.706914344685243e-06,
      "loss": 60.4324,
      "step": 18874
    },
    {
      "epoch": 18.89,
      "grad_norm": 44698.58203125,
      "learning_rate": 5.701754385964912e-06,
      "loss": 38.7053,
      "step": 18875
    },
    {
      "epoch": 18.89,
      "grad_norm": 1384.5587158203125,
      "learning_rate": 5.6965944272445825e-06,
      "loss": 61.3956,
      "step": 18876
    },
    {
      "epoch": 18.9,
      "grad_norm": 1334.8038330078125,
      "learning_rate": 5.691434468524253e-06,
      "loss": 59.4389,
      "step": 18877
    },
    {
      "epoch": 18.9,
      "grad_norm": 7046.63232421875,
      "learning_rate": 5.686274509803921e-06,
      "loss": 52.7352,
      "step": 18878
    },
    {
      "epoch": 18.9,
      "grad_norm": 11583.453125,
      "learning_rate": 5.681114551083592e-06,
      "loss": 55.2957,
      "step": 18879
    },
    {
      "epoch": 18.9,
      "grad_norm": 15527.09765625,
      "learning_rate": 5.675954592363261e-06,
      "loss": 61.3455,
      "step": 18880
    },
    {
      "epoch": 18.9,
      "grad_norm": 14193.9462890625,
      "learning_rate": 5.670794633642931e-06,
      "loss": 39.7061,
      "step": 18881
    },
    {
      "epoch": 18.9,
      "grad_norm": 5281.302734375,
      "learning_rate": 5.665634674922601e-06,
      "loss": 57.2786,
      "step": 18882
    },
    {
      "epoch": 18.9,
      "grad_norm": 8713.638671875,
      "learning_rate": 5.660474716202271e-06,
      "loss": 63.5313,
      "step": 18883
    },
    {
      "epoch": 18.9,
      "grad_norm": 75622.5078125,
      "learning_rate": 5.6553147574819405e-06,
      "loss": 57.2706,
      "step": 18884
    },
    {
      "epoch": 18.9,
      "grad_norm": 4985.4404296875,
      "learning_rate": 5.65015479876161e-06,
      "loss": 68.6593,
      "step": 18885
    },
    {
      "epoch": 18.9,
      "grad_norm": 15356.6591796875,
      "learning_rate": 5.64499484004128e-06,
      "loss": 52.7364,
      "step": 18886
    },
    {
      "epoch": 18.91,
      "grad_norm": 5622.9384765625,
      "learning_rate": 5.63983488132095e-06,
      "loss": 64.1362,
      "step": 18887
    },
    {
      "epoch": 18.91,
      "grad_norm": 123893.546875,
      "learning_rate": 5.63467492260062e-06,
      "loss": 53.0617,
      "step": 18888
    },
    {
      "epoch": 18.91,
      "grad_norm": 1121.1632080078125,
      "learning_rate": 5.629514963880289e-06,
      "loss": 64.4024,
      "step": 18889
    },
    {
      "epoch": 18.91,
      "grad_norm": 3290.7177734375,
      "learning_rate": 5.624355005159959e-06,
      "loss": 58.1301,
      "step": 18890
    },
    {
      "epoch": 18.91,
      "grad_norm": 5610.484375,
      "learning_rate": 5.619195046439628e-06,
      "loss": 62.1424,
      "step": 18891
    },
    {
      "epoch": 18.91,
      "grad_norm": 2320.861572265625,
      "learning_rate": 5.6140350877192985e-06,
      "loss": 63.6016,
      "step": 18892
    },
    {
      "epoch": 18.91,
      "grad_norm": 22134.439453125,
      "learning_rate": 5.608875128998968e-06,
      "loss": 59.8658,
      "step": 18893
    },
    {
      "epoch": 18.91,
      "grad_norm": 17365.865234375,
      "learning_rate": 5.603715170278638e-06,
      "loss": 46.3966,
      "step": 18894
    },
    {
      "epoch": 18.91,
      "grad_norm": 7746.703125,
      "learning_rate": 5.5985552115583085e-06,
      "loss": 52.9431,
      "step": 18895
    },
    {
      "epoch": 18.91,
      "grad_norm": 4003.2744140625,
      "learning_rate": 5.593395252837977e-06,
      "loss": 55.4922,
      "step": 18896
    },
    {
      "epoch": 18.92,
      "grad_norm": 2501.639404296875,
      "learning_rate": 5.588235294117647e-06,
      "loss": 64.7772,
      "step": 18897
    },
    {
      "epoch": 18.92,
      "grad_norm": 14624.6884765625,
      "learning_rate": 5.583075335397317e-06,
      "loss": 64.6795,
      "step": 18898
    },
    {
      "epoch": 18.92,
      "grad_norm": 13860.3251953125,
      "learning_rate": 5.577915376676987e-06,
      "loss": 54.2312,
      "step": 18899
    },
    {
      "epoch": 18.92,
      "grad_norm": 6019.501953125,
      "learning_rate": 5.5727554179566566e-06,
      "loss": 52.6928,
      "step": 18900
    },
    {
      "epoch": 18.92,
      "grad_norm": 9049.310546875,
      "learning_rate": 5.567595459236327e-06,
      "loss": 54.2205,
      "step": 18901
    },
    {
      "epoch": 18.92,
      "grad_norm": 10717.537109375,
      "learning_rate": 5.562435500515996e-06,
      "loss": 64.4924,
      "step": 18902
    },
    {
      "epoch": 18.92,
      "grad_norm": 2700.010009765625,
      "learning_rate": 5.557275541795666e-06,
      "loss": 60.5089,
      "step": 18903
    },
    {
      "epoch": 18.92,
      "grad_norm": 6485.9697265625,
      "learning_rate": 5.552115583075335e-06,
      "loss": 46.7037,
      "step": 18904
    },
    {
      "epoch": 18.92,
      "grad_norm": 16253.501953125,
      "learning_rate": 5.5469556243550054e-06,
      "loss": 40.2194,
      "step": 18905
    },
    {
      "epoch": 18.92,
      "grad_norm": 7936.587890625,
      "learning_rate": 5.541795665634676e-06,
      "loss": 66.0873,
      "step": 18906
    },
    {
      "epoch": 18.93,
      "grad_norm": 16519.43359375,
      "learning_rate": 5.536635706914345e-06,
      "loss": 55.9105,
      "step": 18907
    },
    {
      "epoch": 18.93,
      "grad_norm": 4389.19384765625,
      "learning_rate": 5.531475748194015e-06,
      "loss": 61.1496,
      "step": 18908
    },
    {
      "epoch": 18.93,
      "grad_norm": 2912.90673828125,
      "learning_rate": 5.526315789473684e-06,
      "loss": 57.7227,
      "step": 18909
    },
    {
      "epoch": 18.93,
      "grad_norm": 9781.484375,
      "learning_rate": 5.521155830753354e-06,
      "loss": 57.9432,
      "step": 18910
    },
    {
      "epoch": 18.93,
      "grad_norm": 5203.6328125,
      "learning_rate": 5.515995872033024e-06,
      "loss": 61.3053,
      "step": 18911
    },
    {
      "epoch": 18.93,
      "grad_norm": 20781.9453125,
      "learning_rate": 5.510835913312694e-06,
      "loss": 62.2628,
      "step": 18912
    },
    {
      "epoch": 18.93,
      "grad_norm": 60089.515625,
      "learning_rate": 5.5056759545923635e-06,
      "loss": 55.9338,
      "step": 18913
    },
    {
      "epoch": 18.93,
      "grad_norm": 8039.0849609375,
      "learning_rate": 5.500515995872033e-06,
      "loss": 50.4694,
      "step": 18914
    },
    {
      "epoch": 18.93,
      "grad_norm": 9307.826171875,
      "learning_rate": 5.495356037151703e-06,
      "loss": 55.7422,
      "step": 18915
    },
    {
      "epoch": 18.93,
      "grad_norm": 8820.5908203125,
      "learning_rate": 5.490196078431373e-06,
      "loss": 58.8262,
      "step": 18916
    },
    {
      "epoch": 18.94,
      "grad_norm": 29202.8046875,
      "learning_rate": 5.485036119711043e-06,
      "loss": 57.6084,
      "step": 18917
    },
    {
      "epoch": 18.94,
      "grad_norm": 64781.2265625,
      "learning_rate": 5.479876160990712e-06,
      "loss": 50.8592,
      "step": 18918
    },
    {
      "epoch": 18.94,
      "grad_norm": 17895.697265625,
      "learning_rate": 5.474716202270383e-06,
      "loss": 49.383,
      "step": 18919
    },
    {
      "epoch": 18.94,
      "grad_norm": 8472.66796875,
      "learning_rate": 5.469556243550051e-06,
      "loss": 48.2772,
      "step": 18920
    },
    {
      "epoch": 18.94,
      "grad_norm": 2239.4794921875,
      "learning_rate": 5.4643962848297215e-06,
      "loss": 53.4512,
      "step": 18921
    },
    {
      "epoch": 18.94,
      "grad_norm": 56160.578125,
      "learning_rate": 5.459236326109391e-06,
      "loss": 51.5863,
      "step": 18922
    },
    {
      "epoch": 18.94,
      "grad_norm": 7617.17333984375,
      "learning_rate": 5.454076367389061e-06,
      "loss": 53.3991,
      "step": 18923
    },
    {
      "epoch": 18.94,
      "grad_norm": 4952.73876953125,
      "learning_rate": 5.4489164086687315e-06,
      "loss": 50.9765,
      "step": 18924
    },
    {
      "epoch": 18.94,
      "grad_norm": 6113.52685546875,
      "learning_rate": 5.443756449948401e-06,
      "loss": 59.7158,
      "step": 18925
    },
    {
      "epoch": 18.94,
      "grad_norm": 6740.9189453125,
      "learning_rate": 5.43859649122807e-06,
      "loss": 61.8406,
      "step": 18926
    },
    {
      "epoch": 18.95,
      "grad_norm": 3213.60595703125,
      "learning_rate": 5.43343653250774e-06,
      "loss": 63.791,
      "step": 18927
    },
    {
      "epoch": 18.95,
      "grad_norm": 4775.201171875,
      "learning_rate": 5.42827657378741e-06,
      "loss": 62.6651,
      "step": 18928
    },
    {
      "epoch": 18.95,
      "grad_norm": 2039.5626220703125,
      "learning_rate": 5.4231166150670795e-06,
      "loss": 59.3281,
      "step": 18929
    },
    {
      "epoch": 18.95,
      "grad_norm": 3812.742431640625,
      "learning_rate": 5.41795665634675e-06,
      "loss": 44.1571,
      "step": 18930
    },
    {
      "epoch": 18.95,
      "grad_norm": 1926.15673828125,
      "learning_rate": 5.412796697626419e-06,
      "loss": 58.2174,
      "step": 18931
    },
    {
      "epoch": 18.95,
      "grad_norm": 2356.15087890625,
      "learning_rate": 5.407636738906089e-06,
      "loss": 62.6868,
      "step": 18932
    },
    {
      "epoch": 18.95,
      "grad_norm": 9263.3447265625,
      "learning_rate": 5.402476780185759e-06,
      "loss": 50.7794,
      "step": 18933
    },
    {
      "epoch": 18.95,
      "grad_norm": 1297.9566650390625,
      "learning_rate": 5.397316821465428e-06,
      "loss": 65.3397,
      "step": 18934
    },
    {
      "epoch": 18.95,
      "grad_norm": 5155.37548828125,
      "learning_rate": 5.392156862745099e-06,
      "loss": 58.0022,
      "step": 18935
    },
    {
      "epoch": 18.95,
      "grad_norm": 7992.50341796875,
      "learning_rate": 5.386996904024768e-06,
      "loss": 43.2861,
      "step": 18936
    },
    {
      "epoch": 18.96,
      "grad_norm": 5883.35009765625,
      "learning_rate": 5.381836945304438e-06,
      "loss": 59.1677,
      "step": 18937
    },
    {
      "epoch": 18.96,
      "grad_norm": 30240.986328125,
      "learning_rate": 5.376676986584107e-06,
      "loss": 53.4585,
      "step": 18938
    },
    {
      "epoch": 18.96,
      "grad_norm": 132173.34375,
      "learning_rate": 5.371517027863777e-06,
      "loss": 57.041,
      "step": 18939
    },
    {
      "epoch": 18.96,
      "grad_norm": 15002.19140625,
      "learning_rate": 5.366357069143447e-06,
      "loss": 50.7885,
      "step": 18940
    },
    {
      "epoch": 18.96,
      "grad_norm": 3973.32373046875,
      "learning_rate": 5.361197110423117e-06,
      "loss": 53.2672,
      "step": 18941
    },
    {
      "epoch": 18.96,
      "grad_norm": 5426.0068359375,
      "learning_rate": 5.3560371517027864e-06,
      "loss": 63.8267,
      "step": 18942
    },
    {
      "epoch": 18.96,
      "grad_norm": 7721.74853515625,
      "learning_rate": 5.350877192982457e-06,
      "loss": 51.532,
      "step": 18943
    },
    {
      "epoch": 18.96,
      "grad_norm": 1099.3685302734375,
      "learning_rate": 5.345717234262126e-06,
      "loss": 66.7709,
      "step": 18944
    },
    {
      "epoch": 18.96,
      "grad_norm": 6084.24169921875,
      "learning_rate": 5.340557275541796e-06,
      "loss": 64.0663,
      "step": 18945
    },
    {
      "epoch": 18.96,
      "grad_norm": 4657.70166015625,
      "learning_rate": 5.335397316821466e-06,
      "loss": 56.17,
      "step": 18946
    },
    {
      "epoch": 18.97,
      "grad_norm": 3130.14990234375,
      "learning_rate": 5.330237358101135e-06,
      "loss": 57.2076,
      "step": 18947
    },
    {
      "epoch": 18.97,
      "grad_norm": 3758.186279296875,
      "learning_rate": 5.325077399380806e-06,
      "loss": 56.5648,
      "step": 18948
    },
    {
      "epoch": 18.97,
      "grad_norm": 3304.87890625,
      "learning_rate": 5.319917440660475e-06,
      "loss": 62.3343,
      "step": 18949
    },
    {
      "epoch": 18.97,
      "grad_norm": 41831.19921875,
      "learning_rate": 5.3147574819401445e-06,
      "loss": 59.1914,
      "step": 18950
    },
    {
      "epoch": 18.97,
      "grad_norm": 14895.0078125,
      "learning_rate": 5.309597523219814e-06,
      "loss": 36.0259,
      "step": 18951
    },
    {
      "epoch": 18.97,
      "grad_norm": 3404.97119140625,
      "learning_rate": 5.304437564499484e-06,
      "loss": 64.4232,
      "step": 18952
    },
    {
      "epoch": 18.97,
      "grad_norm": 3866.479248046875,
      "learning_rate": 5.2992776057791545e-06,
      "loss": 57.6781,
      "step": 18953
    },
    {
      "epoch": 18.97,
      "grad_norm": 3555.428466796875,
      "learning_rate": 5.294117647058824e-06,
      "loss": 56.9951,
      "step": 18954
    },
    {
      "epoch": 18.97,
      "grad_norm": 32107.923828125,
      "learning_rate": 5.288957688338494e-06,
      "loss": 48.7656,
      "step": 18955
    },
    {
      "epoch": 18.97,
      "grad_norm": 5026.3359375,
      "learning_rate": 5.283797729618163e-06,
      "loss": 49.7009,
      "step": 18956
    },
    {
      "epoch": 18.98,
      "grad_norm": 8365.3359375,
      "learning_rate": 5.278637770897833e-06,
      "loss": 49.4519,
      "step": 18957
    },
    {
      "epoch": 18.98,
      "grad_norm": 8071.21923828125,
      "learning_rate": 5.2734778121775025e-06,
      "loss": 64.0045,
      "step": 18958
    },
    {
      "epoch": 18.98,
      "grad_norm": 1453.2998046875,
      "learning_rate": 5.268317853457173e-06,
      "loss": 61.0123,
      "step": 18959
    },
    {
      "epoch": 18.98,
      "grad_norm": 7030.5439453125,
      "learning_rate": 5.263157894736842e-06,
      "loss": 62.8354,
      "step": 18960
    },
    {
      "epoch": 18.98,
      "grad_norm": 24778.169921875,
      "learning_rate": 5.2579979360165125e-06,
      "loss": 36.6345,
      "step": 18961
    },
    {
      "epoch": 18.98,
      "grad_norm": 26272.69921875,
      "learning_rate": 5.252837977296182e-06,
      "loss": 60.2845,
      "step": 18962
    },
    {
      "epoch": 18.98,
      "grad_norm": 11124.521484375,
      "learning_rate": 5.247678018575851e-06,
      "loss": 25.5168,
      "step": 18963
    },
    {
      "epoch": 18.98,
      "grad_norm": 44794.4453125,
      "learning_rate": 5.242518059855522e-06,
      "loss": 59.2986,
      "step": 18964
    },
    {
      "epoch": 18.98,
      "grad_norm": 4060.727294921875,
      "learning_rate": 5.237358101135191e-06,
      "loss": 68.0255,
      "step": 18965
    },
    {
      "epoch": 18.98,
      "grad_norm": 47795.65234375,
      "learning_rate": 5.232198142414861e-06,
      "loss": 30.3305,
      "step": 18966
    },
    {
      "epoch": 18.99,
      "grad_norm": 3918.39111328125,
      "learning_rate": 5.227038183694531e-06,
      "loss": 59.9178,
      "step": 18967
    },
    {
      "epoch": 18.99,
      "grad_norm": 9625.5595703125,
      "learning_rate": 5.2218782249742e-06,
      "loss": 64.8741,
      "step": 18968
    },
    {
      "epoch": 18.99,
      "grad_norm": 7598.056640625,
      "learning_rate": 5.21671826625387e-06,
      "loss": 64.103,
      "step": 18969
    },
    {
      "epoch": 18.99,
      "grad_norm": 23582.01171875,
      "learning_rate": 5.21155830753354e-06,
      "loss": 37.6152,
      "step": 18970
    },
    {
      "epoch": 18.99,
      "grad_norm": 3032.0888671875,
      "learning_rate": 5.206398348813209e-06,
      "loss": 60.0416,
      "step": 18971
    },
    {
      "epoch": 18.99,
      "grad_norm": 11707.4375,
      "learning_rate": 5.20123839009288e-06,
      "loss": 60.7093,
      "step": 18972
    },
    {
      "epoch": 18.99,
      "grad_norm": 9329.1162109375,
      "learning_rate": 5.196078431372549e-06,
      "loss": 61.5492,
      "step": 18973
    },
    {
      "epoch": 18.99,
      "grad_norm": 3182.450927734375,
      "learning_rate": 5.1909184726522186e-06,
      "loss": 60.354,
      "step": 18974
    },
    {
      "epoch": 18.99,
      "grad_norm": 2312.690673828125,
      "learning_rate": 5.185758513931889e-06,
      "loss": 61.5045,
      "step": 18975
    },
    {
      "epoch": 18.99,
      "grad_norm": 1037.2249755859375,
      "learning_rate": 5.180598555211558e-06,
      "loss": 62.0475,
      "step": 18976
    },
    {
      "epoch": 19.0,
      "grad_norm": 21782.314453125,
      "learning_rate": 5.1754385964912286e-06,
      "loss": 63.3788,
      "step": 18977
    },
    {
      "epoch": 19.0,
      "grad_norm": 153267.15625,
      "learning_rate": 5.170278637770898e-06,
      "loss": 57.899,
      "step": 18978
    },
    {
      "epoch": 19.0,
      "grad_norm": 29875.916015625,
      "learning_rate": 5.165118679050568e-06,
      "loss": 58.7762,
      "step": 18979
    },
    {
      "epoch": 19.0,
      "grad_norm": 34993.3515625,
      "learning_rate": 5.159958720330237e-06,
      "loss": 55.018,
      "step": 18980
    },
    {
      "epoch": 19.0,
      "grad_norm": 13091.6533203125,
      "learning_rate": 5.154798761609907e-06,
      "loss": 58.7657,
      "step": 18981
    },
    {
      "epoch": 19.0,
      "grad_norm": 8783.16015625,
      "learning_rate": 5.1496388028895774e-06,
      "loss": 50.0966,
      "step": 18982
    },
    {
      "epoch": 19.0,
      "grad_norm": 28285.033203125,
      "learning_rate": 5.144478844169247e-06,
      "loss": 50.5687,
      "step": 18983
    },
    {
      "epoch": 19.0,
      "grad_norm": 2987.727783203125,
      "learning_rate": 5.139318885448917e-06,
      "loss": 65.2755,
      "step": 18984
    },
    {
      "epoch": 19.0,
      "grad_norm": 44282.7890625,
      "learning_rate": 5.134158926728587e-06,
      "loss": 45.7042,
      "step": 18985
    },
    {
      "epoch": 19.01,
      "grad_norm": 895.8269653320312,
      "learning_rate": 5.128998968008256e-06,
      "loss": 60.9156,
      "step": 18986
    },
    {
      "epoch": 19.01,
      "grad_norm": 1071.8955078125,
      "learning_rate": 5.1238390092879255e-06,
      "loss": 62.7368,
      "step": 18987
    },
    {
      "epoch": 19.01,
      "grad_norm": 13077.3154296875,
      "learning_rate": 5.118679050567596e-06,
      "loss": 62.8938,
      "step": 18988
    },
    {
      "epoch": 19.01,
      "grad_norm": 5526.35693359375,
      "learning_rate": 5.113519091847265e-06,
      "loss": 63.246,
      "step": 18989
    },
    {
      "epoch": 19.01,
      "grad_norm": 20436.92578125,
      "learning_rate": 5.1083591331269355e-06,
      "loss": 46.9287,
      "step": 18990
    },
    {
      "epoch": 19.01,
      "grad_norm": 2816.791015625,
      "learning_rate": 5.103199174406605e-06,
      "loss": 59.5328,
      "step": 18991
    },
    {
      "epoch": 19.01,
      "grad_norm": 61993.80859375,
      "learning_rate": 5.098039215686274e-06,
      "loss": 43.7007,
      "step": 18992
    },
    {
      "epoch": 19.01,
      "grad_norm": 7671.7099609375,
      "learning_rate": 5.092879256965945e-06,
      "loss": 65.6934,
      "step": 18993
    },
    {
      "epoch": 19.01,
      "grad_norm": 59355.7578125,
      "learning_rate": 5.087719298245614e-06,
      "loss": 28.4862,
      "step": 18994
    },
    {
      "epoch": 19.01,
      "grad_norm": 5176.609375,
      "learning_rate": 5.082559339525284e-06,
      "loss": 43.949,
      "step": 18995
    },
    {
      "epoch": 19.02,
      "grad_norm": 1687.3531494140625,
      "learning_rate": 5.077399380804954e-06,
      "loss": 64.0005,
      "step": 18996
    },
    {
      "epoch": 19.02,
      "grad_norm": 11811.8466796875,
      "learning_rate": 5.072239422084624e-06,
      "loss": 56.7281,
      "step": 18997
    },
    {
      "epoch": 19.02,
      "grad_norm": 22868.904296875,
      "learning_rate": 5.067079463364293e-06,
      "loss": 43.5936,
      "step": 18998
    },
    {
      "epoch": 19.02,
      "grad_norm": 26610.6796875,
      "learning_rate": 5.061919504643963e-06,
      "loss": 51.418,
      "step": 18999
    },
    {
      "epoch": 19.02,
      "grad_norm": 2029.4849853515625,
      "learning_rate": 5.056759545923633e-06,
      "loss": 52.6687,
      "step": 19000
    },
    {
      "epoch": 19.02,
      "grad_norm": 2674.482421875,
      "learning_rate": 5.051599587203303e-06,
      "loss": 57.4696,
      "step": 19001
    },
    {
      "epoch": 19.02,
      "grad_norm": 6052.771484375,
      "learning_rate": 5.046439628482973e-06,
      "loss": 56.9578,
      "step": 19002
    },
    {
      "epoch": 19.02,
      "grad_norm": 27037.2890625,
      "learning_rate": 5.041279669762642e-06,
      "loss": 53.6533,
      "step": 19003
    },
    {
      "epoch": 19.02,
      "grad_norm": 11456.6552734375,
      "learning_rate": 5.036119711042312e-06,
      "loss": 40.4917,
      "step": 19004
    },
    {
      "epoch": 19.02,
      "grad_norm": 154146.96875,
      "learning_rate": 5.030959752321981e-06,
      "loss": 41.9528,
      "step": 19005
    },
    {
      "epoch": 19.03,
      "grad_norm": 69732.46875,
      "learning_rate": 5.0257997936016515e-06,
      "loss": 63.9514,
      "step": 19006
    },
    {
      "epoch": 19.03,
      "grad_norm": 7119.330078125,
      "learning_rate": 5.020639834881321e-06,
      "loss": 61.5506,
      "step": 19007
    },
    {
      "epoch": 19.03,
      "grad_norm": 3666.645263671875,
      "learning_rate": 5.015479876160991e-06,
      "loss": 62.2756,
      "step": 19008
    },
    {
      "epoch": 19.03,
      "grad_norm": 29528.607421875,
      "learning_rate": 5.010319917440661e-06,
      "loss": 56.1469,
      "step": 19009
    },
    {
      "epoch": 19.03,
      "grad_norm": 9571.2294921875,
      "learning_rate": 5.00515995872033e-06,
      "loss": 54.006,
      "step": 19010
    },
    {
      "epoch": 19.03,
      "grad_norm": 5250.93408203125,
      "learning_rate": 5e-06,
      "loss": 54.5314,
      "step": 19011
    },
    {
      "epoch": 19.03,
      "grad_norm": 4830.125,
      "learning_rate": 4.99484004127967e-06,
      "loss": 55.4303,
      "step": 19012
    },
    {
      "epoch": 19.03,
      "grad_norm": 5299.9365234375,
      "learning_rate": 4.98968008255934e-06,
      "loss": 52.0185,
      "step": 19013
    },
    {
      "epoch": 19.03,
      "grad_norm": 48714.21875,
      "learning_rate": 4.9845201238390096e-06,
      "loss": 60.0621,
      "step": 19014
    },
    {
      "epoch": 19.03,
      "grad_norm": 54617.73828125,
      "learning_rate": 4.979360165118679e-06,
      "loss": 60.7003,
      "step": 19015
    },
    {
      "epoch": 19.04,
      "grad_norm": 5076.8466796875,
      "learning_rate": 4.9742002063983484e-06,
      "loss": 50.624,
      "step": 19016
    },
    {
      "epoch": 19.04,
      "grad_norm": 6664.64697265625,
      "learning_rate": 4.969040247678019e-06,
      "loss": 63.0691,
      "step": 19017
    },
    {
      "epoch": 19.04,
      "grad_norm": 1920.3131103515625,
      "learning_rate": 4.963880288957688e-06,
      "loss": 65.2978,
      "step": 19018
    },
    {
      "epoch": 19.04,
      "grad_norm": 906.1834106445312,
      "learning_rate": 4.9587203302373584e-06,
      "loss": 60.119,
      "step": 19019
    },
    {
      "epoch": 19.04,
      "grad_norm": 1945.2869873046875,
      "learning_rate": 4.953560371517029e-06,
      "loss": 65.2997,
      "step": 19020
    },
    {
      "epoch": 19.04,
      "grad_norm": 6665.57470703125,
      "learning_rate": 4.948400412796698e-06,
      "loss": 52.8429,
      "step": 19021
    },
    {
      "epoch": 19.04,
      "grad_norm": 5782.34326171875,
      "learning_rate": 4.943240454076368e-06,
      "loss": 46.3467,
      "step": 19022
    },
    {
      "epoch": 19.04,
      "grad_norm": 1724.39794921875,
      "learning_rate": 4.938080495356037e-06,
      "loss": 61.2211,
      "step": 19023
    },
    {
      "epoch": 19.04,
      "grad_norm": 9005.884765625,
      "learning_rate": 4.932920536635707e-06,
      "loss": 54.4376,
      "step": 19024
    },
    {
      "epoch": 19.04,
      "grad_norm": 2766.78759765625,
      "learning_rate": 4.927760577915377e-06,
      "loss": 52.9258,
      "step": 19025
    },
    {
      "epoch": 19.05,
      "grad_norm": 17285.369140625,
      "learning_rate": 4.922600619195047e-06,
      "loss": 58.9454,
      "step": 19026
    },
    {
      "epoch": 19.05,
      "grad_norm": 3510.38720703125,
      "learning_rate": 4.9174406604747165e-06,
      "loss": 53.4469,
      "step": 19027
    },
    {
      "epoch": 19.05,
      "grad_norm": 1456986.375,
      "learning_rate": 4.912280701754386e-06,
      "loss": 63.3694,
      "step": 19028
    },
    {
      "epoch": 19.05,
      "grad_norm": 15289.4052734375,
      "learning_rate": 4.907120743034056e-06,
      "loss": 48.1617,
      "step": 19029
    },
    {
      "epoch": 19.05,
      "grad_norm": 4972.6474609375,
      "learning_rate": 4.901960784313726e-06,
      "loss": 34.3956,
      "step": 19030
    },
    {
      "epoch": 19.05,
      "grad_norm": 21467.25,
      "learning_rate": 4.896800825593396e-06,
      "loss": 60.1476,
      "step": 19031
    },
    {
      "epoch": 19.05,
      "grad_norm": 3066.273193359375,
      "learning_rate": 4.891640866873065e-06,
      "loss": 61.2051,
      "step": 19032
    },
    {
      "epoch": 19.05,
      "grad_norm": 72368.125,
      "learning_rate": 4.886480908152735e-06,
      "loss": 64.5226,
      "step": 19033
    },
    {
      "epoch": 19.05,
      "grad_norm": 15693.5126953125,
      "learning_rate": 4.881320949432404e-06,
      "loss": 61.0846,
      "step": 19034
    },
    {
      "epoch": 19.05,
      "grad_norm": 6348.7783203125,
      "learning_rate": 4.8761609907120745e-06,
      "loss": 53.8489,
      "step": 19035
    },
    {
      "epoch": 19.06,
      "grad_norm": 34116.2109375,
      "learning_rate": 4.871001031991744e-06,
      "loss": 36.5966,
      "step": 19036
    },
    {
      "epoch": 19.06,
      "grad_norm": 8658.0625,
      "learning_rate": 4.865841073271414e-06,
      "loss": 61.2543,
      "step": 19037
    },
    {
      "epoch": 19.06,
      "grad_norm": 1821.86865234375,
      "learning_rate": 4.8606811145510845e-06,
      "loss": 67.7452,
      "step": 19038
    },
    {
      "epoch": 19.06,
      "grad_norm": 6141.20068359375,
      "learning_rate": 4.855521155830754e-06,
      "loss": 60.1246,
      "step": 19039
    },
    {
      "epoch": 19.06,
      "grad_norm": 2278.2080078125,
      "learning_rate": 4.850361197110423e-06,
      "loss": 51.1289,
      "step": 19040
    },
    {
      "epoch": 19.06,
      "grad_norm": 6642.875,
      "learning_rate": 4.845201238390093e-06,
      "loss": 52.1893,
      "step": 19041
    },
    {
      "epoch": 19.06,
      "grad_norm": 5576.25732421875,
      "learning_rate": 4.840041279669763e-06,
      "loss": 56.2872,
      "step": 19042
    },
    {
      "epoch": 19.06,
      "grad_norm": 21949.240234375,
      "learning_rate": 4.8348813209494325e-06,
      "loss": 54.4412,
      "step": 19043
    },
    {
      "epoch": 19.06,
      "grad_norm": 99772.8046875,
      "learning_rate": 4.829721362229103e-06,
      "loss": 50.5451,
      "step": 19044
    },
    {
      "epoch": 19.06,
      "grad_norm": 24282.828125,
      "learning_rate": 4.824561403508772e-06,
      "loss": 58.7343,
      "step": 19045
    },
    {
      "epoch": 19.07,
      "grad_norm": 9187.322265625,
      "learning_rate": 4.819401444788442e-06,
      "loss": 56.8517,
      "step": 19046
    },
    {
      "epoch": 19.07,
      "grad_norm": 66358.8125,
      "learning_rate": 4.814241486068111e-06,
      "loss": 48.9277,
      "step": 19047
    },
    {
      "epoch": 19.07,
      "grad_norm": 6063.9541015625,
      "learning_rate": 4.809081527347781e-06,
      "loss": 50.9681,
      "step": 19048
    },
    {
      "epoch": 19.07,
      "grad_norm": 2347.5419921875,
      "learning_rate": 4.803921568627452e-06,
      "loss": 59.9599,
      "step": 19049
    },
    {
      "epoch": 19.07,
      "grad_norm": 52376.27734375,
      "learning_rate": 4.798761609907121e-06,
      "loss": 61.4235,
      "step": 19050
    },
    {
      "epoch": 19.07,
      "grad_norm": 55562.0625,
      "learning_rate": 4.7936016511867906e-06,
      "loss": 60.4084,
      "step": 19051
    },
    {
      "epoch": 19.07,
      "grad_norm": 50990.0390625,
      "learning_rate": 4.78844169246646e-06,
      "loss": 47.1372,
      "step": 19052
    },
    {
      "epoch": 19.07,
      "grad_norm": 4653.2783203125,
      "learning_rate": 4.78328173374613e-06,
      "loss": 56.3634,
      "step": 19053
    },
    {
      "epoch": 19.07,
      "grad_norm": 7277.208984375,
      "learning_rate": 4.7781217750258e-06,
      "loss": 50.844,
      "step": 19054
    },
    {
      "epoch": 19.07,
      "grad_norm": 2407.732666015625,
      "learning_rate": 4.77296181630547e-06,
      "loss": 63.2874,
      "step": 19055
    },
    {
      "epoch": 19.08,
      "grad_norm": 5247.10546875,
      "learning_rate": 4.7678018575851394e-06,
      "loss": 61.3474,
      "step": 19056
    },
    {
      "epoch": 19.08,
      "grad_norm": 1098.185791015625,
      "learning_rate": 4.76264189886481e-06,
      "loss": 59.435,
      "step": 19057
    },
    {
      "epoch": 19.08,
      "grad_norm": 1396.395263671875,
      "learning_rate": 4.757481940144479e-06,
      "loss": 62.2189,
      "step": 19058
    },
    {
      "epoch": 19.08,
      "grad_norm": 2567.698974609375,
      "learning_rate": 4.752321981424149e-06,
      "loss": 60.2196,
      "step": 19059
    },
    {
      "epoch": 19.08,
      "grad_norm": 3616.55615234375,
      "learning_rate": 4.747162022703819e-06,
      "loss": 65.0962,
      "step": 19060
    },
    {
      "epoch": 19.08,
      "grad_norm": 32910.94921875,
      "learning_rate": 4.742002063983488e-06,
      "loss": 58.4614,
      "step": 19061
    },
    {
      "epoch": 19.08,
      "grad_norm": 6623.609375,
      "learning_rate": 4.736842105263159e-06,
      "loss": 64.0205,
      "step": 19062
    },
    {
      "epoch": 19.08,
      "grad_norm": 35040.3359375,
      "learning_rate": 4.731682146542828e-06,
      "loss": 55.1765,
      "step": 19063
    },
    {
      "epoch": 19.08,
      "grad_norm": 3288.58740234375,
      "learning_rate": 4.7265221878224975e-06,
      "loss": 63.5041,
      "step": 19064
    },
    {
      "epoch": 19.08,
      "grad_norm": 2551.69091796875,
      "learning_rate": 4.721362229102167e-06,
      "loss": 63.4401,
      "step": 19065
    },
    {
      "epoch": 19.09,
      "grad_norm": 4761.7734375,
      "learning_rate": 4.716202270381837e-06,
      "loss": 56.1973,
      "step": 19066
    },
    {
      "epoch": 19.09,
      "grad_norm": 33095.35546875,
      "learning_rate": 4.7110423116615075e-06,
      "loss": 57.955,
      "step": 19067
    },
    {
      "epoch": 19.09,
      "grad_norm": 13641.4658203125,
      "learning_rate": 4.705882352941177e-06,
      "loss": 51.0869,
      "step": 19068
    },
    {
      "epoch": 19.09,
      "grad_norm": 9826.2529296875,
      "learning_rate": 4.700722394220846e-06,
      "loss": 57.4632,
      "step": 19069
    },
    {
      "epoch": 19.09,
      "grad_norm": 1002.1292114257812,
      "learning_rate": 4.695562435500516e-06,
      "loss": 59.1876,
      "step": 19070
    },
    {
      "epoch": 19.09,
      "grad_norm": 872.854736328125,
      "learning_rate": 4.690402476780186e-06,
      "loss": 62.2782,
      "step": 19071
    },
    {
      "epoch": 19.09,
      "grad_norm": 8749.6591796875,
      "learning_rate": 4.6852425180598555e-06,
      "loss": 52.1897,
      "step": 19072
    },
    {
      "epoch": 19.09,
      "grad_norm": 10835.810546875,
      "learning_rate": 4.680082559339526e-06,
      "loss": 56.4773,
      "step": 19073
    },
    {
      "epoch": 19.09,
      "grad_norm": 2687.880126953125,
      "learning_rate": 4.674922600619195e-06,
      "loss": 61.8548,
      "step": 19074
    },
    {
      "epoch": 19.09,
      "grad_norm": 9444.5185546875,
      "learning_rate": 4.669762641898865e-06,
      "loss": 59.4259,
      "step": 19075
    },
    {
      "epoch": 19.1,
      "grad_norm": 21093.859375,
      "learning_rate": 4.664602683178535e-06,
      "loss": 61.4533,
      "step": 19076
    },
    {
      "epoch": 19.1,
      "grad_norm": 29679.50390625,
      "learning_rate": 4.659442724458204e-06,
      "loss": 42.6565,
      "step": 19077
    },
    {
      "epoch": 19.1,
      "grad_norm": 7737.20556640625,
      "learning_rate": 4.654282765737875e-06,
      "loss": 56.925,
      "step": 19078
    },
    {
      "epoch": 19.1,
      "grad_norm": 59109.75,
      "learning_rate": 4.649122807017544e-06,
      "loss": 63.3605,
      "step": 19079
    },
    {
      "epoch": 19.1,
      "grad_norm": 1164.27294921875,
      "learning_rate": 4.643962848297214e-06,
      "loss": 45.4809,
      "step": 19080
    },
    {
      "epoch": 19.1,
      "grad_norm": 4192.7431640625,
      "learning_rate": 4.638802889576884e-06,
      "loss": 55.6478,
      "step": 19081
    },
    {
      "epoch": 19.1,
      "grad_norm": 714.375244140625,
      "learning_rate": 4.633642930856553e-06,
      "loss": 56.3598,
      "step": 19082
    },
    {
      "epoch": 19.1,
      "grad_norm": 1687.55615234375,
      "learning_rate": 4.628482972136223e-06,
      "loss": 65.0027,
      "step": 19083
    },
    {
      "epoch": 19.1,
      "grad_norm": 1287.3404541015625,
      "learning_rate": 4.623323013415893e-06,
      "loss": 58.0085,
      "step": 19084
    },
    {
      "epoch": 19.1,
      "grad_norm": 672.04150390625,
      "learning_rate": 4.618163054695562e-06,
      "loss": 59.3277,
      "step": 19085
    },
    {
      "epoch": 19.11,
      "grad_norm": 23198.294921875,
      "learning_rate": 4.613003095975233e-06,
      "loss": 58.8677,
      "step": 19086
    },
    {
      "epoch": 19.11,
      "grad_norm": 3804.8359375,
      "learning_rate": 4.607843137254902e-06,
      "loss": 60.2398,
      "step": 19087
    },
    {
      "epoch": 19.11,
      "grad_norm": 6487.240234375,
      "learning_rate": 4.6026831785345716e-06,
      "loss": 59.0266,
      "step": 19088
    },
    {
      "epoch": 19.11,
      "grad_norm": 30711.49609375,
      "learning_rate": 4.597523219814242e-06,
      "loss": 55.0604,
      "step": 19089
    },
    {
      "epoch": 19.11,
      "grad_norm": 6033.50732421875,
      "learning_rate": 4.592363261093911e-06,
      "loss": 45.4897,
      "step": 19090
    },
    {
      "epoch": 19.11,
      "grad_norm": 48302.40234375,
      "learning_rate": 4.5872033023735816e-06,
      "loss": 57.7197,
      "step": 19091
    },
    {
      "epoch": 19.11,
      "grad_norm": 18029.171875,
      "learning_rate": 4.582043343653251e-06,
      "loss": 39.5447,
      "step": 19092
    },
    {
      "epoch": 19.11,
      "grad_norm": 3645.320068359375,
      "learning_rate": 4.5768833849329204e-06,
      "loss": 59.5974,
      "step": 19093
    },
    {
      "epoch": 19.11,
      "grad_norm": 72778.546875,
      "learning_rate": 4.57172342621259e-06,
      "loss": 66.0724,
      "step": 19094
    },
    {
      "epoch": 19.11,
      "grad_norm": 27616.4765625,
      "learning_rate": 4.56656346749226e-06,
      "loss": 55.6828,
      "step": 19095
    },
    {
      "epoch": 19.12,
      "grad_norm": 36916.26171875,
      "learning_rate": 4.5614035087719304e-06,
      "loss": 23.5602,
      "step": 19096
    },
    {
      "epoch": 19.12,
      "grad_norm": 12071.705078125,
      "learning_rate": 4.5562435500516e-06,
      "loss": 49.3496,
      "step": 19097
    },
    {
      "epoch": 19.12,
      "grad_norm": 6873.1767578125,
      "learning_rate": 4.55108359133127e-06,
      "loss": 55.961,
      "step": 19098
    },
    {
      "epoch": 19.12,
      "grad_norm": 12985.98046875,
      "learning_rate": 4.54592363261094e-06,
      "loss": 61.5312,
      "step": 19099
    },
    {
      "epoch": 19.12,
      "grad_norm": 16454.283203125,
      "learning_rate": 4.540763673890609e-06,
      "loss": 56.5713,
      "step": 19100
    },
    {
      "epoch": 19.12,
      "grad_norm": 5097.41552734375,
      "learning_rate": 4.5356037151702785e-06,
      "loss": 44.9961,
      "step": 19101
    },
    {
      "epoch": 19.12,
      "grad_norm": 4937.794921875,
      "learning_rate": 4.530443756449949e-06,
      "loss": 56.5546,
      "step": 19102
    },
    {
      "epoch": 19.12,
      "grad_norm": 5523.05712890625,
      "learning_rate": 4.525283797729618e-06,
      "loss": 55.5358,
      "step": 19103
    },
    {
      "epoch": 19.12,
      "grad_norm": 11397.529296875,
      "learning_rate": 4.5201238390092885e-06,
      "loss": 65.5009,
      "step": 19104
    },
    {
      "epoch": 19.12,
      "grad_norm": 5318.3837890625,
      "learning_rate": 4.514963880288958e-06,
      "loss": 64.9901,
      "step": 19105
    },
    {
      "epoch": 19.13,
      "grad_norm": 1817.3824462890625,
      "learning_rate": 4.509803921568627e-06,
      "loss": 56.2329,
      "step": 19106
    },
    {
      "epoch": 19.13,
      "grad_norm": 28188.8125,
      "learning_rate": 4.504643962848298e-06,
      "loss": 59.7697,
      "step": 19107
    },
    {
      "epoch": 19.13,
      "grad_norm": 3755.032470703125,
      "learning_rate": 4.499484004127967e-06,
      "loss": 63.2803,
      "step": 19108
    },
    {
      "epoch": 19.13,
      "grad_norm": 7433.51708984375,
      "learning_rate": 4.494324045407637e-06,
      "loss": 54.6081,
      "step": 19109
    },
    {
      "epoch": 19.13,
      "grad_norm": 10505.9931640625,
      "learning_rate": 4.489164086687307e-06,
      "loss": 42.2936,
      "step": 19110
    },
    {
      "epoch": 19.13,
      "grad_norm": 6619.1484375,
      "learning_rate": 4.484004127966976e-06,
      "loss": 63.7731,
      "step": 19111
    },
    {
      "epoch": 19.13,
      "grad_norm": 26289.873046875,
      "learning_rate": 4.478844169246646e-06,
      "loss": 60.6114,
      "step": 19112
    },
    {
      "epoch": 19.13,
      "grad_norm": 5345.9873046875,
      "learning_rate": 4.473684210526316e-06,
      "loss": 52.5613,
      "step": 19113
    },
    {
      "epoch": 19.13,
      "grad_norm": 91061.3515625,
      "learning_rate": 4.468524251805986e-06,
      "loss": 23.9243,
      "step": 19114
    },
    {
      "epoch": 19.13,
      "grad_norm": 5911.71044921875,
      "learning_rate": 4.463364293085656e-06,
      "loss": 55.5662,
      "step": 19115
    },
    {
      "epoch": 19.14,
      "grad_norm": 41431.6640625,
      "learning_rate": 4.458204334365326e-06,
      "loss": 41.1636,
      "step": 19116
    },
    {
      "epoch": 19.14,
      "grad_norm": 13204.96875,
      "learning_rate": 4.4530443756449945e-06,
      "loss": 65.6008,
      "step": 19117
    },
    {
      "epoch": 19.14,
      "grad_norm": 13508.568359375,
      "learning_rate": 4.447884416924665e-06,
      "loss": 60.4452,
      "step": 19118
    },
    {
      "epoch": 19.14,
      "grad_norm": 3961.801025390625,
      "learning_rate": 4.442724458204334e-06,
      "loss": 64.8836,
      "step": 19119
    },
    {
      "epoch": 19.14,
      "grad_norm": 19445.310546875,
      "learning_rate": 4.4375644994840045e-06,
      "loss": 56.0114,
      "step": 19120
    },
    {
      "epoch": 19.14,
      "grad_norm": 1382.9320068359375,
      "learning_rate": 4.432404540763674e-06,
      "loss": 59.7334,
      "step": 19121
    },
    {
      "epoch": 19.14,
      "grad_norm": 3293.81005859375,
      "learning_rate": 4.427244582043344e-06,
      "loss": 67.561,
      "step": 19122
    },
    {
      "epoch": 19.14,
      "grad_norm": 9773.9443359375,
      "learning_rate": 4.422084623323014e-06,
      "loss": 61.3,
      "step": 19123
    },
    {
      "epoch": 19.14,
      "grad_norm": 3328.61328125,
      "learning_rate": 4.416924664602683e-06,
      "loss": 62.534,
      "step": 19124
    },
    {
      "epoch": 19.14,
      "grad_norm": 3268.89013671875,
      "learning_rate": 4.411764705882353e-06,
      "loss": 56.1845,
      "step": 19125
    },
    {
      "epoch": 19.15,
      "grad_norm": 36619.046875,
      "learning_rate": 4.406604747162023e-06,
      "loss": 60.2167,
      "step": 19126
    },
    {
      "epoch": 19.15,
      "grad_norm": 23095.654296875,
      "learning_rate": 4.401444788441693e-06,
      "loss": 60.3673,
      "step": 19127
    },
    {
      "epoch": 19.15,
      "grad_norm": 8718.6943359375,
      "learning_rate": 4.3962848297213626e-06,
      "loss": 60.5588,
      "step": 19128
    },
    {
      "epoch": 19.15,
      "grad_norm": 17158.76953125,
      "learning_rate": 4.391124871001032e-06,
      "loss": 55.0933,
      "step": 19129
    },
    {
      "epoch": 19.15,
      "grad_norm": 1121.810302734375,
      "learning_rate": 4.3859649122807014e-06,
      "loss": 62.239,
      "step": 19130
    },
    {
      "epoch": 19.15,
      "grad_norm": 14471.1923828125,
      "learning_rate": 4.380804953560372e-06,
      "loss": 60.3924,
      "step": 19131
    },
    {
      "epoch": 19.15,
      "grad_norm": 18863.416015625,
      "learning_rate": 4.375644994840041e-06,
      "loss": 49.8423,
      "step": 19132
    },
    {
      "epoch": 19.15,
      "grad_norm": 1817.4239501953125,
      "learning_rate": 4.3704850361197114e-06,
      "loss": 51.5503,
      "step": 19133
    },
    {
      "epoch": 19.15,
      "grad_norm": 131634.046875,
      "learning_rate": 4.365325077399382e-06,
      "loss": 60.6679,
      "step": 19134
    },
    {
      "epoch": 19.15,
      "grad_norm": 6588.86474609375,
      "learning_rate": 4.36016511867905e-06,
      "loss": 57.8088,
      "step": 19135
    },
    {
      "epoch": 19.16,
      "grad_norm": 2634.394287109375,
      "learning_rate": 4.355005159958721e-06,
      "loss": 62.8447,
      "step": 19136
    },
    {
      "epoch": 19.16,
      "grad_norm": 3803.28369140625,
      "learning_rate": 4.34984520123839e-06,
      "loss": 55.1378,
      "step": 19137
    },
    {
      "epoch": 19.16,
      "grad_norm": 54088.40625,
      "learning_rate": 4.34468524251806e-06,
      "loss": 62.5678,
      "step": 19138
    },
    {
      "epoch": 19.16,
      "grad_norm": 12943.04296875,
      "learning_rate": 4.33952528379773e-06,
      "loss": 58.2476,
      "step": 19139
    },
    {
      "epoch": 19.16,
      "grad_norm": 26676.755859375,
      "learning_rate": 4.3343653250774e-06,
      "loss": 62.1337,
      "step": 19140
    },
    {
      "epoch": 19.16,
      "grad_norm": 3048.330810546875,
      "learning_rate": 4.3292053663570695e-06,
      "loss": 57.6363,
      "step": 19141
    },
    {
      "epoch": 19.16,
      "grad_norm": 14220.515625,
      "learning_rate": 4.324045407636739e-06,
      "loss": 51.7519,
      "step": 19142
    },
    {
      "epoch": 19.16,
      "grad_norm": 185065.390625,
      "learning_rate": 4.318885448916409e-06,
      "loss": 45.1159,
      "step": 19143
    },
    {
      "epoch": 19.16,
      "grad_norm": 3488.934326171875,
      "learning_rate": 4.313725490196079e-06,
      "loss": 54.9032,
      "step": 19144
    },
    {
      "epoch": 19.16,
      "grad_norm": 23907.091796875,
      "learning_rate": 4.308565531475749e-06,
      "loss": 42.9402,
      "step": 19145
    },
    {
      "epoch": 19.17,
      "grad_norm": 26446.51953125,
      "learning_rate": 4.303405572755418e-06,
      "loss": 35.3161,
      "step": 19146
    },
    {
      "epoch": 19.17,
      "grad_norm": 10875.771484375,
      "learning_rate": 4.298245614035088e-06,
      "loss": 63.8033,
      "step": 19147
    },
    {
      "epoch": 19.17,
      "grad_norm": 13237.1171875,
      "learning_rate": 4.293085655314757e-06,
      "loss": 57.1548,
      "step": 19148
    },
    {
      "epoch": 19.17,
      "grad_norm": 2673.81591796875,
      "learning_rate": 4.2879256965944275e-06,
      "loss": 55.8396,
      "step": 19149
    },
    {
      "epoch": 19.17,
      "grad_norm": 5547.3701171875,
      "learning_rate": 4.282765737874097e-06,
      "loss": 59.0541,
      "step": 19150
    },
    {
      "epoch": 19.17,
      "grad_norm": 2528.717041015625,
      "learning_rate": 4.277605779153767e-06,
      "loss": 66.3304,
      "step": 19151
    },
    {
      "epoch": 19.17,
      "grad_norm": 658.611328125,
      "learning_rate": 4.2724458204334375e-06,
      "loss": 54.4427,
      "step": 19152
    },
    {
      "epoch": 19.17,
      "grad_norm": 53290.16796875,
      "learning_rate": 4.267285861713106e-06,
      "loss": 42.9616,
      "step": 19153
    },
    {
      "epoch": 19.17,
      "grad_norm": 4237.23388671875,
      "learning_rate": 4.262125902992776e-06,
      "loss": 44.4464,
      "step": 19154
    },
    {
      "epoch": 19.17,
      "grad_norm": 16749.318359375,
      "learning_rate": 4.256965944272446e-06,
      "loss": 62.711,
      "step": 19155
    },
    {
      "epoch": 19.18,
      "grad_norm": 41071.62109375,
      "learning_rate": 4.251805985552116e-06,
      "loss": 66.7955,
      "step": 19156
    },
    {
      "epoch": 19.18,
      "grad_norm": 21571.236328125,
      "learning_rate": 4.2466460268317855e-06,
      "loss": 50.4497,
      "step": 19157
    },
    {
      "epoch": 19.18,
      "grad_norm": 122720.859375,
      "learning_rate": 4.241486068111456e-06,
      "loss": 31.1048,
      "step": 19158
    },
    {
      "epoch": 19.18,
      "grad_norm": 2502.2548828125,
      "learning_rate": 4.236326109391124e-06,
      "loss": 63.9739,
      "step": 19159
    },
    {
      "epoch": 19.18,
      "grad_norm": 3648.382080078125,
      "learning_rate": 4.231166150670795e-06,
      "loss": 65.3686,
      "step": 19160
    },
    {
      "epoch": 19.18,
      "grad_norm": 5305.39306640625,
      "learning_rate": 4.226006191950464e-06,
      "loss": 58.1684,
      "step": 19161
    },
    {
      "epoch": 19.18,
      "grad_norm": 1886.026123046875,
      "learning_rate": 4.220846233230134e-06,
      "loss": 60.4586,
      "step": 19162
    },
    {
      "epoch": 19.18,
      "grad_norm": 137911.875,
      "learning_rate": 4.215686274509805e-06,
      "loss": 34.0013,
      "step": 19163
    },
    {
      "epoch": 19.18,
      "grad_norm": 6696.89892578125,
      "learning_rate": 4.210526315789474e-06,
      "loss": 58.7444,
      "step": 19164
    },
    {
      "epoch": 19.18,
      "grad_norm": 6453.099609375,
      "learning_rate": 4.2053663570691436e-06,
      "loss": 58.7138,
      "step": 19165
    },
    {
      "epoch": 19.19,
      "grad_norm": 22778.5,
      "learning_rate": 4.200206398348813e-06,
      "loss": 57.5268,
      "step": 19166
    },
    {
      "epoch": 19.19,
      "grad_norm": 29113.3046875,
      "learning_rate": 4.195046439628483e-06,
      "loss": 50.7444,
      "step": 19167
    },
    {
      "epoch": 19.19,
      "grad_norm": 4149.36767578125,
      "learning_rate": 4.189886480908153e-06,
      "loss": 64.5713,
      "step": 19168
    },
    {
      "epoch": 19.19,
      "grad_norm": 8451.03125,
      "learning_rate": 4.184726522187823e-06,
      "loss": 52.2,
      "step": 19169
    },
    {
      "epoch": 19.19,
      "grad_norm": 24484.521484375,
      "learning_rate": 4.1795665634674924e-06,
      "loss": 39.7805,
      "step": 19170
    },
    {
      "epoch": 19.19,
      "grad_norm": 9844.341796875,
      "learning_rate": 4.174406604747162e-06,
      "loss": 60.3021,
      "step": 19171
    },
    {
      "epoch": 19.19,
      "grad_norm": 6911.95654296875,
      "learning_rate": 4.169246646026832e-06,
      "loss": 58.243,
      "step": 19172
    },
    {
      "epoch": 19.19,
      "grad_norm": 2337.885498046875,
      "learning_rate": 4.164086687306502e-06,
      "loss": 40.9834,
      "step": 19173
    },
    {
      "epoch": 19.19,
      "grad_norm": 6911.9013671875,
      "learning_rate": 4.158926728586172e-06,
      "loss": 62.3524,
      "step": 19174
    },
    {
      "epoch": 19.19,
      "grad_norm": 6299.34423828125,
      "learning_rate": 4.153766769865841e-06,
      "loss": 63.3015,
      "step": 19175
    },
    {
      "epoch": 19.2,
      "grad_norm": 17432.869140625,
      "learning_rate": 4.148606811145512e-06,
      "loss": 38.9046,
      "step": 19176
    },
    {
      "epoch": 19.2,
      "grad_norm": 8978.66796875,
      "learning_rate": 4.14344685242518e-06,
      "loss": 61.4758,
      "step": 19177
    },
    {
      "epoch": 19.2,
      "grad_norm": 16524.861328125,
      "learning_rate": 4.1382868937048505e-06,
      "loss": 49.2108,
      "step": 19178
    },
    {
      "epoch": 19.2,
      "grad_norm": 11681.23828125,
      "learning_rate": 4.13312693498452e-06,
      "loss": 53.9627,
      "step": 19179
    },
    {
      "epoch": 19.2,
      "grad_norm": 10740.5634765625,
      "learning_rate": 4.12796697626419e-06,
      "loss": 58.5226,
      "step": 19180
    },
    {
      "epoch": 19.2,
      "grad_norm": 5674.60498046875,
      "learning_rate": 4.1228070175438605e-06,
      "loss": 57.2725,
      "step": 19181
    },
    {
      "epoch": 19.2,
      "grad_norm": 5825.71630859375,
      "learning_rate": 4.11764705882353e-06,
      "loss": 63.6144,
      "step": 19182
    },
    {
      "epoch": 19.2,
      "grad_norm": 12043.3720703125,
      "learning_rate": 4.112487100103199e-06,
      "loss": 61.6155,
      "step": 19183
    },
    {
      "epoch": 19.2,
      "grad_norm": 3279.322998046875,
      "learning_rate": 4.107327141382869e-06,
      "loss": 62.5436,
      "step": 19184
    },
    {
      "epoch": 19.2,
      "grad_norm": 6944.40576171875,
      "learning_rate": 4.102167182662539e-06,
      "loss": 45.9756,
      "step": 19185
    },
    {
      "epoch": 19.21,
      "grad_norm": 35606.4921875,
      "learning_rate": 4.0970072239422085e-06,
      "loss": 59.8738,
      "step": 19186
    },
    {
      "epoch": 19.21,
      "grad_norm": 12861.1787109375,
      "learning_rate": 4.091847265221879e-06,
      "loss": 65.8328,
      "step": 19187
    },
    {
      "epoch": 19.21,
      "grad_norm": 2886.595458984375,
      "learning_rate": 4.086687306501548e-06,
      "loss": 60.4096,
      "step": 19188
    },
    {
      "epoch": 19.21,
      "grad_norm": 5157.89990234375,
      "learning_rate": 4.081527347781218e-06,
      "loss": 51.8215,
      "step": 19189
    },
    {
      "epoch": 19.21,
      "grad_norm": 8920.4921875,
      "learning_rate": 4.076367389060888e-06,
      "loss": 65.073,
      "step": 19190
    },
    {
      "epoch": 19.21,
      "grad_norm": 9124.544921875,
      "learning_rate": 4.071207430340557e-06,
      "loss": 57.3129,
      "step": 19191
    },
    {
      "epoch": 19.21,
      "grad_norm": 4091.224609375,
      "learning_rate": 4.066047471620228e-06,
      "loss": 60.1978,
      "step": 19192
    },
    {
      "epoch": 19.21,
      "grad_norm": 8993.40234375,
      "learning_rate": 4.060887512899897e-06,
      "loss": 59.999,
      "step": 19193
    },
    {
      "epoch": 19.21,
      "grad_norm": 122334.21875,
      "learning_rate": 4.055727554179567e-06,
      "loss": 62.4739,
      "step": 19194
    },
    {
      "epoch": 19.21,
      "grad_norm": 2597.77490234375,
      "learning_rate": 4.050567595459236e-06,
      "loss": 42.601,
      "step": 19195
    },
    {
      "epoch": 19.22,
      "grad_norm": 25265.70703125,
      "learning_rate": 4.045407636738906e-06,
      "loss": 56.0678,
      "step": 19196
    },
    {
      "epoch": 19.22,
      "grad_norm": 123166.4296875,
      "learning_rate": 4.040247678018576e-06,
      "loss": 40.6248,
      "step": 19197
    },
    {
      "epoch": 19.22,
      "grad_norm": 2852.832275390625,
      "learning_rate": 4.035087719298246e-06,
      "loss": 60.2413,
      "step": 19198
    },
    {
      "epoch": 19.22,
      "grad_norm": 2601.932861328125,
      "learning_rate": 4.029927760577915e-06,
      "loss": 67.0301,
      "step": 19199
    },
    {
      "epoch": 19.22,
      "grad_norm": 26424.06640625,
      "learning_rate": 4.024767801857586e-06,
      "loss": 60.2185,
      "step": 19200
    },
    {
      "epoch": 19.22,
      "grad_norm": 16275.37890625,
      "learning_rate": 4.019607843137255e-06,
      "loss": 54.9713,
      "step": 19201
    },
    {
      "epoch": 19.22,
      "grad_norm": 13211.638671875,
      "learning_rate": 4.0144478844169246e-06,
      "loss": 58.3737,
      "step": 19202
    },
    {
      "epoch": 19.22,
      "grad_norm": 15504.0048828125,
      "learning_rate": 4.009287925696595e-06,
      "loss": 59.1867,
      "step": 19203
    },
    {
      "epoch": 19.22,
      "grad_norm": 1299.9727783203125,
      "learning_rate": 4.004127966976264e-06,
      "loss": 57.4957,
      "step": 19204
    },
    {
      "epoch": 19.22,
      "grad_norm": 9242.9619140625,
      "learning_rate": 3.9989680082559346e-06,
      "loss": 53.7823,
      "step": 19205
    },
    {
      "epoch": 19.23,
      "grad_norm": 6167.359375,
      "learning_rate": 3.993808049535604e-06,
      "loss": 56.0196,
      "step": 19206
    },
    {
      "epoch": 19.23,
      "grad_norm": 1184.833740234375,
      "learning_rate": 3.9886480908152734e-06,
      "loss": 62.834,
      "step": 19207
    },
    {
      "epoch": 19.23,
      "grad_norm": 8015.8935546875,
      "learning_rate": 3.983488132094943e-06,
      "loss": 61.8856,
      "step": 19208
    },
    {
      "epoch": 19.23,
      "grad_norm": 5054.3916015625,
      "learning_rate": 3.978328173374613e-06,
      "loss": 65.2478,
      "step": 19209
    },
    {
      "epoch": 19.23,
      "grad_norm": 10644.615234375,
      "learning_rate": 3.9731682146542834e-06,
      "loss": 57.9702,
      "step": 19210
    },
    {
      "epoch": 19.23,
      "grad_norm": 14475.4677734375,
      "learning_rate": 3.968008255933953e-06,
      "loss": 35.2004,
      "step": 19211
    },
    {
      "epoch": 19.23,
      "grad_norm": 6112.6767578125,
      "learning_rate": 3.962848297213622e-06,
      "loss": 63.1272,
      "step": 19212
    },
    {
      "epoch": 19.23,
      "grad_norm": 20608.755859375,
      "learning_rate": 3.957688338493292e-06,
      "loss": 57.2856,
      "step": 19213
    },
    {
      "epoch": 19.23,
      "grad_norm": 9517.658203125,
      "learning_rate": 3.952528379772962e-06,
      "loss": 58.3501,
      "step": 19214
    },
    {
      "epoch": 19.23,
      "grad_norm": 226614.34375,
      "learning_rate": 3.9473684210526315e-06,
      "loss": 38.9578,
      "step": 19215
    },
    {
      "epoch": 19.24,
      "grad_norm": 64186.33984375,
      "learning_rate": 3.942208462332302e-06,
      "loss": 63.8808,
      "step": 19216
    },
    {
      "epoch": 19.24,
      "grad_norm": 17989.423828125,
      "learning_rate": 3.937048503611971e-06,
      "loss": 53.8377,
      "step": 19217
    },
    {
      "epoch": 19.24,
      "grad_norm": 8149.85009765625,
      "learning_rate": 3.9318885448916415e-06,
      "loss": 58.9552,
      "step": 19218
    },
    {
      "epoch": 19.24,
      "grad_norm": 4364.06103515625,
      "learning_rate": 3.926728586171311e-06,
      "loss": 59.9106,
      "step": 19219
    },
    {
      "epoch": 19.24,
      "grad_norm": 3882.272216796875,
      "learning_rate": 3.92156862745098e-06,
      "loss": 60.286,
      "step": 19220
    },
    {
      "epoch": 19.24,
      "grad_norm": 5323.43701171875,
      "learning_rate": 3.916408668730651e-06,
      "loss": 66.224,
      "step": 19221
    },
    {
      "epoch": 19.24,
      "grad_norm": 2508.131591796875,
      "learning_rate": 3.91124871001032e-06,
      "loss": 65.1865,
      "step": 19222
    },
    {
      "epoch": 19.24,
      "grad_norm": 2180.442626953125,
      "learning_rate": 3.90608875128999e-06,
      "loss": 55.7696,
      "step": 19223
    },
    {
      "epoch": 19.24,
      "grad_norm": 13473.6591796875,
      "learning_rate": 3.90092879256966e-06,
      "loss": 51.853,
      "step": 19224
    },
    {
      "epoch": 19.24,
      "grad_norm": 23140.923828125,
      "learning_rate": 3.895768833849329e-06,
      "loss": 38.8719,
      "step": 19225
    },
    {
      "epoch": 19.25,
      "grad_norm": 4048.431884765625,
      "learning_rate": 3.890608875128999e-06,
      "loss": 53.4512,
      "step": 19226
    },
    {
      "epoch": 19.25,
      "grad_norm": 11380.0595703125,
      "learning_rate": 3.885448916408669e-06,
      "loss": 48.1951,
      "step": 19227
    },
    {
      "epoch": 19.25,
      "grad_norm": 101875.109375,
      "learning_rate": 3.880288957688338e-06,
      "loss": 62.2719,
      "step": 19228
    },
    {
      "epoch": 19.25,
      "grad_norm": 10177.69921875,
      "learning_rate": 3.875128998968009e-06,
      "loss": 42.2814,
      "step": 19229
    },
    {
      "epoch": 19.25,
      "grad_norm": 34111.70703125,
      "learning_rate": 3.869969040247678e-06,
      "loss": 59.8547,
      "step": 19230
    },
    {
      "epoch": 19.25,
      "grad_norm": 7805.0849609375,
      "learning_rate": 3.8648090815273475e-06,
      "loss": 57.5404,
      "step": 19231
    },
    {
      "epoch": 19.25,
      "grad_norm": 5976.4501953125,
      "learning_rate": 3.859649122807018e-06,
      "loss": 51.4622,
      "step": 19232
    },
    {
      "epoch": 19.25,
      "grad_norm": 50495.8125,
      "learning_rate": 3.854489164086687e-06,
      "loss": 26.7222,
      "step": 19233
    },
    {
      "epoch": 19.25,
      "grad_norm": 9972.8310546875,
      "learning_rate": 3.8493292053663575e-06,
      "loss": 61.0203,
      "step": 19234
    },
    {
      "epoch": 19.25,
      "grad_norm": 5092.4921875,
      "learning_rate": 3.844169246646027e-06,
      "loss": 65.8129,
      "step": 19235
    },
    {
      "epoch": 19.26,
      "grad_norm": 33917.59765625,
      "learning_rate": 3.839009287925697e-06,
      "loss": 62.9122,
      "step": 19236
    },
    {
      "epoch": 19.26,
      "grad_norm": 12474.2265625,
      "learning_rate": 3.833849329205366e-06,
      "loss": 57.7927,
      "step": 19237
    },
    {
      "epoch": 19.26,
      "grad_norm": 6104.2158203125,
      "learning_rate": 3.828689370485036e-06,
      "loss": 50.4267,
      "step": 19238
    },
    {
      "epoch": 19.26,
      "grad_norm": 3992.494140625,
      "learning_rate": 3.823529411764706e-06,
      "loss": 62.3577,
      "step": 19239
    },
    {
      "epoch": 19.26,
      "grad_norm": 8063.1455078125,
      "learning_rate": 3.818369453044376e-06,
      "loss": 58.4289,
      "step": 19240
    },
    {
      "epoch": 19.26,
      "grad_norm": 2028.8485107421875,
      "learning_rate": 3.8132094943240457e-06,
      "loss": 51.0616,
      "step": 19241
    },
    {
      "epoch": 19.26,
      "grad_norm": 13584.921875,
      "learning_rate": 3.808049535603715e-06,
      "loss": 51.8622,
      "step": 19242
    },
    {
      "epoch": 19.26,
      "grad_norm": 2474.76513671875,
      "learning_rate": 3.8028895768833854e-06,
      "loss": 64.1044,
      "step": 19243
    },
    {
      "epoch": 19.26,
      "grad_norm": 3147.466552734375,
      "learning_rate": 3.797729618163055e-06,
      "loss": 56.2598,
      "step": 19244
    },
    {
      "epoch": 19.26,
      "grad_norm": 26660.12890625,
      "learning_rate": 3.7925696594427247e-06,
      "loss": 54.4431,
      "step": 19245
    },
    {
      "epoch": 19.27,
      "grad_norm": 108905.2421875,
      "learning_rate": 3.787409700722394e-06,
      "loss": 39.8778,
      "step": 19246
    },
    {
      "epoch": 19.27,
      "grad_norm": 2151.0576171875,
      "learning_rate": 3.782249742002064e-06,
      "loss": 59.2474,
      "step": 19247
    },
    {
      "epoch": 19.27,
      "grad_norm": 46713.72265625,
      "learning_rate": 3.7770897832817343e-06,
      "loss": 56.5025,
      "step": 19248
    },
    {
      "epoch": 19.27,
      "grad_norm": 5967.5224609375,
      "learning_rate": 3.7719298245614037e-06,
      "loss": 54.6748,
      "step": 19249
    },
    {
      "epoch": 19.27,
      "grad_norm": 11340.09765625,
      "learning_rate": 3.7667698658410736e-06,
      "loss": 60.3446,
      "step": 19250
    },
    {
      "epoch": 19.27,
      "grad_norm": 1881.6331787109375,
      "learning_rate": 3.761609907120743e-06,
      "loss": 62.3656,
      "step": 19251
    },
    {
      "epoch": 19.27,
      "grad_norm": 3109.920654296875,
      "learning_rate": 3.7564499484004133e-06,
      "loss": 61.7243,
      "step": 19252
    },
    {
      "epoch": 19.27,
      "grad_norm": 8098.35888671875,
      "learning_rate": 3.7512899896800823e-06,
      "loss": 59.6007,
      "step": 19253
    },
    {
      "epoch": 19.27,
      "grad_norm": 7407.17333984375,
      "learning_rate": 3.7461300309597526e-06,
      "loss": 47.9164,
      "step": 19254
    },
    {
      "epoch": 19.27,
      "grad_norm": 2614.2177734375,
      "learning_rate": 3.740970072239422e-06,
      "loss": 56.4868,
      "step": 19255
    },
    {
      "epoch": 19.28,
      "grad_norm": 3196.684814453125,
      "learning_rate": 3.735810113519092e-06,
      "loss": 48.672,
      "step": 19256
    },
    {
      "epoch": 19.28,
      "grad_norm": 1282.21875,
      "learning_rate": 3.730650154798762e-06,
      "loss": 64.2885,
      "step": 19257
    },
    {
      "epoch": 19.28,
      "grad_norm": 9759.2099609375,
      "learning_rate": 3.7254901960784316e-06,
      "loss": 63.52,
      "step": 19258
    },
    {
      "epoch": 19.28,
      "grad_norm": 22518.548828125,
      "learning_rate": 3.7203302373581015e-06,
      "loss": 48.928,
      "step": 19259
    },
    {
      "epoch": 19.28,
      "grad_norm": 7092.47998046875,
      "learning_rate": 3.715170278637771e-06,
      "loss": 63.2764,
      "step": 19260
    },
    {
      "epoch": 19.28,
      "grad_norm": 6496.49609375,
      "learning_rate": 3.710010319917441e-06,
      "loss": 66.6727,
      "step": 19261
    },
    {
      "epoch": 19.28,
      "grad_norm": 9862.8896484375,
      "learning_rate": 3.70485036119711e-06,
      "loss": 63.3817,
      "step": 19262
    },
    {
      "epoch": 19.28,
      "grad_norm": 6030.658203125,
      "learning_rate": 3.6996904024767805e-06,
      "loss": 51.226,
      "step": 19263
    },
    {
      "epoch": 19.28,
      "grad_norm": 10424.7763671875,
      "learning_rate": 3.69453044375645e-06,
      "loss": 52.4394,
      "step": 19264
    },
    {
      "epoch": 19.28,
      "grad_norm": 8917.25,
      "learning_rate": 3.6893704850361198e-06,
      "loss": 56.8802,
      "step": 19265
    },
    {
      "epoch": 19.29,
      "grad_norm": 1119.928466796875,
      "learning_rate": 3.6842105263157892e-06,
      "loss": 66.2516,
      "step": 19266
    },
    {
      "epoch": 19.29,
      "grad_norm": 3108.844482421875,
      "learning_rate": 3.6790505675954595e-06,
      "loss": 54.671,
      "step": 19267
    },
    {
      "epoch": 19.29,
      "grad_norm": 27986.994140625,
      "learning_rate": 3.6738906088751294e-06,
      "loss": 44.1115,
      "step": 19268
    },
    {
      "epoch": 19.29,
      "grad_norm": 3809.03515625,
      "learning_rate": 3.668730650154799e-06,
      "loss": 66.6235,
      "step": 19269
    },
    {
      "epoch": 19.29,
      "grad_norm": 40102.16796875,
      "learning_rate": 3.663570691434469e-06,
      "loss": 62.6259,
      "step": 19270
    },
    {
      "epoch": 19.29,
      "grad_norm": 9076.306640625,
      "learning_rate": 3.658410732714138e-06,
      "loss": 63.4206,
      "step": 19271
    },
    {
      "epoch": 19.29,
      "grad_norm": 12341.0234375,
      "learning_rate": 3.6532507739938084e-06,
      "loss": 62.2874,
      "step": 19272
    },
    {
      "epoch": 19.29,
      "grad_norm": 7941.60546875,
      "learning_rate": 3.648090815273478e-06,
      "loss": 58.0994,
      "step": 19273
    },
    {
      "epoch": 19.29,
      "grad_norm": 21207.58203125,
      "learning_rate": 3.6429308565531477e-06,
      "loss": 18.5953,
      "step": 19274
    },
    {
      "epoch": 19.29,
      "grad_norm": 2684.6640625,
      "learning_rate": 3.637770897832817e-06,
      "loss": 58.1202,
      "step": 19275
    },
    {
      "epoch": 19.3,
      "grad_norm": 34713.8828125,
      "learning_rate": 3.6326109391124874e-06,
      "loss": 55.9546,
      "step": 19276
    },
    {
      "epoch": 19.3,
      "grad_norm": 1899.6671142578125,
      "learning_rate": 3.6274509803921573e-06,
      "loss": 57.6694,
      "step": 19277
    },
    {
      "epoch": 19.3,
      "grad_norm": 1411.293701171875,
      "learning_rate": 3.6222910216718267e-06,
      "loss": 60.1045,
      "step": 19278
    },
    {
      "epoch": 19.3,
      "grad_norm": 2773.40673828125,
      "learning_rate": 3.617131062951497e-06,
      "loss": 53.6424,
      "step": 19279
    },
    {
      "epoch": 19.3,
      "grad_norm": 2756.2158203125,
      "learning_rate": 3.611971104231166e-06,
      "loss": 51.4593,
      "step": 19280
    },
    {
      "epoch": 19.3,
      "grad_norm": 132298.515625,
      "learning_rate": 3.6068111455108363e-06,
      "loss": 60.3633,
      "step": 19281
    },
    {
      "epoch": 19.3,
      "grad_norm": 3859.65380859375,
      "learning_rate": 3.6016511867905057e-06,
      "loss": 55.1698,
      "step": 19282
    },
    {
      "epoch": 19.3,
      "grad_norm": 7965.55224609375,
      "learning_rate": 3.5964912280701756e-06,
      "loss": 49.5922,
      "step": 19283
    },
    {
      "epoch": 19.3,
      "grad_norm": 12575.7978515625,
      "learning_rate": 3.591331269349845e-06,
      "loss": 62.9197,
      "step": 19284
    },
    {
      "epoch": 19.3,
      "grad_norm": 3092.70068359375,
      "learning_rate": 3.5861713106295153e-06,
      "loss": 43.8564,
      "step": 19285
    },
    {
      "epoch": 19.31,
      "grad_norm": 29212.69140625,
      "learning_rate": 3.581011351909185e-06,
      "loss": 59.4306,
      "step": 19286
    },
    {
      "epoch": 19.31,
      "grad_norm": 2655.27783203125,
      "learning_rate": 3.5758513931888546e-06,
      "loss": 49.6325,
      "step": 19287
    },
    {
      "epoch": 19.31,
      "grad_norm": 4991.2421875,
      "learning_rate": 3.570691434468525e-06,
      "loss": 56.9215,
      "step": 19288
    },
    {
      "epoch": 19.31,
      "grad_norm": 3992.763916015625,
      "learning_rate": 3.565531475748194e-06,
      "loss": 55.735,
      "step": 19289
    },
    {
      "epoch": 19.31,
      "grad_norm": 1859.731689453125,
      "learning_rate": 3.560371517027864e-06,
      "loss": 66.7508,
      "step": 19290
    },
    {
      "epoch": 19.31,
      "grad_norm": 33642.39453125,
      "learning_rate": 3.5552115583075336e-06,
      "loss": 53.4047,
      "step": 19291
    },
    {
      "epoch": 19.31,
      "grad_norm": 7614.484375,
      "learning_rate": 3.5500515995872035e-06,
      "loss": 59.4043,
      "step": 19292
    },
    {
      "epoch": 19.31,
      "grad_norm": 2413.82568359375,
      "learning_rate": 3.544891640866873e-06,
      "loss": 55.2049,
      "step": 19293
    },
    {
      "epoch": 19.31,
      "grad_norm": 9767.0947265625,
      "learning_rate": 3.539731682146543e-06,
      "loss": 58.5263,
      "step": 19294
    },
    {
      "epoch": 19.31,
      "grad_norm": 2897.906982421875,
      "learning_rate": 3.534571723426213e-06,
      "loss": 61.3586,
      "step": 19295
    },
    {
      "epoch": 19.32,
      "grad_norm": 1209.6356201171875,
      "learning_rate": 3.5294117647058825e-06,
      "loss": 61.4798,
      "step": 19296
    },
    {
      "epoch": 19.32,
      "grad_norm": 5801.96923828125,
      "learning_rate": 3.5242518059855528e-06,
      "loss": 62.8012,
      "step": 19297
    },
    {
      "epoch": 19.32,
      "grad_norm": 581.531494140625,
      "learning_rate": 3.5190918472652218e-06,
      "loss": 67.6632,
      "step": 19298
    },
    {
      "epoch": 19.32,
      "grad_norm": 3265.656494140625,
      "learning_rate": 3.513931888544892e-06,
      "loss": 53.924,
      "step": 19299
    },
    {
      "epoch": 19.32,
      "grad_norm": 926.2743530273438,
      "learning_rate": 3.5087719298245615e-06,
      "loss": 59.4347,
      "step": 19300
    },
    {
      "epoch": 19.32,
      "grad_norm": 7412.93505859375,
      "learning_rate": 3.5036119711042313e-06,
      "loss": 53.367,
      "step": 19301
    },
    {
      "epoch": 19.32,
      "grad_norm": 4701.20751953125,
      "learning_rate": 3.4984520123839008e-06,
      "loss": 63.8879,
      "step": 19302
    },
    {
      "epoch": 19.32,
      "grad_norm": 1869.189697265625,
      "learning_rate": 3.493292053663571e-06,
      "loss": 59.0673,
      "step": 19303
    },
    {
      "epoch": 19.32,
      "grad_norm": 15478.55078125,
      "learning_rate": 3.48813209494324e-06,
      "loss": 37.3854,
      "step": 19304
    },
    {
      "epoch": 19.32,
      "grad_norm": 5970.5634765625,
      "learning_rate": 3.4829721362229104e-06,
      "loss": 63.1001,
      "step": 19305
    },
    {
      "epoch": 19.33,
      "grad_norm": 4421.44384765625,
      "learning_rate": 3.4778121775025802e-06,
      "loss": 50.9836,
      "step": 19306
    },
    {
      "epoch": 19.33,
      "grad_norm": 46645.1640625,
      "learning_rate": 3.4726522187822497e-06,
      "loss": 58.17,
      "step": 19307
    },
    {
      "epoch": 19.33,
      "grad_norm": 11286.138671875,
      "learning_rate": 3.46749226006192e-06,
      "loss": 51.5188,
      "step": 19308
    },
    {
      "epoch": 19.33,
      "grad_norm": 12518.4892578125,
      "learning_rate": 3.4623323013415894e-06,
      "loss": 53.4719,
      "step": 19309
    },
    {
      "epoch": 19.33,
      "grad_norm": 6594.63037109375,
      "learning_rate": 3.4571723426212592e-06,
      "loss": 65.0775,
      "step": 19310
    },
    {
      "epoch": 19.33,
      "grad_norm": 14194.1689453125,
      "learning_rate": 3.4520123839009287e-06,
      "loss": 38.959,
      "step": 19311
    },
    {
      "epoch": 19.33,
      "grad_norm": 24548.36328125,
      "learning_rate": 3.446852425180599e-06,
      "loss": 57.8335,
      "step": 19312
    },
    {
      "epoch": 19.33,
      "grad_norm": 10594.1318359375,
      "learning_rate": 3.441692466460268e-06,
      "loss": 62.1602,
      "step": 19313
    },
    {
      "epoch": 19.33,
      "grad_norm": 8118.34423828125,
      "learning_rate": 3.4365325077399382e-06,
      "loss": 52.8434,
      "step": 19314
    },
    {
      "epoch": 19.33,
      "grad_norm": 4088.816650390625,
      "learning_rate": 3.431372549019608e-06,
      "loss": 58.4493,
      "step": 19315
    },
    {
      "epoch": 19.34,
      "grad_norm": 5614.13720703125,
      "learning_rate": 3.4262125902992775e-06,
      "loss": 58.9944,
      "step": 19316
    },
    {
      "epoch": 19.34,
      "grad_norm": 17010.5546875,
      "learning_rate": 3.421052631578948e-06,
      "loss": 31.4493,
      "step": 19317
    },
    {
      "epoch": 19.34,
      "grad_norm": 198961.515625,
      "learning_rate": 3.4158926728586173e-06,
      "loss": 48.2593,
      "step": 19318
    },
    {
      "epoch": 19.34,
      "grad_norm": 11513.1025390625,
      "learning_rate": 3.410732714138287e-06,
      "loss": 57.9472,
      "step": 19319
    },
    {
      "epoch": 19.34,
      "grad_norm": 4930.8232421875,
      "learning_rate": 3.4055727554179566e-06,
      "loss": 64.6002,
      "step": 19320
    },
    {
      "epoch": 19.34,
      "grad_norm": 15041.2412109375,
      "learning_rate": 3.400412796697627e-06,
      "loss": 57.7233,
      "step": 19321
    },
    {
      "epoch": 19.34,
      "grad_norm": 4652.7333984375,
      "learning_rate": 3.395252837977296e-06,
      "loss": 71.0011,
      "step": 19322
    },
    {
      "epoch": 19.34,
      "grad_norm": 2398.795654296875,
      "learning_rate": 3.390092879256966e-06,
      "loss": 56.9595,
      "step": 19323
    },
    {
      "epoch": 19.34,
      "grad_norm": 1324.4051513671875,
      "learning_rate": 3.384932920536636e-06,
      "loss": 55.3609,
      "step": 19324
    },
    {
      "epoch": 19.34,
      "grad_norm": 8270.6357421875,
      "learning_rate": 3.3797729618163054e-06,
      "loss": 62.3355,
      "step": 19325
    },
    {
      "epoch": 19.35,
      "grad_norm": 20779.7734375,
      "learning_rate": 3.3746130030959757e-06,
      "loss": 54.2441,
      "step": 19326
    },
    {
      "epoch": 19.35,
      "grad_norm": 128158.984375,
      "learning_rate": 3.369453044375645e-06,
      "loss": 57.4749,
      "step": 19327
    },
    {
      "epoch": 19.35,
      "grad_norm": 5954.69921875,
      "learning_rate": 3.364293085655315e-06,
      "loss": 63.5667,
      "step": 19328
    },
    {
      "epoch": 19.35,
      "grad_norm": 148138.265625,
      "learning_rate": 3.3591331269349844e-06,
      "loss": 38.0473,
      "step": 19329
    },
    {
      "epoch": 19.35,
      "grad_norm": 23802.00390625,
      "learning_rate": 3.3539731682146547e-06,
      "loss": 54.8515,
      "step": 19330
    },
    {
      "epoch": 19.35,
      "grad_norm": 44790.12890625,
      "learning_rate": 3.3488132094943237e-06,
      "loss": 64.5784,
      "step": 19331
    },
    {
      "epoch": 19.35,
      "grad_norm": 850.51123046875,
      "learning_rate": 3.343653250773994e-06,
      "loss": 65.2278,
      "step": 19332
    },
    {
      "epoch": 19.35,
      "grad_norm": 2892.850830078125,
      "learning_rate": 3.338493292053664e-06,
      "loss": 63.1619,
      "step": 19333
    },
    {
      "epoch": 19.35,
      "grad_norm": 5458.3857421875,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 50.8453,
      "step": 19334
    },
    {
      "epoch": 19.35,
      "grad_norm": 7584.7060546875,
      "learning_rate": 3.3281733746130036e-06,
      "loss": 65.9641,
      "step": 19335
    },
    {
      "epoch": 19.36,
      "grad_norm": 22032.48828125,
      "learning_rate": 3.323013415892673e-06,
      "loss": 59.3661,
      "step": 19336
    },
    {
      "epoch": 19.36,
      "grad_norm": 8110.42822265625,
      "learning_rate": 3.317853457172343e-06,
      "loss": 50.4277,
      "step": 19337
    },
    {
      "epoch": 19.36,
      "grad_norm": 13789.3173828125,
      "learning_rate": 3.3126934984520123e-06,
      "loss": 52.8846,
      "step": 19338
    },
    {
      "epoch": 19.36,
      "grad_norm": 7869.6484375,
      "learning_rate": 3.3075335397316826e-06,
      "loss": 58.7934,
      "step": 19339
    },
    {
      "epoch": 19.36,
      "grad_norm": 11779.1083984375,
      "learning_rate": 3.3023735810113516e-06,
      "loss": 34.5537,
      "step": 19340
    },
    {
      "epoch": 19.36,
      "grad_norm": 2952.459228515625,
      "learning_rate": 3.297213622291022e-06,
      "loss": 65.1821,
      "step": 19341
    },
    {
      "epoch": 19.36,
      "grad_norm": 7669.2451171875,
      "learning_rate": 3.2920536635706914e-06,
      "loss": 54.679,
      "step": 19342
    },
    {
      "epoch": 19.36,
      "grad_norm": 21130.35546875,
      "learning_rate": 3.2868937048503612e-06,
      "loss": 43.3945,
      "step": 19343
    },
    {
      "epoch": 19.36,
      "grad_norm": 12744.2333984375,
      "learning_rate": 3.2817337461300315e-06,
      "loss": 59.7253,
      "step": 19344
    },
    {
      "epoch": 19.36,
      "grad_norm": 6187.82666015625,
      "learning_rate": 3.276573787409701e-06,
      "loss": 57.5083,
      "step": 19345
    },
    {
      "epoch": 19.37,
      "grad_norm": 3534.84521484375,
      "learning_rate": 3.271413828689371e-06,
      "loss": 54.6251,
      "step": 19346
    },
    {
      "epoch": 19.37,
      "grad_norm": 1306.6309814453125,
      "learning_rate": 3.2662538699690402e-06,
      "loss": 69.0136,
      "step": 19347
    },
    {
      "epoch": 19.37,
      "grad_norm": 2559.84814453125,
      "learning_rate": 3.2610939112487105e-06,
      "loss": 61.6872,
      "step": 19348
    },
    {
      "epoch": 19.37,
      "grad_norm": 11965.69921875,
      "learning_rate": 3.2559339525283795e-06,
      "loss": 60.8557,
      "step": 19349
    },
    {
      "epoch": 19.37,
      "grad_norm": 101896.8125,
      "learning_rate": 3.25077399380805e-06,
      "loss": 46.7976,
      "step": 19350
    },
    {
      "epoch": 19.37,
      "grad_norm": 3402.628173828125,
      "learning_rate": 3.2456140350877192e-06,
      "loss": 59.7717,
      "step": 19351
    },
    {
      "epoch": 19.37,
      "grad_norm": 3001.991943359375,
      "learning_rate": 3.240454076367389e-06,
      "loss": 58.0211,
      "step": 19352
    },
    {
      "epoch": 19.37,
      "grad_norm": 5972.87646484375,
      "learning_rate": 3.2352941176470594e-06,
      "loss": 56.0685,
      "step": 19353
    },
    {
      "epoch": 19.37,
      "grad_norm": 15008.42578125,
      "learning_rate": 3.230134158926729e-06,
      "loss": 42.2584,
      "step": 19354
    },
    {
      "epoch": 19.37,
      "grad_norm": 2116.0810546875,
      "learning_rate": 3.2249742002063987e-06,
      "loss": 63.4779,
      "step": 19355
    },
    {
      "epoch": 19.38,
      "grad_norm": 166666.5625,
      "learning_rate": 3.219814241486068e-06,
      "loss": 49.7013,
      "step": 19356
    },
    {
      "epoch": 19.38,
      "grad_norm": 39336.1640625,
      "learning_rate": 3.214654282765738e-06,
      "loss": 59.6331,
      "step": 19357
    },
    {
      "epoch": 19.38,
      "grad_norm": 4242.10205078125,
      "learning_rate": 3.2094943240454074e-06,
      "loss": 47.516,
      "step": 19358
    },
    {
      "epoch": 19.38,
      "grad_norm": 4015.088134765625,
      "learning_rate": 3.2043343653250777e-06,
      "loss": 62.4342,
      "step": 19359
    },
    {
      "epoch": 19.38,
      "grad_norm": 7969.9423828125,
      "learning_rate": 3.199174406604747e-06,
      "loss": 64.7704,
      "step": 19360
    },
    {
      "epoch": 19.38,
      "grad_norm": 3231.35791015625,
      "learning_rate": 3.194014447884417e-06,
      "loss": 60.3977,
      "step": 19361
    },
    {
      "epoch": 19.38,
      "grad_norm": 1800.633544921875,
      "learning_rate": 3.1888544891640873e-06,
      "loss": 55.6622,
      "step": 19362
    },
    {
      "epoch": 19.38,
      "grad_norm": 7406.044921875,
      "learning_rate": 3.1836945304437567e-06,
      "loss": 62.8002,
      "step": 19363
    },
    {
      "epoch": 19.38,
      "grad_norm": 38933.1171875,
      "learning_rate": 3.1785345717234266e-06,
      "loss": 44.8672,
      "step": 19364
    },
    {
      "epoch": 19.38,
      "grad_norm": 11342.8447265625,
      "learning_rate": 3.173374613003096e-06,
      "loss": 50.3802,
      "step": 19365
    },
    {
      "epoch": 19.39,
      "grad_norm": 12083.384765625,
      "learning_rate": 3.168214654282766e-06,
      "loss": 63.2004,
      "step": 19366
    },
    {
      "epoch": 19.39,
      "grad_norm": 63421.8671875,
      "learning_rate": 3.1630546955624353e-06,
      "loss": 55.9986,
      "step": 19367
    },
    {
      "epoch": 19.39,
      "grad_norm": 14328.5107421875,
      "learning_rate": 3.1578947368421056e-06,
      "loss": 68.707,
      "step": 19368
    },
    {
      "epoch": 19.39,
      "grad_norm": 70197.859375,
      "learning_rate": 3.152734778121775e-06,
      "loss": 32.5462,
      "step": 19369
    },
    {
      "epoch": 19.39,
      "grad_norm": 1685.7279052734375,
      "learning_rate": 3.147574819401445e-06,
      "loss": 67.4072,
      "step": 19370
    },
    {
      "epoch": 19.39,
      "grad_norm": 10320.9248046875,
      "learning_rate": 3.142414860681115e-06,
      "loss": 68.0428,
      "step": 19371
    },
    {
      "epoch": 19.39,
      "grad_norm": 5310.24609375,
      "learning_rate": 3.1372549019607846e-06,
      "loss": 51.2147,
      "step": 19372
    },
    {
      "epoch": 19.39,
      "grad_norm": 11221.2373046875,
      "learning_rate": 3.1320949432404545e-06,
      "loss": 54.1116,
      "step": 19373
    },
    {
      "epoch": 19.39,
      "grad_norm": 29314.79296875,
      "learning_rate": 3.126934984520124e-06,
      "loss": 58.8214,
      "step": 19374
    },
    {
      "epoch": 19.39,
      "grad_norm": 23666.021484375,
      "learning_rate": 3.1217750257997938e-06,
      "loss": 43.2095,
      "step": 19375
    },
    {
      "epoch": 19.4,
      "grad_norm": 5259.7216796875,
      "learning_rate": 3.1166150670794636e-06,
      "loss": 61.4289,
      "step": 19376
    },
    {
      "epoch": 19.4,
      "grad_norm": 35319.19921875,
      "learning_rate": 3.1114551083591335e-06,
      "loss": 22.7502,
      "step": 19377
    },
    {
      "epoch": 19.4,
      "grad_norm": 5813.1904296875,
      "learning_rate": 3.106295149638803e-06,
      "loss": 61.9573,
      "step": 19378
    },
    {
      "epoch": 19.4,
      "grad_norm": 2388.876953125,
      "learning_rate": 3.1011351909184728e-06,
      "loss": 52.6206,
      "step": 19379
    },
    {
      "epoch": 19.4,
      "grad_norm": 2848.066162109375,
      "learning_rate": 3.0959752321981426e-06,
      "loss": 56.4412,
      "step": 19380
    },
    {
      "epoch": 19.4,
      "grad_norm": 37896.21875,
      "learning_rate": 3.0908152734778125e-06,
      "loss": 70.95,
      "step": 19381
    },
    {
      "epoch": 19.4,
      "grad_norm": 9802.7236328125,
      "learning_rate": 3.085655314757482e-06,
      "loss": 52.446,
      "step": 19382
    },
    {
      "epoch": 19.4,
      "grad_norm": 2252.593505859375,
      "learning_rate": 3.080495356037152e-06,
      "loss": 65.1932,
      "step": 19383
    },
    {
      "epoch": 19.4,
      "grad_norm": 1736.0965576171875,
      "learning_rate": 3.0753353973168216e-06,
      "loss": 59.4754,
      "step": 19384
    },
    {
      "epoch": 19.4,
      "grad_norm": 10590.80859375,
      "learning_rate": 3.070175438596491e-06,
      "loss": 37.4944,
      "step": 19385
    },
    {
      "epoch": 19.41,
      "grad_norm": 21403.544921875,
      "learning_rate": 3.0650154798761614e-06,
      "loss": 53.9809,
      "step": 19386
    },
    {
      "epoch": 19.41,
      "grad_norm": 4096.97021484375,
      "learning_rate": 3.059855521155831e-06,
      "loss": 60.922,
      "step": 19387
    },
    {
      "epoch": 19.41,
      "grad_norm": 16349.8115234375,
      "learning_rate": 3.0546955624355007e-06,
      "loss": 52.0275,
      "step": 19388
    },
    {
      "epoch": 19.41,
      "grad_norm": 3484.298095703125,
      "learning_rate": 3.0495356037151705e-06,
      "loss": 67.5164,
      "step": 19389
    },
    {
      "epoch": 19.41,
      "grad_norm": 31321.501953125,
      "learning_rate": 3.0443756449948404e-06,
      "loss": 52.7326,
      "step": 19390
    },
    {
      "epoch": 19.41,
      "grad_norm": 183281.828125,
      "learning_rate": 3.03921568627451e-06,
      "loss": 50.7865,
      "step": 19391
    },
    {
      "epoch": 19.41,
      "grad_norm": 11196.0078125,
      "learning_rate": 3.0340557275541797e-06,
      "loss": 57.068,
      "step": 19392
    },
    {
      "epoch": 19.41,
      "grad_norm": 143758.21875,
      "learning_rate": 3.0288957688338495e-06,
      "loss": 56.4402,
      "step": 19393
    },
    {
      "epoch": 19.41,
      "grad_norm": 2481.686767578125,
      "learning_rate": 3.023735810113519e-06,
      "loss": 57.9805,
      "step": 19394
    },
    {
      "epoch": 19.41,
      "grad_norm": 6610.9453125,
      "learning_rate": 3.0185758513931893e-06,
      "loss": 57.9886,
      "step": 19395
    },
    {
      "epoch": 19.42,
      "grad_norm": 1275.125244140625,
      "learning_rate": 3.0134158926728587e-06,
      "loss": 64.8136,
      "step": 19396
    },
    {
      "epoch": 19.42,
      "grad_norm": 3779.45166015625,
      "learning_rate": 3.0082559339525286e-06,
      "loss": 60.1462,
      "step": 19397
    },
    {
      "epoch": 19.42,
      "grad_norm": 5458.71484375,
      "learning_rate": 3.0030959752321984e-06,
      "loss": 62.2284,
      "step": 19398
    },
    {
      "epoch": 19.42,
      "grad_norm": 62954.59375,
      "learning_rate": 2.997936016511868e-06,
      "loss": 51.1963,
      "step": 19399
    },
    {
      "epoch": 19.42,
      "grad_norm": 6000.5361328125,
      "learning_rate": 2.9927760577915377e-06,
      "loss": 55.0032,
      "step": 19400
    },
    {
      "epoch": 19.42,
      "grad_norm": 20592.83203125,
      "learning_rate": 2.9876160990712076e-06,
      "loss": 49.5246,
      "step": 19401
    },
    {
      "epoch": 19.42,
      "grad_norm": 11810.2666015625,
      "learning_rate": 2.9824561403508774e-06,
      "loss": 52.788,
      "step": 19402
    },
    {
      "epoch": 19.42,
      "grad_norm": 8703.04296875,
      "learning_rate": 2.977296181630547e-06,
      "loss": 29.4182,
      "step": 19403
    },
    {
      "epoch": 19.42,
      "grad_norm": 1309.562255859375,
      "learning_rate": 2.9721362229102167e-06,
      "loss": 65.0472,
      "step": 19404
    },
    {
      "epoch": 19.42,
      "grad_norm": 4911.66796875,
      "learning_rate": 2.9669762641898866e-06,
      "loss": 67.8542,
      "step": 19405
    },
    {
      "epoch": 19.43,
      "grad_norm": 2454.665771484375,
      "learning_rate": 2.9618163054695564e-06,
      "loss": 56.5287,
      "step": 19406
    },
    {
      "epoch": 19.43,
      "grad_norm": 8635.2216796875,
      "learning_rate": 2.9566563467492263e-06,
      "loss": 57.6233,
      "step": 19407
    },
    {
      "epoch": 19.43,
      "grad_norm": 7510.43798828125,
      "learning_rate": 2.9514963880288957e-06,
      "loss": 55.4161,
      "step": 19408
    },
    {
      "epoch": 19.43,
      "grad_norm": 14019.8720703125,
      "learning_rate": 2.9463364293085656e-06,
      "loss": 62.6935,
      "step": 19409
    },
    {
      "epoch": 19.43,
      "grad_norm": 38942.09375,
      "learning_rate": 2.9411764705882355e-06,
      "loss": 46.74,
      "step": 19410
    },
    {
      "epoch": 19.43,
      "grad_norm": 3834.282470703125,
      "learning_rate": 2.9360165118679053e-06,
      "loss": 61.6912,
      "step": 19411
    },
    {
      "epoch": 19.43,
      "grad_norm": 6844.98193359375,
      "learning_rate": 2.9308565531475748e-06,
      "loss": 59.5233,
      "step": 19412
    },
    {
      "epoch": 19.43,
      "grad_norm": 140116.921875,
      "learning_rate": 2.9256965944272446e-06,
      "loss": 53.9056,
      "step": 19413
    },
    {
      "epoch": 19.43,
      "grad_norm": 106707.546875,
      "learning_rate": 2.9205366357069145e-06,
      "loss": 32.2139,
      "step": 19414
    },
    {
      "epoch": 19.43,
      "grad_norm": 5065.91650390625,
      "learning_rate": 2.9153766769865843e-06,
      "loss": 64.906,
      "step": 19415
    },
    {
      "epoch": 19.44,
      "grad_norm": 4158.74853515625,
      "learning_rate": 2.910216718266254e-06,
      "loss": 59.0387,
      "step": 19416
    },
    {
      "epoch": 19.44,
      "grad_norm": 1656.672607421875,
      "learning_rate": 2.9050567595459236e-06,
      "loss": 62.036,
      "step": 19417
    },
    {
      "epoch": 19.44,
      "grad_norm": 7377.1025390625,
      "learning_rate": 2.8998968008255935e-06,
      "loss": 60.3739,
      "step": 19418
    },
    {
      "epoch": 19.44,
      "grad_norm": 38959.18359375,
      "learning_rate": 2.8947368421052634e-06,
      "loss": 47.7443,
      "step": 19419
    },
    {
      "epoch": 19.44,
      "grad_norm": 16205.357421875,
      "learning_rate": 2.889576883384933e-06,
      "loss": 58.0079,
      "step": 19420
    },
    {
      "epoch": 19.44,
      "grad_norm": 2939.03125,
      "learning_rate": 2.8844169246646026e-06,
      "loss": 43.9252,
      "step": 19421
    },
    {
      "epoch": 19.44,
      "grad_norm": 9878.650390625,
      "learning_rate": 2.8792569659442725e-06,
      "loss": 51.2539,
      "step": 19422
    },
    {
      "epoch": 19.44,
      "grad_norm": 6636.68310546875,
      "learning_rate": 2.8740970072239424e-06,
      "loss": 55.8729,
      "step": 19423
    },
    {
      "epoch": 19.44,
      "grad_norm": 6718.001953125,
      "learning_rate": 2.8689370485036122e-06,
      "loss": 63.7862,
      "step": 19424
    },
    {
      "epoch": 19.44,
      "grad_norm": 4749.45166015625,
      "learning_rate": 2.863777089783282e-06,
      "loss": 49.0295,
      "step": 19425
    },
    {
      "epoch": 19.45,
      "grad_norm": 12611.962890625,
      "learning_rate": 2.8586171310629515e-06,
      "loss": 49.1683,
      "step": 19426
    },
    {
      "epoch": 19.45,
      "grad_norm": 1303.4290771484375,
      "learning_rate": 2.8534571723426214e-06,
      "loss": 56.1064,
      "step": 19427
    },
    {
      "epoch": 19.45,
      "grad_norm": 1204.254150390625,
      "learning_rate": 2.8482972136222912e-06,
      "loss": 62.2415,
      "step": 19428
    },
    {
      "epoch": 19.45,
      "grad_norm": 9642.978515625,
      "learning_rate": 2.8431372549019607e-06,
      "loss": 56.4774,
      "step": 19429
    },
    {
      "epoch": 19.45,
      "grad_norm": 5636.12353515625,
      "learning_rate": 2.8379772961816305e-06,
      "loss": 60.2636,
      "step": 19430
    },
    {
      "epoch": 19.45,
      "grad_norm": 7702.84423828125,
      "learning_rate": 2.8328173374613004e-06,
      "loss": 47.0117,
      "step": 19431
    },
    {
      "epoch": 19.45,
      "grad_norm": 6608.7763671875,
      "learning_rate": 2.8276573787409703e-06,
      "loss": 60.8422,
      "step": 19432
    },
    {
      "epoch": 19.45,
      "grad_norm": 19005.12890625,
      "learning_rate": 2.82249742002064e-06,
      "loss": 60.3979,
      "step": 19433
    },
    {
      "epoch": 19.45,
      "grad_norm": 3490.34619140625,
      "learning_rate": 2.81733746130031e-06,
      "loss": 56.8452,
      "step": 19434
    },
    {
      "epoch": 19.45,
      "grad_norm": 16731.64453125,
      "learning_rate": 2.8121775025799794e-06,
      "loss": 64.128,
      "step": 19435
    },
    {
      "epoch": 19.46,
      "grad_norm": 5086.54296875,
      "learning_rate": 2.8070175438596493e-06,
      "loss": 60.7525,
      "step": 19436
    },
    {
      "epoch": 19.46,
      "grad_norm": 3061.052490234375,
      "learning_rate": 2.801857585139319e-06,
      "loss": 62.1818,
      "step": 19437
    },
    {
      "epoch": 19.46,
      "grad_norm": 26640.373046875,
      "learning_rate": 2.7966976264189886e-06,
      "loss": 55.5814,
      "step": 19438
    },
    {
      "epoch": 19.46,
      "grad_norm": 11264.1865234375,
      "learning_rate": 2.7915376676986584e-06,
      "loss": 43.158,
      "step": 19439
    },
    {
      "epoch": 19.46,
      "grad_norm": 17150.33984375,
      "learning_rate": 2.7863777089783283e-06,
      "loss": 53.6248,
      "step": 19440
    },
    {
      "epoch": 19.46,
      "grad_norm": 8978.439453125,
      "learning_rate": 2.781217750257998e-06,
      "loss": 57.9161,
      "step": 19441
    },
    {
      "epoch": 19.46,
      "grad_norm": 51616.01171875,
      "learning_rate": 2.7760577915376676e-06,
      "loss": 55.7499,
      "step": 19442
    },
    {
      "epoch": 19.46,
      "grad_norm": 8652.94140625,
      "learning_rate": 2.770897832817338e-06,
      "loss": 41.3816,
      "step": 19443
    },
    {
      "epoch": 19.46,
      "grad_norm": 23845.4296875,
      "learning_rate": 2.7657378740970073e-06,
      "loss": 43.6156,
      "step": 19444
    },
    {
      "epoch": 19.46,
      "grad_norm": 62197.8046875,
      "learning_rate": 2.760577915376677e-06,
      "loss": 62.6388,
      "step": 19445
    },
    {
      "epoch": 19.47,
      "grad_norm": 2203.800048828125,
      "learning_rate": 2.755417956656347e-06,
      "loss": 55.954,
      "step": 19446
    },
    {
      "epoch": 19.47,
      "grad_norm": 9598.3291015625,
      "learning_rate": 2.7502579979360165e-06,
      "loss": 65.6398,
      "step": 19447
    },
    {
      "epoch": 19.47,
      "grad_norm": 10050.478515625,
      "learning_rate": 2.7450980392156863e-06,
      "loss": 52.8494,
      "step": 19448
    },
    {
      "epoch": 19.47,
      "grad_norm": 63792.78515625,
      "learning_rate": 2.739938080495356e-06,
      "loss": 55.9513,
      "step": 19449
    },
    {
      "epoch": 19.47,
      "grad_norm": 3018.28369140625,
      "learning_rate": 2.7347781217750256e-06,
      "loss": 47.5697,
      "step": 19450
    },
    {
      "epoch": 19.47,
      "grad_norm": 9997.7861328125,
      "learning_rate": 2.7296181630546955e-06,
      "loss": 33.251,
      "step": 19451
    },
    {
      "epoch": 19.47,
      "grad_norm": 6782.0771484375,
      "learning_rate": 2.7244582043343658e-06,
      "loss": 57.0311,
      "step": 19452
    },
    {
      "epoch": 19.47,
      "grad_norm": 4967.00341796875,
      "learning_rate": 2.719298245614035e-06,
      "loss": 36.7846,
      "step": 19453
    },
    {
      "epoch": 19.47,
      "grad_norm": 61095.1171875,
      "learning_rate": 2.714138286893705e-06,
      "loss": 55.8502,
      "step": 19454
    },
    {
      "epoch": 19.47,
      "grad_norm": 28094.087890625,
      "learning_rate": 2.708978328173375e-06,
      "loss": 61.1102,
      "step": 19455
    },
    {
      "epoch": 19.48,
      "grad_norm": 18412.029296875,
      "learning_rate": 2.7038183694530443e-06,
      "loss": 41.2753,
      "step": 19456
    },
    {
      "epoch": 19.48,
      "grad_norm": 1949.2716064453125,
      "learning_rate": 2.698658410732714e-06,
      "loss": 61.4304,
      "step": 19457
    },
    {
      "epoch": 19.48,
      "grad_norm": 2264.4404296875,
      "learning_rate": 2.693498452012384e-06,
      "loss": 65.7179,
      "step": 19458
    },
    {
      "epoch": 19.48,
      "grad_norm": 3168.587158203125,
      "learning_rate": 2.6883384932920535e-06,
      "loss": 61.4284,
      "step": 19459
    },
    {
      "epoch": 19.48,
      "grad_norm": 8917.7666015625,
      "learning_rate": 2.6831785345717234e-06,
      "loss": 59.6229,
      "step": 19460
    },
    {
      "epoch": 19.48,
      "grad_norm": 78611.25,
      "learning_rate": 2.6780185758513932e-06,
      "loss": 50.3664,
      "step": 19461
    },
    {
      "epoch": 19.48,
      "grad_norm": 3391.68359375,
      "learning_rate": 2.672858617131063e-06,
      "loss": 59.1194,
      "step": 19462
    },
    {
      "epoch": 19.48,
      "grad_norm": 14388.357421875,
      "learning_rate": 2.667698658410733e-06,
      "loss": 44.1398,
      "step": 19463
    },
    {
      "epoch": 19.48,
      "grad_norm": 8723.826171875,
      "learning_rate": 2.662538699690403e-06,
      "loss": 63.5364,
      "step": 19464
    },
    {
      "epoch": 19.48,
      "grad_norm": 179680.9375,
      "learning_rate": 2.6573787409700722e-06,
      "loss": 56.7043,
      "step": 19465
    },
    {
      "epoch": 19.49,
      "grad_norm": 7311.7470703125,
      "learning_rate": 2.652218782249742e-06,
      "loss": 54.4799,
      "step": 19466
    },
    {
      "epoch": 19.49,
      "grad_norm": 2826.2919921875,
      "learning_rate": 2.647058823529412e-06,
      "loss": 60.8207,
      "step": 19467
    },
    {
      "epoch": 19.49,
      "grad_norm": 87665.71875,
      "learning_rate": 2.6418988648090814e-06,
      "loss": 41.8988,
      "step": 19468
    },
    {
      "epoch": 19.49,
      "grad_norm": 3395.357177734375,
      "learning_rate": 2.6367389060887513e-06,
      "loss": 60.5688,
      "step": 19469
    },
    {
      "epoch": 19.49,
      "grad_norm": 6376.23046875,
      "learning_rate": 2.631578947368421e-06,
      "loss": 58.1518,
      "step": 19470
    },
    {
      "epoch": 19.49,
      "grad_norm": 2758.167236328125,
      "learning_rate": 2.626418988648091e-06,
      "loss": 59.7979,
      "step": 19471
    },
    {
      "epoch": 19.49,
      "grad_norm": 5879.34228515625,
      "learning_rate": 2.621259029927761e-06,
      "loss": 62.0006,
      "step": 19472
    },
    {
      "epoch": 19.49,
      "grad_norm": 1300.93896484375,
      "learning_rate": 2.6160990712074307e-06,
      "loss": 65.8104,
      "step": 19473
    },
    {
      "epoch": 19.49,
      "grad_norm": 6585.48486328125,
      "learning_rate": 2.6109391124871e-06,
      "loss": 50.6311,
      "step": 19474
    },
    {
      "epoch": 19.49,
      "grad_norm": 5693.56787109375,
      "learning_rate": 2.60577915376677e-06,
      "loss": 51.8247,
      "step": 19475
    },
    {
      "epoch": 19.5,
      "grad_norm": 11289.330078125,
      "learning_rate": 2.60061919504644e-06,
      "loss": 57.455,
      "step": 19476
    },
    {
      "epoch": 19.5,
      "grad_norm": 3293.686279296875,
      "learning_rate": 2.5954592363261093e-06,
      "loss": 58.8683,
      "step": 19477
    },
    {
      "epoch": 19.5,
      "grad_norm": 448.5351867675781,
      "learning_rate": 2.590299277605779e-06,
      "loss": 64.6988,
      "step": 19478
    },
    {
      "epoch": 19.5,
      "grad_norm": 22237.34375,
      "learning_rate": 2.585139318885449e-06,
      "loss": 58.1549,
      "step": 19479
    },
    {
      "epoch": 19.5,
      "grad_norm": 8363.763671875,
      "learning_rate": 2.5799793601651184e-06,
      "loss": 59.6089,
      "step": 19480
    },
    {
      "epoch": 19.5,
      "grad_norm": 26961.533203125,
      "learning_rate": 2.5748194014447887e-06,
      "loss": 56.4544,
      "step": 19481
    },
    {
      "epoch": 19.5,
      "grad_norm": 11635.5224609375,
      "learning_rate": 2.5696594427244586e-06,
      "loss": 64.0272,
      "step": 19482
    },
    {
      "epoch": 19.5,
      "grad_norm": 7715.15380859375,
      "learning_rate": 2.564499484004128e-06,
      "loss": 54.0714,
      "step": 19483
    },
    {
      "epoch": 19.5,
      "grad_norm": 45080.7421875,
      "learning_rate": 2.559339525283798e-06,
      "loss": 45.5594,
      "step": 19484
    },
    {
      "epoch": 19.5,
      "grad_norm": 56241.06640625,
      "learning_rate": 2.5541795665634677e-06,
      "loss": 29.9839,
      "step": 19485
    },
    {
      "epoch": 19.51,
      "grad_norm": 2252.25537109375,
      "learning_rate": 2.549019607843137e-06,
      "loss": 59.985,
      "step": 19486
    },
    {
      "epoch": 19.51,
      "grad_norm": 21112.369140625,
      "learning_rate": 2.543859649122807e-06,
      "loss": 66.971,
      "step": 19487
    },
    {
      "epoch": 19.51,
      "grad_norm": 10976.390625,
      "learning_rate": 2.538699690402477e-06,
      "loss": 67.8607,
      "step": 19488
    },
    {
      "epoch": 19.51,
      "grad_norm": 5462.81591796875,
      "learning_rate": 2.5335397316821463e-06,
      "loss": 58.8527,
      "step": 19489
    },
    {
      "epoch": 19.51,
      "grad_norm": 2438.423583984375,
      "learning_rate": 2.5283797729618166e-06,
      "loss": 55.978,
      "step": 19490
    },
    {
      "epoch": 19.51,
      "grad_norm": 37846.38671875,
      "learning_rate": 2.5232198142414865e-06,
      "loss": 58.4386,
      "step": 19491
    },
    {
      "epoch": 19.51,
      "grad_norm": 2699.73779296875,
      "learning_rate": 2.518059855521156e-06,
      "loss": 56.8413,
      "step": 19492
    },
    {
      "epoch": 19.51,
      "grad_norm": 13499.4521484375,
      "learning_rate": 2.5128998968008258e-06,
      "loss": 38.9682,
      "step": 19493
    },
    {
      "epoch": 19.51,
      "grad_norm": 1449.93408203125,
      "learning_rate": 2.5077399380804956e-06,
      "loss": 59.1662,
      "step": 19494
    },
    {
      "epoch": 19.51,
      "grad_norm": 1732.5115966796875,
      "learning_rate": 2.502579979360165e-06,
      "loss": 53.733,
      "step": 19495
    },
    {
      "epoch": 19.52,
      "grad_norm": 4948.87890625,
      "learning_rate": 2.497420020639835e-06,
      "loss": 54.1541,
      "step": 19496
    },
    {
      "epoch": 19.52,
      "grad_norm": 8061.75439453125,
      "learning_rate": 2.4922600619195048e-06,
      "loss": 65.0469,
      "step": 19497
    },
    {
      "epoch": 19.52,
      "grad_norm": 18967.576171875,
      "learning_rate": 2.4871001031991742e-06,
      "loss": 62.4542,
      "step": 19498
    },
    {
      "epoch": 19.52,
      "grad_norm": 16496.8203125,
      "learning_rate": 2.481940144478844e-06,
      "loss": 62.5417,
      "step": 19499
    },
    {
      "epoch": 19.52,
      "grad_norm": 9444.7724609375,
      "learning_rate": 2.4767801857585144e-06,
      "loss": 63.1304,
      "step": 19500
    },
    {
      "epoch": 19.52,
      "grad_norm": 3836.8779296875,
      "learning_rate": 2.471620227038184e-06,
      "loss": 53.6621,
      "step": 19501
    },
    {
      "epoch": 19.52,
      "grad_norm": 2121.760009765625,
      "learning_rate": 2.4664602683178537e-06,
      "loss": 58.1976,
      "step": 19502
    },
    {
      "epoch": 19.52,
      "grad_norm": 4631.4482421875,
      "learning_rate": 2.4613003095975235e-06,
      "loss": 52.5786,
      "step": 19503
    },
    {
      "epoch": 19.52,
      "grad_norm": 1657.00439453125,
      "learning_rate": 2.456140350877193e-06,
      "loss": 54.1196,
      "step": 19504
    },
    {
      "epoch": 19.52,
      "grad_norm": 13038.0810546875,
      "learning_rate": 2.450980392156863e-06,
      "loss": 54.178,
      "step": 19505
    },
    {
      "epoch": 19.53,
      "grad_norm": 15537.1357421875,
      "learning_rate": 2.4458204334365327e-06,
      "loss": 58.4668,
      "step": 19506
    },
    {
      "epoch": 19.53,
      "grad_norm": 26092.1328125,
      "learning_rate": 2.440660474716202e-06,
      "loss": 60.9863,
      "step": 19507
    },
    {
      "epoch": 19.53,
      "grad_norm": 2757.970458984375,
      "learning_rate": 2.435500515995872e-06,
      "loss": 55.6591,
      "step": 19508
    },
    {
      "epoch": 19.53,
      "grad_norm": 37707.921875,
      "learning_rate": 2.4303405572755423e-06,
      "loss": 58.3074,
      "step": 19509
    },
    {
      "epoch": 19.53,
      "grad_norm": 6870.71240234375,
      "learning_rate": 2.4251805985552117e-06,
      "loss": 64.7887,
      "step": 19510
    },
    {
      "epoch": 19.53,
      "grad_norm": 5479.46533203125,
      "learning_rate": 2.4200206398348815e-06,
      "loss": 65.4571,
      "step": 19511
    },
    {
      "epoch": 19.53,
      "grad_norm": 5798.673828125,
      "learning_rate": 2.4148606811145514e-06,
      "loss": 62.1525,
      "step": 19512
    },
    {
      "epoch": 19.53,
      "grad_norm": 3322.119140625,
      "learning_rate": 2.409700722394221e-06,
      "loss": 64.3434,
      "step": 19513
    },
    {
      "epoch": 19.53,
      "grad_norm": 6364.943359375,
      "learning_rate": 2.4045407636738907e-06,
      "loss": 56.7782,
      "step": 19514
    },
    {
      "epoch": 19.53,
      "grad_norm": 4912.99365234375,
      "learning_rate": 2.3993808049535606e-06,
      "loss": 54.7877,
      "step": 19515
    },
    {
      "epoch": 19.54,
      "grad_norm": 9149.4775390625,
      "learning_rate": 2.39422084623323e-06,
      "loss": 55.0937,
      "step": 19516
    },
    {
      "epoch": 19.54,
      "grad_norm": 5743.890625,
      "learning_rate": 2.3890608875129e-06,
      "loss": 52.5987,
      "step": 19517
    },
    {
      "epoch": 19.54,
      "grad_norm": 26062.72265625,
      "learning_rate": 2.3839009287925697e-06,
      "loss": 33.0648,
      "step": 19518
    },
    {
      "epoch": 19.54,
      "grad_norm": 1458.8759765625,
      "learning_rate": 2.3787409700722396e-06,
      "loss": 64.003,
      "step": 19519
    },
    {
      "epoch": 19.54,
      "grad_norm": 3417.63623046875,
      "learning_rate": 2.3735810113519094e-06,
      "loss": 67.5852,
      "step": 19520
    },
    {
      "epoch": 19.54,
      "grad_norm": 6503.5546875,
      "learning_rate": 2.3684210526315793e-06,
      "loss": 56.7423,
      "step": 19521
    },
    {
      "epoch": 19.54,
      "grad_norm": 9346.6865234375,
      "learning_rate": 2.3632610939112487e-06,
      "loss": 62.7908,
      "step": 19522
    },
    {
      "epoch": 19.54,
      "grad_norm": 1136.4981689453125,
      "learning_rate": 2.3581011351909186e-06,
      "loss": 55.5227,
      "step": 19523
    },
    {
      "epoch": 19.54,
      "grad_norm": 2700.406982421875,
      "learning_rate": 2.3529411764705885e-06,
      "loss": 68.6371,
      "step": 19524
    },
    {
      "epoch": 19.54,
      "grad_norm": 1373.591064453125,
      "learning_rate": 2.347781217750258e-06,
      "loss": 61.0404,
      "step": 19525
    },
    {
      "epoch": 19.55,
      "grad_norm": 1859.6759033203125,
      "learning_rate": 2.3426212590299277e-06,
      "loss": 57.5013,
      "step": 19526
    },
    {
      "epoch": 19.55,
      "grad_norm": 65915.6015625,
      "learning_rate": 2.3374613003095976e-06,
      "loss": 59.7308,
      "step": 19527
    },
    {
      "epoch": 19.55,
      "grad_norm": 2734.12890625,
      "learning_rate": 2.3323013415892675e-06,
      "loss": 57.8327,
      "step": 19528
    },
    {
      "epoch": 19.55,
      "grad_norm": 5593.2890625,
      "learning_rate": 2.3271413828689373e-06,
      "loss": 45.9149,
      "step": 19529
    },
    {
      "epoch": 19.55,
      "grad_norm": 3550.42919921875,
      "learning_rate": 2.321981424148607e-06,
      "loss": 51.7191,
      "step": 19530
    },
    {
      "epoch": 19.55,
      "grad_norm": 6370.32470703125,
      "learning_rate": 2.3168214654282766e-06,
      "loss": 65.1381,
      "step": 19531
    },
    {
      "epoch": 19.55,
      "grad_norm": 14363.9189453125,
      "learning_rate": 2.3116615067079465e-06,
      "loss": 49.1108,
      "step": 19532
    },
    {
      "epoch": 19.55,
      "grad_norm": 7855.91259765625,
      "learning_rate": 2.3065015479876163e-06,
      "loss": 62.1353,
      "step": 19533
    },
    {
      "epoch": 19.55,
      "grad_norm": 16652.275390625,
      "learning_rate": 2.3013415892672858e-06,
      "loss": 58.7264,
      "step": 19534
    },
    {
      "epoch": 19.55,
      "grad_norm": 16636.43359375,
      "learning_rate": 2.2961816305469556e-06,
      "loss": 57.9973,
      "step": 19535
    },
    {
      "epoch": 19.56,
      "grad_norm": 4352.36669921875,
      "learning_rate": 2.2910216718266255e-06,
      "loss": 50.5773,
      "step": 19536
    },
    {
      "epoch": 19.56,
      "grad_norm": 15630.98828125,
      "learning_rate": 2.285861713106295e-06,
      "loss": 45.2199,
      "step": 19537
    },
    {
      "epoch": 19.56,
      "grad_norm": 1839.8197021484375,
      "learning_rate": 2.2807017543859652e-06,
      "loss": 52.1939,
      "step": 19538
    },
    {
      "epoch": 19.56,
      "grad_norm": 136726.71875,
      "learning_rate": 2.275541795665635e-06,
      "loss": 34.876,
      "step": 19539
    },
    {
      "epoch": 19.56,
      "grad_norm": 6113.83935546875,
      "learning_rate": 2.2703818369453045e-06,
      "loss": 61.9983,
      "step": 19540
    },
    {
      "epoch": 19.56,
      "grad_norm": 1425.8623046875,
      "learning_rate": 2.2652218782249744e-06,
      "loss": 59.7673,
      "step": 19541
    },
    {
      "epoch": 19.56,
      "grad_norm": 13880.1826171875,
      "learning_rate": 2.2600619195046442e-06,
      "loss": 60.0948,
      "step": 19542
    },
    {
      "epoch": 19.56,
      "grad_norm": 44602.765625,
      "learning_rate": 2.2549019607843137e-06,
      "loss": 59.3138,
      "step": 19543
    },
    {
      "epoch": 19.56,
      "grad_norm": 20874.025390625,
      "learning_rate": 2.2497420020639835e-06,
      "loss": 59.0119,
      "step": 19544
    },
    {
      "epoch": 19.56,
      "grad_norm": 13461.3515625,
      "learning_rate": 2.2445820433436534e-06,
      "loss": 64.3425,
      "step": 19545
    },
    {
      "epoch": 19.57,
      "grad_norm": 47379.046875,
      "learning_rate": 2.239422084623323e-06,
      "loss": 62.2917,
      "step": 19546
    },
    {
      "epoch": 19.57,
      "grad_norm": 16453.82421875,
      "learning_rate": 2.234262125902993e-06,
      "loss": 56.2486,
      "step": 19547
    },
    {
      "epoch": 19.57,
      "grad_norm": 7668.85888671875,
      "learning_rate": 2.229102167182663e-06,
      "loss": 58.5199,
      "step": 19548
    },
    {
      "epoch": 19.57,
      "grad_norm": 10083.330078125,
      "learning_rate": 2.2239422084623324e-06,
      "loss": 56.2758,
      "step": 19549
    },
    {
      "epoch": 19.57,
      "grad_norm": 3355.966552734375,
      "learning_rate": 2.2187822497420023e-06,
      "loss": 59.1746,
      "step": 19550
    },
    {
      "epoch": 19.57,
      "grad_norm": 4516.41357421875,
      "learning_rate": 2.213622291021672e-06,
      "loss": 56.4689,
      "step": 19551
    },
    {
      "epoch": 19.57,
      "grad_norm": 13463.521484375,
      "learning_rate": 2.2084623323013416e-06,
      "loss": 60.0136,
      "step": 19552
    },
    {
      "epoch": 19.57,
      "grad_norm": 30098.216796875,
      "learning_rate": 2.2033023735810114e-06,
      "loss": 52.7867,
      "step": 19553
    },
    {
      "epoch": 19.57,
      "grad_norm": 2513.44287109375,
      "learning_rate": 2.1981424148606813e-06,
      "loss": 54.9662,
      "step": 19554
    },
    {
      "epoch": 19.57,
      "grad_norm": 105755.15625,
      "learning_rate": 2.1929824561403507e-06,
      "loss": 48.0569,
      "step": 19555
    },
    {
      "epoch": 19.58,
      "grad_norm": 2809.30078125,
      "learning_rate": 2.1878224974200206e-06,
      "loss": 57.1811,
      "step": 19556
    },
    {
      "epoch": 19.58,
      "grad_norm": 2812.8125,
      "learning_rate": 2.182662538699691e-06,
      "loss": 65.797,
      "step": 19557
    },
    {
      "epoch": 19.58,
      "grad_norm": 4827.02783203125,
      "learning_rate": 2.1775025799793603e-06,
      "loss": 58.9484,
      "step": 19558
    },
    {
      "epoch": 19.58,
      "grad_norm": 25935.427734375,
      "learning_rate": 2.17234262125903e-06,
      "loss": 63.0368,
      "step": 19559
    },
    {
      "epoch": 19.58,
      "grad_norm": 6873.21337890625,
      "learning_rate": 2.1671826625387e-06,
      "loss": 45.7734,
      "step": 19560
    },
    {
      "epoch": 19.58,
      "grad_norm": 13972.43359375,
      "learning_rate": 2.1620227038183694e-06,
      "loss": 48.4167,
      "step": 19561
    },
    {
      "epoch": 19.58,
      "grad_norm": 4860.14453125,
      "learning_rate": 2.1568627450980393e-06,
      "loss": 53.6485,
      "step": 19562
    },
    {
      "epoch": 19.58,
      "grad_norm": 12871.80859375,
      "learning_rate": 2.151702786377709e-06,
      "loss": 43.3652,
      "step": 19563
    },
    {
      "epoch": 19.58,
      "grad_norm": 1958.0262451171875,
      "learning_rate": 2.1465428276573786e-06,
      "loss": 63.2862,
      "step": 19564
    },
    {
      "epoch": 19.58,
      "grad_norm": 6756.93310546875,
      "learning_rate": 2.1413828689370485e-06,
      "loss": 64.4717,
      "step": 19565
    },
    {
      "epoch": 19.59,
      "grad_norm": 4563.25244140625,
      "learning_rate": 2.1362229102167187e-06,
      "loss": 60.0187,
      "step": 19566
    },
    {
      "epoch": 19.59,
      "grad_norm": 8613.3349609375,
      "learning_rate": 2.131062951496388e-06,
      "loss": 62.2331,
      "step": 19567
    },
    {
      "epoch": 19.59,
      "grad_norm": 14861.44140625,
      "learning_rate": 2.125902992776058e-06,
      "loss": 53.4055,
      "step": 19568
    },
    {
      "epoch": 19.59,
      "grad_norm": 6140.5341796875,
      "learning_rate": 2.120743034055728e-06,
      "loss": 54.3624,
      "step": 19569
    },
    {
      "epoch": 19.59,
      "grad_norm": 9980.541015625,
      "learning_rate": 2.1155830753353973e-06,
      "loss": 67.9881,
      "step": 19570
    },
    {
      "epoch": 19.59,
      "grad_norm": 7862.0263671875,
      "learning_rate": 2.110423116615067e-06,
      "loss": 61.3955,
      "step": 19571
    },
    {
      "epoch": 19.59,
      "grad_norm": 7465.50048828125,
      "learning_rate": 2.105263157894737e-06,
      "loss": 55.715,
      "step": 19572
    },
    {
      "epoch": 19.59,
      "grad_norm": 1424.0257568359375,
      "learning_rate": 2.1001031991744065e-06,
      "loss": 65.8206,
      "step": 19573
    },
    {
      "epoch": 19.59,
      "grad_norm": 8399.9892578125,
      "learning_rate": 2.0949432404540764e-06,
      "loss": 59.4569,
      "step": 19574
    },
    {
      "epoch": 19.59,
      "grad_norm": 12055.4619140625,
      "learning_rate": 2.0897832817337462e-06,
      "loss": 53.2071,
      "step": 19575
    },
    {
      "epoch": 19.6,
      "grad_norm": 5270.5029296875,
      "learning_rate": 2.084623323013416e-06,
      "loss": 58.7526,
      "step": 19576
    },
    {
      "epoch": 19.6,
      "grad_norm": 87913.046875,
      "learning_rate": 2.079463364293086e-06,
      "loss": 39.5574,
      "step": 19577
    },
    {
      "epoch": 19.6,
      "grad_norm": 1637.918701171875,
      "learning_rate": 2.074303405572756e-06,
      "loss": 54.6491,
      "step": 19578
    },
    {
      "epoch": 19.6,
      "grad_norm": 5138.486328125,
      "learning_rate": 2.0691434468524252e-06,
      "loss": 58.7192,
      "step": 19579
    },
    {
      "epoch": 19.6,
      "grad_norm": 7193.6025390625,
      "learning_rate": 2.063983488132095e-06,
      "loss": 62.7175,
      "step": 19580
    },
    {
      "epoch": 19.6,
      "grad_norm": 2808.544189453125,
      "learning_rate": 2.058823529411765e-06,
      "loss": 63.4319,
      "step": 19581
    },
    {
      "epoch": 19.6,
      "grad_norm": 11352.4931640625,
      "learning_rate": 2.0536635706914344e-06,
      "loss": 60.3113,
      "step": 19582
    },
    {
      "epoch": 19.6,
      "grad_norm": 3810.02001953125,
      "learning_rate": 2.0485036119711042e-06,
      "loss": 62.6736,
      "step": 19583
    },
    {
      "epoch": 19.6,
      "grad_norm": 27784.5234375,
      "learning_rate": 2.043343653250774e-06,
      "loss": 33.6596,
      "step": 19584
    },
    {
      "epoch": 19.6,
      "grad_norm": 9848.9013671875,
      "learning_rate": 2.038183694530444e-06,
      "loss": 47.4863,
      "step": 19585
    },
    {
      "epoch": 19.61,
      "grad_norm": 1090.30859375,
      "learning_rate": 2.033023735810114e-06,
      "loss": 60.4718,
      "step": 19586
    },
    {
      "epoch": 19.61,
      "grad_norm": 2054.017822265625,
      "learning_rate": 2.0278637770897837e-06,
      "loss": 54.2053,
      "step": 19587
    },
    {
      "epoch": 19.61,
      "grad_norm": 8703.9404296875,
      "learning_rate": 2.022703818369453e-06,
      "loss": 56.9097,
      "step": 19588
    },
    {
      "epoch": 19.61,
      "grad_norm": 7012.43310546875,
      "learning_rate": 2.017543859649123e-06,
      "loss": 61.8003,
      "step": 19589
    },
    {
      "epoch": 19.61,
      "grad_norm": 2136.08740234375,
      "learning_rate": 2.012383900928793e-06,
      "loss": 58.8053,
      "step": 19590
    },
    {
      "epoch": 19.61,
      "grad_norm": 18167.39453125,
      "learning_rate": 2.0072239422084623e-06,
      "loss": 54.5294,
      "step": 19591
    },
    {
      "epoch": 19.61,
      "grad_norm": 8494.9677734375,
      "learning_rate": 2.002063983488132e-06,
      "loss": 56.6149,
      "step": 19592
    },
    {
      "epoch": 19.61,
      "grad_norm": 44716.44921875,
      "learning_rate": 1.996904024767802e-06,
      "loss": 52.3103,
      "step": 19593
    },
    {
      "epoch": 19.61,
      "grad_norm": 123365.140625,
      "learning_rate": 1.9917440660474714e-06,
      "loss": 50.9617,
      "step": 19594
    },
    {
      "epoch": 19.61,
      "grad_norm": 11371.5009765625,
      "learning_rate": 1.9865841073271417e-06,
      "loss": 61.5439,
      "step": 19595
    },
    {
      "epoch": 19.62,
      "grad_norm": 12155.041015625,
      "learning_rate": 1.981424148606811e-06,
      "loss": 25.844,
      "step": 19596
    },
    {
      "epoch": 19.62,
      "grad_norm": 7954.3505859375,
      "learning_rate": 1.976264189886481e-06,
      "loss": 57.5152,
      "step": 19597
    },
    {
      "epoch": 19.62,
      "grad_norm": 9838.751953125,
      "learning_rate": 1.971104231166151e-06,
      "loss": 60.5306,
      "step": 19598
    },
    {
      "epoch": 19.62,
      "grad_norm": 4925.93798828125,
      "learning_rate": 1.9659442724458207e-06,
      "loss": 56.0751,
      "step": 19599
    },
    {
      "epoch": 19.62,
      "grad_norm": 30219.05859375,
      "learning_rate": 1.96078431372549e-06,
      "loss": 55.4757,
      "step": 19600
    },
    {
      "epoch": 19.62,
      "grad_norm": 26767.501953125,
      "learning_rate": 1.95562435500516e-06,
      "loss": 57.0977,
      "step": 19601
    },
    {
      "epoch": 19.62,
      "grad_norm": 1029.035400390625,
      "learning_rate": 1.95046439628483e-06,
      "loss": 64.8436,
      "step": 19602
    },
    {
      "epoch": 19.62,
      "grad_norm": 11676.7783203125,
      "learning_rate": 1.9453044375644993e-06,
      "loss": 62.5783,
      "step": 19603
    },
    {
      "epoch": 19.62,
      "grad_norm": 1382.8531494140625,
      "learning_rate": 1.940144478844169e-06,
      "loss": 53.6101,
      "step": 19604
    },
    {
      "epoch": 19.62,
      "grad_norm": 2585.112060546875,
      "learning_rate": 1.934984520123839e-06,
      "loss": 61.6746,
      "step": 19605
    },
    {
      "epoch": 19.63,
      "grad_norm": 2004.687255859375,
      "learning_rate": 1.929824561403509e-06,
      "loss": 50.3293,
      "step": 19606
    },
    {
      "epoch": 19.63,
      "grad_norm": 1453.7178955078125,
      "learning_rate": 1.9246646026831788e-06,
      "loss": 62.0365,
      "step": 19607
    },
    {
      "epoch": 19.63,
      "grad_norm": 3865.68505859375,
      "learning_rate": 1.9195046439628486e-06,
      "loss": 60.6046,
      "step": 19608
    },
    {
      "epoch": 19.63,
      "grad_norm": 30266.51171875,
      "learning_rate": 1.914344685242518e-06,
      "loss": 56.7361,
      "step": 19609
    },
    {
      "epoch": 19.63,
      "grad_norm": 14208.380859375,
      "learning_rate": 1.909184726522188e-06,
      "loss": 65.9259,
      "step": 19610
    },
    {
      "epoch": 19.63,
      "grad_norm": 2694.645751953125,
      "learning_rate": 1.9040247678018576e-06,
      "loss": 61.1207,
      "step": 19611
    },
    {
      "epoch": 19.63,
      "grad_norm": 7799.53173828125,
      "learning_rate": 1.8988648090815274e-06,
      "loss": 35.0238,
      "step": 19612
    },
    {
      "epoch": 19.63,
      "grad_norm": 7117.1748046875,
      "learning_rate": 1.893704850361197e-06,
      "loss": 50.5151,
      "step": 19613
    },
    {
      "epoch": 19.63,
      "grad_norm": 4806.35595703125,
      "learning_rate": 1.8885448916408671e-06,
      "loss": 60.8563,
      "step": 19614
    },
    {
      "epoch": 19.63,
      "grad_norm": 25471.884765625,
      "learning_rate": 1.8833849329205368e-06,
      "loss": 63.4301,
      "step": 19615
    },
    {
      "epoch": 19.64,
      "grad_norm": 34985.77734375,
      "learning_rate": 1.8782249742002066e-06,
      "loss": 57.299,
      "step": 19616
    },
    {
      "epoch": 19.64,
      "grad_norm": 12254.8583984375,
      "learning_rate": 1.8730650154798763e-06,
      "loss": 63.0643,
      "step": 19617
    },
    {
      "epoch": 19.64,
      "grad_norm": 5143.42138671875,
      "learning_rate": 1.867905056759546e-06,
      "loss": 62.3311,
      "step": 19618
    },
    {
      "epoch": 19.64,
      "grad_norm": 12594.5205078125,
      "learning_rate": 1.8627450980392158e-06,
      "loss": 65.5398,
      "step": 19619
    },
    {
      "epoch": 19.64,
      "grad_norm": 46217.1328125,
      "learning_rate": 1.8575851393188855e-06,
      "loss": 56.9774,
      "step": 19620
    },
    {
      "epoch": 19.64,
      "grad_norm": 2749.861572265625,
      "learning_rate": 1.852425180598555e-06,
      "loss": 62.2612,
      "step": 19621
    },
    {
      "epoch": 19.64,
      "grad_norm": 4366.99365234375,
      "learning_rate": 1.847265221878225e-06,
      "loss": 61.7378,
      "step": 19622
    },
    {
      "epoch": 19.64,
      "grad_norm": 2900.73779296875,
      "learning_rate": 1.8421052631578946e-06,
      "loss": 58.8421,
      "step": 19623
    },
    {
      "epoch": 19.64,
      "grad_norm": 11140.6640625,
      "learning_rate": 1.8369453044375647e-06,
      "loss": 52.9236,
      "step": 19624
    },
    {
      "epoch": 19.64,
      "grad_norm": 17484.458984375,
      "learning_rate": 1.8317853457172345e-06,
      "loss": 56.0435,
      "step": 19625
    },
    {
      "epoch": 19.65,
      "grad_norm": 28358.513671875,
      "learning_rate": 1.8266253869969042e-06,
      "loss": 62.3276,
      "step": 19626
    },
    {
      "epoch": 19.65,
      "grad_norm": 49108.6796875,
      "learning_rate": 1.8214654282765738e-06,
      "loss": 48.2931,
      "step": 19627
    },
    {
      "epoch": 19.65,
      "grad_norm": 1905.38671875,
      "learning_rate": 1.8163054695562437e-06,
      "loss": 64.7384,
      "step": 19628
    },
    {
      "epoch": 19.65,
      "grad_norm": 5391.09130859375,
      "learning_rate": 1.8111455108359133e-06,
      "loss": 59.7083,
      "step": 19629
    },
    {
      "epoch": 19.65,
      "grad_norm": 8580.46875,
      "learning_rate": 1.805985552115583e-06,
      "loss": 63.9502,
      "step": 19630
    },
    {
      "epoch": 19.65,
      "grad_norm": 4512.13671875,
      "learning_rate": 1.8008255933952529e-06,
      "loss": 51.7291,
      "step": 19631
    },
    {
      "epoch": 19.65,
      "grad_norm": 4313.33837890625,
      "learning_rate": 1.7956656346749225e-06,
      "loss": 55.4755,
      "step": 19632
    },
    {
      "epoch": 19.65,
      "grad_norm": 5609.21728515625,
      "learning_rate": 1.7905056759545926e-06,
      "loss": 58.1522,
      "step": 19633
    },
    {
      "epoch": 19.65,
      "grad_norm": 9189.86328125,
      "learning_rate": 1.7853457172342624e-06,
      "loss": 53.8603,
      "step": 19634
    },
    {
      "epoch": 19.65,
      "grad_norm": 24994.859375,
      "learning_rate": 1.780185758513932e-06,
      "loss": 60.8788,
      "step": 19635
    },
    {
      "epoch": 19.66,
      "grad_norm": 2691.01513671875,
      "learning_rate": 1.7750257997936017e-06,
      "loss": 60.9911,
      "step": 19636
    },
    {
      "epoch": 19.66,
      "grad_norm": 4593.22021484375,
      "learning_rate": 1.7698658410732716e-06,
      "loss": 56.6966,
      "step": 19637
    },
    {
      "epoch": 19.66,
      "grad_norm": 85093.2265625,
      "learning_rate": 1.7647058823529412e-06,
      "loss": 51.7866,
      "step": 19638
    },
    {
      "epoch": 19.66,
      "grad_norm": 1815.9150390625,
      "learning_rate": 1.7595459236326109e-06,
      "loss": 66.5554,
      "step": 19639
    },
    {
      "epoch": 19.66,
      "grad_norm": 23121.77734375,
      "learning_rate": 1.7543859649122807e-06,
      "loss": 64.6082,
      "step": 19640
    },
    {
      "epoch": 19.66,
      "grad_norm": 5487.54345703125,
      "learning_rate": 1.7492260061919504e-06,
      "loss": 55.9147,
      "step": 19641
    },
    {
      "epoch": 19.66,
      "grad_norm": 55113.4453125,
      "learning_rate": 1.74406604747162e-06,
      "loss": 60.754,
      "step": 19642
    },
    {
      "epoch": 19.66,
      "grad_norm": 2315.55224609375,
      "learning_rate": 1.7389060887512901e-06,
      "loss": 59.6531,
      "step": 19643
    },
    {
      "epoch": 19.66,
      "grad_norm": 10553.45703125,
      "learning_rate": 1.73374613003096e-06,
      "loss": 50.9276,
      "step": 19644
    },
    {
      "epoch": 19.66,
      "grad_norm": 3880.56298828125,
      "learning_rate": 1.7285861713106296e-06,
      "loss": 50.5464,
      "step": 19645
    },
    {
      "epoch": 19.67,
      "grad_norm": 1574.0052490234375,
      "learning_rate": 1.7234262125902995e-06,
      "loss": 52.7092,
      "step": 19646
    },
    {
      "epoch": 19.67,
      "grad_norm": 2534.973388671875,
      "learning_rate": 1.7182662538699691e-06,
      "loss": 61.7876,
      "step": 19647
    },
    {
      "epoch": 19.67,
      "grad_norm": 6548.98388671875,
      "learning_rate": 1.7131062951496388e-06,
      "loss": 45.1571,
      "step": 19648
    },
    {
      "epoch": 19.67,
      "grad_norm": 2496.529541015625,
      "learning_rate": 1.7079463364293086e-06,
      "loss": 59.486,
      "step": 19649
    },
    {
      "epoch": 19.67,
      "grad_norm": 8584.044921875,
      "learning_rate": 1.7027863777089783e-06,
      "loss": 60.3677,
      "step": 19650
    },
    {
      "epoch": 19.67,
      "grad_norm": 6249.47216796875,
      "learning_rate": 1.697626418988648e-06,
      "loss": 61.7316,
      "step": 19651
    },
    {
      "epoch": 19.67,
      "grad_norm": 4835.47021484375,
      "learning_rate": 1.692466460268318e-06,
      "loss": 64.5727,
      "step": 19652
    },
    {
      "epoch": 19.67,
      "grad_norm": 31756.806640625,
      "learning_rate": 1.6873065015479879e-06,
      "loss": 50.7341,
      "step": 19653
    },
    {
      "epoch": 19.67,
      "grad_norm": 7044.06982421875,
      "learning_rate": 1.6821465428276575e-06,
      "loss": 50.8417,
      "step": 19654
    },
    {
      "epoch": 19.67,
      "grad_norm": 7732.75390625,
      "learning_rate": 1.6769865841073274e-06,
      "loss": 48.3308,
      "step": 19655
    },
    {
      "epoch": 19.68,
      "grad_norm": 76428.6171875,
      "learning_rate": 1.671826625386997e-06,
      "loss": 44.3761,
      "step": 19656
    },
    {
      "epoch": 19.68,
      "grad_norm": 11200.4384765625,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 58.3627,
      "step": 19657
    },
    {
      "epoch": 19.68,
      "grad_norm": 1318.0562744140625,
      "learning_rate": 1.6615067079463365e-06,
      "loss": 60.8731,
      "step": 19658
    },
    {
      "epoch": 19.68,
      "grad_norm": 62692.1640625,
      "learning_rate": 1.6563467492260062e-06,
      "loss": 56.0504,
      "step": 19659
    },
    {
      "epoch": 19.68,
      "grad_norm": 157160.71875,
      "learning_rate": 1.6511867905056758e-06,
      "loss": 47.1189,
      "step": 19660
    },
    {
      "epoch": 19.68,
      "grad_norm": 28430.30859375,
      "learning_rate": 1.6460268317853457e-06,
      "loss": 46.6908,
      "step": 19661
    },
    {
      "epoch": 19.68,
      "grad_norm": 1926.9957275390625,
      "learning_rate": 1.6408668730650157e-06,
      "loss": 60.2311,
      "step": 19662
    },
    {
      "epoch": 19.68,
      "grad_norm": 3395.759033203125,
      "learning_rate": 1.6357069143446854e-06,
      "loss": 58.1074,
      "step": 19663
    },
    {
      "epoch": 19.68,
      "grad_norm": 19246.021484375,
      "learning_rate": 1.6305469556243553e-06,
      "loss": 64.8403,
      "step": 19664
    },
    {
      "epoch": 19.68,
      "grad_norm": 3645.8720703125,
      "learning_rate": 1.625386996904025e-06,
      "loss": 57.224,
      "step": 19665
    },
    {
      "epoch": 19.69,
      "grad_norm": 1119.7100830078125,
      "learning_rate": 1.6202270381836946e-06,
      "loss": 60.843,
      "step": 19666
    },
    {
      "epoch": 19.69,
      "grad_norm": 12711.365234375,
      "learning_rate": 1.6150670794633644e-06,
      "loss": 55.9011,
      "step": 19667
    },
    {
      "epoch": 19.69,
      "grad_norm": 6238.68505859375,
      "learning_rate": 1.609907120743034e-06,
      "loss": 61.7594,
      "step": 19668
    },
    {
      "epoch": 19.69,
      "grad_norm": 3839.011962890625,
      "learning_rate": 1.6047471620227037e-06,
      "loss": 57.8633,
      "step": 19669
    },
    {
      "epoch": 19.69,
      "grad_norm": 976.5161743164062,
      "learning_rate": 1.5995872033023736e-06,
      "loss": 60.6399,
      "step": 19670
    },
    {
      "epoch": 19.69,
      "grad_norm": 10445.8740234375,
      "learning_rate": 1.5944272445820436e-06,
      "loss": 62.9548,
      "step": 19671
    },
    {
      "epoch": 19.69,
      "grad_norm": 6791.513671875,
      "learning_rate": 1.5892672858617133e-06,
      "loss": 65.9201,
      "step": 19672
    },
    {
      "epoch": 19.69,
      "grad_norm": 3790.89697265625,
      "learning_rate": 1.584107327141383e-06,
      "loss": 62.3288,
      "step": 19673
    },
    {
      "epoch": 19.69,
      "grad_norm": 3056.05517578125,
      "learning_rate": 1.5789473684210528e-06,
      "loss": 55.2447,
      "step": 19674
    },
    {
      "epoch": 19.69,
      "grad_norm": 7885.49609375,
      "learning_rate": 1.5737874097007224e-06,
      "loss": 53.215,
      "step": 19675
    },
    {
      "epoch": 19.7,
      "grad_norm": 1474.4071044921875,
      "learning_rate": 1.5686274509803923e-06,
      "loss": 64.1454,
      "step": 19676
    },
    {
      "epoch": 19.7,
      "grad_norm": 114788.484375,
      "learning_rate": 1.563467492260062e-06,
      "loss": 66.1208,
      "step": 19677
    },
    {
      "epoch": 19.7,
      "grad_norm": 29809.2890625,
      "learning_rate": 1.5583075335397318e-06,
      "loss": 51.6189,
      "step": 19678
    },
    {
      "epoch": 19.7,
      "grad_norm": 1874.7713623046875,
      "learning_rate": 1.5531475748194015e-06,
      "loss": 61.2843,
      "step": 19679
    },
    {
      "epoch": 19.7,
      "grad_norm": 1953.2200927734375,
      "learning_rate": 1.5479876160990713e-06,
      "loss": 58.6691,
      "step": 19680
    },
    {
      "epoch": 19.7,
      "grad_norm": 4843.580078125,
      "learning_rate": 1.542827657378741e-06,
      "loss": 50.305,
      "step": 19681
    },
    {
      "epoch": 19.7,
      "grad_norm": 29876.326171875,
      "learning_rate": 1.5376676986584108e-06,
      "loss": 52.4279,
      "step": 19682
    },
    {
      "epoch": 19.7,
      "grad_norm": 13996.5576171875,
      "learning_rate": 1.5325077399380807e-06,
      "loss": 60.2837,
      "step": 19683
    },
    {
      "epoch": 19.7,
      "grad_norm": 9507.0517578125,
      "learning_rate": 1.5273477812177503e-06,
      "loss": 40.3946,
      "step": 19684
    },
    {
      "epoch": 19.7,
      "grad_norm": 2805.12744140625,
      "learning_rate": 1.5221878224974202e-06,
      "loss": 48.0847,
      "step": 19685
    },
    {
      "epoch": 19.71,
      "grad_norm": 3074.4033203125,
      "learning_rate": 1.5170278637770898e-06,
      "loss": 62.0336,
      "step": 19686
    },
    {
      "epoch": 19.71,
      "grad_norm": 24689.439453125,
      "learning_rate": 1.5118679050567595e-06,
      "loss": 52.238,
      "step": 19687
    },
    {
      "epoch": 19.71,
      "grad_norm": 3029.640625,
      "learning_rate": 1.5067079463364293e-06,
      "loss": 61.268,
      "step": 19688
    },
    {
      "epoch": 19.71,
      "grad_norm": 24311.033203125,
      "learning_rate": 1.5015479876160992e-06,
      "loss": 63.9151,
      "step": 19689
    },
    {
      "epoch": 19.71,
      "grad_norm": 15598.8154296875,
      "learning_rate": 1.4963880288957689e-06,
      "loss": 39.6023,
      "step": 19690
    },
    {
      "epoch": 19.71,
      "grad_norm": 7062.31396484375,
      "learning_rate": 1.4912280701754387e-06,
      "loss": 57.8644,
      "step": 19691
    },
    {
      "epoch": 19.71,
      "grad_norm": 3696.44287109375,
      "learning_rate": 1.4860681114551084e-06,
      "loss": 58.7891,
      "step": 19692
    },
    {
      "epoch": 19.71,
      "grad_norm": 30806.330078125,
      "learning_rate": 1.4809081527347782e-06,
      "loss": 63.4325,
      "step": 19693
    },
    {
      "epoch": 19.71,
      "grad_norm": 404651.40625,
      "learning_rate": 1.4757481940144479e-06,
      "loss": 59.1172,
      "step": 19694
    },
    {
      "epoch": 19.71,
      "grad_norm": 52223.78515625,
      "learning_rate": 1.4705882352941177e-06,
      "loss": 51.9424,
      "step": 19695
    },
    {
      "epoch": 19.72,
      "grad_norm": 33199.83984375,
      "learning_rate": 1.4654282765737874e-06,
      "loss": 57.8264,
      "step": 19696
    },
    {
      "epoch": 19.72,
      "grad_norm": 645.6300659179688,
      "learning_rate": 1.4602683178534572e-06,
      "loss": 64.0215,
      "step": 19697
    },
    {
      "epoch": 19.72,
      "grad_norm": 8989.0703125,
      "learning_rate": 1.455108359133127e-06,
      "loss": 57.9981,
      "step": 19698
    },
    {
      "epoch": 19.72,
      "grad_norm": 1495.4532470703125,
      "learning_rate": 1.4499484004127967e-06,
      "loss": 56.2552,
      "step": 19699
    },
    {
      "epoch": 19.72,
      "grad_norm": 35502.15234375,
      "learning_rate": 1.4447884416924666e-06,
      "loss": 59.0305,
      "step": 19700
    },
    {
      "epoch": 19.72,
      "grad_norm": 15585.2431640625,
      "learning_rate": 1.4396284829721363e-06,
      "loss": 50.5241,
      "step": 19701
    },
    {
      "epoch": 19.72,
      "grad_norm": 75957.9921875,
      "learning_rate": 1.4344685242518061e-06,
      "loss": 36.1947,
      "step": 19702
    },
    {
      "epoch": 19.72,
      "grad_norm": 2904.25390625,
      "learning_rate": 1.4293085655314758e-06,
      "loss": 62.8281,
      "step": 19703
    },
    {
      "epoch": 19.72,
      "grad_norm": 7911.7626953125,
      "learning_rate": 1.4241486068111456e-06,
      "loss": 52.6234,
      "step": 19704
    },
    {
      "epoch": 19.72,
      "grad_norm": 1866.877197265625,
      "learning_rate": 1.4189886480908153e-06,
      "loss": 64.8926,
      "step": 19705
    },
    {
      "epoch": 19.73,
      "grad_norm": 1958.83349609375,
      "learning_rate": 1.4138286893704851e-06,
      "loss": 59.0655,
      "step": 19706
    },
    {
      "epoch": 19.73,
      "grad_norm": 10997.7568359375,
      "learning_rate": 1.408668730650155e-06,
      "loss": 43.6936,
      "step": 19707
    },
    {
      "epoch": 19.73,
      "grad_norm": 2658.693115234375,
      "learning_rate": 1.4035087719298246e-06,
      "loss": 65.0663,
      "step": 19708
    },
    {
      "epoch": 19.73,
      "grad_norm": 16195.5830078125,
      "learning_rate": 1.3983488132094943e-06,
      "loss": 56.0912,
      "step": 19709
    },
    {
      "epoch": 19.73,
      "grad_norm": 34441.91015625,
      "learning_rate": 1.3931888544891641e-06,
      "loss": 59.8004,
      "step": 19710
    },
    {
      "epoch": 19.73,
      "grad_norm": 15600.56640625,
      "learning_rate": 1.3880288957688338e-06,
      "loss": 62.2584,
      "step": 19711
    },
    {
      "epoch": 19.73,
      "grad_norm": 4693.58154296875,
      "learning_rate": 1.3828689370485037e-06,
      "loss": 59.8796,
      "step": 19712
    },
    {
      "epoch": 19.73,
      "grad_norm": 11069.2548828125,
      "learning_rate": 1.3777089783281735e-06,
      "loss": 60.0506,
      "step": 19713
    },
    {
      "epoch": 19.73,
      "grad_norm": 5300.3408203125,
      "learning_rate": 1.3725490196078432e-06,
      "loss": 58.7267,
      "step": 19714
    },
    {
      "epoch": 19.73,
      "grad_norm": 2685.703125,
      "learning_rate": 1.3673890608875128e-06,
      "loss": 60.0855,
      "step": 19715
    },
    {
      "epoch": 19.74,
      "grad_norm": 15797.6708984375,
      "learning_rate": 1.3622291021671829e-06,
      "loss": 53.0343,
      "step": 19716
    },
    {
      "epoch": 19.74,
      "grad_norm": 7302.123046875,
      "learning_rate": 1.3570691434468525e-06,
      "loss": 50.3617,
      "step": 19717
    },
    {
      "epoch": 19.74,
      "grad_norm": 33991.79296875,
      "learning_rate": 1.3519091847265222e-06,
      "loss": 46.0035,
      "step": 19718
    },
    {
      "epoch": 19.74,
      "grad_norm": 76769.296875,
      "learning_rate": 1.346749226006192e-06,
      "loss": 46.9297,
      "step": 19719
    },
    {
      "epoch": 19.74,
      "grad_norm": 15730.35546875,
      "learning_rate": 1.3415892672858617e-06,
      "loss": 52.1997,
      "step": 19720
    },
    {
      "epoch": 19.74,
      "grad_norm": 2776.527587890625,
      "learning_rate": 1.3364293085655315e-06,
      "loss": 61.7576,
      "step": 19721
    },
    {
      "epoch": 19.74,
      "grad_norm": 12567.23828125,
      "learning_rate": 1.3312693498452014e-06,
      "loss": 63.2611,
      "step": 19722
    },
    {
      "epoch": 19.74,
      "grad_norm": 99526.40625,
      "learning_rate": 1.326109391124871e-06,
      "loss": 55.7625,
      "step": 19723
    },
    {
      "epoch": 19.74,
      "grad_norm": 9994.7529296875,
      "learning_rate": 1.3209494324045407e-06,
      "loss": 57.1657,
      "step": 19724
    },
    {
      "epoch": 19.74,
      "grad_norm": 99862.8515625,
      "learning_rate": 1.3157894736842106e-06,
      "loss": 59.4209,
      "step": 19725
    },
    {
      "epoch": 19.75,
      "grad_norm": 3626.449951171875,
      "learning_rate": 1.3106295149638804e-06,
      "loss": 62.8687,
      "step": 19726
    },
    {
      "epoch": 19.75,
      "grad_norm": 17364.587890625,
      "learning_rate": 1.30546955624355e-06,
      "loss": 46.9562,
      "step": 19727
    },
    {
      "epoch": 19.75,
      "grad_norm": 2404.513671875,
      "learning_rate": 1.30030959752322e-06,
      "loss": 50.4691,
      "step": 19728
    },
    {
      "epoch": 19.75,
      "grad_norm": 4296.22900390625,
      "learning_rate": 1.2951496388028896e-06,
      "loss": 59.5102,
      "step": 19729
    },
    {
      "epoch": 19.75,
      "grad_norm": 3867.919677734375,
      "learning_rate": 1.2899896800825592e-06,
      "loss": 56.1512,
      "step": 19730
    },
    {
      "epoch": 19.75,
      "grad_norm": 18004.90625,
      "learning_rate": 1.2848297213622293e-06,
      "loss": 34.094,
      "step": 19731
    },
    {
      "epoch": 19.75,
      "grad_norm": 22997.263671875,
      "learning_rate": 1.279669762641899e-06,
      "loss": 53.668,
      "step": 19732
    },
    {
      "epoch": 19.75,
      "grad_norm": 61801.0078125,
      "learning_rate": 1.2745098039215686e-06,
      "loss": 56.9541,
      "step": 19733
    },
    {
      "epoch": 19.75,
      "grad_norm": 6187.72314453125,
      "learning_rate": 1.2693498452012384e-06,
      "loss": 52.5489,
      "step": 19734
    },
    {
      "epoch": 19.75,
      "grad_norm": 28788.833984375,
      "learning_rate": 1.2641898864809083e-06,
      "loss": 46.921,
      "step": 19735
    },
    {
      "epoch": 19.76,
      "grad_norm": 13927.8037109375,
      "learning_rate": 1.259029927760578e-06,
      "loss": 60.2931,
      "step": 19736
    },
    {
      "epoch": 19.76,
      "grad_norm": 4942.1171875,
      "learning_rate": 1.2538699690402478e-06,
      "loss": 66.9175,
      "step": 19737
    },
    {
      "epoch": 19.76,
      "grad_norm": 1592.42529296875,
      "learning_rate": 1.2487100103199175e-06,
      "loss": 56.015,
      "step": 19738
    },
    {
      "epoch": 19.76,
      "grad_norm": 6537.2041015625,
      "learning_rate": 1.2435500515995871e-06,
      "loss": 59.8104,
      "step": 19739
    },
    {
      "epoch": 19.76,
      "grad_norm": 7309.8916015625,
      "learning_rate": 1.2383900928792572e-06,
      "loss": 59.7597,
      "step": 19740
    },
    {
      "epoch": 19.76,
      "grad_norm": 2187.708740234375,
      "learning_rate": 1.2332301341589268e-06,
      "loss": 55.0505,
      "step": 19741
    },
    {
      "epoch": 19.76,
      "grad_norm": 31125.56640625,
      "learning_rate": 1.2280701754385965e-06,
      "loss": 59.5901,
      "step": 19742
    },
    {
      "epoch": 19.76,
      "grad_norm": 28065.31640625,
      "learning_rate": 1.2229102167182663e-06,
      "loss": 56.7599,
      "step": 19743
    },
    {
      "epoch": 19.76,
      "grad_norm": 59010.43359375,
      "learning_rate": 1.217750257997936e-06,
      "loss": 54.8694,
      "step": 19744
    },
    {
      "epoch": 19.76,
      "grad_norm": 3882.3388671875,
      "learning_rate": 1.2125902992776058e-06,
      "loss": 62.6378,
      "step": 19745
    },
    {
      "epoch": 19.77,
      "grad_norm": 3322.46240234375,
      "learning_rate": 1.2074303405572757e-06,
      "loss": 63.2114,
      "step": 19746
    },
    {
      "epoch": 19.77,
      "grad_norm": 29741.60546875,
      "learning_rate": 1.2022703818369454e-06,
      "loss": 27.1158,
      "step": 19747
    },
    {
      "epoch": 19.77,
      "grad_norm": 5933.935546875,
      "learning_rate": 1.197110423116615e-06,
      "loss": 60.2745,
      "step": 19748
    },
    {
      "epoch": 19.77,
      "grad_norm": 2336.73779296875,
      "learning_rate": 1.1919504643962849e-06,
      "loss": 59.2318,
      "step": 19749
    },
    {
      "epoch": 19.77,
      "grad_norm": 15624.7958984375,
      "learning_rate": 1.1867905056759547e-06,
      "loss": 52.5165,
      "step": 19750
    },
    {
      "epoch": 19.77,
      "grad_norm": 24844.46875,
      "learning_rate": 1.1816305469556244e-06,
      "loss": 53.4777,
      "step": 19751
    },
    {
      "epoch": 19.77,
      "grad_norm": 2082.400634765625,
      "learning_rate": 1.1764705882352942e-06,
      "loss": 57.3427,
      "step": 19752
    },
    {
      "epoch": 19.77,
      "grad_norm": 1784.0858154296875,
      "learning_rate": 1.1713106295149639e-06,
      "loss": 62.0671,
      "step": 19753
    },
    {
      "epoch": 19.77,
      "grad_norm": 3990.178955078125,
      "learning_rate": 1.1661506707946337e-06,
      "loss": 58.8908,
      "step": 19754
    },
    {
      "epoch": 19.77,
      "grad_norm": 2330.649169921875,
      "learning_rate": 1.1609907120743036e-06,
      "loss": 59.5906,
      "step": 19755
    },
    {
      "epoch": 19.78,
      "grad_norm": 2416.842529296875,
      "learning_rate": 1.1558307533539732e-06,
      "loss": 55.6098,
      "step": 19756
    },
    {
      "epoch": 19.78,
      "grad_norm": 14160.9599609375,
      "learning_rate": 1.1506707946336429e-06,
      "loss": 54.082,
      "step": 19757
    },
    {
      "epoch": 19.78,
      "grad_norm": 1341.3209228515625,
      "learning_rate": 1.1455108359133127e-06,
      "loss": 66.2788,
      "step": 19758
    },
    {
      "epoch": 19.78,
      "grad_norm": 4032.94482421875,
      "learning_rate": 1.1403508771929826e-06,
      "loss": 66.1526,
      "step": 19759
    },
    {
      "epoch": 19.78,
      "grad_norm": 31358.08203125,
      "learning_rate": 1.1351909184726523e-06,
      "loss": 57.3458,
      "step": 19760
    },
    {
      "epoch": 19.78,
      "grad_norm": 7613.4404296875,
      "learning_rate": 1.1300309597523221e-06,
      "loss": 45.021,
      "step": 19761
    },
    {
      "epoch": 19.78,
      "grad_norm": 29439.29296875,
      "learning_rate": 1.1248710010319918e-06,
      "loss": 59.6748,
      "step": 19762
    },
    {
      "epoch": 19.78,
      "grad_norm": 7168.029296875,
      "learning_rate": 1.1197110423116614e-06,
      "loss": 43.684,
      "step": 19763
    },
    {
      "epoch": 19.78,
      "grad_norm": 5841.5986328125,
      "learning_rate": 1.1145510835913315e-06,
      "loss": 60.16,
      "step": 19764
    },
    {
      "epoch": 19.78,
      "grad_norm": 2953.59716796875,
      "learning_rate": 1.1093911248710011e-06,
      "loss": 59.317,
      "step": 19765
    },
    {
      "epoch": 19.79,
      "grad_norm": 5684.62939453125,
      "learning_rate": 1.1042311661506708e-06,
      "loss": 59.9958,
      "step": 19766
    },
    {
      "epoch": 19.79,
      "grad_norm": 27930.837890625,
      "learning_rate": 1.0990712074303406e-06,
      "loss": 60.6202,
      "step": 19767
    },
    {
      "epoch": 19.79,
      "grad_norm": 7072.73876953125,
      "learning_rate": 1.0939112487100103e-06,
      "loss": 61.5956,
      "step": 19768
    },
    {
      "epoch": 19.79,
      "grad_norm": 1048.1552734375,
      "learning_rate": 1.0887512899896801e-06,
      "loss": 55.7624,
      "step": 19769
    },
    {
      "epoch": 19.79,
      "grad_norm": 2112.764892578125,
      "learning_rate": 1.08359133126935e-06,
      "loss": 57.3351,
      "step": 19770
    },
    {
      "epoch": 19.79,
      "grad_norm": 2244.74072265625,
      "learning_rate": 1.0784313725490197e-06,
      "loss": 41.2002,
      "step": 19771
    },
    {
      "epoch": 19.79,
      "grad_norm": 12551.1767578125,
      "learning_rate": 1.0732714138286893e-06,
      "loss": 62.0654,
      "step": 19772
    },
    {
      "epoch": 19.79,
      "grad_norm": 4567.2294921875,
      "learning_rate": 1.0681114551083594e-06,
      "loss": 55.5933,
      "step": 19773
    },
    {
      "epoch": 19.79,
      "grad_norm": 19574.2421875,
      "learning_rate": 1.062951496388029e-06,
      "loss": 65.7364,
      "step": 19774
    },
    {
      "epoch": 19.79,
      "grad_norm": 13758.7333984375,
      "learning_rate": 1.0577915376676987e-06,
      "loss": 58.0368,
      "step": 19775
    },
    {
      "epoch": 19.8,
      "grad_norm": 1242.18408203125,
      "learning_rate": 1.0526315789473685e-06,
      "loss": 49.6011,
      "step": 19776
    },
    {
      "epoch": 19.8,
      "grad_norm": 3630.99658203125,
      "learning_rate": 1.0474716202270382e-06,
      "loss": 60.5149,
      "step": 19777
    },
    {
      "epoch": 19.8,
      "grad_norm": 1413.10107421875,
      "learning_rate": 1.042311661506708e-06,
      "loss": 54.9772,
      "step": 19778
    },
    {
      "epoch": 19.8,
      "grad_norm": 6197.55908203125,
      "learning_rate": 1.037151702786378e-06,
      "loss": 44.4478,
      "step": 19779
    },
    {
      "epoch": 19.8,
      "grad_norm": 47921.66796875,
      "learning_rate": 1.0319917440660475e-06,
      "loss": 24.5798,
      "step": 19780
    },
    {
      "epoch": 19.8,
      "grad_norm": 17784.068359375,
      "learning_rate": 1.0268317853457172e-06,
      "loss": 56.2657,
      "step": 19781
    },
    {
      "epoch": 19.8,
      "grad_norm": 19011.75,
      "learning_rate": 1.021671826625387e-06,
      "loss": 47.4338,
      "step": 19782
    },
    {
      "epoch": 19.8,
      "grad_norm": 4762.423828125,
      "learning_rate": 1.016511867905057e-06,
      "loss": 68.0773,
      "step": 19783
    },
    {
      "epoch": 19.8,
      "grad_norm": 2385.366943359375,
      "learning_rate": 1.0113519091847266e-06,
      "loss": 64.0437,
      "step": 19784
    },
    {
      "epoch": 19.8,
      "grad_norm": 20519.90234375,
      "learning_rate": 1.0061919504643964e-06,
      "loss": 58.0575,
      "step": 19785
    },
    {
      "epoch": 19.81,
      "grad_norm": 43388.95703125,
      "learning_rate": 1.001031991744066e-06,
      "loss": 47.5373,
      "step": 19786
    },
    {
      "epoch": 19.81,
      "grad_norm": 9388.13671875,
      "learning_rate": 9.958720330237357e-07,
      "loss": 62.1582,
      "step": 19787
    },
    {
      "epoch": 19.81,
      "grad_norm": 27260.01953125,
      "learning_rate": 9.907120743034056e-07,
      "loss": 54.6481,
      "step": 19788
    },
    {
      "epoch": 19.81,
      "grad_norm": 7760.1669921875,
      "learning_rate": 9.855521155830754e-07,
      "loss": 62.1027,
      "step": 19789
    },
    {
      "epoch": 19.81,
      "grad_norm": 3513.666259765625,
      "learning_rate": 9.80392156862745e-07,
      "loss": 40.4067,
      "step": 19790
    },
    {
      "epoch": 19.81,
      "grad_norm": 11987.0869140625,
      "learning_rate": 9.75232198142415e-07,
      "loss": 36.8946,
      "step": 19791
    },
    {
      "epoch": 19.81,
      "grad_norm": 23425.166015625,
      "learning_rate": 9.700722394220846e-07,
      "loss": 61.074,
      "step": 19792
    },
    {
      "epoch": 19.81,
      "grad_norm": 2877.705322265625,
      "learning_rate": 9.649122807017545e-07,
      "loss": 52.508,
      "step": 19793
    },
    {
      "epoch": 19.81,
      "grad_norm": 75084.75,
      "learning_rate": 9.597523219814243e-07,
      "loss": 45.5842,
      "step": 19794
    },
    {
      "epoch": 19.81,
      "grad_norm": 1244.0411376953125,
      "learning_rate": 9.54592363261094e-07,
      "loss": 46.668,
      "step": 19795
    },
    {
      "epoch": 19.82,
      "grad_norm": 7557.5126953125,
      "learning_rate": 9.494324045407637e-07,
      "loss": 55.7265,
      "step": 19796
    },
    {
      "epoch": 19.82,
      "grad_norm": 6730.81494140625,
      "learning_rate": 9.442724458204336e-07,
      "loss": 39.5562,
      "step": 19797
    },
    {
      "epoch": 19.82,
      "grad_norm": 2623.09912109375,
      "learning_rate": 9.391124871001033e-07,
      "loss": 63.2171,
      "step": 19798
    },
    {
      "epoch": 19.82,
      "grad_norm": 12594.06640625,
      "learning_rate": 9.33952528379773e-07,
      "loss": 59.5434,
      "step": 19799
    },
    {
      "epoch": 19.82,
      "grad_norm": 21001.482421875,
      "learning_rate": 9.287925696594427e-07,
      "loss": 68.8981,
      "step": 19800
    },
    {
      "epoch": 19.82,
      "grad_norm": 28640.701171875,
      "learning_rate": 9.236326109391125e-07,
      "loss": 52.137,
      "step": 19801
    },
    {
      "epoch": 19.82,
      "grad_norm": 4524.0537109375,
      "learning_rate": 9.184726522187823e-07,
      "loss": 62.4063,
      "step": 19802
    },
    {
      "epoch": 19.82,
      "grad_norm": 21278.798828125,
      "learning_rate": 9.133126934984521e-07,
      "loss": 59.7024,
      "step": 19803
    },
    {
      "epoch": 19.82,
      "grad_norm": 3544.953125,
      "learning_rate": 9.081527347781218e-07,
      "loss": 56.7581,
      "step": 19804
    },
    {
      "epoch": 19.82,
      "grad_norm": 13621.2109375,
      "learning_rate": 9.029927760577915e-07,
      "loss": 55.5638,
      "step": 19805
    },
    {
      "epoch": 19.83,
      "grad_norm": 691.5633544921875,
      "learning_rate": 8.978328173374612e-07,
      "loss": 64.7731,
      "step": 19806
    },
    {
      "epoch": 19.83,
      "grad_norm": 2012.4307861328125,
      "learning_rate": 8.926728586171312e-07,
      "loss": 60.3038,
      "step": 19807
    },
    {
      "epoch": 19.83,
      "grad_norm": 2855.17626953125,
      "learning_rate": 8.875128998968009e-07,
      "loss": 63.4596,
      "step": 19808
    },
    {
      "epoch": 19.83,
      "grad_norm": 6274.36572265625,
      "learning_rate": 8.823529411764706e-07,
      "loss": 60.588,
      "step": 19809
    },
    {
      "epoch": 19.83,
      "grad_norm": 16918.90625,
      "learning_rate": 8.771929824561404e-07,
      "loss": 64.0126,
      "step": 19810
    },
    {
      "epoch": 19.83,
      "grad_norm": 13640.0478515625,
      "learning_rate": 8.7203302373581e-07,
      "loss": 41.8796,
      "step": 19811
    },
    {
      "epoch": 19.83,
      "grad_norm": 11647.3701171875,
      "learning_rate": 8.6687306501548e-07,
      "loss": 56.6794,
      "step": 19812
    },
    {
      "epoch": 19.83,
      "grad_norm": 4281.69091796875,
      "learning_rate": 8.617131062951497e-07,
      "loss": 60.2077,
      "step": 19813
    },
    {
      "epoch": 19.83,
      "grad_norm": 5316.462890625,
      "learning_rate": 8.565531475748194e-07,
      "loss": 63.0608,
      "step": 19814
    },
    {
      "epoch": 19.83,
      "grad_norm": 10709.583984375,
      "learning_rate": 8.513931888544891e-07,
      "loss": 59.6053,
      "step": 19815
    },
    {
      "epoch": 19.84,
      "grad_norm": 3040.852783203125,
      "learning_rate": 8.46233230134159e-07,
      "loss": 52.9492,
      "step": 19816
    },
    {
      "epoch": 19.84,
      "grad_norm": 4383.05419921875,
      "learning_rate": 8.410732714138288e-07,
      "loss": 57.4296,
      "step": 19817
    },
    {
      "epoch": 19.84,
      "grad_norm": 2335.903564453125,
      "learning_rate": 8.359133126934985e-07,
      "loss": 55.9228,
      "step": 19818
    },
    {
      "epoch": 19.84,
      "grad_norm": 6330.5849609375,
      "learning_rate": 8.307533539731683e-07,
      "loss": 62.3655,
      "step": 19819
    },
    {
      "epoch": 19.84,
      "grad_norm": 34783.55078125,
      "learning_rate": 8.255933952528379e-07,
      "loss": 58.745,
      "step": 19820
    },
    {
      "epoch": 19.84,
      "grad_norm": 9390.482421875,
      "learning_rate": 8.204334365325079e-07,
      "loss": 41.6101,
      "step": 19821
    },
    {
      "epoch": 19.84,
      "grad_norm": 5887.65185546875,
      "learning_rate": 8.152734778121776e-07,
      "loss": 58.7924,
      "step": 19822
    },
    {
      "epoch": 19.84,
      "grad_norm": 2964.95166015625,
      "learning_rate": 8.101135190918473e-07,
      "loss": 50.2675,
      "step": 19823
    },
    {
      "epoch": 19.84,
      "grad_norm": 64694.17578125,
      "learning_rate": 8.04953560371517e-07,
      "loss": 66.4905,
      "step": 19824
    },
    {
      "epoch": 19.84,
      "grad_norm": 26663.68359375,
      "learning_rate": 7.997936016511868e-07,
      "loss": 55.4349,
      "step": 19825
    },
    {
      "epoch": 19.85,
      "grad_norm": 12788.841796875,
      "learning_rate": 7.946336429308566e-07,
      "loss": 58.5393,
      "step": 19826
    },
    {
      "epoch": 19.85,
      "grad_norm": 5284.69580078125,
      "learning_rate": 7.894736842105264e-07,
      "loss": 54.003,
      "step": 19827
    },
    {
      "epoch": 19.85,
      "grad_norm": 30741.498046875,
      "learning_rate": 7.843137254901962e-07,
      "loss": 62.1261,
      "step": 19828
    },
    {
      "epoch": 19.85,
      "grad_norm": 16110.001953125,
      "learning_rate": 7.791537667698659e-07,
      "loss": 37.6728,
      "step": 19829
    },
    {
      "epoch": 19.85,
      "grad_norm": 11602.2724609375,
      "learning_rate": 7.739938080495357e-07,
      "loss": 47.6925,
      "step": 19830
    },
    {
      "epoch": 19.85,
      "grad_norm": 10446.119140625,
      "learning_rate": 7.688338493292054e-07,
      "loss": 54.651,
      "step": 19831
    },
    {
      "epoch": 19.85,
      "grad_norm": 1716.7879638671875,
      "learning_rate": 7.636738906088752e-07,
      "loss": 65.1619,
      "step": 19832
    },
    {
      "epoch": 19.85,
      "grad_norm": 2545.222900390625,
      "learning_rate": 7.585139318885449e-07,
      "loss": 63.8945,
      "step": 19833
    },
    {
      "epoch": 19.85,
      "grad_norm": 5571.07373046875,
      "learning_rate": 7.533539731682147e-07,
      "loss": 57.8088,
      "step": 19834
    },
    {
      "epoch": 19.85,
      "grad_norm": 10115.5546875,
      "learning_rate": 7.481940144478844e-07,
      "loss": 54.4729,
      "step": 19835
    },
    {
      "epoch": 19.86,
      "grad_norm": 21571.314453125,
      "learning_rate": 7.430340557275542e-07,
      "loss": 52.8715,
      "step": 19836
    },
    {
      "epoch": 19.86,
      "grad_norm": 76249.40625,
      "learning_rate": 7.378740970072239e-07,
      "loss": 61.8848,
      "step": 19837
    },
    {
      "epoch": 19.86,
      "grad_norm": 6783.70361328125,
      "learning_rate": 7.327141382868937e-07,
      "loss": 46.009,
      "step": 19838
    },
    {
      "epoch": 19.86,
      "grad_norm": 4750.07177734375,
      "learning_rate": 7.275541795665635e-07,
      "loss": 65.1862,
      "step": 19839
    },
    {
      "epoch": 19.86,
      "grad_norm": 1035.398193359375,
      "learning_rate": 7.223942208462333e-07,
      "loss": 55.3512,
      "step": 19840
    },
    {
      "epoch": 19.86,
      "grad_norm": 7716.23388671875,
      "learning_rate": 7.172342621259031e-07,
      "loss": 53.1076,
      "step": 19841
    },
    {
      "epoch": 19.86,
      "grad_norm": 10722.8984375,
      "learning_rate": 7.120743034055728e-07,
      "loss": 50.0997,
      "step": 19842
    },
    {
      "epoch": 19.86,
      "grad_norm": 2754.421630859375,
      "learning_rate": 7.069143446852426e-07,
      "loss": 44.658,
      "step": 19843
    },
    {
      "epoch": 19.86,
      "grad_norm": 17497.470703125,
      "learning_rate": 7.017543859649123e-07,
      "loss": 62.0007,
      "step": 19844
    },
    {
      "epoch": 19.86,
      "grad_norm": 2654.959228515625,
      "learning_rate": 6.965944272445821e-07,
      "loss": 64.5727,
      "step": 19845
    },
    {
      "epoch": 19.87,
      "grad_norm": 12547.5361328125,
      "learning_rate": 6.914344685242518e-07,
      "loss": 60.1621,
      "step": 19846
    },
    {
      "epoch": 19.87,
      "grad_norm": 40829.87109375,
      "learning_rate": 6.862745098039216e-07,
      "loss": 26.8058,
      "step": 19847
    },
    {
      "epoch": 19.87,
      "grad_norm": 16261.1748046875,
      "learning_rate": 6.811145510835914e-07,
      "loss": 54.2341,
      "step": 19848
    },
    {
      "epoch": 19.87,
      "grad_norm": 30186.970703125,
      "learning_rate": 6.759545923632611e-07,
      "loss": 56.8897,
      "step": 19849
    },
    {
      "epoch": 19.87,
      "grad_norm": 3774.310791015625,
      "learning_rate": 6.707946336429308e-07,
      "loss": 51.8565,
      "step": 19850
    },
    {
      "epoch": 19.87,
      "grad_norm": 2051.369873046875,
      "learning_rate": 6.656346749226007e-07,
      "loss": 61.9631,
      "step": 19851
    },
    {
      "epoch": 19.87,
      "grad_norm": 116823.984375,
      "learning_rate": 6.604747162022703e-07,
      "loss": 52.7645,
      "step": 19852
    },
    {
      "epoch": 19.87,
      "grad_norm": 12564.4990234375,
      "learning_rate": 6.553147574819402e-07,
      "loss": 58.4149,
      "step": 19853
    },
    {
      "epoch": 19.87,
      "grad_norm": 10297.603515625,
      "learning_rate": 6.5015479876161e-07,
      "loss": 55.2974,
      "step": 19854
    },
    {
      "epoch": 19.87,
      "grad_norm": 3054.392578125,
      "learning_rate": 6.449948400412796e-07,
      "loss": 41.1264,
      "step": 19855
    },
    {
      "epoch": 19.88,
      "grad_norm": 34582.71484375,
      "learning_rate": 6.398348813209495e-07,
      "loss": 60.657,
      "step": 19856
    },
    {
      "epoch": 19.88,
      "grad_norm": 4534.05419921875,
      "learning_rate": 6.346749226006192e-07,
      "loss": 57.7216,
      "step": 19857
    },
    {
      "epoch": 19.88,
      "grad_norm": 9013.9931640625,
      "learning_rate": 6.29514963880289e-07,
      "loss": 43.0632,
      "step": 19858
    },
    {
      "epoch": 19.88,
      "grad_norm": 8505.5087890625,
      "learning_rate": 6.243550051599587e-07,
      "loss": 51.2256,
      "step": 19859
    },
    {
      "epoch": 19.88,
      "grad_norm": 10800.2880859375,
      "learning_rate": 6.191950464396286e-07,
      "loss": 59.6397,
      "step": 19860
    },
    {
      "epoch": 19.88,
      "grad_norm": 3570.870361328125,
      "learning_rate": 6.140350877192982e-07,
      "loss": 65.8025,
      "step": 19861
    },
    {
      "epoch": 19.88,
      "grad_norm": 5868.9970703125,
      "learning_rate": 6.08875128998968e-07,
      "loss": 59.2464,
      "step": 19862
    },
    {
      "epoch": 19.88,
      "grad_norm": 3269.101318359375,
      "learning_rate": 6.037151702786379e-07,
      "loss": 57.1654,
      "step": 19863
    },
    {
      "epoch": 19.88,
      "grad_norm": 14309.0341796875,
      "learning_rate": 5.985552115583075e-07,
      "loss": 62.3226,
      "step": 19864
    },
    {
      "epoch": 19.88,
      "grad_norm": 3944.41455078125,
      "learning_rate": 5.933952528379774e-07,
      "loss": 63.6319,
      "step": 19865
    },
    {
      "epoch": 19.89,
      "grad_norm": 6006.158203125,
      "learning_rate": 5.882352941176471e-07,
      "loss": 54.3337,
      "step": 19866
    },
    {
      "epoch": 19.89,
      "grad_norm": 132568.890625,
      "learning_rate": 5.830753353973169e-07,
      "loss": 29.7756,
      "step": 19867
    },
    {
      "epoch": 19.89,
      "grad_norm": 1684.1385498046875,
      "learning_rate": 5.779153766769866e-07,
      "loss": 58.993,
      "step": 19868
    },
    {
      "epoch": 19.89,
      "grad_norm": 2570.850830078125,
      "learning_rate": 5.727554179566564e-07,
      "loss": 59.2248,
      "step": 19869
    },
    {
      "epoch": 19.89,
      "grad_norm": 1457.7628173828125,
      "learning_rate": 5.675954592363261e-07,
      "loss": 65.9325,
      "step": 19870
    },
    {
      "epoch": 19.89,
      "grad_norm": 3469.37353515625,
      "learning_rate": 5.624355005159959e-07,
      "loss": 58.5824,
      "step": 19871
    },
    {
      "epoch": 19.89,
      "grad_norm": 7049.92578125,
      "learning_rate": 5.572755417956657e-07,
      "loss": 57.976,
      "step": 19872
    },
    {
      "epoch": 19.89,
      "grad_norm": 7594.88134765625,
      "learning_rate": 5.521155830753354e-07,
      "loss": 59.676,
      "step": 19873
    },
    {
      "epoch": 19.89,
      "grad_norm": 1849.596435546875,
      "learning_rate": 5.469556243550051e-07,
      "loss": 68.9548,
      "step": 19874
    },
    {
      "epoch": 19.89,
      "grad_norm": 28773.70703125,
      "learning_rate": 5.41795665634675e-07,
      "loss": 62.4461,
      "step": 19875
    },
    {
      "epoch": 19.9,
      "grad_norm": 5650.0625,
      "learning_rate": 5.366357069143447e-07,
      "loss": 67.7082,
      "step": 19876
    },
    {
      "epoch": 19.9,
      "grad_norm": 3200.218994140625,
      "learning_rate": 5.314757481940145e-07,
      "loss": 66.3409,
      "step": 19877
    },
    {
      "epoch": 19.9,
      "grad_norm": 8445.4560546875,
      "learning_rate": 5.263157894736843e-07,
      "loss": 35.2482,
      "step": 19878
    },
    {
      "epoch": 19.9,
      "grad_norm": 7437.8642578125,
      "learning_rate": 5.21155830753354e-07,
      "loss": 65.8778,
      "step": 19879
    },
    {
      "epoch": 19.9,
      "grad_norm": 1573.6912841796875,
      "learning_rate": 5.159958720330238e-07,
      "loss": 60.1632,
      "step": 19880
    },
    {
      "epoch": 19.9,
      "grad_norm": 4096.099609375,
      "learning_rate": 5.108359133126935e-07,
      "loss": 62.591,
      "step": 19881
    },
    {
      "epoch": 19.9,
      "grad_norm": 47398.5546875,
      "learning_rate": 5.056759545923633e-07,
      "loss": 64.5239,
      "step": 19882
    },
    {
      "epoch": 19.9,
      "grad_norm": 1061.7418212890625,
      "learning_rate": 5.00515995872033e-07,
      "loss": 64.9395,
      "step": 19883
    },
    {
      "epoch": 19.9,
      "grad_norm": 10939.876953125,
      "learning_rate": 4.953560371517028e-07,
      "loss": 51.4949,
      "step": 19884
    },
    {
      "epoch": 19.9,
      "grad_norm": 2546.236328125,
      "learning_rate": 4.901960784313725e-07,
      "loss": 59.3206,
      "step": 19885
    },
    {
      "epoch": 19.91,
      "grad_norm": 1333.665283203125,
      "learning_rate": 4.850361197110423e-07,
      "loss": 57.4368,
      "step": 19886
    },
    {
      "epoch": 19.91,
      "grad_norm": 6477.96240234375,
      "learning_rate": 4.798761609907122e-07,
      "loss": 49.4685,
      "step": 19887
    },
    {
      "epoch": 19.91,
      "grad_norm": 2696.1484375,
      "learning_rate": 4.7471620227038186e-07,
      "loss": 61.6153,
      "step": 19888
    },
    {
      "epoch": 19.91,
      "grad_norm": 3273.478515625,
      "learning_rate": 4.6955624355005166e-07,
      "loss": 54.5231,
      "step": 19889
    },
    {
      "epoch": 19.91,
      "grad_norm": 17214.205078125,
      "learning_rate": 4.6439628482972136e-07,
      "loss": 59.3639,
      "step": 19890
    },
    {
      "epoch": 19.91,
      "grad_norm": 5800.8837890625,
      "learning_rate": 4.5923632610939117e-07,
      "loss": 59.5078,
      "step": 19891
    },
    {
      "epoch": 19.91,
      "grad_norm": 31175.84375,
      "learning_rate": 4.540763673890609e-07,
      "loss": 64.6347,
      "step": 19892
    },
    {
      "epoch": 19.91,
      "grad_norm": 4079.0947265625,
      "learning_rate": 4.489164086687306e-07,
      "loss": 51.2938,
      "step": 19893
    },
    {
      "epoch": 19.91,
      "grad_norm": 4395.23779296875,
      "learning_rate": 4.4375644994840043e-07,
      "loss": 61.9796,
      "step": 19894
    },
    {
      "epoch": 19.91,
      "grad_norm": 12728.095703125,
      "learning_rate": 4.385964912280702e-07,
      "loss": 58.1741,
      "step": 19895
    },
    {
      "epoch": 19.92,
      "grad_norm": 44806.640625,
      "learning_rate": 4.3343653250774e-07,
      "loss": 55.7352,
      "step": 19896
    },
    {
      "epoch": 19.92,
      "grad_norm": 16409.09375,
      "learning_rate": 4.282765737874097e-07,
      "loss": 67.2547,
      "step": 19897
    },
    {
      "epoch": 19.92,
      "grad_norm": 14001.8017578125,
      "learning_rate": 4.231166150670795e-07,
      "loss": 65.299,
      "step": 19898
    },
    {
      "epoch": 19.92,
      "grad_norm": 5064.861328125,
      "learning_rate": 4.1795665634674925e-07,
      "loss": 54.1266,
      "step": 19899
    },
    {
      "epoch": 19.92,
      "grad_norm": 14653.529296875,
      "learning_rate": 4.1279669762641895e-07,
      "loss": 58.4519,
      "step": 19900
    },
    {
      "epoch": 19.92,
      "grad_norm": 27912.49609375,
      "learning_rate": 4.076367389060888e-07,
      "loss": 43.5581,
      "step": 19901
    },
    {
      "epoch": 19.92,
      "grad_norm": 4430.68896484375,
      "learning_rate": 4.024767801857585e-07,
      "loss": 64.6358,
      "step": 19902
    },
    {
      "epoch": 19.92,
      "grad_norm": 1450.9544677734375,
      "learning_rate": 3.973168214654283e-07,
      "loss": 62.5742,
      "step": 19903
    },
    {
      "epoch": 19.92,
      "grad_norm": 5005.62548828125,
      "learning_rate": 3.921568627450981e-07,
      "loss": 54.4123,
      "step": 19904
    },
    {
      "epoch": 19.92,
      "grad_norm": 1530.3936767578125,
      "learning_rate": 3.8699690402476783e-07,
      "loss": 55.2396,
      "step": 19905
    },
    {
      "epoch": 19.93,
      "grad_norm": 1364.829833984375,
      "learning_rate": 3.818369453044376e-07,
      "loss": 65.7995,
      "step": 19906
    },
    {
      "epoch": 19.93,
      "grad_norm": 3998.39404296875,
      "learning_rate": 3.7667698658410734e-07,
      "loss": 67.2976,
      "step": 19907
    },
    {
      "epoch": 19.93,
      "grad_norm": 76953.359375,
      "learning_rate": 3.715170278637771e-07,
      "loss": 61.9776,
      "step": 19908
    },
    {
      "epoch": 19.93,
      "grad_norm": 23332.30078125,
      "learning_rate": 3.6635706914344684e-07,
      "loss": 54.491,
      "step": 19909
    },
    {
      "epoch": 19.93,
      "grad_norm": 17448.23046875,
      "learning_rate": 3.6119711042311665e-07,
      "loss": 58.1342,
      "step": 19910
    },
    {
      "epoch": 19.93,
      "grad_norm": 6961.76318359375,
      "learning_rate": 3.560371517027864e-07,
      "loss": 55.3556,
      "step": 19911
    },
    {
      "epoch": 19.93,
      "grad_norm": 10277.388671875,
      "learning_rate": 3.5087719298245616e-07,
      "loss": 46.7746,
      "step": 19912
    },
    {
      "epoch": 19.93,
      "grad_norm": 8854.54296875,
      "learning_rate": 3.457172342621259e-07,
      "loss": 56.3812,
      "step": 19913
    },
    {
      "epoch": 19.93,
      "grad_norm": 8339.642578125,
      "learning_rate": 3.405572755417957e-07,
      "loss": 66.4031,
      "step": 19914
    },
    {
      "epoch": 19.93,
      "grad_norm": 3492.289306640625,
      "learning_rate": 3.353973168214654e-07,
      "loss": 57.8302,
      "step": 19915
    },
    {
      "epoch": 19.94,
      "grad_norm": 7546.49609375,
      "learning_rate": 3.302373581011352e-07,
      "loss": 62.236,
      "step": 19916
    },
    {
      "epoch": 19.94,
      "grad_norm": 3281.818359375,
      "learning_rate": 3.25077399380805e-07,
      "loss": 49.5471,
      "step": 19917
    },
    {
      "epoch": 19.94,
      "grad_norm": 6950.05712890625,
      "learning_rate": 3.1991744066047473e-07,
      "loss": 63.7956,
      "step": 19918
    },
    {
      "epoch": 19.94,
      "grad_norm": 1691.1312255859375,
      "learning_rate": 3.147574819401445e-07,
      "loss": 52.6599,
      "step": 19919
    },
    {
      "epoch": 19.94,
      "grad_norm": 13024.4677734375,
      "learning_rate": 3.095975232198143e-07,
      "loss": 61.0045,
      "step": 19920
    },
    {
      "epoch": 19.94,
      "grad_norm": 8135.78564453125,
      "learning_rate": 3.04437564499484e-07,
      "loss": 45.9925,
      "step": 19921
    },
    {
      "epoch": 19.94,
      "grad_norm": 10566.8369140625,
      "learning_rate": 2.9927760577915375e-07,
      "loss": 47.8255,
      "step": 19922
    },
    {
      "epoch": 19.94,
      "grad_norm": 10467.427734375,
      "learning_rate": 2.9411764705882356e-07,
      "loss": 59.5797,
      "step": 19923
    },
    {
      "epoch": 19.94,
      "grad_norm": 1870.72265625,
      "learning_rate": 2.889576883384933e-07,
      "loss": 57.4777,
      "step": 19924
    },
    {
      "epoch": 19.94,
      "grad_norm": 5384.130859375,
      "learning_rate": 2.8379772961816306e-07,
      "loss": 64.8522,
      "step": 19925
    },
    {
      "epoch": 19.95,
      "grad_norm": 3543.812255859375,
      "learning_rate": 2.7863777089783287e-07,
      "loss": 56.6183,
      "step": 19926
    },
    {
      "epoch": 19.95,
      "grad_norm": 6383.4560546875,
      "learning_rate": 2.7347781217750257e-07,
      "loss": 56.5374,
      "step": 19927
    },
    {
      "epoch": 19.95,
      "grad_norm": 33664.2109375,
      "learning_rate": 2.683178534571723e-07,
      "loss": 59.2143,
      "step": 19928
    },
    {
      "epoch": 19.95,
      "grad_norm": 8113.638671875,
      "learning_rate": 2.6315789473684213e-07,
      "loss": 63.6763,
      "step": 19929
    },
    {
      "epoch": 19.95,
      "grad_norm": 9158.2548828125,
      "learning_rate": 2.579979360165119e-07,
      "loss": 62.5045,
      "step": 19930
    },
    {
      "epoch": 19.95,
      "grad_norm": 5520.03173828125,
      "learning_rate": 2.5283797729618164e-07,
      "loss": 57.1405,
      "step": 19931
    },
    {
      "epoch": 19.95,
      "grad_norm": 2966.3203125,
      "learning_rate": 2.476780185758514e-07,
      "loss": 61.2998,
      "step": 19932
    },
    {
      "epoch": 19.95,
      "grad_norm": 372818.21875,
      "learning_rate": 2.4251805985552115e-07,
      "loss": 40.7978,
      "step": 19933
    },
    {
      "epoch": 19.95,
      "grad_norm": 7204.68408203125,
      "learning_rate": 2.3735810113519093e-07,
      "loss": 63.7031,
      "step": 19934
    },
    {
      "epoch": 19.95,
      "grad_norm": 7357.9091796875,
      "learning_rate": 2.3219814241486068e-07,
      "loss": 62.8565,
      "step": 19935
    },
    {
      "epoch": 19.96,
      "grad_norm": 7555.8525390625,
      "learning_rate": 2.2703818369453046e-07,
      "loss": 64.2935,
      "step": 19936
    },
    {
      "epoch": 19.96,
      "grad_norm": 6530.384765625,
      "learning_rate": 2.2187822497420022e-07,
      "loss": 45.4213,
      "step": 19937
    },
    {
      "epoch": 19.96,
      "grad_norm": 7550.2978515625,
      "learning_rate": 2.1671826625387e-07,
      "loss": 58.0755,
      "step": 19938
    },
    {
      "epoch": 19.96,
      "grad_norm": 3145.977783203125,
      "learning_rate": 2.1155830753353975e-07,
      "loss": 68.3567,
      "step": 19939
    },
    {
      "epoch": 19.96,
      "grad_norm": 8879.7607421875,
      "learning_rate": 2.0639834881320948e-07,
      "loss": 46.197,
      "step": 19940
    },
    {
      "epoch": 19.96,
      "grad_norm": 16911.873046875,
      "learning_rate": 2.0123839009287926e-07,
      "loss": 60.9534,
      "step": 19941
    },
    {
      "epoch": 19.96,
      "grad_norm": 5479.81640625,
      "learning_rate": 1.9607843137254904e-07,
      "loss": 55.4146,
      "step": 19942
    },
    {
      "epoch": 19.96,
      "grad_norm": 2549.745849609375,
      "learning_rate": 1.909184726522188e-07,
      "loss": 56.7101,
      "step": 19943
    },
    {
      "epoch": 19.96,
      "grad_norm": 4241.06787109375,
      "learning_rate": 1.8575851393188855e-07,
      "loss": 64.559,
      "step": 19944
    },
    {
      "epoch": 19.96,
      "grad_norm": 3460.554931640625,
      "learning_rate": 1.8059855521155833e-07,
      "loss": 54.6457,
      "step": 19945
    },
    {
      "epoch": 19.97,
      "grad_norm": 4829.11865234375,
      "learning_rate": 1.7543859649122808e-07,
      "loss": 60.3605,
      "step": 19946
    },
    {
      "epoch": 19.97,
      "grad_norm": 4044.637939453125,
      "learning_rate": 1.7027863777089786e-07,
      "loss": 55.02,
      "step": 19947
    },
    {
      "epoch": 19.97,
      "grad_norm": 810.6956787109375,
      "learning_rate": 1.651186790505676e-07,
      "loss": 57.4957,
      "step": 19948
    },
    {
      "epoch": 19.97,
      "grad_norm": 12768.1376953125,
      "learning_rate": 1.5995872033023737e-07,
      "loss": 61.3965,
      "step": 19949
    },
    {
      "epoch": 19.97,
      "grad_norm": 586.4673461914062,
      "learning_rate": 1.5479876160990715e-07,
      "loss": 58.6679,
      "step": 19950
    },
    {
      "epoch": 19.97,
      "grad_norm": 7312.22119140625,
      "learning_rate": 1.4963880288957687e-07,
      "loss": 52.7187,
      "step": 19951
    },
    {
      "epoch": 19.97,
      "grad_norm": 117920.0078125,
      "learning_rate": 1.4447884416924666e-07,
      "loss": 55.704,
      "step": 19952
    },
    {
      "epoch": 19.97,
      "grad_norm": 38471.578125,
      "learning_rate": 1.3931888544891644e-07,
      "loss": 59.0264,
      "step": 19953
    },
    {
      "epoch": 19.97,
      "grad_norm": 3142.28857421875,
      "learning_rate": 1.3415892672858616e-07,
      "loss": 49.8204,
      "step": 19954
    },
    {
      "epoch": 19.97,
      "grad_norm": 8021.01318359375,
      "learning_rate": 1.2899896800825594e-07,
      "loss": 53.9794,
      "step": 19955
    },
    {
      "epoch": 19.98,
      "grad_norm": 3168.787841796875,
      "learning_rate": 1.238390092879257e-07,
      "loss": 50.9405,
      "step": 19956
    },
    {
      "epoch": 19.98,
      "grad_norm": 3365.4384765625,
      "learning_rate": 1.1867905056759546e-07,
      "loss": 67.6629,
      "step": 19957
    },
    {
      "epoch": 19.98,
      "grad_norm": 4805.298828125,
      "learning_rate": 1.1351909184726523e-07,
      "loss": 45.7334,
      "step": 19958
    },
    {
      "epoch": 19.98,
      "grad_norm": 15914.61328125,
      "learning_rate": 1.08359133126935e-07,
      "loss": 39.4064,
      "step": 19959
    },
    {
      "epoch": 19.98,
      "grad_norm": 1813.2818603515625,
      "learning_rate": 1.0319917440660474e-07,
      "loss": 53.011,
      "step": 19960
    },
    {
      "epoch": 19.98,
      "grad_norm": 7625.56201171875,
      "learning_rate": 9.803921568627452e-08,
      "loss": 66.2289,
      "step": 19961
    },
    {
      "epoch": 19.98,
      "grad_norm": 1680.70458984375,
      "learning_rate": 9.287925696594427e-08,
      "loss": 55.8347,
      "step": 19962
    },
    {
      "epoch": 19.98,
      "grad_norm": 4073.26904296875,
      "learning_rate": 8.771929824561404e-08,
      "loss": 62.158,
      "step": 19963
    },
    {
      "epoch": 19.98,
      "grad_norm": 11427.5234375,
      "learning_rate": 8.25593395252838e-08,
      "loss": 50.8009,
      "step": 19964
    },
    {
      "epoch": 19.98,
      "grad_norm": 2026.036865234375,
      "learning_rate": 7.739938080495357e-08,
      "loss": 41.198,
      "step": 19965
    },
    {
      "epoch": 19.99,
      "grad_norm": 37241.19921875,
      "learning_rate": 7.223942208462333e-08,
      "loss": 61.4396,
      "step": 19966
    },
    {
      "epoch": 19.99,
      "grad_norm": 5609.8701171875,
      "learning_rate": 6.707946336429308e-08,
      "loss": 56.2832,
      "step": 19967
    },
    {
      "epoch": 19.99,
      "grad_norm": 31703.837890625,
      "learning_rate": 6.191950464396285e-08,
      "loss": 53.737,
      "step": 19968
    },
    {
      "epoch": 19.99,
      "grad_norm": 17252.509765625,
      "learning_rate": 5.6759545923632615e-08,
      "loss": 50.8088,
      "step": 19969
    },
    {
      "epoch": 19.99,
      "grad_norm": 5437.8349609375,
      "learning_rate": 5.159958720330237e-08,
      "loss": 60.1124,
      "step": 19970
    },
    {
      "epoch": 19.99,
      "grad_norm": 25524.046875,
      "learning_rate": 4.6439628482972136e-08,
      "loss": 51.1098,
      "step": 19971
    },
    {
      "epoch": 19.99,
      "grad_norm": 2622.18505859375,
      "learning_rate": 4.12796697626419e-08,
      "loss": 60.7008,
      "step": 19972
    },
    {
      "epoch": 19.99,
      "grad_norm": 16945.96875,
      "learning_rate": 3.6119711042311664e-08,
      "loss": 38.2095,
      "step": 19973
    },
    {
      "epoch": 19.99,
      "grad_norm": 15421.173828125,
      "learning_rate": 3.0959752321981424e-08,
      "loss": 41.0181,
      "step": 19974
    },
    {
      "epoch": 19.99,
      "grad_norm": 9372.17578125,
      "learning_rate": 2.5799793601651185e-08,
      "loss": 63.6282,
      "step": 19975
    },
    {
      "epoch": 20.0,
      "grad_norm": 13345.1328125,
      "learning_rate": 2.063983488132095e-08,
      "loss": 55.9536,
      "step": 19976
    },
    {
      "epoch": 20.0,
      "grad_norm": 7264.6240234375,
      "learning_rate": 1.5479876160990712e-08,
      "loss": 30.8921,
      "step": 19977
    },
    {
      "epoch": 20.0,
      "grad_norm": 5759.04345703125,
      "learning_rate": 1.0319917440660474e-08,
      "loss": 56.1159,
      "step": 19978
    },
    {
      "epoch": 20.0,
      "grad_norm": 3871.244384765625,
      "learning_rate": 5.159958720330237e-09,
      "loss": 58.3386,
      "step": 19979
    },
    {
      "epoch": 20.0,
      "grad_norm": 57235.109375,
      "learning_rate": 0.0,
      "loss": 62.5532,
      "step": 19980
    }
  ],
  "logging_steps": 1,
  "max_steps": 19980,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "total_flos": 5290119455047680.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
